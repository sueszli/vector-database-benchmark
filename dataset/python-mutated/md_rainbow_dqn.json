[
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\n\n        Arguments:\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\n\n        Returns:\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\n                - cur_lr (:obj:`float`): current learning rate\n                - total_loss (:obj:`float`): the calculated loss\n                - priority (:obj:`list`): the priority of samples\n        \"\"\"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}",
        "mutated": [
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\\n\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\\n\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\\n                - cur_lr (:obj:`float`): current learning rate\\n                - total_loss (:obj:`float`): the calculated loss\\n                - priority (:obj:`list`): the priority of samples\\n        \"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\\n\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\\n\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\\n                - cur_lr (:obj:`float`): current learning rate\\n                - total_loss (:obj:`float`): the calculated loss\\n                - priority (:obj:`list`): the priority of samples\\n        \"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\\n\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\\n\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\\n                - cur_lr (:obj:`float`): current learning rate\\n                - total_loss (:obj:`float`): the calculated loss\\n                - priority (:obj:`list`): the priority of samples\\n        \"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\\n\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\\n\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\\n                - cur_lr (:obj:`float`): current learning rate\\n                - total_loss (:obj:`float`): the calculated loss\\n                - priority (:obj:`list`): the priority of samples\\n        \"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Forward and backward function of learn mode, acquire the data and calculate the loss and             optimize learner model\\n\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data, including at least ['obs', 'next_obs', 'reward', 'action']\\n\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Including cur_lr, total_loss and priority\\n                - cur_lr (:obj:`float`): current learning rate\\n                - total_loss (:obj:`float`): the calculated loss\\n                - priority (:obj:`list`): the priority of samples\\n        \"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    self._reset_noise(self._learn_model)\n    self._reset_noise(self._target_model)\n    q_dist = self._learn_model.forward(data['obs'])['distribution']\n    with torch.no_grad():\n        target_q_dist = self._target_model.forward(data['next_obs'])['distribution']\n        self._reset_noise(self._learn_model)\n        target_q_action = self._learn_model.forward(data['next_obs'])['action']\n    value_gamma = data.get('value_gamma', None)\n    if isinstance(q_dist, torch.Tensor):\n        td_data = dist_nstep_td_data(q_dist, target_q_dist, data['action'], target_q_action, data['reward'], data['done'], data['weight'])\n        (loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n    else:\n        act_num = len(q_dist)\n        losses = []\n        td_error_per_samples = []\n        for i in range(act_num):\n            td_data = dist_nstep_td_data(q_dist[i], target_q_dist[i], data['action'][i], target_q_action[i], data['reward'], data['done'], data['weight'])\n            (td_loss, td_error_per_sample) = dist_nstep_td_error(td_data, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma)\n            losses.append(td_loss)\n            td_error_per_samples.append(td_error_per_sample)\n        loss = sum(losses) / (len(losses) + 1e-08)\n        td_error_per_sample_mean = sum(td_error_per_samples) / (len(td_error_per_samples) + 1e-08)\n    self._optimizer.zero_grad()\n    loss.backward()\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'priority': td_error_per_sample_mean.abs().tolist()}"
        ]
    }
]