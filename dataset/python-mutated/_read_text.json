[
    {
        "func_name": "_resolve_format",
        "original": "def _resolve_format(read_format: str) -> Any:\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')",
        "mutated": [
            "def _resolve_format(read_format: str) -> Any:\n    if False:\n        i = 10\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')",
            "def _resolve_format(read_format: str) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')",
            "def _resolve_format(read_format: str) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')",
            "def _resolve_format(read_format: str) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')",
            "def _resolve_format(read_format: str) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if read_format == 'csv':\n        return pd.read_csv\n    if read_format == 'fwf':\n        return pd.read_fwf\n    if read_format == 'json':\n        return pd.read_json\n    raise exceptions.UnsupportedType('Unsupported read format')"
        ]
    },
    {
        "func_name": "_read_text",
        "original": "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)",
        "mutated": [
            "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    if False:\n        i = 10\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)",
            "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)",
            "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)",
            "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)",
            "@engine.dispatch_on_engine\ndef _read_text(read_format: str, paths: List[str], path_root: Optional[str], use_threads: Union[bool, int], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], dataset: bool, ignore_index: bool, parallelism: int, version_ids: Optional[Dict[str, str]], pandas_kwargs: Dict[str, Any]) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser_func = _resolve_format(read_format)\n    executor: _BaseExecutor = _get_executor(use_threads=use_threads)\n    tables = executor.map(_read_text_file, s3_client, paths, [version_ids.get(p) if isinstance(version_ids, dict) else None for p in paths], itertools.repeat(parser_func), itertools.repeat(path_root), itertools.repeat(pandas_kwargs), itertools.repeat(s3_additional_kwargs), itertools.repeat(dataset))\n    return _union(dfs=tables, ignore_index=ignore_index)"
        ]
    },
    {
        "func_name": "_read_text_format",
        "original": "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)",
        "mutated": [
            "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)",
            "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)",
            "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)",
            "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)",
            "def _read_text_format(read_format: str, path: Union[str, List[str]], path_suffix: Union[str, List[str], None], path_ignore_suffix: Union[str, List[str], None], ignore_empty: bool, use_threads: Union[bool, int], last_modified_begin: Optional[datetime.datetime], last_modified_end: Optional[datetime.datetime], s3_client: 'S3Client', s3_additional_kwargs: Optional[Dict[str, str]], chunksize: Optional[int], dataset: bool, partition_filter: Optional[Callable[[Dict[str, str]], bool]], ignore_index: bool, ray_args: Optional[RaySettings], version_id: Optional[Union[str, Dict[str, str]]]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'iterator' in pandas_kwargs:\n        raise exceptions.InvalidArgument('Please, use the chunksize argument instead of iterator.')\n    paths: List[str] = _path2list(path=path, s3_client=s3_client, suffix=path_suffix, ignore_suffix=_get_path_ignore_suffix(path_ignore_suffix=path_ignore_suffix), ignore_empty=ignore_empty, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, s3_additional_kwargs=s3_additional_kwargs)\n    path_root: Optional[str] = _get_path_root(path=path, dataset=dataset)\n    if path_root is not None:\n        paths = _apply_partition_filter(path_root=path_root, paths=paths, filter_func=partition_filter)\n    if len(paths) < 1:\n        raise exceptions.NoFilesFound(f'No files Found on: {path}.')\n    version_ids = _check_version_id(paths=paths, version_id=version_id)\n    args: Dict[str, Any] = {'parser_func': _resolve_format(read_format), 's3_client': s3_client, 'dataset': dataset, 'path_root': path_root, 'pandas_kwargs': pandas_kwargs, 's3_additional_kwargs': s3_additional_kwargs, 'use_threads': use_threads}\n    _logger.debug('Read args:\\n%s', pprint.pformat(args))\n    if chunksize is not None:\n        return _read_text_files_chunked(paths=paths, version_ids=version_ids, chunksize=chunksize, **args)\n    ray_args = ray_args if ray_args else {}\n    return _read_text(read_format, paths=paths, path_root=path_root, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, dataset=dataset, ignore_index=ignore_index, parallelism=ray_args.get('parallelism', -1), version_ids=version_ids, pandas_kwargs=pandas_kwargs)"
        ]
    },
    {
        "func_name": "read_csv",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\n\n    This function accepts Unix shell-style wildcards in the path argument.\n    * (matches everything), ? (matches any single character),\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\n    you can use `glob.escape(path)` before passing the path to this function.\n\n    Note\n    ----\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Note\n    ----\n    The filter by last_modified begin last_modified end is applied after list all S3 files\n\n    Parameters\n    ----------\n    path : Union[str, List[str]]\n        S3 prefix (accepts Unix shell-style wildcards)\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\n    path_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\n        If None, will try to read all files. (default)\n    path_ignore_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\n        If None, will try to read all files. (default)\n    version_id: Optional[Union[str, Dict[str, str]]]\n        Version id of the object or mapping of object path to version id.\n        (e.g. {'s3://bucket/key0': '121212', 's3://bucket/key1': '343434'})\n    ignore_empty: bool\n        Ignore files with 0 bytes.\n    use_threads : Union[bool, int]\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    last_modified_begin\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    last_modified_end: datetime, optional\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Optional[Dict[str, Any]]\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    chunksize: int, optional\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\n    dataset : bool\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\n        This function MUST return a bool, True to read the partition or False to ignore it.\n        Ignored if `dataset=False`.\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\n    ray_args: typing.RaySettings, optional\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\n    pandas_kwargs :\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\n        Pandas arguments in the function call and awswrangler will accept it.\n        e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', na_values=['null', 'none'], skip_blank_lines=True)\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\n\n    Examples\n    --------\n    Reading all CSV files under a prefix\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_csv(path='s3://bucket/prefix/')\n\n    Reading all CSV files under a prefix and using pandas_kwargs\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_csv('s3://bucket/prefix/', sep='|', na_values=['null', 'none'], skip_blank_lines=True)\n\n    Reading all CSV files from a list\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_csv(path=['s3://bucket/filename0.csv', 's3://bucket/filename1.csv'])\n\n    Reading in chunks of 100 lines\n\n    >>> import awswrangler as wr\n    >>> dfs = wr.s3.read_csv(path=['s3://bucket/filename0.csv', 's3://bucket/filename1.csv'], chunksize=100)\n    >>> for df in dfs:\n    >>>     print(df)  # 100 lines Pandas DataFrame\n\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\n\n    >>> import awswrangler as wr\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\n\n    \"\"\"\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset : bool\\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs :\\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all CSV files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n\\n    Reading all CSV files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'], chunksize=100)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset : bool\\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs :\\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all CSV files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n\\n    Reading all CSV files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'], chunksize=100)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset : bool\\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs :\\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all CSV files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n\\n    Reading all CSV files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'], chunksize=100)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset : bool\\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs :\\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all CSV files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n\\n    Reading all CSV files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'], chunksize=100)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_csv(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read CSV file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".csv\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset : bool\\n        If `True` read a CSV dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter : Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs :\\n        KEYWORD arguments forwarded to pandas.read_csv(). You can NOT pass `pandas_kwargs` explicitly, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all CSV files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(\\'s3://bucket/prefix/\\', sep=\\'|\\', na_values=[\\'null\\', \\'none\\'], skip_blank_lines=True)\\n\\n    Reading all CSV files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_csv(path=[\\'s3://bucket/filename0.csv\\', \\'s3://bucket/filename1.csv\\'], chunksize=100)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading CSV Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_csv(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicitly, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_csv('s3://bucket/prefix/', sep='|', skip_blank_lines=True)\")\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    ignore_index: bool = 'index_col' not in pandas_kwargs\n    return _read_text_format(read_format='csv', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)"
        ]
    },
    {
        "func_name": "read_fwf",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\n\n    This function accepts Unix shell-style wildcards in the path argument.\n    * (matches everything), ? (matches any single character),\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\n    you can use `glob.escape(path)` before passing the path to this function.\n\n    Note\n    ----\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Note\n    ----\n    The filter by last_modified begin last_modified end is applied after list all S3 files\n\n    Parameters\n    ----------\n    path : Union[str, List[str]]\n        S3 prefix (accepts Unix shell-style wildcards)\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\n    path_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\n        If None, will try to read all files. (default)\n    path_ignore_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\n        If None, will try to read all files. (default)\n    version_id: Optional[Union[str, Dict[str, str]]]\n        Version id of the object or mapping of object path to version id.\n        (e.g. {'s3://bucket/key0': '121212', 's3://bucket/key1': '343434'})\n    ignore_empty: bool\n        Ignore files with 0 bytes.\n    use_threads : Union[bool, int]\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    last_modified_begin\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    last_modified_end: datetime, optional\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Optional[Dict[str, Any]]\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\n    chunksize: int, optional\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\n    dataset: bool\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\n        This function MUST return a bool, True to read the partition or False to ignore it.\n        Ignored if `dataset=False`.\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\n    ray_args: typing.RaySettings, optional\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\n    pandas_kwargs:\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\n        Pandas arguments in the function call and awswrangler will accept it.\n        e.g. wr.s3.read_fwf(path='s3://bucket/prefix/', widths=[1, 3], names=[\"c0\", \"c1\"])\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\n\n    Examples\n    --------\n    Reading all fixed-width formatted (FWF) files under a prefix\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_fwf(path='s3://bucket/prefix/', widths=[1, 3], names=['c0', 'c1'])\n\n    Reading all fixed-width formatted (FWF) files from a list\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_fwf(path=['s3://bucket/0.txt', 's3://bucket/1.txt'], widths=[1, 3], names=['c0', 'c1'])\n\n    Reading in chunks of 100 lines\n\n    >>> import awswrangler as wr\n    >>> dfs = wr.s3.read_fwf(\n    ...     path=['s3://bucket/0.txt', 's3://bucket/1.txt'],\n    ...     chunksize=100,\n    ...     widths=[1, 3],\n    ...     names=[\"c0\", \"c1\"]\n    ... )\n    >>> for df in dfs:\n    >>>     print(df)  # 100 lines Pandas DataFrame\n\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\n\n    >>> import awswrangler as wr\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\n\n    \"\"\"\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\"c0\", \"c1\"])\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all fixed-width formatted (FWF) files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading all fixed-width formatted (FWF) files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'], widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_fwf(\\n    ...     path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'],\\n    ...     chunksize=100,\\n    ...     widths=[1, 3],\\n    ...     names=[\"c0\", \"c1\"]\\n    ... )\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\"c0\", \"c1\"])\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all fixed-width formatted (FWF) files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading all fixed-width formatted (FWF) files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'], widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_fwf(\\n    ...     path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'],\\n    ...     chunksize=100,\\n    ...     widths=[1, 3],\\n    ...     names=[\"c0\", \"c1\"]\\n    ... )\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\"c0\", \"c1\"])\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all fixed-width formatted (FWF) files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading all fixed-width formatted (FWF) files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'], widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_fwf(\\n    ...     path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'],\\n    ...     chunksize=100,\\n    ...     widths=[1, 3],\\n    ...     names=[\"c0\", \"c1\"]\\n    ... )\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\"c0\", \"c1\"])\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all fixed-width formatted (FWF) files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading all fixed-width formatted (FWF) files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'], widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_fwf(\\n    ...     path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'],\\n    ...     chunksize=100,\\n    ...     widths=[1, 3],\\n    ...     names=[\"c0\", \"c1\"]\\n    ... )\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_fwf(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read fixed-width formatted file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".txt\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a FWF dataset instead of simple file(s) loading all the related partitions as columns.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_fwf(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\"c0\", \"c1\"])\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all fixed-width formatted (FWF) files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=\\'s3://bucket/prefix/\\', widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading all fixed-width formatted (FWF) files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_fwf(path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'], widths=[1, 3], names=[\\'c0\\', \\'c1\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_fwf(\\n    ...     path=[\\'s3://bucket/0.txt\\', \\'s3://bucket/1.txt\\'],\\n    ...     chunksize=100,\\n    ...     widths=[1, 3],\\n    ...     names=[\"c0\", \"c1\"]\\n    ... )\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading FWF Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_fwf(path, dataset=True, partition_filter=my_filter, widths=[1, 3], names=[\"c0\", \"c1\"])\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument(\"You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_fwf(path, widths=[1, 3], names=['c0', 'c1'])\")\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    return _read_text_format(read_format='fwf', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=True, sort_index=False, ray_args=ray_args, **pandas_kwargs)"
        ]
    },
    {
        "func_name": "read_json",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\n\n    This function accepts Unix shell-style wildcards in the path argument.\n    * (matches everything), ? (matches any single character),\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\n    you can use `glob.escape(path)` before passing the path to this function.\n\n    Note\n    ----\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Note\n    ----\n    The filter by last_modified begin last_modified end is applied after list all S3 files\n\n    Parameters\n    ----------\n    path : Union[str, List[str]]\n        S3 prefix (accepts Unix shell-style wildcards)\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\n    path_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\n        If None, will try to read all files. (default)\n    path_ignore_suffix: Union[str, List[str], None]\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\n        If None, will try to read all files. (default)\n    version_id: Optional[Union[str, Dict[str, str]]]\n        Version id of the object or mapping of object path to version id.\n        (e.g. {'s3://bucket/key0': '121212', 's3://bucket/key1': '343434'})\n    ignore_empty: bool\n        Ignore files with 0 bytes.\n    orient : str\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\n    use_threads : Union[bool, int]\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    last_modified_begin\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    last_modified_end: datetime, optional\n        Filter the s3 files by the Last modified date of the object.\n        The filter is applied only after list all s3 files.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Optional[Dict[str, Any]]\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    chunksize: int, optional\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\n    dataset: bool\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\n        If `True`, the `lines=True` will be assumed by default.\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\n        This function MUST return a bool, True to read the partition or False to ignore it.\n        Ignored if `dataset=False`.\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\n    ray_args: typing.RaySettings, optional\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\n    pandas_kwargs:\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\n        Pandas arguments in the function call and awswrangler will accept it.\n        e.g. wr.s3.read_json('s3://bucket/prefix/', lines=True, keep_default_dates=True)\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\n\n    Examples\n    --------\n    Reading all JSON files under a prefix\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_json(path='s3://bucket/prefix/')\n\n    Reading all CSV files under a prefix and using pandas_kwargs\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_json('s3://bucket/prefix/', lines=True, keep_default_dates=True)\n\n    Reading all JSON files from a list\n\n    >>> import awswrangler as wr\n    >>> df = wr.s3.read_json(path=['s3://bucket/filename0.json', 's3://bucket/filename1.json'])\n\n    Reading in chunks of 100 lines\n\n    >>> import awswrangler as wr\n    >>> dfs = wr.s3.read_json(path=['s3://bucket/0.json', 's3://bucket/1.json'], chunksize=100, lines=True)\n    >>> for df in dfs:\n    >>>     print(df)  # 100 lines Pandas DataFrame\n\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\n\n    >>> import awswrangler as wr\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\n\n    \"\"\"\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    orient : str\\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\\n        If `True`, the `lines=True` will be assumed by default.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all JSON files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n\\n    Reading all JSON files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=[\\'s3://bucket/filename0.json\\', \\'s3://bucket/filename1.json\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_json(path=[\\'s3://bucket/0.json\\', \\'s3://bucket/1.json\\'], chunksize=100, lines=True)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    orient : str\\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\\n        If `True`, the `lines=True` will be assumed by default.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all JSON files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n\\n    Reading all JSON files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=[\\'s3://bucket/filename0.json\\', \\'s3://bucket/filename1.json\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_json(path=[\\'s3://bucket/0.json\\', \\'s3://bucket/1.json\\'], chunksize=100, lines=True)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    orient : str\\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\\n        If `True`, the `lines=True` will be assumed by default.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all JSON files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n\\n    Reading all JSON files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=[\\'s3://bucket/filename0.json\\', \\'s3://bucket/filename1.json\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_json(path=[\\'s3://bucket/0.json\\', \\'s3://bucket/1.json\\'], chunksize=100, lines=True)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    orient : str\\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\\n        If `True`, the `lines=True` will be assumed by default.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all JSON files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n\\n    Reading all JSON files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=[\\'s3://bucket/filename0.json\\', \\'s3://bucket/filename1.json\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_json(path=[\\'s3://bucket/0.json\\', \\'s3://bucket/1.json\\'], chunksize=100, lines=True)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session'])\ndef read_json(path: Union[str, List[str]], path_suffix: Union[str, List[str], None]=None, path_ignore_suffix: Union[str, List[str], None]=None, version_id: Optional[Union[str, Dict[str, str]]]=None, ignore_empty: bool=True, orient: str='columns', use_threads: Union[bool, int]=True, last_modified_begin: Optional[datetime.datetime]=None, last_modified_end: Optional[datetime.datetime]=None, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, Any]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dataset: bool=False, partition_filter: Optional[Callable[[Dict[str, str]], bool]]=None, ray_args: Optional[RaySettings]=None, **pandas_kwargs: Any) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Read JSON file(s) from a received S3 prefix or list of S3 objects paths.\\n\\n    This function accepts Unix shell-style wildcards in the path argument.\\n    * (matches everything), ? (matches any single character),\\n    [seq] (matches any character in seq), [!seq] (matches any character not in seq).\\n    If you want to use a path which includes Unix shell-style wildcard characters (`*, ?, []`),\\n    you can use `glob.escape(path)` before passing the path to this function.\\n\\n    Note\\n    ----\\n    For partial and gradual reading use the argument ``chunksize`` instead of ``iterator``.\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Note\\n    ----\\n    The filter by last_modified begin last_modified end is applied after list all S3 files\\n\\n    Parameters\\n    ----------\\n    path : Union[str, List[str]]\\n        S3 prefix (accepts Unix shell-style wildcards)\\n        (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. ``[s3://bucket/key0, s3://bucket/key1]``).\\n    path_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes to be read (e.g. [\".json\"]).\\n        If None, will try to read all files. (default)\\n    path_ignore_suffix: Union[str, List[str], None]\\n        Suffix or List of suffixes for S3 keys to be ignored.(e.g. [\"_SUCCESS\"]).\\n        If None, will try to read all files. (default)\\n    version_id: Optional[Union[str, Dict[str, str]]]\\n        Version id of the object or mapping of object path to version id.\\n        (e.g. {\\'s3://bucket/key0\\': \\'121212\\', \\'s3://bucket/key1\\': \\'343434\\'})\\n    ignore_empty: bool\\n        Ignore files with 0 bytes.\\n    orient : str\\n        Same as Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n    use_threads : Union[bool, int]\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    last_modified_begin\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    last_modified_end: datetime, optional\\n        Filter the s3 files by the Last modified date of the object.\\n        The filter is applied only after list all s3 files.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Optional[Dict[str, Any]]\\n        Forward to botocore requests, only \"SSECustomerAlgorithm\" and \"SSECustomerKey\" arguments will be considered.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize: int, optional\\n        If specified, return an generator where chunksize is the number of rows to include in each chunk.\\n    dataset: bool\\n        If `True` read a JSON dataset instead of simple file(s) loading all the related partitions as columns.\\n        If `True`, the `lines=True` will be assumed by default.\\n    partition_filter: Optional[Callable[[Dict[str, str]], bool]]\\n        Callback Function filters to apply on PARTITION columns (PUSH-DOWN filter).\\n        This function MUST receive a single argument (Dict[str, str]) where keys are partitions\\n        names and values are partitions values. Partitions values will be always strings extracted from S3.\\n        This function MUST return a bool, True to read the partition or False to ignore it.\\n        Ignored if `dataset=False`.\\n        E.g ``lambda x: True if x[\"year\"] == \"2020\" and x[\"month\"] == \"1\" else False``\\n        https://aws-sdk-pandas.readthedocs.io/en/3.4.2/tutorials/023%20-%20Flexible%20Partitions%20Filter.html\\n    ray_args: typing.RaySettings, optional\\n        Parameters of the Ray Modin settings. Only used when distributed computing is used with Ray and Modin installed.\\n    pandas_kwargs:\\n        KEYWORD arguments forwarded to pandas.read_json(). You can NOT pass `pandas_kwargs` explicit, just add valid\\n        Pandas arguments in the function call and awswrangler will accept it.\\n        e.g. wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Generator[pandas.DataFrame, None, None]]\\n        Pandas DataFrame or a Generator in case of `chunksize != None`.\\n\\n    Examples\\n    --------\\n    Reading all JSON files under a prefix\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=\\'s3://bucket/prefix/\\')\\n\\n    Reading all CSV files under a prefix and using pandas_kwargs\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(\\'s3://bucket/prefix/\\', lines=True, keep_default_dates=True)\\n\\n    Reading all JSON files from a list\\n\\n    >>> import awswrangler as wr\\n    >>> df = wr.s3.read_json(path=[\\'s3://bucket/filename0.json\\', \\'s3://bucket/filename1.json\\'])\\n\\n    Reading in chunks of 100 lines\\n\\n    >>> import awswrangler as wr\\n    >>> dfs = wr.s3.read_json(path=[\\'s3://bucket/0.json\\', \\'s3://bucket/1.json\\'], chunksize=100, lines=True)\\n    >>> for df in dfs:\\n    >>>     print(df)  # 100 lines Pandas DataFrame\\n\\n    Reading JSON Dataset with PUSH-DOWN filter over partitions\\n\\n    >>> import awswrangler as wr\\n    >>> my_filter = lambda x: True if x[\"city\"].startswith(\"new\") else False\\n    >>> df = wr.s3.read_json(path, dataset=True, partition_filter=my_filter)\\n\\n    '\n    if 'pandas_kwargs' in pandas_kwargs:\n        raise exceptions.InvalidArgument('You can NOT pass `pandas_kwargs` explicit, just add valid Pandas arguments in the function call and awswrangler will accept it.e.g. wr.s3.read_json(path, lines=True, keep_default_dates=True)')\n    if dtype_backend != 'numpy_nullable':\n        pandas_kwargs['dtype_backend'] = dtype_backend\n    s3_client = _utils.client(service_name='s3', session=boto3_session)\n    if dataset is True and 'lines' not in pandas_kwargs:\n        pandas_kwargs['lines'] = True\n    pandas_kwargs['orient'] = orient\n    ignore_index: bool = orient not in ('split', 'index', 'columns')\n    return _read_text_format(read_format='json', path=path, path_suffix=path_suffix, path_ignore_suffix=path_ignore_suffix, version_id=version_id, ignore_empty=ignore_empty, use_threads=use_threads, s3_client=s3_client, s3_additional_kwargs=s3_additional_kwargs, chunksize=chunksize, dataset=dataset, partition_filter=partition_filter, last_modified_begin=last_modified_begin, last_modified_end=last_modified_end, ignore_index=ignore_index, ray_args=ray_args, **pandas_kwargs)"
        ]
    }
]