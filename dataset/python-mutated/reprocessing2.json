[
    {
        "func_name": "reprocess_group",
        "original": "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)",
        "mutated": [
            "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    if False:\n        i = 10\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.reprocess_group', queue='events.reprocessing.process_event', time_limit=120, soft_time_limit=110, silo_mode=SiloMode.REGION)\ndef reprocess_group(project_id, group_id, remaining_events='delete', new_group_id=None, query_state=None, start_time=None, max_events=None, acting_user_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('group_id', group_id)\n    from sentry.reprocessing2 import CannotReprocess, buffered_handle_remaining_events, logger, reprocess_event, start_group_reprocessing\n    sentry_sdk.set_tag('is_start', 'false')\n    if start_time is None:\n        assert new_group_id is None\n        start_time = time.time()\n        metrics.incr('events.reprocessing.start_group_reprocessing', sample_rate=1.0)\n        sentry_sdk.set_tag('is_start', 'true')\n        new_group_id = start_group_reprocessing(project_id, group_id, max_events=max_events, acting_user_id=acting_user_id, remaining_events=remaining_events)\n    assert new_group_id is not None\n    (query_state, events) = celery_run_batch_query(filter=eventstore.Filter(project_ids=[project_id], group_ids=[group_id]), batch_size=settings.SENTRY_REPROCESSING_PAGE_SIZE, state=query_state, referrer='reprocessing2.reprocess_group', tenant_ids={'organization_id': Project.objects.get_from_cache(id=project_id).organization_id})\n    if not events:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=[], remaining_events=remaining_events, force_flush_batch=True)\n        return\n    remaining_event_ids = []\n    for event in events:\n        if max_events is None or max_events > 0:\n            with sentry_sdk.start_span(op='reprocess_event'):\n                try:\n                    reprocess_event(project_id=project_id, event_id=event.event_id, start_time=start_time)\n                except CannotReprocess as e:\n                    logger.error(f'reprocessing2.{e}')\n                except Exception:\n                    sentry_sdk.capture_exception()\n                else:\n                    if max_events is not None:\n                        max_events -= 1\n                    continue\n        remaining_event_ids.append((event.datetime, event.event_id))\n    if remaining_event_ids:\n        buffered_handle_remaining_events(project_id=project_id, old_group_id=group_id, new_group_id=new_group_id, datetime_to_event=remaining_event_ids, remaining_events=remaining_events)\n    reprocess_group.delay(project_id=project_id, group_id=group_id, new_group_id=new_group_id, query_state=query_state, start_time=start_time, max_events=max_events, remaining_events=remaining_events)"
        ]
    },
    {
        "func_name": "handle_remaining_events",
        "original": "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    \"\"\"\n    Delete or merge/move associated per-event data: nodestore, event\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\n\n    This is not full event deletion. Snuba can still only delete entire groups,\n    however we must only run this task for event IDs that we don't intend to\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\n    inserted over in eventstream.\n\n    See doc comment in sentry.reprocessing2.\n    \"\"\"\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))",
        "mutated": [
            "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    if False:\n        i = 10\n    '\\n    Delete or merge/move associated per-event data: nodestore, event\\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\\n\\n    This is not full event deletion. Snuba can still only delete entire groups,\\n    however we must only run this task for event IDs that we don\\'t intend to\\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\\n    inserted over in eventstream.\\n\\n    See doc comment in sentry.reprocessing2.\\n    '\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))",
            "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Delete or merge/move associated per-event data: nodestore, event\\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\\n\\n    This is not full event deletion. Snuba can still only delete entire groups,\\n    however we must only run this task for event IDs that we don\\'t intend to\\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\\n    inserted over in eventstream.\\n\\n    See doc comment in sentry.reprocessing2.\\n    '\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))",
            "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Delete or merge/move associated per-event data: nodestore, event\\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\\n\\n    This is not full event deletion. Snuba can still only delete entire groups,\\n    however we must only run this task for event IDs that we don\\'t intend to\\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\\n    inserted over in eventstream.\\n\\n    See doc comment in sentry.reprocessing2.\\n    '\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))",
            "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Delete or merge/move associated per-event data: nodestore, event\\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\\n\\n    This is not full event deletion. Snuba can still only delete entire groups,\\n    however we must only run this task for event IDs that we don\\'t intend to\\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\\n    inserted over in eventstream.\\n\\n    See doc comment in sentry.reprocessing2.\\n    '\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))",
            "@instrumented_task(name='sentry.tasks.reprocessing2.handle_remaining_events', queue='events.reprocessing.process_event', time_limit=60 * 5, max_retries=5, silo_mode=SiloMode.REGION)\n@retry\ndef handle_remaining_events(project_id, new_group_id, remaining_events, event_ids_redis_key=None, old_group_id=None, event_ids=None, from_timestamp=None, to_timestamp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Delete or merge/move associated per-event data: nodestore, event\\n    attachments, user reports. Mark the event as \"tombstoned\" in Snuba.\\n\\n    This is not full event deletion. Snuba can still only delete entire groups,\\n    however we must only run this task for event IDs that we don\\'t intend to\\n    reuse for reprocessed events. An event ID that is once tombstoned cannot be\\n    inserted over in eventstream.\\n\\n    See doc comment in sentry.reprocessing2.\\n    '\n    sentry_sdk.set_tag('project', project_id)\n    sentry_sdk.set_tag('old_group_id', old_group_id)\n    sentry_sdk.set_tag('new_group_id', new_group_id)\n    from sentry.models.group import Group\n    from sentry.reprocessing2 import EVENT_MODELS_TO_MIGRATE, pop_batched_events_from_redis\n    if event_ids_redis_key is not None:\n        (event_ids, from_timestamp, to_timestamp) = pop_batched_events_from_redis(event_ids_redis_key)\n    metrics.timing('events.reprocessing.handle_remaining_events.batch_size', len(event_ids), sample_rate=1.0)\n    assert remaining_events in ('delete', 'keep')\n    if remaining_events == 'delete':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).delete()\n        node_ids = [Event.generate_node_id(project_id, event_id) for event_id in event_ids]\n        nodestore.delete_multi(node_ids)\n        eventstream.tombstone_events_unsafe(project_id, event_ids, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n    elif remaining_events == 'keep':\n        for cls in EVENT_MODELS_TO_MIGRATE:\n            cls.objects.filter(project_id=project_id, event_id__in=event_ids).update(group_id=new_group_id)\n        eventstream.replace_group_unsafe(project_id, event_ids, new_group_id=new_group_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n        buffer_incr(Group, {'times_seen': len(event_ids)}, {'id': new_group_id})\n    else:\n        raise ValueError(f'Invalid value for remaining_events: {remaining_events}')\n    if old_group_id is not None:\n        from sentry.reprocessing2 import mark_event_reprocessed\n        mark_event_reprocessed(group_id=old_group_id, project_id=project_id, num_events=len(event_ids))"
        ]
    },
    {
        "func_name": "finish_reprocessing",
        "original": "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)",
        "mutated": [
            "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    if False:\n        i = 10\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)",
            "@instrumented_task(name='sentry.tasks.reprocessing2.finish_reprocessing', queue='events.reprocessing.process_event', time_limit=60 * 5 + 5, soft_time_limit=60 * 5)\ndef finish_reprocessing(project_id, group_id):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from sentry.models.activity import Activity\n    from sentry.models.group import Group\n    from sentry.models.groupredirect import GroupRedirect\n    with transaction.atomic(router.db_for_write(Group)):\n        group = Group.objects.get(id=group_id)\n        activity = Activity.objects.get(group_id=group_id, type=ActivityType.REPROCESS.value)\n        new_group_id = activity.group_id = activity.data['newGroupId']\n        activity.save()\n        new_group = Group.objects.get(id=new_group_id)\n        GroupRedirect.objects.create(organization_id=new_group.project.organization_id, group_id=new_group_id, previous_group_id=group_id)\n        group.delete()\n    buffered_delete_old_primary_hash(project_id=project_id, group_id=group_id, force_flush_batch=True)\n    eventstream.exclude_groups(project_id, [group_id])\n    from sentry import similarity\n    similarity.delete(None, group)"
        ]
    }
]