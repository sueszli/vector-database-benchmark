[
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.conv1 = M.Conv2d(3, 6, 5, bias=False)\n    self.bn1 = M.BatchNorm2d(6)\n    self.conv2 = M.Conv2d(6, 16, 5, bias=False)\n    self.bn2 = M.BatchNorm2d(16)\n    self.fc1 = M.Linear(16 * 5 * 5, 120)\n    self.fc2 = M.Linear(120, 84)\n    self.classifier = M.Linear(84, 10)\n    self.pool = M.AvgPool2d(2, 2)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = self.pool(self.bn1(self.conv1(x)))\n    x = self.pool(self.bn2(self.conv2(x)))\n    x = F.flatten(x, 1)\n    x = self.fc1(x)\n    x = self.fc2(x)\n    x = self.classifier(x)\n    return x"
        ]
    },
    {
        "func_name": "func",
        "original": "def func(model, optimizer, timage, tlabel):\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
        "mutated": [
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss"
        ]
    },
    {
        "func_name": "runner",
        "original": "def runner(is_trace):\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
        "mutated": [
            "def runner(is_trace):\n    if False:\n        i = 10\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = AdamW(model.parameters(), lr=0.012)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))"
        ]
    },
    {
        "func_name": "test_xla_trace_bn_opt_state_update",
        "original": "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)",
        "mutated": [
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n    if False:\n        i = 10\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_bn_opt_state_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = AdamW(model.parameters(), lr=0.012)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[3]['exp_avg'].numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=1e-05)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=1e-05)"
        ]
    },
    {
        "func_name": "runner",
        "original": "def runner(is_trace):\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
        "mutated": [
            "def runner(is_trace):\n    if False:\n        i = 10\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, bn_states, opt_states) = ([], [], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.006\n                pg['weight_decay'] = 0.2\n        losses.append(loss.item())\n        bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n        opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n    return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))"
        ]
    },
    {
        "func_name": "test_partial_trace_bn_opt_update",
        "original": "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)",
        "mutated": [
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n    if False:\n        i = 10\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_bn_opt_update():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = Adam(model.parameters(), lr=0.012, weight_decay=0.1)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, bn_states, opt_states) = ([], [], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.006\n                    pg['weight_decay'] = 0.2\n            losses.append(loss.item())\n            bn_states.append(model.bn1.running_mean.numpy().reshape(-1))\n            opt_states.append(list(optimizer._state.values())[7]['exp_avg'].numpy().reshape(-1))\n        return (np.asarray(losses), np.stack(bn_states), np.stack(opt_states))\n    (imp_loss, imp_bn_states, imp_opt_states) = runner(False)\n    (xla_loss, xla_bn_states, xla_opt_states) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=0.0005, rtol=0.001)\n    np.testing.assert_allclose(imp_bn_states, xla_bn_states, atol=0.0001)\n    np.testing.assert_allclose(imp_opt_states, xla_opt_states, atol=0.0001)"
        ]
    },
    {
        "func_name": "func",
        "original": "def func(model, optimizer, timage, tlabel):\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
        "mutated": [
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss",
            "def func(model, optimizer, timage, tlabel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with gm:\n        score = model(timage)\n        loss = F.nn.cross_entropy(score, tlabel)\n        gm.backward(loss)\n        optimizer.step().clear_grad()\n    return loss"
        ]
    },
    {
        "func_name": "runner",
        "original": "def runner(is_trace):\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
        "mutated": [
            "def runner(is_trace):\n    if False:\n        i = 10\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n\n    def func(model, optimizer, timage, tlabel):\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        return loss\n    if is_trace:\n        func = xla_trace(func, without_host=True, capture_as_const=True)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        loss = func(model, optimizer, timage, tlabel)\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))"
        ]
    },
    {
        "func_name": "tester",
        "original": "def tester(OptCls, **kwargs):\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
        "mutated": [
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n\n        def func(model, optimizer, timage, tlabel):\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            return loss\n        if is_trace:\n            func = xla_trace(func, without_host=True, capture_as_const=True)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            loss = func(model, optimizer, timage, tlabel)\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)"
        ]
    },
    {
        "func_name": "test_xla_trace_optimizer",
        "original": "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
        "mutated": [
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n    if False:\n        i = 10\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_xla_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n\n            def func(model, optimizer, timage, tlabel):\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                return loss\n            if is_trace:\n                func = xla_trace(func, without_host=True, capture_as_const=True)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                loss = func(model, optimizer, timage, tlabel)\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)"
        ]
    },
    {
        "func_name": "runner",
        "original": "def runner(is_trace):\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
        "mutated": [
            "def runner(is_trace):\n    if False:\n        i = 10\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))",
            "def runner(is_trace):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(123)\n    megengine.random.seed(123)\n    model = ConvNet()\n    model.train()\n    image = np.random.randn(3, 8, 3, 32, 32)\n    label = np.random.randint(0, 10, (3, 8))\n    gm = autodiff.GradManager().attach(model.parameters())\n    optimizer = OptCls(model.parameters(), **kwargs)\n    if is_trace:\n        model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n        optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n    (losses, updated_weights) = ([], [])\n    for i in range(10):\n        timage = megengine.Tensor(image[i % 3])\n        tlabel = megengine.Tensor(label[i % 3])\n        with gm:\n            score = model(timage)\n            loss = F.nn.cross_entropy(score, tlabel)\n            gm.backward(loss)\n            optimizer.step().clear_grad()\n        losses.append(loss.item())\n        updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n        if i == 4:\n            for pg in optimizer.param_groups:\n                pg['lr'] = 0.005\n    return (np.asarray(losses), np.stack(updated_weights))"
        ]
    },
    {
        "func_name": "tester",
        "original": "def tester(OptCls, **kwargs):\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
        "mutated": [
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)",
            "def tester(OptCls, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def runner(is_trace):\n        np.random.seed(123)\n        megengine.random.seed(123)\n        model = ConvNet()\n        model.train()\n        image = np.random.randn(3, 8, 3, 32, 32)\n        label = np.random.randint(0, 10, (3, 8))\n        gm = autodiff.GradManager().attach(model.parameters())\n        optimizer = OptCls(model.parameters(), **kwargs)\n        if is_trace:\n            model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n            optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n        (losses, updated_weights) = ([], [])\n        for i in range(10):\n            timage = megengine.Tensor(image[i % 3])\n            tlabel = megengine.Tensor(label[i % 3])\n            with gm:\n                score = model(timage)\n                loss = F.nn.cross_entropy(score, tlabel)\n                gm.backward(loss)\n                optimizer.step().clear_grad()\n            losses.append(loss.item())\n            updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n            if i == 4:\n                for pg in optimizer.param_groups:\n                    pg['lr'] = 0.005\n        return (np.asarray(losses), np.stack(updated_weights))\n    (imp_loss, imp_weight) = runner(False)\n    (xla_loss, xla_weight) = runner(True)\n    np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n    np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)"
        ]
    },
    {
        "func_name": "test_partial_trace_optimizer",
        "original": "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
        "mutated": [
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n    if False:\n        i = 10\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_partial_trace_optimizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def tester(OptCls, **kwargs):\n\n        def runner(is_trace):\n            np.random.seed(123)\n            megengine.random.seed(123)\n            model = ConvNet()\n            model.train()\n            image = np.random.randn(3, 8, 3, 32, 32)\n            label = np.random.randint(0, 10, (3, 8))\n            gm = autodiff.GradManager().attach(model.parameters())\n            optimizer = OptCls(model.parameters(), **kwargs)\n            if is_trace:\n                model.forward = partial(partial_trace(func=type(model).forward, backend='xla', capture_as_const=True), model)\n                optimizer._updates = partial(partial_trace(func=type(optimizer)._updates, backend='xla', capture_as_const=True), optimizer)\n            (losses, updated_weights) = ([], [])\n            for i in range(10):\n                timage = megengine.Tensor(image[i % 3])\n                tlabel = megengine.Tensor(label[i % 3])\n                with gm:\n                    score = model(timage)\n                    loss = F.nn.cross_entropy(score, tlabel)\n                    gm.backward(loss)\n                    optimizer.step().clear_grad()\n                losses.append(loss.item())\n                updated_weights.append(model.conv1.weight.numpy().reshape(-1))\n                if i == 4:\n                    for pg in optimizer.param_groups:\n                        pg['lr'] = 0.005\n            return (np.asarray(losses), np.stack(updated_weights))\n        (imp_loss, imp_weight) = runner(False)\n        (xla_loss, xla_weight) = runner(True)\n        np.testing.assert_allclose(imp_loss, xla_loss, atol=5e-05, rtol=10000000000.0)\n        np.testing.assert_allclose(imp_weight, xla_weight, atol=5e-05, rtol=10000000000.0)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.9, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0, nesterov=False, weight_decay=0.1)\n    tester(SGD, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adadelta, lr=0.01, rho=0.9, eps=1e-05, weight_decay=0)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.9, eps=1e-05, weight_decay=0.0)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0.1)\n    tester(Adagrad, lr=0.01, lr_decay=0.0, eps=1e-05, weight_decay=0)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(Adam, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.0)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0.1)\n    tester(AdamW, lr=0.01, betas=(0.1, 0.01), eps=1e-05, weight_decay=0)"
        ]
    }
]