[
    {
        "func_name": "recursive_print",
        "original": "def recursive_print(name, val, spaces=0):\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
        "mutated": [
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)"
        ]
    },
    {
        "func_name": "fix_query_key_value_ordering",
        "original": "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
        "mutated": [
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param"
        ]
    },
    {
        "func_name": "convert_megatron_checkpoint",
        "original": "def convert_megatron_checkpoint(args, input_state_dict, config):\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict",
        "mutated": [
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict",
            "def convert_megatron_checkpoint(args, input_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output_state_dict = {}\n    ds_args = input_state_dict.get('args', None)\n    if ds_args is not None:\n        config.tokenizer_type = ds_args.tokenizer_type\n        config.vocab_size = ds_args.padded_vocab_size\n        config.max_position_embeddings = ds_args.max_position_embeddings\n        config.hidden_size = ds_args.hidden_size\n        config.num_hidden_layers = ds_args.num_layers\n        config.num_attention_heads = ds_args.num_attention_heads\n        config.intermediate_size = ds_args.ffn_hidden_size if 'ffn_hidden_size' in ds_args else 4 * ds_args.hidden_size\n    heads = config.num_attention_heads\n    hidden_size_per_head = config.hidden_size // heads\n    if 'checkpoint_version' in input_state_dict.keys():\n        checkpoint_version = input_state_dict['checkpoint_version']\n    else:\n        checkpoint_version = 0.0\n    model = input_state_dict['model']\n    lm = model['language_model']\n    embeddings = lm['embedding']\n    word_embeddings = embeddings['word_embeddings']['weight']\n    word_embeddings = word_embeddings[:config.vocab_size, :]\n    output_state_dict['bert.embeddings.word_embeddings.weight'] = word_embeddings\n    pos_embeddings = embeddings['position_embeddings']['weight']\n    assert pos_embeddings.size(0) == config.max_position_embeddings and pos_embeddings.size(1) == config.hidden_size\n    output_state_dict['bert.embeddings.position_embeddings.weight'] = pos_embeddings\n    tokentype_embeddings = embeddings['tokentype_embeddings']['weight']\n    output_state_dict['bert.embeddings.token_type_embeddings.weight'] = tokentype_embeddings\n    transformer = lm['transformer'] if 'transformer' in lm.keys() else lm['encoder']\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    megatron_to_transformers = {'attention.dense': '.attention.output.dense.', 'self_attention.dense': '.attention.output.dense.', 'mlp.dense_h_to_4h': '.intermediate.dense.', 'mlp.dense_4h_to_h': '.output.dense.'}\n    attention_qkv_weight = None\n    for (key, val) in transformer.items():\n        m = layer_re.match(key)\n        if m is None:\n            break\n        layer_idx = int(m.group(1))\n        op_name = m.group(2)\n        weight_or_bias = m.group(3)\n        layer_name = f'bert.encoder.layer.{layer_idx}'\n        if op_name.endswith('layernorm'):\n            ln_name = 'attention.ln' if op_name.startswith('input') else 'ln'\n            output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n            assert attention_qkv_weight is None, ''\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            attention_qkv_weight = out_val\n        elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n            assert attention_qkv_weight is not None, ''\n            q = attention_qkv_weight[0 * config.hidden_size:1 * config.hidden_size, :]\n            k = attention_qkv_weight[1 * config.hidden_size:2 * config.hidden_size, :]\n            v = attention_qkv_weight[2 * config.hidden_size:3 * config.hidden_size, :]\n            out_val = fix_query_key_value_ordering(val, checkpoint_version, 3, heads, hidden_size_per_head)\n            q_bias = out_val[0 * config.hidden_size:1 * config.hidden_size]\n            k_bias = out_val[1 * config.hidden_size:2 * config.hidden_size]\n            v_bias = out_val[2 * config.hidden_size:3 * config.hidden_size]\n            output_state_dict[f'{layer_name}.attention.self.query.weight'] = q\n            output_state_dict[f'{layer_name}.attention.self.query.bias'] = q_bias\n            output_state_dict[f'{layer_name}.attention.self.key.weight'] = k\n            output_state_dict[f'{layer_name}.attention.self.key.bias'] = k_bias\n            output_state_dict[f'{layer_name}.attention.self.value.weight'] = v\n            output_state_dict[f'{layer_name}.attention.self.value.bias'] = v_bias\n            attention_qkv_weight = None\n        elif weight_or_bias in ['weight', 'bias']:\n            out_name = megatron_to_transformers[op_name]\n            output_state_dict[layer_name + out_name + weight_or_bias] = val\n    output_state_dict['bert.encoder.ln.weight'] = transformer['final_layernorm.weight']\n    output_state_dict['bert.encoder.ln.bias'] = transformer['final_layernorm.bias']\n    pooler = lm['pooler']\n    output_state_dict['bert.pooler.dense.weight'] = pooler['dense.weight']\n    output_state_dict['bert.pooler.dense.bias'] = pooler['dense.bias']\n    lm_head = model['lm_head']\n    output_state_dict['cls.predictions.transform.dense.weight'] = lm_head['dense.weight']\n    output_state_dict['cls.predictions.transform.dense.bias'] = lm_head['dense.bias']\n    output_state_dict['cls.predictions.transform.LayerNorm.weight'] = lm_head['layernorm.weight']\n    output_state_dict['cls.predictions.transform.LayerNorm.bias'] = lm_head['layernorm.bias']\n    output_state_dict['cls.predictions.decoder.weight'] = word_embeddings\n    output_state_dict['cls.predictions.bias'] = lm_head['bias']\n    binary_head = model['binary_head']\n    output_state_dict['cls.seq_relationship.weight'] = binary_head['weight']\n    output_state_dict['cls.seq_relationship.bias'] = binary_head['bias']\n    return output_state_dict"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    parser.add_argument('path_to_checkpoint', type=str, help='Path to the ZIP file containing the checkpoint')\n    parser.add_argument('--config_file', default='', type=str, help='An optional config json file describing the pre-trained model.')\n    args = parser.parse_args()\n    basename = os.path.dirname(args.path_to_checkpoint)\n    print(f'Extracting PyTorch state dictionary from \"{args.path_to_checkpoint}\"')\n    if args.path_to_checkpoint.endswith('.zip'):\n        with zipfile.ZipFile(args.path_to_checkpoint, 'r') as checkpoint:\n            with checkpoint.open('release/mp_rank_00/model_optim_rng.pt') as pytorch_dict:\n                input_state_dict = torch.load(pytorch_dict, map_location='cpu')\n    else:\n        input_state_dict = torch.load(args.path_to_checkpoint, map_location='cpu')\n    if args.config_file == '':\n        config = MegatronBertConfig()\n        config.vocab_size = input_state_dict['model']['lm_head']['bias'].numel()\n    else:\n        config = MegatronBertConfig.from_json_file(args.config_file)\n    print('Converting')\n    output_state_dict = convert_megatron_checkpoint(args, input_state_dict, config)\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    print('Saving config')\n    config.save_pretrained(basename)\n    output_checkpoint_file = os.path.join(basename, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(output_state_dict, output_checkpoint_file)"
        ]
    }
]