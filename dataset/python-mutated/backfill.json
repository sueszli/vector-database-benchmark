[
    {
        "func_name": "_assert_permission_for_asset_graph",
        "original": "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)",
        "mutated": [
            "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    if False:\n        i = 10\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)",
            "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)",
            "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)",
            "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)",
            "def _assert_permission_for_asset_graph(graphene_info: 'ResolveInfo', asset_graph: ExternalAssetGraph, asset_selection: Optional[Sequence[AssetKey]], permission: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    asset_keys = set(asset_selection or [])\n    if asset_keys.difference(asset_graph.repository_handles_by_key.keys()):\n        assert_permission(graphene_info, permission)\n        return\n    if asset_keys:\n        repo_handles = [asset_graph.get_repository_handle(asset_key) for asset_key in asset_keys]\n    else:\n        repo_handles = asset_graph.repository_handles_by_key.values()\n    location_names = set((repo_handle.code_location_origin.location_name for repo_handle in repo_handles))\n    if not location_names:\n        assert_permission(graphene_info, permission)\n    else:\n        for location_name in location_names:\n            assert_permission_for_location(graphene_info, permission, location_name)"
        ]
    },
    {
        "func_name": "get_asset_backfill_preview",
        "original": "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions",
        "mutated": [
            "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    if False:\n        i = 10\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions",
            "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions",
            "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions",
            "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions",
            "def get_asset_backfill_preview(graphene_info: 'ResolveInfo', backfill_preview_params: AssetBackfillPreviewParams) -> Sequence['GrapheneAssetPartitions']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from ...schema.backfill import GrapheneAssetPartitions\n    asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n    check.invariant(backfill_preview_params.get('assetSelection') is not None)\n    check.invariant(backfill_preview_params.get('partitionNames') is not None)\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_preview_params['assetSelection']]\n    partition_names: List[str] = backfill_preview_params['partitionNames']\n    asset_backfill_data = create_asset_backfill_data_from_asset_partitions(asset_graph, asset_selection, partition_names, graphene_info.context.instance)\n    asset_partitions = []\n    for asset_key in asset_backfill_data.get_targeted_asset_keys_topological_order():\n        if asset_graph.get_partitions_def(asset_key):\n            partitions_subset = asset_backfill_data.target_subset.partitions_subsets_by_asset_key[asset_key]\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=partitions_subset))\n        else:\n            asset_partitions.append(GrapheneAssetPartitions(asset_key=asset_key, partitions_subset=None))\n    return asset_partitions"
        ]
    },
    {
        "func_name": "create_and_launch_partition_backfill",
        "original": "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)",
        "mutated": [
            "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    if False:\n        i = 10\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)",
            "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)",
            "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)",
            "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)",
            "def create_and_launch_partition_backfill(graphene_info: 'ResolveInfo', backfill_params: BackfillParams) -> Union['GrapheneLaunchBackfillSuccess', 'GraphenePartitionSetNotFoundError']:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from ...schema.backfill import GrapheneLaunchBackfillSuccess\n    from ...schema.errors import GraphenePartitionSetNotFoundError\n    backfill_id = make_new_backfill_id()\n    asset_selection = [cast(AssetKey, AssetKey.from_graphql_input(asset_key)) for asset_key in backfill_params['assetSelection']] if backfill_params.get('assetSelection') else None\n    partitions_by_assets = backfill_params.get('partitionsByAssets')\n    check.invariant(asset_selection is None and backfill_params.get('selector') is None and (backfill_params.get('partitionNames') is None) if partitions_by_assets else True, 'partitions_by_assets cannot be used together with asset_selection, selector, or partitionNames')\n    tags = {t['key']: t['value'] for t in backfill_params.get('tags', [])}\n    tags = {**tags, **graphene_info.context.get_viewer_tags()}\n    backfill_timestamp = pendulum.now('UTC').timestamp()\n    if backfill_params.get('selector') is not None:\n        partition_set_selector = backfill_params['selector']\n        partition_set_name = partition_set_selector.get('partitionSetName')\n        repository_selector = RepositorySelector.from_graphql_input(partition_set_selector.get('repositorySelector'))\n        assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, repository_selector.location_name)\n        location = graphene_info.context.get_code_location(repository_selector.location_name)\n        repository = location.get_repository(repository_selector.repository_name)\n        matches = [partition_set for partition_set in repository.get_external_partition_sets() if partition_set.name == partition_set_selector.get('partitionSetName')]\n        if not matches:\n            return GraphenePartitionSetNotFoundError(partition_set_name)\n        check.invariant(len(matches) == 1, 'Partition set names must be unique: found {num} matches for {partition_set_name}'.format(num=len(matches), partition_set_name=partition_set_name))\n        external_partition_set = next(iter(matches))\n        if backfill_params.get('allPartitions'):\n            result = graphene_info.context.get_external_partition_names(external_partition_set, instance=graphene_info.context.instance)\n            if isinstance(result, ExternalPartitionExecutionErrorData):\n                raise DagsterUserCodeProcessError.from_error_info(result.error)\n            partition_names = result.partition_names\n        elif backfill_params.get('partitionNames'):\n            partition_names = backfill_params['partitionNames']\n        else:\n            raise DagsterError('Backfill requested without specifying either \"allPartitions\" or \"partitionNames\" arguments')\n        backfill = PartitionBackfill(backfill_id=backfill_id, partition_set_origin=external_partition_set.get_external_origin(), status=BulkActionStatus.REQUESTED, partition_names=partition_names, from_failure=bool(backfill_params.get('fromFailure')), reexecution_steps=backfill_params.get('reexecutionSteps'), tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection)\n        if backfill_params.get('forceSynchronousSubmission'):\n            to_submit = [name for name in partition_names]\n            submitted_run_ids: List[str] = []\n            while to_submit:\n                chunk = to_submit[:BACKFILL_CHUNK_SIZE]\n                to_submit = to_submit[BACKFILL_CHUNK_SIZE:]\n                submitted_run_ids.extend((run_id for run_id in submit_backfill_runs(graphene_info.context.instance, create_workspace=lambda : graphene_info.context, backfill_job=backfill, partition_names=chunk) if run_id is not None))\n            return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id, launched_run_ids=submitted_run_ids)\n    elif asset_selection is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_asset_partitions(asset_graph=asset_graph, backfill_id=backfill_id, tags=tags, backfill_timestamp=backfill_timestamp, asset_selection=asset_selection, partition_names=backfill_params.get('partitionNames'), dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), all_partitions=backfill_params.get('allPartitions', False))\n    elif partitions_by_assets is not None:\n        if backfill_params.get('forceSynchronousSubmission'):\n            raise DagsterError('forceSynchronousSubmission is not supported for pure asset backfills')\n        if backfill_params.get('fromFailure'):\n            raise DagsterError('fromFailure is not supported for pure asset backfills')\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, asset_selection, Permissions.LAUNCH_PARTITION_BACKFILL)\n        backfill = PartitionBackfill.from_partitions_by_assets(backfill_id=backfill_id, asset_graph=asset_graph, backfill_timestamp=backfill_timestamp, tags=tags, dynamic_partitions_store=CachingInstanceQueryer(graphene_info.context.instance, asset_graph, utc_datetime_from_timestamp(backfill_timestamp)), partitions_by_assets=[PartitionsByAssetSelector.from_graphql_input(partitions_by_asset_selector) for partitions_by_asset_selector in partitions_by_assets])\n    else:\n        raise DagsterError('Backfill requested without specifying partition set selector or asset selection')\n    graphene_info.context.instance.add_backfill(backfill)\n    return GrapheneLaunchBackfillSuccess(backfill_id=backfill_id)"
        ]
    },
    {
        "func_name": "cancel_partition_backfill",
        "original": "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)",
        "mutated": [
            "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    if False:\n        i = 10\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)",
            "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)",
            "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)",
            "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)",
            "def cancel_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneCancelBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from ...schema.backfill import GrapheneCancelBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    if backfill.serialized_asset_backfill_data:\n        asset_graph = ExternalAssetGraph.from_workspace(graphene_info.context)\n        _assert_permission_for_asset_graph(graphene_info, asset_graph, backfill.asset_selection, Permissions.CANCEL_PARTITION_BACKFILL)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELING))\n    else:\n        partition_set_origin = check.not_none(backfill.partition_set_origin)\n        location_name = partition_set_origin.selector.location_name\n        assert_permission_for_location(graphene_info, Permissions.CANCEL_PARTITION_BACKFILL, location_name)\n        graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.CANCELED))\n    return GrapheneCancelBackfillSuccess(backfill_id=backfill_id)"
        ]
    },
    {
        "func_name": "resume_partition_backfill",
        "original": "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)",
        "mutated": [
            "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    if False:\n        i = 10\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)",
            "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)",
            "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)",
            "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)",
            "def resume_partition_backfill(graphene_info: 'ResolveInfo', backfill_id: str) -> 'GrapheneResumeBackfillSuccess':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from ...schema.backfill import GrapheneResumeBackfillSuccess\n    backfill = graphene_info.context.instance.get_backfill(backfill_id)\n    if not backfill:\n        check.failed(f'No backfill found for id: {backfill_id}')\n    partition_set_origin = check.not_none(backfill.partition_set_origin)\n    location_name = partition_set_origin.selector.location_name\n    assert_permission_for_location(graphene_info, Permissions.LAUNCH_PARTITION_BACKFILL, location_name)\n    graphene_info.context.instance.update_backfill(backfill.with_status(BulkActionStatus.REQUESTED))\n    return GrapheneResumeBackfillSuccess(backfill_id=backfill_id)"
        ]
    }
]