[
    {
        "func_name": "__call__",
        "original": "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)",
        "mutated": [
            "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if False:\n        i = 10\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)",
            "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)",
            "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)",
            "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)",
            "def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if titles is None and texts is None:\n        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    elif titles is None or texts is None:\n        text_pair = titles if texts is None else texts\n        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)\n    titles = titles if not isinstance(titles, str) else [titles]\n    texts = texts if not isinstance(texts, str) else [texts]\n    n_passages = len(titles)\n    questions = questions if not isinstance(questions, str) else [questions] * n_passages\n    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'\n    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']\n    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']\n    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}\n    if return_attention_mask is not False:\n        attention_mask = []\n        for input_ids in encoded_inputs['input_ids']:\n            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])\n        encoded_inputs['attention_mask'] = attention_mask\n    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)"
        ]
    },
    {
        "func_name": "decode_best_spans",
        "original": "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    \"\"\"\n        Get the span predictions for the extractive Q&A model.\n\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\n        *DPRReaderOutput* is a *Tuple* with:\n\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\n\n        Examples:\n\n        ```python\n        >>> from transformers import DPRReader, DPRReaderTokenizer\n\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\n        >>> encoded_inputs = tokenizer(\n        ...     questions=[\"What is love ?\"],\n        ...     titles=[\"Haddaway\"],\n        ...     texts=[\"'What Is Love' is a song recorded by the artist Haddaway\"],\n        ...     return_tensors=\"pt\",\n        ... )\n        >>> outputs = model(**encoded_inputs)\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\n        >>> print(predicted_spans[0].text)  # best span\n        a song\n        ```\"\"\"\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]",
        "mutated": [
            "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n    '\\n        Get the span predictions for the extractive Q&A model.\\n\\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\\n        *DPRReaderOutput* is a *Tuple* with:\\n\\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\\n\\n        Examples:\\n\\n        ```python\\n        >>> from transformers import DPRReader, DPRReaderTokenizer\\n\\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> encoded_inputs = tokenizer(\\n        ...     questions=[\"What is love ?\"],\\n        ...     titles=[\"Haddaway\"],\\n        ...     texts=[\"\\'What Is Love\\' is a song recorded by the artist Haddaway\"],\\n        ...     return_tensors=\"pt\",\\n        ... )\\n        >>> outputs = model(**encoded_inputs)\\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\\n        >>> print(predicted_spans[0].text)  # best span\\n        a song\\n        ```'\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]",
            "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Get the span predictions for the extractive Q&A model.\\n\\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\\n        *DPRReaderOutput* is a *Tuple* with:\\n\\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\\n\\n        Examples:\\n\\n        ```python\\n        >>> from transformers import DPRReader, DPRReaderTokenizer\\n\\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> encoded_inputs = tokenizer(\\n        ...     questions=[\"What is love ?\"],\\n        ...     titles=[\"Haddaway\"],\\n        ...     texts=[\"\\'What Is Love\\' is a song recorded by the artist Haddaway\"],\\n        ...     return_tensors=\"pt\",\\n        ... )\\n        >>> outputs = model(**encoded_inputs)\\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\\n        >>> print(predicted_spans[0].text)  # best span\\n        a song\\n        ```'\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]",
            "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Get the span predictions for the extractive Q&A model.\\n\\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\\n        *DPRReaderOutput* is a *Tuple* with:\\n\\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\\n\\n        Examples:\\n\\n        ```python\\n        >>> from transformers import DPRReader, DPRReaderTokenizer\\n\\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> encoded_inputs = tokenizer(\\n        ...     questions=[\"What is love ?\"],\\n        ...     titles=[\"Haddaway\"],\\n        ...     texts=[\"\\'What Is Love\\' is a song recorded by the artist Haddaway\"],\\n        ...     return_tensors=\"pt\",\\n        ... )\\n        >>> outputs = model(**encoded_inputs)\\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\\n        >>> print(predicted_spans[0].text)  # best span\\n        a song\\n        ```'\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]",
            "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Get the span predictions for the extractive Q&A model.\\n\\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\\n        *DPRReaderOutput* is a *Tuple* with:\\n\\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\\n\\n        Examples:\\n\\n        ```python\\n        >>> from transformers import DPRReader, DPRReaderTokenizer\\n\\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> encoded_inputs = tokenizer(\\n        ...     questions=[\"What is love ?\"],\\n        ...     titles=[\"Haddaway\"],\\n        ...     texts=[\"\\'What Is Love\\' is a song recorded by the artist Haddaway\"],\\n        ...     return_tensors=\"pt\",\\n        ... )\\n        >>> outputs = model(**encoded_inputs)\\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\\n        >>> print(predicted_spans[0].text)  # best span\\n        a song\\n        ```'\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]",
            "def decode_best_spans(self, reader_input: BatchEncoding, reader_output: DPRReaderOutput, num_spans: int=16, max_answer_length: int=64, num_spans_per_passage: int=4) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Get the span predictions for the extractive Q&A model.\\n\\n        Returns: *List* of *DPRReaderOutput* sorted by descending *(relevance_score, span_score)*. Each\\n        *DPRReaderOutput* is a *Tuple* with:\\n\\n            - **span_score**: `float` that corresponds to the score given by the reader for this span compared to other\\n              spans in the same passage. It corresponds to the sum of the start and end logits of the span.\\n            - **relevance_score**: `float` that corresponds to the score of the each passage to answer the question,\\n              compared to all the other passages. It corresponds to the output of the QA classifier of the DPRReader.\\n            - **doc_id**: `int` the id of the passage. - ***start_index**: `int` the start index of the span\\n              (inclusive). - **end_index**: `int` the end index of the span (inclusive).\\n\\n        Examples:\\n\\n        ```python\\n        >>> from transformers import DPRReader, DPRReaderTokenizer\\n\\n        >>> tokenizer = DPRReaderTokenizer.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> model = DPRReader.from_pretrained(\"facebook/dpr-reader-single-nq-base\")\\n        >>> encoded_inputs = tokenizer(\\n        ...     questions=[\"What is love ?\"],\\n        ...     titles=[\"Haddaway\"],\\n        ...     texts=[\"\\'What Is Love\\' is a song recorded by the artist Haddaway\"],\\n        ...     return_tensors=\"pt\",\\n        ... )\\n        >>> outputs = model(**encoded_inputs)\\n        >>> predicted_spans = tokenizer.decode_best_spans(encoded_inputs, outputs)\\n        >>> print(predicted_spans[0].text)  # best span\\n        a song\\n        ```'\n    input_ids = reader_input['input_ids']\n    (start_logits, end_logits, relevance_logits) = reader_output[:3]\n    n_passages = len(relevance_logits)\n    sorted_docs = sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)\n    nbest_spans_predictions: List[DPRReaderOutput] = []\n    for doc_id in sorted_docs:\n        sequence_ids = list(input_ids[doc_id])\n        passage_offset = sequence_ids.index(self.sep_token_id, 2) + 1\n        if sequence_ids[-1] == self.pad_token_id:\n            sequence_len = sequence_ids.index(self.pad_token_id)\n        else:\n            sequence_len = len(sequence_ids)\n        best_spans = self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)\n        for (start_index, end_index) in best_spans:\n            start_index += passage_offset\n            end_index += passage_offset\n            nbest_spans_predictions.append(DPRSpanPrediction(span_score=start_logits[doc_id][start_index] + end_logits[doc_id][end_index], relevance_score=relevance_logits[doc_id], doc_id=doc_id, start_index=start_index, end_index=end_index, text=self.decode(sequence_ids[start_index:end_index + 1])))\n        if len(nbest_spans_predictions) >= num_spans:\n            break\n    return nbest_spans_predictions[:num_spans]"
        ]
    },
    {
        "func_name": "_get_best_spans",
        "original": "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    \"\"\"\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\n        \"\"\"\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals",
        "mutated": [
            "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n    '\\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\\n        '\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals",
            "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\\n        '\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals",
            "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\\n        '\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals",
            "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\\n        '\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals",
            "def _get_best_spans(self, start_logits: List[int], end_logits: List[int], max_answer_length: int, top_spans: int) -> List[DPRSpanPrediction]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Finds the best answer span for the extractive Q&A model for one passage. It returns the best span by descending\\n        `span_score` order and keeping max `top_spans` spans. Spans longer that `max_answer_length` are ignored.\\n        '\n    scores = []\n    for (start_index, start_score) in enumerate(start_logits):\n        for (answer_length, end_score) in enumerate(end_logits[start_index:start_index + max_answer_length]):\n            scores.append(((start_index, start_index + answer_length), start_score + end_score))\n    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n    chosen_span_intervals = []\n    for ((start_index, end_index), score) in scores:\n        assert start_index <= end_index, f'Wrong span indices: [{start_index}:{end_index}]'\n        length = end_index - start_index + 1\n        assert length <= max_answer_length, f'Span is too long: {length} > {max_answer_length}'\n        if any((start_index <= prev_start_index <= prev_end_index <= end_index or prev_start_index <= start_index <= end_index <= prev_end_index for (prev_start_index, prev_end_index) in chosen_span_intervals)):\n            continue\n        chosen_span_intervals.append((start_index, end_index))\n        if len(chosen_span_intervals) == top_spans:\n            break\n    return chosen_span_intervals"
        ]
    }
]