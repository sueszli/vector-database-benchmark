[
    {
        "func_name": "simple_save",
        "original": "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    \"\"\"Convenience function to build a SavedModel suitable for serving.\n\n  In many common cases, saving models for serving will be as simple as:\n\n      simple_save(session,\n                  export_dir,\n                  inputs={\"x\": x, \"y\": y},\n                  outputs={\"z\": z})\n\n  Although in many cases it's not necessary to understand all of the many ways\n      to configure a SavedModel, this method has a few practical implications:\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\n      `saved_model.SERVING`)\n    - The SavedModel will load in TensorFlow Serving and supports the\n      [Predict\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\n      To use the Classify, Regress, or MultiInference APIs, please\n      use either\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\n      or the lower level\n      [SavedModel\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\n    - Some TensorFlow ops depend on information on disk or other information\n      called \"assets\". These are generally handled automatically by adding the\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\n      collection are exported; if you need more custom behavior, you'll need to\n      use the\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\n\n  More information about SavedModel and signatures can be found here:\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\n\n  Args:\n    session: The TensorFlow session from which to save the meta graph and\n        variables.\n    export_dir: The path to which the SavedModel will be stored.\n    inputs: dict mapping string input names to tensors. These are added\n        to the SignatureDef as the inputs.\n    outputs:  dict mapping string output names to tensors. These are added\n        to the SignatureDef as the outputs.\n    legacy_init_op: Legacy support for op or group of ops to execute after the\n        restore op upon a load.\n  \"\"\"\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()",
        "mutated": [
            "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    if False:\n        i = 10\n    'Convenience function to build a SavedModel suitable for serving.\\n\\n  In many common cases, saving models for serving will be as simple as:\\n\\n      simple_save(session,\\n                  export_dir,\\n                  inputs={\"x\": x, \"y\": y},\\n                  outputs={\"z\": z})\\n\\n  Although in many cases it\\'s not necessary to understand all of the many ways\\n      to configure a SavedModel, this method has a few practical implications:\\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\\n      `saved_model.SERVING`)\\n    - The SavedModel will load in TensorFlow Serving and supports the\\n      [Predict\\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\\n      To use the Classify, Regress, or MultiInference APIs, please\\n      use either\\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\\n      or the lower level\\n      [SavedModel\\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\\n    - Some TensorFlow ops depend on information on disk or other information\\n      called \"assets\". These are generally handled automatically by adding the\\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\\n      collection are exported; if you need more custom behavior, you\\'ll need to\\n      use the\\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\\n\\n  More information about SavedModel and signatures can be found here:\\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\\n\\n  Args:\\n    session: The TensorFlow session from which to save the meta graph and\\n        variables.\\n    export_dir: The path to which the SavedModel will be stored.\\n    inputs: dict mapping string input names to tensors. These are added\\n        to the SignatureDef as the inputs.\\n    outputs:  dict mapping string output names to tensors. These are added\\n        to the SignatureDef as the outputs.\\n    legacy_init_op: Legacy support for op or group of ops to execute after the\\n        restore op upon a load.\\n  '\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()",
            "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convenience function to build a SavedModel suitable for serving.\\n\\n  In many common cases, saving models for serving will be as simple as:\\n\\n      simple_save(session,\\n                  export_dir,\\n                  inputs={\"x\": x, \"y\": y},\\n                  outputs={\"z\": z})\\n\\n  Although in many cases it\\'s not necessary to understand all of the many ways\\n      to configure a SavedModel, this method has a few practical implications:\\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\\n      `saved_model.SERVING`)\\n    - The SavedModel will load in TensorFlow Serving and supports the\\n      [Predict\\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\\n      To use the Classify, Regress, or MultiInference APIs, please\\n      use either\\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\\n      or the lower level\\n      [SavedModel\\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\\n    - Some TensorFlow ops depend on information on disk or other information\\n      called \"assets\". These are generally handled automatically by adding the\\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\\n      collection are exported; if you need more custom behavior, you\\'ll need to\\n      use the\\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\\n\\n  More information about SavedModel and signatures can be found here:\\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\\n\\n  Args:\\n    session: The TensorFlow session from which to save the meta graph and\\n        variables.\\n    export_dir: The path to which the SavedModel will be stored.\\n    inputs: dict mapping string input names to tensors. These are added\\n        to the SignatureDef as the inputs.\\n    outputs:  dict mapping string output names to tensors. These are added\\n        to the SignatureDef as the outputs.\\n    legacy_init_op: Legacy support for op or group of ops to execute after the\\n        restore op upon a load.\\n  '\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()",
            "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convenience function to build a SavedModel suitable for serving.\\n\\n  In many common cases, saving models for serving will be as simple as:\\n\\n      simple_save(session,\\n                  export_dir,\\n                  inputs={\"x\": x, \"y\": y},\\n                  outputs={\"z\": z})\\n\\n  Although in many cases it\\'s not necessary to understand all of the many ways\\n      to configure a SavedModel, this method has a few practical implications:\\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\\n      `saved_model.SERVING`)\\n    - The SavedModel will load in TensorFlow Serving and supports the\\n      [Predict\\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\\n      To use the Classify, Regress, or MultiInference APIs, please\\n      use either\\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\\n      or the lower level\\n      [SavedModel\\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\\n    - Some TensorFlow ops depend on information on disk or other information\\n      called \"assets\". These are generally handled automatically by adding the\\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\\n      collection are exported; if you need more custom behavior, you\\'ll need to\\n      use the\\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\\n\\n  More information about SavedModel and signatures can be found here:\\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\\n\\n  Args:\\n    session: The TensorFlow session from which to save the meta graph and\\n        variables.\\n    export_dir: The path to which the SavedModel will be stored.\\n    inputs: dict mapping string input names to tensors. These are added\\n        to the SignatureDef as the inputs.\\n    outputs:  dict mapping string output names to tensors. These are added\\n        to the SignatureDef as the outputs.\\n    legacy_init_op: Legacy support for op or group of ops to execute after the\\n        restore op upon a load.\\n  '\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()",
            "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convenience function to build a SavedModel suitable for serving.\\n\\n  In many common cases, saving models for serving will be as simple as:\\n\\n      simple_save(session,\\n                  export_dir,\\n                  inputs={\"x\": x, \"y\": y},\\n                  outputs={\"z\": z})\\n\\n  Although in many cases it\\'s not necessary to understand all of the many ways\\n      to configure a SavedModel, this method has a few practical implications:\\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\\n      `saved_model.SERVING`)\\n    - The SavedModel will load in TensorFlow Serving and supports the\\n      [Predict\\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\\n      To use the Classify, Regress, or MultiInference APIs, please\\n      use either\\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\\n      or the lower level\\n      [SavedModel\\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\\n    - Some TensorFlow ops depend on information on disk or other information\\n      called \"assets\". These are generally handled automatically by adding the\\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\\n      collection are exported; if you need more custom behavior, you\\'ll need to\\n      use the\\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\\n\\n  More information about SavedModel and signatures can be found here:\\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\\n\\n  Args:\\n    session: The TensorFlow session from which to save the meta graph and\\n        variables.\\n    export_dir: The path to which the SavedModel will be stored.\\n    inputs: dict mapping string input names to tensors. These are added\\n        to the SignatureDef as the inputs.\\n    outputs:  dict mapping string output names to tensors. These are added\\n        to the SignatureDef as the outputs.\\n    legacy_init_op: Legacy support for op or group of ops to execute after the\\n        restore op upon a load.\\n  '\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()",
            "@tf_export(v1=['saved_model.simple_save'])\n@deprecation.deprecated(None, 'This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.')\ndef simple_save(session, export_dir, inputs, outputs, legacy_init_op=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convenience function to build a SavedModel suitable for serving.\\n\\n  In many common cases, saving models for serving will be as simple as:\\n\\n      simple_save(session,\\n                  export_dir,\\n                  inputs={\"x\": x, \"y\": y},\\n                  outputs={\"z\": z})\\n\\n  Although in many cases it\\'s not necessary to understand all of the many ways\\n      to configure a SavedModel, this method has a few practical implications:\\n    - It will be treated as a graph for inference / serving (i.e. uses the tag\\n      `saved_model.SERVING`)\\n    - The SavedModel will load in TensorFlow Serving and supports the\\n      [Predict\\n      API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto).\\n      To use the Classify, Regress, or MultiInference APIs, please\\n      use either\\n      [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)\\n      or the lower level\\n      [SavedModel\\n      APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\\n    - Some TensorFlow ops depend on information on disk or other information\\n      called \"assets\". These are generally handled automatically by adding the\\n      assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that\\n      collection are exported; if you need more custom behavior, you\\'ll need to\\n      use the\\n      [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\\n\\n  More information about SavedModel and signatures can be found here:\\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\\n\\n  Args:\\n    session: The TensorFlow session from which to save the meta graph and\\n        variables.\\n    export_dir: The path to which the SavedModel will be stored.\\n    inputs: dict mapping string input names to tensors. These are added\\n        to the SignatureDef as the inputs.\\n    outputs:  dict mapping string output names to tensors. These are added\\n        to the SignatureDef as the outputs.\\n    legacy_init_op: Legacy support for op or group of ops to execute after the\\n        restore op upon a load.\\n  '\n    signature_def_map = {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def_utils.predict_signature_def(inputs, outputs)}\n    b = builder.SavedModelBuilder(export_dir)\n    b.add_meta_graph_and_variables(session, tags=[tag_constants.SERVING], signature_def_map=signature_def_map, assets_collection=ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS), main_op=legacy_init_op, clear_devices=True)\n    b.save()"
        ]
    }
]