[
    {
        "func_name": "_vol_fn",
        "original": "def _vol_fn(t, state):\n    \"\"\"Volatility function of LSV model.\"\"\"\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho",
        "mutated": [
            "def _vol_fn(t, state):\n    if False:\n        i = 10\n    'Volatility function of LSV model.'\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho",
            "def _vol_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Volatility function of LSV model.'\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho",
            "def _vol_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Volatility function of LSV model.'\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho",
            "def _vol_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Volatility function of LSV model.'\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho",
            "def _vol_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Volatility function of LSV model.'\n    num_samples = state.shape.as_list()[0]\n    broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n    spot_prices = state[:, 0]\n    variance = state[:, 1:]\n    level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n    spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n    variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n    diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n    diffusion = tf.expand_dims(diffusion, axis=-2)\n    return diffusion * self._sqrt_rho"
        ]
    },
    {
        "func_name": "_drift_fn",
        "original": "def _drift_fn(t, state):\n    \"\"\"Drift function of LSV model.\"\"\"\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)",
        "mutated": [
            "def _drift_fn(t, state):\n    if False:\n        i = 10\n    'Drift function of LSV model.'\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)",
            "def _drift_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Drift function of LSV model.'\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)",
            "def _drift_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Drift function of LSV model.'\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)",
            "def _drift_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Drift function of LSV model.'\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)",
            "def _drift_fn(t, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Drift function of LSV model.'\n    spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n    variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n    return tf.concat([spot_drift, variance_drift], axis=1)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    \"\"\"Initializes the Local stochastic volatility model.\n\n    Args:\n      leverage_fn: A Python callable which returns the Leverage function\n        `L(t, S(t))` as a function of state and time. The function must accept\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\n        `[num_samples, 1]` corresponding to the underlying price (S) as\n        inputs  and return a real `Tensor` containing the leverage function\n        computed at (S,t).\n      variance_process: An instance of `ItoProcess` specifying the\n        dynamics of the variance process of the LSV model. The\n        `variance_process` should implement a one-factor stochastic process.\n        For the common version of Heston like variance model use\n        `LSVVarianceModel`.\n      risk_free_rate: An optional scalar real `Tensor` specifying the\n        (continuously compounded) risk free interest rate. If the underlying is\n        an FX rate, then use this input to specify the domestic interest rate.\n        Note that the current implementation supports constant interest rates\n        and dividend yield.\n        Default value: `None` in which case the input is set to zero.\n      dividend_yield: An optional real scalar `Tensor` specifying the\n        (continuosly compounded) dividend yield. If the underlying is an FX\n        rate, then use this input to specify the foreign interest rate.\n        Note that the currect implementation supports constant interest rates\n        and dividend yield.\n        Default value: `None` in which case the input is set to zero.\n      rho: A real scalar `Tensor` specifying the correlation between the\n        underlying spot price and the variance process.\n        Default value: `None` in which case cross correlations are assumed\n        to be zero.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n        TensorFlow are used.\n      name: Python string. The name to give to the ops created by this class.\n        Default value: `None` which maps to the default name\n        `local_stochastic_volatility_model`.\n    \"\"\"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
        "mutated": [
            "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    if False:\n        i = 10\n    \"Initializes the Local stochastic volatility model.\\n\\n    Args:\\n      leverage_fn: A Python callable which returns the Leverage function\\n        `L(t, S(t))` as a function of state and time. The function must accept\\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\\n        `[num_samples, 1]` corresponding to the underlying price (S) as\\n        inputs  and return a real `Tensor` containing the leverage function\\n        computed at (S,t).\\n      variance_process: An instance of `ItoProcess` specifying the\\n        dynamics of the variance process of the LSV model. The\\n        `variance_process` should implement a one-factor stochastic process.\\n        For the common version of Heston like variance model use\\n        `LSVVarianceModel`.\\n      risk_free_rate: An optional scalar real `Tensor` specifying the\\n        (continuously compounded) risk free interest rate. If the underlying is\\n        an FX rate, then use this input to specify the domestic interest rate.\\n        Note that the current implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: An optional real scalar `Tensor` specifying the\\n        (continuosly compounded) dividend yield. If the underlying is an FX\\n        rate, then use this input to specify the foreign interest rate.\\n        Note that the currect implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      rho: A real scalar `Tensor` specifying the correlation between the\\n        underlying spot price and the variance process.\\n        Default value: `None` in which case cross correlations are assumed\\n        to be zero.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n        TensorFlow are used.\\n      name: Python string. The name to give to the ops created by this class.\\n        Default value: `None` which maps to the default name\\n        `local_stochastic_volatility_model`.\\n    \"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Initializes the Local stochastic volatility model.\\n\\n    Args:\\n      leverage_fn: A Python callable which returns the Leverage function\\n        `L(t, S(t))` as a function of state and time. The function must accept\\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\\n        `[num_samples, 1]` corresponding to the underlying price (S) as\\n        inputs  and return a real `Tensor` containing the leverage function\\n        computed at (S,t).\\n      variance_process: An instance of `ItoProcess` specifying the\\n        dynamics of the variance process of the LSV model. The\\n        `variance_process` should implement a one-factor stochastic process.\\n        For the common version of Heston like variance model use\\n        `LSVVarianceModel`.\\n      risk_free_rate: An optional scalar real `Tensor` specifying the\\n        (continuously compounded) risk free interest rate. If the underlying is\\n        an FX rate, then use this input to specify the domestic interest rate.\\n        Note that the current implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: An optional real scalar `Tensor` specifying the\\n        (continuosly compounded) dividend yield. If the underlying is an FX\\n        rate, then use this input to specify the foreign interest rate.\\n        Note that the currect implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      rho: A real scalar `Tensor` specifying the correlation between the\\n        underlying spot price and the variance process.\\n        Default value: `None` in which case cross correlations are assumed\\n        to be zero.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n        TensorFlow are used.\\n      name: Python string. The name to give to the ops created by this class.\\n        Default value: `None` which maps to the default name\\n        `local_stochastic_volatility_model`.\\n    \"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Initializes the Local stochastic volatility model.\\n\\n    Args:\\n      leverage_fn: A Python callable which returns the Leverage function\\n        `L(t, S(t))` as a function of state and time. The function must accept\\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\\n        `[num_samples, 1]` corresponding to the underlying price (S) as\\n        inputs  and return a real `Tensor` containing the leverage function\\n        computed at (S,t).\\n      variance_process: An instance of `ItoProcess` specifying the\\n        dynamics of the variance process of the LSV model. The\\n        `variance_process` should implement a one-factor stochastic process.\\n        For the common version of Heston like variance model use\\n        `LSVVarianceModel`.\\n      risk_free_rate: An optional scalar real `Tensor` specifying the\\n        (continuously compounded) risk free interest rate. If the underlying is\\n        an FX rate, then use this input to specify the domestic interest rate.\\n        Note that the current implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: An optional real scalar `Tensor` specifying the\\n        (continuosly compounded) dividend yield. If the underlying is an FX\\n        rate, then use this input to specify the foreign interest rate.\\n        Note that the currect implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      rho: A real scalar `Tensor` specifying the correlation between the\\n        underlying spot price and the variance process.\\n        Default value: `None` in which case cross correlations are assumed\\n        to be zero.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n        TensorFlow are used.\\n      name: Python string. The name to give to the ops created by this class.\\n        Default value: `None` which maps to the default name\\n        `local_stochastic_volatility_model`.\\n    \"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Initializes the Local stochastic volatility model.\\n\\n    Args:\\n      leverage_fn: A Python callable which returns the Leverage function\\n        `L(t, S(t))` as a function of state and time. The function must accept\\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\\n        `[num_samples, 1]` corresponding to the underlying price (S) as\\n        inputs  and return a real `Tensor` containing the leverage function\\n        computed at (S,t).\\n      variance_process: An instance of `ItoProcess` specifying the\\n        dynamics of the variance process of the LSV model. The\\n        `variance_process` should implement a one-factor stochastic process.\\n        For the common version of Heston like variance model use\\n        `LSVVarianceModel`.\\n      risk_free_rate: An optional scalar real `Tensor` specifying the\\n        (continuously compounded) risk free interest rate. If the underlying is\\n        an FX rate, then use this input to specify the domestic interest rate.\\n        Note that the current implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: An optional real scalar `Tensor` specifying the\\n        (continuosly compounded) dividend yield. If the underlying is an FX\\n        rate, then use this input to specify the foreign interest rate.\\n        Note that the currect implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      rho: A real scalar `Tensor` specifying the correlation between the\\n        underlying spot price and the variance process.\\n        Default value: `None` in which case cross correlations are assumed\\n        to be zero.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n        TensorFlow are used.\\n      name: Python string. The name to give to the ops created by this class.\\n        Default value: `None` which maps to the default name\\n        `local_stochastic_volatility_model`.\\n    \"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "def __init__(self, leverage_fn, variance_process, risk_free_rate=None, dividend_yield=None, rho=None, dtype=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Initializes the Local stochastic volatility model.\\n\\n    Args:\\n      leverage_fn: A Python callable which returns the Leverage function\\n        `L(t, S(t))` as a function of state and time. The function must accept\\n        a scalar `Tensor` corresponding to time 't' and a real `Tensor` of shape\\n        `[num_samples, 1]` corresponding to the underlying price (S) as\\n        inputs  and return a real `Tensor` containing the leverage function\\n        computed at (S,t).\\n      variance_process: An instance of `ItoProcess` specifying the\\n        dynamics of the variance process of the LSV model. The\\n        `variance_process` should implement a one-factor stochastic process.\\n        For the common version of Heston like variance model use\\n        `LSVVarianceModel`.\\n      risk_free_rate: An optional scalar real `Tensor` specifying the\\n        (continuously compounded) risk free interest rate. If the underlying is\\n        an FX rate, then use this input to specify the domestic interest rate.\\n        Note that the current implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: An optional real scalar `Tensor` specifying the\\n        (continuosly compounded) dividend yield. If the underlying is an FX\\n        rate, then use this input to specify the foreign interest rate.\\n        Note that the currect implementation supports constant interest rates\\n        and dividend yield.\\n        Default value: `None` in which case the input is set to zero.\\n      rho: A real scalar `Tensor` specifying the correlation between the\\n        underlying spot price and the variance process.\\n        Default value: `None` in which case cross correlations are assumed\\n        to be zero.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n        TensorFlow are used.\\n      name: Python string. The name to give to the ops created by this class.\\n        Default value: `None` which maps to the default name\\n        `local_stochastic_volatility_model`.\\n    \"\n    self._name = name or 'local_stochastic_volatility_model'\n    with tf.name_scope(self._name):\n        if risk_free_rate is None:\n            risk_free_rate = 0.0\n        if dividend_yield is None:\n            dividend_yield = 0.0\n        self._risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        self._dtype = dtype or self._domestic_rate.dtype\n        self._dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n        self._leverage_fn = leverage_fn\n        self._variance_process = variance_process\n        dim = 1 + variance_process.dim()\n        rho = rho or 0.0\n        self._rho = _create_corr_matrix(rho, self._dtype)\n        self._sqrt_rho = tf.linalg.cholesky(self._rho)\n\n        def _vol_fn(t, state):\n            \"\"\"Volatility function of LSV model.\"\"\"\n            num_samples = state.shape.as_list()[0]\n            broadcasted_t = tf.broadcast_to(t, [1, num_samples])\n            spot_prices = state[:, 0]\n            variance = state[:, 1:]\n            level_fun = self._leverage_fn(broadcasted_t, tf.expand_dims(spot_prices, axis=0))\n            spot_diffusion = tf.expand_dims(level_fun[0, :], axis=-1) * tf.expand_dims(spot_prices, axis=-1) * tf.math.sqrt(variance)\n            variance_diffusion = self._variance_process.volatility_fn()(t, variance)\n            diffusion = tf.concat([spot_diffusion, variance_diffusion], axis=1)\n            diffusion = tf.expand_dims(diffusion, axis=-2)\n            return diffusion * self._sqrt_rho\n\n        def _drift_fn(t, state):\n            \"\"\"Drift function of LSV model.\"\"\"\n            spot_drift = (self._risk_free_rate - self._dividend_yield) * state[:, :1]\n            variance_drift = self._variance_process.drift_fn()(t, state[:, 1:])\n            return tf.concat([spot_drift, variance_drift], axis=1)\n        super(LocalStochasticVolatilityModel, self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)"
        ]
    },
    {
        "func_name": "from_market_data",
        "original": "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    \"\"\"Creates a `LocalStochasticVolatilityModel` from market data.\n\n    This function computes the leverage function for the LSV model by first\n    computing the joint probability density function `p(t, X(t), v(t))` where\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\n    The joint probablity density is computed using the Fokker-Planck equation of\n    the LSV model (see 6.8.2 in Ref [1]):\n\n    ```None\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\n            d[a(v) p]/dv\n    ```\n\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\n    variance process. Defining\n\n    ```None\n    I_n(k,t) = int v^n p(t, k, v) dv\n    ```\n\n    we can calculate the leverage function as follows:\n    ```None\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\n    ```\n\n    Note that the computation of `I_0` and `I_1` require the knowledge of\n    leverage function and hence the computation of the leverage function is\n    implicit in nature.\n\n    Args:\n      valuation_date: A scalar `DateTensor` specifying the valuation\n        (or settlement) date for the market data.\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\n        expiry dates on which the implied volatilities are specified.\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\n        num_strikes)` specifying the strike prices at which implied volatilities\n        are specified.\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\n        num_strikes)` specifying the implied volatilities.\n      variance_process: An instance of `LSVVarianceModel` or\n        `ItoProcess` specifying the dynamics of the variance process of\n        the LSV model.\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\n        on the valuation date.\n      initial_variance: A real scalar `Tensor` specifying the initial variance\n        on the valuation date.\n      rho: A real scalar `Tensor` specifying the correlation between spot price\n        and the stochastic variance.\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\n        compounded) risk free interest rate. If the underlying is an FX rate,\n        then use this input to specify the domestic interest rate.\n        Default value: `None` in which case the input is set to zero.\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\n        compounded) divident yield. If the underlying is an FX rate, then use\n        this input to specify the foreign interest rate.\n        Default value: `None` in which case the input is set to zero.\n      time_step: A real scalar `Tensor` specifying the time step during the\n        numerical solution of the Fokker-Planck PDE.\n        Default value: None, in which case `time_step` corresponding to 100 time\n          steps is used.\n      num_grid_points: A scalar integer `Tensor` specifying the number of\n        discretization points for each spatial dimension.\n        Default value: None, in which case number of grid points is set to 100.\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\n        correspond to the minimum variance value.\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\n        correspond to the maximum variance value.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n\n    Returns:\n      An instance of `LocalStochasticVolatilityModel` constructed using the\n      input data.\n    \"\"\"\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
        "mutated": [
            "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n    'Creates a `LocalStochasticVolatilityModel` from market data.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probability density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Note that the computation of `I_0` and `I_1` require the knowledge of\\n    leverage function and hence the computation of the leverage function is\\n    implicit in nature.\\n\\n    Args:\\n      valuation_date: A scalar `DateTensor` specifying the valuation\\n        (or settlement) date for the market data.\\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\\n        expiry dates on which the implied volatilities are specified.\\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the strike prices at which implied volatilities\\n        are specified.\\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the implied volatilities.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess` specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: A real scalar `Tensor` specifying the time step during the\\n        numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a `LocalStochasticVolatilityModel` from market data.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probability density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Note that the computation of `I_0` and `I_1` require the knowledge of\\n    leverage function and hence the computation of the leverage function is\\n    implicit in nature.\\n\\n    Args:\\n      valuation_date: A scalar `DateTensor` specifying the valuation\\n        (or settlement) date for the market data.\\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\\n        expiry dates on which the implied volatilities are specified.\\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the strike prices at which implied volatilities\\n        are specified.\\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the implied volatilities.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess` specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: A real scalar `Tensor` specifying the time step during the\\n        numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a `LocalStochasticVolatilityModel` from market data.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probability density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Note that the computation of `I_0` and `I_1` require the knowledge of\\n    leverage function and hence the computation of the leverage function is\\n    implicit in nature.\\n\\n    Args:\\n      valuation_date: A scalar `DateTensor` specifying the valuation\\n        (or settlement) date for the market data.\\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\\n        expiry dates on which the implied volatilities are specified.\\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the strike prices at which implied volatilities\\n        are specified.\\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the implied volatilities.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess` specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: A real scalar `Tensor` specifying the time step during the\\n        numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a `LocalStochasticVolatilityModel` from market data.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probability density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Note that the computation of `I_0` and `I_1` require the knowledge of\\n    leverage function and hence the computation of the leverage function is\\n    implicit in nature.\\n\\n    Args:\\n      valuation_date: A scalar `DateTensor` specifying the valuation\\n        (or settlement) date for the market data.\\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\\n        expiry dates on which the implied volatilities are specified.\\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the strike prices at which implied volatilities\\n        are specified.\\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the implied volatilities.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess` specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: A real scalar `Tensor` specifying the time step during the\\n        numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_market_data(cls, valuation_date, expiry_dates, strikes, implied_volatilities, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a `LocalStochasticVolatilityModel` from market data.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probability density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Note that the computation of `I_0` and `I_1` require the knowledge of\\n    leverage function and hence the computation of the leverage function is\\n    implicit in nature.\\n\\n    Args:\\n      valuation_date: A scalar `DateTensor` specifying the valuation\\n        (or settlement) date for the market data.\\n      expiry_dates: A `DateTensor` of shape `(num_expiries,)` containing the\\n        expiry dates on which the implied volatilities are specified.\\n      strikes: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the strike prices at which implied volatilities\\n        are specified.\\n      implied_volatilities: A `Tensor` of real dtype and shape `(num_expiries,\\n        num_strikes)` specifying the implied volatilities.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess` specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: A real scalar `Tensor` specifying the time step during the\\n        numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_market_data(dim=1, valuation_date=valuation_date, expiry_dates=expiry_dates, strikes=strikes, implied_volatilities=implied_volatilities, spot=initial_spot, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    max_time = tf.math.reduce_max(dates.daycount_actual_365_fixed(start_date=valuation_date, end_date=expiry_dates, dtype=dtype))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)"
        ]
    },
    {
        "func_name": "from_volatility_surface",
        "original": "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    \"\"\"Creates a `LocalStochasticVolatilityModel` from volatility surface.\n\n    This function computes the leverage function for the LSV model by first\n    computing the joint probablity density function `p(t, X(t), v(t))` where\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\n    The joint probablity density is computed using the Fokker-Planck equation of\n    the LSV model (see 6.8.2 in Ref [1]):\n    ```None\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\n            d[a(v) p]/dv\n    ```\n\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\n    variance process. Defining\n\n    ```None\n    I_n(k,t) = int v^n p(t, k, v) dv\n    ```\n\n    we can calculate the leverage function as follows:\n    ```None\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\n    ```\n\n    Args:\n      implied_volatility_surface: Either an instance of\n        `processed_market_data.VolatilitySurface` or a Python object containing\n        the implied volatility market data. If the input is a Python object,\n        then the object must implement a function `volatility(strike,\n        expiry_times)` which takes real `Tensor`s corresponding to option\n        strikes and time to expiry and returns a real `Tensor` containing the\n        corresponding market implied volatility.\n      variance_process: An instance of `LSVVarianceModel` or\n        `ItoProcess`specifying the dynamics of the variance process of\n        the LSV model.\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\n        on the valuation date.\n      initial_variance: A real scalar `Tensor` specifying the initial variance\n        on the valuation date.\n      rho: A real scalar `Tensor` specifying the correlation between spot price\n        and the stochastic variance.\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\n        compounded) risk free interest rate. If the underlying is an FX rate,\n        then use this input to specify the domestic interest rate.\n        Default value: `None` in which case the input is set to zero.\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\n        compounded) divident yield. If the underlying is an FX rate, then use\n        this input to specify the foreign interest rate.\n        Default value: `None` in which case the input is set to zero.\n      time_step: An optional real scalar `Tensor` specifying the time step\n        during the numerical solution of the Fokker-Planck PDE.\n        Default value: None, in which case `time_step` corresponding to 100 time\n          steps is used.\n      num_grid_points: A scalar integer `Tensor` specifying the number of\n        discretization points for each spatial dimension.\n        Default value: None, in which case number of grid points is set to 100.\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\n        correspond to the minimum variance value.\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\n        correspond to the maximum variance value.\n      dtype: The default dtype to use when converting values to `Tensor`s.\n        Default value: `None` which means that default dtypes inferred by\n          TensorFlow are used.\n\n    Returns:\n      An instance of `LocalStochasticVolatilityModel` constructed using the\n      input data.\n    \"\"\"\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
        "mutated": [
            "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n    'Creates a `LocalStochasticVolatilityModel` from volatility surface.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probablity density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Args:\\n      implied_volatility_surface: Either an instance of\\n        `processed_market_data.VolatilitySurface` or a Python object containing\\n        the implied volatility market data. If the input is a Python object,\\n        then the object must implement a function `volatility(strike,\\n        expiry_times)` which takes real `Tensor`s corresponding to option\\n        strikes and time to expiry and returns a real `Tensor` containing the\\n        corresponding market implied volatility.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess`specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: An optional real scalar `Tensor` specifying the time step\\n        during the numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates a `LocalStochasticVolatilityModel` from volatility surface.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probablity density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Args:\\n      implied_volatility_surface: Either an instance of\\n        `processed_market_data.VolatilitySurface` or a Python object containing\\n        the implied volatility market data. If the input is a Python object,\\n        then the object must implement a function `volatility(strike,\\n        expiry_times)` which takes real `Tensor`s corresponding to option\\n        strikes and time to expiry and returns a real `Tensor` containing the\\n        corresponding market implied volatility.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess`specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: An optional real scalar `Tensor` specifying the time step\\n        during the numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates a `LocalStochasticVolatilityModel` from volatility surface.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probablity density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Args:\\n      implied_volatility_surface: Either an instance of\\n        `processed_market_data.VolatilitySurface` or a Python object containing\\n        the implied volatility market data. If the input is a Python object,\\n        then the object must implement a function `volatility(strike,\\n        expiry_times)` which takes real `Tensor`s corresponding to option\\n        strikes and time to expiry and returns a real `Tensor` containing the\\n        corresponding market implied volatility.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess`specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: An optional real scalar `Tensor` specifying the time step\\n        during the numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates a `LocalStochasticVolatilityModel` from volatility surface.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probablity density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Args:\\n      implied_volatility_surface: Either an instance of\\n        `processed_market_data.VolatilitySurface` or a Python object containing\\n        the implied volatility market data. If the input is a Python object,\\n        then the object must implement a function `volatility(strike,\\n        expiry_times)` which takes real `Tensor`s corresponding to option\\n        strikes and time to expiry and returns a real `Tensor` containing the\\n        corresponding market implied volatility.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess`specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: An optional real scalar `Tensor` specifying the time step\\n        during the numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)",
            "@classmethod\ndef from_volatility_surface(cls, implied_volatility_surface, variance_process, initial_spot, initial_variance, rho=None, risk_free_rate=None, dividend_yield=None, time_step=None, num_grid_points=None, grid_minimums=None, grid_maximums=None, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates a `LocalStochasticVolatilityModel` from volatility surface.\\n\\n    This function computes the leverage function for the LSV model by first\\n    computing the joint probablity density function `p(t, X(t), v(t))` where\\n    `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n    The joint probablity density is computed using the Fokker-Planck equation of\\n    the LSV model (see 6.8.2 in Ref [1]):\\n    ```None\\n    dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n            rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv -\\n            d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n            d[a(v) p]/dv\\n    ```\\n\\n    where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n    variance process. Defining\\n\\n    ```None\\n    I_n(k,t) = int v^n p(t, k, v) dv\\n    ```\\n\\n    we can calculate the leverage function as follows:\\n    ```None\\n    L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n    ```\\n\\n    Args:\\n      implied_volatility_surface: Either an instance of\\n        `processed_market_data.VolatilitySurface` or a Python object containing\\n        the implied volatility market data. If the input is a Python object,\\n        then the object must implement a function `volatility(strike,\\n        expiry_times)` which takes real `Tensor`s corresponding to option\\n        strikes and time to expiry and returns a real `Tensor` containing the\\n        corresponding market implied volatility.\\n      variance_process: An instance of `LSVVarianceModel` or\\n        `ItoProcess`specifying the dynamics of the variance process of\\n        the LSV model.\\n      initial_spot: A real scalar `Tensor` specifying the underlying spot price\\n        on the valuation date.\\n      initial_variance: A real scalar `Tensor` specifying the initial variance\\n        on the valuation date.\\n      rho: A real scalar `Tensor` specifying the correlation between spot price\\n        and the stochastic variance.\\n      risk_free_rate: A real scalar `Tensor` specifying the (continuosly\\n        compounded) risk free interest rate. If the underlying is an FX rate,\\n        then use this input to specify the domestic interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n        compounded) divident yield. If the underlying is an FX rate, then use\\n        this input to specify the foreign interest rate.\\n        Default value: `None` in which case the input is set to zero.\\n      time_step: An optional real scalar `Tensor` specifying the time step\\n        during the numerical solution of the Fokker-Planck PDE.\\n        Default value: None, in which case `time_step` corresponding to 100 time\\n          steps is used.\\n      num_grid_points: A scalar integer `Tensor` specifying the number of\\n        discretization points for each spatial dimension.\\n        Default value: None, in which case number of grid points is set to 100.\\n      grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n        points for PDE spatial discretization. `grid_minimums[0]` correspond\\n        to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n        correspond to the minimum variance value.\\n      grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n        points for PDE spatial discretization. `grid_maximums[0]` correspond\\n        to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n        correspond to the maximum variance value.\\n      dtype: The default dtype to use when converting values to `Tensor`s.\\n        Default value: `None` which means that default dtypes inferred by\\n          TensorFlow are used.\\n\\n    Returns:\\n      An instance of `LocalStochasticVolatilityModel` constructed using the\\n      input data.\\n    '\n    if risk_free_rate is None:\n        discount_factor_fn = lambda t: tf.ones_like(t, dtype=dtype)\n    else:\n        r = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n        discount_factor_fn = lambda t: tf.math.exp(-r * t)\n    lv_model = lvm.LocalVolatilityModel.from_volatility_surface(dim=1, spot=initial_spot, implied_volatility_surface=implied_volatility_surface, discount_factor_fn=discount_factor_fn, dividend_yield=dividend_yield, dtype=dtype)\n    dtype = dtype or lv_model.dtype()\n    day_count_fn = utils.get_daycount_fn(implied_volatility_surface.daycount_convention, dtype=dtype)\n    max_time = tf.math.reduce_max(day_count_fn(start_date=implied_volatility_surface.settlement_date(), end_date=implied_volatility_surface.node_expiries()))\n    if time_step is None:\n        time_step = max_time / 100.0\n    rho = rho or 0.0\n    num_grid_points = num_grid_points or 100\n    leverage_fn = _leverage_function_using_pde(risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, lv_model=lv_model, variance_model=variance_process, rho=[rho], initial_spot=initial_spot, initial_variance=initial_variance, time_step=time_step, max_time=max_time, num_grid_points=num_grid_points, grid_minimums=grid_minimums, grid_maximums=grid_maximums, dtype=dtype)\n    return LocalStochasticVolatilityModel(leverage_fn, variance_process, risk_free_rate=risk_free_rate, dividend_yield=dividend_yield, rho=rho, dtype=dtype)"
        ]
    },
    {
        "func_name": "_create_corr_matrix",
        "original": "def _create_corr_matrix(rho, dtype):\n    \"\"\"Create correlation matrix with scalar `rho`.\"\"\"\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])",
        "mutated": [
            "def _create_corr_matrix(rho, dtype):\n    if False:\n        i = 10\n    'Create correlation matrix with scalar `rho`.'\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])",
            "def _create_corr_matrix(rho, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create correlation matrix with scalar `rho`.'\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])",
            "def _create_corr_matrix(rho, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create correlation matrix with scalar `rho`.'\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])",
            "def _create_corr_matrix(rho, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create correlation matrix with scalar `rho`.'\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])",
            "def _create_corr_matrix(rho, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create correlation matrix with scalar `rho`.'\n    one = tf.constant(1.0, dtype=dtype)\n    m1 = tf.stack([one, rho], axis=0)\n    m2 = tf.stack([rho, one], axis=0)\n    return tf.stack([m1, m2])"
        ]
    },
    {
        "func_name": "_machine_eps",
        "original": "def _machine_eps(dtype):\n    \"\"\"Returns the machine epsilon for the supplied dtype.\"\"\"\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps",
        "mutated": [
            "def _machine_eps(dtype):\n    if False:\n        i = 10\n    'Returns the machine epsilon for the supplied dtype.'\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps",
            "def _machine_eps(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the machine epsilon for the supplied dtype.'\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps",
            "def _machine_eps(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the machine epsilon for the supplied dtype.'\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps",
            "def _machine_eps(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the machine epsilon for the supplied dtype.'\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps",
            "def _machine_eps(dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the machine epsilon for the supplied dtype.'\n    dtype = tf.as_dtype(dtype).as_numpy_dtype\n    eps = 1e-06 if dtype == np.float32 else 1e-10\n    return eps"
        ]
    },
    {
        "func_name": "_two_d_integration",
        "original": "def _two_d_integration(grid, value_grid):\n    \"\"\"Perform 2-D integration numerically.\"\"\"\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral",
        "mutated": [
            "def _two_d_integration(grid, value_grid):\n    if False:\n        i = 10\n    'Perform 2-D integration numerically.'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral",
            "def _two_d_integration(grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Perform 2-D integration numerically.'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral",
            "def _two_d_integration(grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Perform 2-D integration numerically.'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral",
            "def _two_d_integration(grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Perform 2-D integration numerically.'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral",
            "def _two_d_integration(grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Perform 2-D integration numerically.'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_v = variance_grid[1:, :] - variance_grid[:-1, :]\n    delta_s = log_spot_grid[:, 1:] - log_spot_grid[:, :-1]\n    integral = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_v, axis=0)\n    integral = tf.math.reduce_sum(integral[:-1] * delta_s[0, :])\n    return integral"
        ]
    },
    {
        "func_name": "_tavella_randell_nonuniform_grid",
        "original": "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    \"\"\"Creates non-uniform grid clustered around a specified point.\n\n  Args:\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\n      grid.\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\n      upper limit of the grid.\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\n      location on the grid around which higher grid density is desired.\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\n      on the grid.\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\n      grid. The smaller values of `alpha` correspond to greater degree of\n      clustering around `x_star`.\n    dtype: The default dtype to use when converting values to `Tensor`s.\n\n  Returns:\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\n    non-uniform grid.\n  \"\"\"\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)",
        "mutated": [
            "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    if False:\n        i = 10\n    'Creates non-uniform grid clustered around a specified point.\\n\\n  Args:\\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\\n      grid.\\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      upper limit of the grid.\\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      location on the grid around which higher grid density is desired.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\\n      on the grid.\\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\\n      grid. The smaller values of `alpha` correspond to greater degree of\\n      clustering around `x_star`.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\\n    non-uniform grid.\\n  '\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)",
            "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Creates non-uniform grid clustered around a specified point.\\n\\n  Args:\\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\\n      grid.\\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      upper limit of the grid.\\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      location on the grid around which higher grid density is desired.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\\n      on the grid.\\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\\n      grid. The smaller values of `alpha` correspond to greater degree of\\n      clustering around `x_star`.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\\n    non-uniform grid.\\n  '\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)",
            "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Creates non-uniform grid clustered around a specified point.\\n\\n  Args:\\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\\n      grid.\\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      upper limit of the grid.\\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      location on the grid around which higher grid density is desired.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\\n      on the grid.\\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\\n      grid. The smaller values of `alpha` correspond to greater degree of\\n      clustering around `x_star`.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\\n    non-uniform grid.\\n  '\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)",
            "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Creates non-uniform grid clustered around a specified point.\\n\\n  Args:\\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\\n      grid.\\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      upper limit of the grid.\\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      location on the grid around which higher grid density is desired.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\\n      on the grid.\\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\\n      grid. The smaller values of `alpha` correspond to greater degree of\\n      clustering around `x_star`.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\\n    non-uniform grid.\\n  '\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)",
            "def _tavella_randell_nonuniform_grid(x_min, x_max, x_star, num_grid_points, alpha, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Creates non-uniform grid clustered around a specified point.\\n\\n  Args:\\n    x_min: A real `Tensor` of shape `(dim,)` specifying the lower limit of the\\n      grid.\\n    x_max: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      upper limit of the grid.\\n    x_star: A real `Tensor` of same shape and dtype as `x_min` specifying the\\n      location on the grid around which higher grid density is desired.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of points\\n      on the grid.\\n    alpha: A scalar parameter which controls the degree of non-uniformity of the\\n      grid. The smaller values of `alpha` correspond to greater degree of\\n      clustering around `x_star`.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A real `Tensor` of shape `(dim, num_grid_points+1)` containing the\\n    non-uniform grid.\\n  '\n    c1 = tf.math.asinh((x_min - x_star) / alpha)\n    c2 = tf.math.asinh((x_max - x_star) / alpha)\n    i = tf.expand_dims(tf.range(0, num_grid_points + 1, 1, dtype=dtype), axis=-1)\n    grid = x_star + alpha * tf.math.sinh(c2 * i / num_grid_points + c1 * (1 - i / num_grid_points))\n    return tf.transpose(grid)"
        ]
    },
    {
        "func_name": "_conditional_expected_variance_from_pde_solution",
        "original": "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    \"\"\"Computes E[variance|log_spot=k].\"\"\"\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)",
        "mutated": [
            "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    if False:\n        i = 10\n    'Computes E[variance|log_spot=k].'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)",
            "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes E[variance|log_spot=k].'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)",
            "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes E[variance|log_spot=k].'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)",
            "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes E[variance|log_spot=k].'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)",
            "def _conditional_expected_variance_from_pde_solution(grid, value_grid, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes E[variance|log_spot=k].'\n    (log_spot_grid, variance_grid) = tf.meshgrid(*grid)\n    delta_s = variance_grid[1:, :] - variance_grid[:-1, :]\n    integral_0 = tf.math.reduce_sum(value_grid[0, :-1, :] * delta_s, axis=0)\n    integral_1 = tf.math.reduce_sum(variance_grid[:-1, :] * value_grid[0, :-1, :] * delta_s, axis=0)\n    variance_given_logspot = tf.math.divide_no_nan(integral_1, integral_0)\n    return functools.partial(linear.interpolate, x_data=log_spot_grid[0, :], y_data=variance_given_logspot, dtype=dtype)"
        ]
    },
    {
        "func_name": "_initial_value",
        "original": "def _initial_value():\n    \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)",
        "mutated": [
            "def _initial_value():\n    if False:\n        i = 10\n    'Computes initial value as a delta function delta(log_spot(t), var(0)).'\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)",
            "def _initial_value():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes initial value as a delta function delta(log_spot(t), var(0)).'\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)",
            "def _initial_value():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes initial value as a delta function delta(log_spot(t), var(0)).'\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)",
            "def _initial_value():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes initial value as a delta function delta(log_spot(t), var(0)).'\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)",
            "def _initial_value():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes initial value as a delta function delta(log_spot(t), var(0)).'\n    (log_spot, variance) = tf.meshgrid(*grid)\n    init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n    return tf.expand_dims(init_value, axis=0)"
        ]
    },
    {
        "func_name": "_second_order_coeff_fn",
        "original": "def _second_order_coeff_fn(t, grid):\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]",
        "mutated": [
            "def _second_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]",
            "def _second_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]",
            "def _second_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]",
            "def _second_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]",
            "def _second_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n    val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n    val_yx = val_xy\n    val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n    return [[-val_yy, -val_yx], [-val_xy, -val_xx]]"
        ]
    },
    {
        "func_name": "_first_order_coeff_fn",
        "original": "def _first_order_coeff_fn(t, grid):\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]",
        "mutated": [
            "def _first_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]",
            "def _first_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]",
            "def _first_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]",
            "def _first_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]",
            "def _first_order_coeff_fn(t, grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    log_spot = grid[0] + x_scale\n    variance = grid[1] * y_scale\n    leverage_fn_t_x = leverage_fn(log_spot)\n    val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n    val_y = variance_model.drift_fn()(t, variance)\n    return [val_y / y_scale, val_x]"
        ]
    },
    {
        "func_name": "_compute_leverage_fn",
        "original": "def _compute_leverage_fn(t, coord_grid, value_grid):\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn",
        "mutated": [
            "def _compute_leverage_fn(t, coord_grid, value_grid):\n    if False:\n        i = 10\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn",
            "def _compute_leverage_fn(t, coord_grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn",
            "def _compute_leverage_fn(t, coord_grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn",
            "def _compute_leverage_fn(t, coord_grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn",
            "def _compute_leverage_fn(t, coord_grid, value_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n    local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n    local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n    variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n    leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n    leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n    return leverage_fn"
        ]
    },
    {
        "func_name": "_trivial_neumann_boundary",
        "original": "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    del t, location_grid\n    return 0.0",
        "mutated": [
            "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    if False:\n        i = 10\n    del t, location_grid\n    return 0.0",
            "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    del t, location_grid\n    return 0.0",
            "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    del t, location_grid\n    return 0.0",
            "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    del t, location_grid\n    return 0.0",
            "@pde.boundary_conditions.neumann\ndef _trivial_neumann_boundary(t, location_grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    del t, location_grid\n    return 0.0"
        ]
    },
    {
        "func_name": "loop_body",
        "original": "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)",
        "mutated": [
            "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    if False:\n        i = 10\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)",
            "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)",
            "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)",
            "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)",
            "def loop_body(i, tstart, joint_density, leverage_fn_values):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n    joint_density = tf.math.maximum(joint_density, 0.0)\n    area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n    joint_density = joint_density / area_under_joint_density\n    leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n    leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n    leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n    return (i + 1, tstart + time_step, joint_density, leverage_fn_values)"
        ]
    },
    {
        "func_name": "_return_fn",
        "original": "def _return_fn(t, spot):\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))",
        "mutated": [
            "def _return_fn(t, spot):\n    if False:\n        i = 10\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))",
            "def _return_fn(t, spot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))",
            "def _return_fn(t, spot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))",
            "def _return_fn(t, spot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))",
            "def _return_fn(t, spot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n    return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))"
        ]
    },
    {
        "func_name": "_leverage_function_using_pde",
        "original": "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    \"\"\"Computes Leverage function using Fokker-Planck PDE for joint density.\n\n  This function computes the leverage function for the LSV model by first\n  computing the joint probablity density function `p(t, X(t), v(t))` where\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\n  The joint probablity density is computed using the Fokker-Planck equation of\n  the LSV model (see 6.8.2 in Ref [1]):\n  ```None\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\n          d[a(v) p]/dv\n  ```\n\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\n  variance process. Defining\n\n  ```None\n  I_n(k,t) = int v^n p(t, k, v) dv\n  ```\n\n  we can calculate the leverage function as follows:\n  ```None\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\n  ```\n\n  Args:\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\n      compounded) risk free interest rate. If the underlying is an FX rate, then\n      use this input to specify the domestic interest rate.\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\n      compounded) dividend yield. If the underlying is an FX rate, then use this\n      input to specify the foreign interest rate.\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\n      volatility for the spot price.\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\n      the variance process of the LSV model.\n    rho: A real scalar `Tensor` specifying the correlation between spot price\n      and the stochastic variance.\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\n      the valuation date.\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\n      the valuation date.\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\n      Fokker-Planck PDE is evolved.\n    time_step: A real scalar `Tensor` specifying the time step during the\n      numerical solution of the Fokker-Planck PDE.\n    num_grid_points: A scalar integer `Tensor` specifying the number of\n      discretization points for each spatial dimension.\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\n      correspond to the minimum variance value.\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\n      correspond to the maximum variance value.\n    dtype: The default dtype to use when converting values to `Tensor`s.\n\n  Returns:\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\n    inputs  and return a real `Tensor` corresponding to the leverage function\n    computed at (S,t).\n\n  \"\"\"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn",
        "mutated": [
            "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    if False:\n        i = 10\n    \"Computes Leverage function using Fokker-Planck PDE for joint density.\\n\\n  This function computes the leverage function for the LSV model by first\\n  computing the joint probablity density function `p(t, X(t), v(t))` where\\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n  The joint probablity density is computed using the Fokker-Planck equation of\\n  the LSV model (see 6.8.2 in Ref [1]):\\n  ```None\\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n          d[a(v) p]/dv\\n  ```\\n\\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n  variance process. Defining\\n\\n  ```None\\n  I_n(k,t) = int v^n p(t, k, v) dv\\n  ```\\n\\n  we can calculate the leverage function as follows:\\n  ```None\\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n  ```\\n\\n  Args:\\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\\n      compounded) risk free interest rate. If the underlying is an FX rate, then\\n      use this input to specify the domestic interest rate.\\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n      compounded) dividend yield. If the underlying is an FX rate, then use this\\n      input to specify the foreign interest rate.\\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\\n      volatility for the spot price.\\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\\n      the variance process of the LSV model.\\n    rho: A real scalar `Tensor` specifying the correlation between spot price\\n      and the stochastic variance.\\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\\n      the valuation date.\\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\\n      the valuation date.\\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\\n      Fokker-Planck PDE is evolved.\\n    time_step: A real scalar `Tensor` specifying the time step during the\\n      numerical solution of the Fokker-Planck PDE.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of\\n      discretization points for each spatial dimension.\\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n      correspond to the minimum variance value.\\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n      correspond to the maximum variance value.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\\n    inputs  and return a real `Tensor` corresponding to the leverage function\\n    computed at (S,t).\\n\\n  \"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn",
            "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes Leverage function using Fokker-Planck PDE for joint density.\\n\\n  This function computes the leverage function for the LSV model by first\\n  computing the joint probablity density function `p(t, X(t), v(t))` where\\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n  The joint probablity density is computed using the Fokker-Planck equation of\\n  the LSV model (see 6.8.2 in Ref [1]):\\n  ```None\\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n          d[a(v) p]/dv\\n  ```\\n\\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n  variance process. Defining\\n\\n  ```None\\n  I_n(k,t) = int v^n p(t, k, v) dv\\n  ```\\n\\n  we can calculate the leverage function as follows:\\n  ```None\\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n  ```\\n\\n  Args:\\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\\n      compounded) risk free interest rate. If the underlying is an FX rate, then\\n      use this input to specify the domestic interest rate.\\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n      compounded) dividend yield. If the underlying is an FX rate, then use this\\n      input to specify the foreign interest rate.\\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\\n      volatility for the spot price.\\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\\n      the variance process of the LSV model.\\n    rho: A real scalar `Tensor` specifying the correlation between spot price\\n      and the stochastic variance.\\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\\n      the valuation date.\\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\\n      the valuation date.\\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\\n      Fokker-Planck PDE is evolved.\\n    time_step: A real scalar `Tensor` specifying the time step during the\\n      numerical solution of the Fokker-Planck PDE.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of\\n      discretization points for each spatial dimension.\\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n      correspond to the minimum variance value.\\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n      correspond to the maximum variance value.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\\n    inputs  and return a real `Tensor` corresponding to the leverage function\\n    computed at (S,t).\\n\\n  \"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn",
            "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes Leverage function using Fokker-Planck PDE for joint density.\\n\\n  This function computes the leverage function for the LSV model by first\\n  computing the joint probablity density function `p(t, X(t), v(t))` where\\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n  The joint probablity density is computed using the Fokker-Planck equation of\\n  the LSV model (see 6.8.2 in Ref [1]):\\n  ```None\\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n          d[a(v) p]/dv\\n  ```\\n\\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n  variance process. Defining\\n\\n  ```None\\n  I_n(k,t) = int v^n p(t, k, v) dv\\n  ```\\n\\n  we can calculate the leverage function as follows:\\n  ```None\\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n  ```\\n\\n  Args:\\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\\n      compounded) risk free interest rate. If the underlying is an FX rate, then\\n      use this input to specify the domestic interest rate.\\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n      compounded) dividend yield. If the underlying is an FX rate, then use this\\n      input to specify the foreign interest rate.\\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\\n      volatility for the spot price.\\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\\n      the variance process of the LSV model.\\n    rho: A real scalar `Tensor` specifying the correlation between spot price\\n      and the stochastic variance.\\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\\n      the valuation date.\\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\\n      the valuation date.\\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\\n      Fokker-Planck PDE is evolved.\\n    time_step: A real scalar `Tensor` specifying the time step during the\\n      numerical solution of the Fokker-Planck PDE.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of\\n      discretization points for each spatial dimension.\\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n      correspond to the minimum variance value.\\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n      correspond to the maximum variance value.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\\n    inputs  and return a real `Tensor` corresponding to the leverage function\\n    computed at (S,t).\\n\\n  \"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn",
            "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes Leverage function using Fokker-Planck PDE for joint density.\\n\\n  This function computes the leverage function for the LSV model by first\\n  computing the joint probablity density function `p(t, X(t), v(t))` where\\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n  The joint probablity density is computed using the Fokker-Planck equation of\\n  the LSV model (see 6.8.2 in Ref [1]):\\n  ```None\\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n          d[a(v) p]/dv\\n  ```\\n\\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n  variance process. Defining\\n\\n  ```None\\n  I_n(k,t) = int v^n p(t, k, v) dv\\n  ```\\n\\n  we can calculate the leverage function as follows:\\n  ```None\\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n  ```\\n\\n  Args:\\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\\n      compounded) risk free interest rate. If the underlying is an FX rate, then\\n      use this input to specify the domestic interest rate.\\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n      compounded) dividend yield. If the underlying is an FX rate, then use this\\n      input to specify the foreign interest rate.\\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\\n      volatility for the spot price.\\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\\n      the variance process of the LSV model.\\n    rho: A real scalar `Tensor` specifying the correlation between spot price\\n      and the stochastic variance.\\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\\n      the valuation date.\\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\\n      the valuation date.\\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\\n      Fokker-Planck PDE is evolved.\\n    time_step: A real scalar `Tensor` specifying the time step during the\\n      numerical solution of the Fokker-Planck PDE.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of\\n      discretization points for each spatial dimension.\\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n      correspond to the minimum variance value.\\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n      correspond to the maximum variance value.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\\n    inputs  and return a real `Tensor` corresponding to the leverage function\\n    computed at (S,t).\\n\\n  \"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn",
            "def _leverage_function_using_pde(*, risk_free_rate, dividend_yield, lv_model, variance_model, rho, initial_spot, initial_variance, max_time, time_step, num_grid_points, grid_minimums, grid_maximums, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes Leverage function using Fokker-Planck PDE for joint density.\\n\\n  This function computes the leverage function for the LSV model by first\\n  computing the joint probablity density function `p(t, X(t), v(t))` where\\n  `X(t)` is the log of the spot price and `v(t)` is the variance at time `t`.\\n  The joint probablity density is computed using the Fokker-Planck equation of\\n  the LSV model (see 6.8.2 in Ref [1]):\\n  ```None\\n  dp/dt = 1/2 d^2 [v L(t,X)^2 p]/dX^2 + 1/2 d^2 [b(v)^2 p]/dv^2 +\\n          rho d^2 [sqrt(v)L(t,X)b(v) p]/dXdv - d[(r - d - 1/2 v L(t,X)^2)p]/dX -\\n          d[a(v) p]/dv\\n  ```\\n\\n  where `a(v)` and `b(v)` are the drift and diffusion functions for the\\n  variance process. Defining\\n\\n  ```None\\n  I_n(k,t) = int v^n p(t, k, v) dv\\n  ```\\n\\n  we can calculate the leverage function as follows:\\n  ```None\\n  L(k, t) = sigma(exp(k), t) sqrt(I_0(k, t)/I_1(k, t)).\\n  ```\\n\\n  Args:\\n    risk_free_rate: A scalar real `Tensor` specifying the (continuosly\\n      compounded) risk free interest rate. If the underlying is an FX rate, then\\n      use this input to specify the domestic interest rate.\\n    dividend_yield: A real scalar `Tensor` specifying the (continuosly\\n      compounded) dividend yield. If the underlying is an FX rate, then use this\\n      input to specify the foreign interest rate.\\n    lv_model: An instance of `LocalVolatilityModel` specifying the local\\n      volatility for the spot price.\\n    variance_model: An instance of `LSVVarianceModel` specifying the dynamics of\\n      the variance process of the LSV model.\\n    rho: A real scalar `Tensor` specifying the correlation between spot price\\n      and the stochastic variance.\\n    initial_spot: A real scalar `Tensor` specifying the underlying spot price on\\n      the valuation date.\\n    initial_variance: A real scalar `Tensor` specifying the initial variance on\\n      the valuation date.\\n    max_time: A real scalar `Tensor` specifying the maximum time to which the\\n      Fokker-Planck PDE is evolved.\\n    time_step: A real scalar `Tensor` specifying the time step during the\\n      numerical solution of the Fokker-Planck PDE.\\n    num_grid_points: A scalar integer `Tensor` specifying the number of\\n      discretization points for each spatial dimension.\\n    grid_minimums: An optional `Tensor` of size 2 containing the minimum grid\\n      points for PDE spatial discretization. `grid_minimums[0]` correspond\\n      to the minimum spot price in the spatial grid and `grid_minimums[1]`\\n      correspond to the minimum variance value.\\n    grid_maximums: An optional `Tensor` of size 2 containing the maximum grid\\n      points for PDE spatial discretization. `grid_maximums[0]` correspond\\n      to the maximum spot price in the spatial grid and `grid_maximums[1]`\\n      correspond to the maximum variance value.\\n    dtype: The default dtype to use when converting values to `Tensor`s.\\n\\n  Returns:\\n    A Python callable which computes the Leverage function `L(t, S(t))`. The\\n    function accepts a scalar `Tensor` corresponding to time 't' and a real\\n    `Tensor` of shape `[num_samples, 1]` corresponding to the spot price (S) as\\n    inputs  and return a real `Tensor` corresponding to the leverage function\\n    computed at (S,t).\\n\\n  \"\n    if variance_model.dim() > 1:\n        raise ValueError(\"The default model of Leverage function doesn't support the variance process with more than 1 factor.\")\n    pde_grid_tol = _machine_eps(dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    initial_spot = tf.convert_to_tensor(initial_spot, dtype=dtype)\n    initial_log_spot = tf.math.log(tf.convert_to_tensor(initial_spot, dtype=dtype))\n    initial_variance = tf.convert_to_tensor(initial_variance, dtype=dtype)\n    risk_free_rate = tf.convert_to_tensor(risk_free_rate, dtype=dtype)\n    dividend_yield = tf.convert_to_tensor(dividend_yield, dtype=dtype)\n    rho = tf.convert_to_tensor(rho, dtype=dtype)\n    x_scale = initial_log_spot\n    y_scale = initial_variance\n    scaled_initial_point = tf.convert_to_tensor([0.0, 1.0], dtype=dtype)\n    if grid_minimums is None:\n        grid_minimums = [0.01, 0.0001]\n    else:\n        grid_minimums = tf.convert_to_tensor(grid_minimums, dtype=dtype)\n        grid_minimums = [grid_minimums[0] / initial_spot, grid_minimums[1] / initial_variance]\n    if grid_maximums is None:\n        grid_maximums = [10.0, 5.0]\n    else:\n        grid_maximums = tf.convert_to_tensor(grid_maximums, dtype=dtype)\n        grid_maximums = [grid_maximums[0] / initial_spot, grid_maximums[1] / initial_variance]\n    log_spot_min = tf.math.log(tf.convert_to_tensor([grid_minimums[0]], dtype=dtype))\n    log_spot_max = tf.math.log(tf.convert_to_tensor([grid_maximums[0]], dtype=dtype))\n    variance_min = tf.convert_to_tensor([grid_minimums[1]], dtype=dtype)\n    variance_max = tf.convert_to_tensor([grid_maximums[1]], dtype=dtype)\n    grid_minimums = tf.concat([log_spot_min, variance_min], axis=0)\n    grid_maximums = tf.concat([log_spot_max, variance_max], axis=0)\n    grid = _tavella_randell_nonuniform_grid(grid_minimums, grid_maximums, scaled_initial_point, num_grid_points, 0.3, dtype)\n    grid = [tf.expand_dims(grid[0], axis=0), tf.expand_dims(grid[1], axis=0)]\n    delta_x = tf.math.reduce_min(grid[0][0, 1:] - grid[0][0, :-1])\n    delta_y = tf.math.reduce_min(grid[1][0, 1:] - grid[1][0, :-1])\n    leverage_fn = functools.partial(linear.interpolate, x_data=[[0.0, 1.0]], y_data=[[1.0, 1.0]], dtype=dtype)\n\n    def _initial_value():\n        \"\"\"Computes initial value as a delta function delta(log_spot(t), var(0)).\"\"\"\n        (log_spot, variance) = tf.meshgrid(*grid)\n        init_value = tf.where(tf.math.logical_and(tf.math.abs(log_spot - scaled_initial_point[0]) < delta_x + pde_grid_tol, tf.math.abs(variance - scaled_initial_point[1]) < delta_y + pde_grid_tol), 1.0 / (delta_x * delta_y * 4), 0.0)\n        return tf.expand_dims(init_value, axis=0)\n\n    def _second_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_xx = 0.5 * variance * leverage_fn_t_x ** 2\n        val_xy = 0.5 * (rho * tf.math.sqrt(variance) * leverage_fn_t_x * variance_model.volatility_fn()(t, variance)) / y_scale\n        val_yx = val_xy\n        val_yy = 0.5 * variance_model.volatility_fn()(t, variance) ** 2 / y_scale ** 2\n        return [[-val_yy, -val_yx], [-val_xy, -val_xx]]\n\n    def _first_order_coeff_fn(t, grid):\n        log_spot = grid[0] + x_scale\n        variance = grid[1] * y_scale\n        leverage_fn_t_x = leverage_fn(log_spot)\n        val_x = risk_free_rate - dividend_yield - 0.5 * variance * leverage_fn_t_x ** 2\n        val_y = variance_model.drift_fn()(t, variance)\n        return [val_y / y_scale, val_x]\n\n    def _compute_leverage_fn(t, coord_grid, value_grid):\n        log_spot = tf.expand_dims(coord_grid[0], axis=-1) + x_scale\n        local_volatility_values = lv_model.local_volatility_fn()(t, tf.math.exp(log_spot))\n        local_volatility_values = tf.where(tf.math.abs(local_volatility_values) > 10000.0, tf.constant(0.0, dtype=dtype), local_volatility_values)\n        variance_given_logspot = _conditional_expected_variance_from_pde_solution([coord_grid[0] + x_scale, coord_grid[1] * y_scale], value_grid, dtype)(log_spot)\n        leverage_fn_values = tf.math.divide_no_nan(local_volatility_values, tf.math.sqrt(variance_given_logspot))\n        leverage_fn = functools.partial(linear.interpolate, x_data=grid[0] + x_scale, y_data=tf.transpose(leverage_fn_values), dtype=dtype)\n        return leverage_fn\n\n    @pde.boundary_conditions.neumann\n    def _trivial_neumann_boundary(t, location_grid):\n        del t, location_grid\n        return 0.0\n    joint_density = _initial_value()\n\n    def loop_body(i, tstart, joint_density, leverage_fn_values):\n        (joint_density, coord_grid, _, _) = pde.fd_solvers.solve_forward(tstart, tstart + time_step, coord_grid=[grid[0][0], grid[1][0]], values_grid=joint_density, time_step=time_step / 10.0, values_transform_fn=None, inner_second_order_coeff_fn=_second_order_coeff_fn, inner_first_order_coeff_fn=_first_order_coeff_fn, zeroth_order_coeff_fn=None, boundary_conditions=[[_trivial_neumann_boundary, _trivial_neumann_boundary], [_trivial_neumann_boundary, _trivial_neumann_boundary]], dtype=dtype)\n        joint_density = tf.math.maximum(joint_density, 0.0)\n        area_under_joint_density = _two_d_integration([grid[0][0, :], grid[1][0, :]], joint_density)\n        joint_density = joint_density / area_under_joint_density\n        leverage_fn = _compute_leverage_fn(tstart + time_step, coord_grid, joint_density)\n        leverage_v = leverage_fn(grid[0][0, :] + x_scale)[0, :]\n        leverage_fn_values = leverage_fn_values.write(i, leverage_v)\n        return (i + 1, tstart + time_step, joint_density, leverage_fn_values)\n    times = tf.range(0.0, max_time + time_step, time_step, dtype=dtype)\n    tstart = times[0]\n    first_leverage_value = leverage_fn(grid[0][0])[0]\n    leverage_fn_values = tf.TensorArray(dtype=dtype, size=tff_utils.get_shape(times)[0], element_shape=tff_utils.get_shape(first_leverage_value), clear_after_read=False)\n    leverage_fn_values.write(0, first_leverage_value)\n    loop_cond = lambda i, tstart, *args: tf.less(tstart, max_time)\n    initial_args = (1, tstart, joint_density, leverage_fn_values)\n    (_, _, _, leverage_fn_values) = tf.while_loop(loop_cond, loop_body, initial_args)\n    leverage_fn_values = leverage_fn_values.stack()\n    leverage_fn_values = tf.convert_to_tensor(leverage_fn_values, dtype=dtype)\n\n    def _return_fn(t, spot):\n        leverage_fn_interpolator = math.interpolation.interpolation_2d.Interpolation2D(x_data=[times], y_data=tf.expand_dims(tf.repeat(grid[0] + x_scale, times.shape[0], axis=0), axis=0), z_data=tf.expand_dims(leverage_fn_values, axis=0), dtype=dtype)\n        return leverage_fn_interpolator.interpolate(t, tf.math.log(spot))\n    return _return_fn"
        ]
    }
]