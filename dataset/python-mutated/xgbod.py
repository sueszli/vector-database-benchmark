"""XGBOD: Improving Supervised Outlier Detection with Unsupervised
Representation Learning. A semi-supervised outlier detection framework.
"""
from __future__ import division
from __future__ import print_function
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.utils import check_array
from sklearn.utils.validation import check_X_y
from sklearn.utils.validation import check_is_fitted
try:
    import xgboost
except ImportError:
    print('please install xgboost first for running XGBOD')
from xgboost.sklearn import XGBClassifier
from .base import BaseDetector
from .knn import KNN
from .lof import LOF
from .iforest import IForest
from .hbos import HBOS
from .ocsvm import OCSVM
from ..utils.utility import check_parameter
from ..utils.utility import check_detector
from ..utils.utility import standardizer
from ..utils.utility import precision_n_scores

class XGBOD(BaseDetector):
    """XGBOD class for outlier detection.
    It first uses the passed in unsupervised outlier detectors to extract
    richer representation of the data and then concatenates the newly
    generated features to the original feature for constructing the augmented
    feature space. An XGBoost classifier is then applied on this augmented
    feature space. Read more in the :cite:`zhao2018xgbod`.

    Parameters
    ----------
    estimator_list : list, optional (default=None)
        The list of pyod detectors passed in for unsupervised learning

    standardization_flag_list : list, optional (default=None)
        The list of boolean flags for indicating whether to perform
        standardization for each detector.

    max_depth : int
        Maximum tree depth for base learners.

    learning_rate : float
        Boosting learning rate (xgb's "eta")

    n_estimators : int
        Number of boosted trees to fit.

    silent : bool
        Whether to print messages while running boosting.

    objective : string or callable
        Specify the learning task and the corresponding learning objective or
        a custom objective function to be used (see note below).

    booster : string
        Specify which booster to use: gbtree, gblinear or dart.

    n_jobs : int
        Number of parallel threads used to run xgboost.  (replaces ``nthread``)

    gamma : float
        Minimum loss reduction required to make a further partition on a leaf
        node of the tree.

    min_child_weight : int
        Minimum sum of instance weight(hessian) needed in a child.

    max_delta_step : int
        Maximum delta step we allow each tree's weight estimation to be.

    subsample : float
        Subsample ratio of the training instance.

    colsample_bytree : float
        Subsample ratio of columns when constructing each tree.

    colsample_bylevel : float
        Subsample ratio of columns for each split, in each level.

    reg_alpha : float (xgb's alpha)
        L1 regularization term on weights.

    reg_lambda : float (xgb's lambda)
        L2 regularization term on weights.

    scale_pos_weight : float
        Balancing of positive and negative weights.

    base_score:
        The initial prediction score of all instances, global bias.

    random_state : int
        Random number seed.  (replaces seed)

    # missing : float, optional
    #     Value in the data which needs to be present as a missing value. If
    #     None, defaults to np.nan.

    importance_type: string, default "gain"
        The feature importance type for the ``feature_importances_``
        property: either "gain",
        "weight", "cover", "total_gain" or "total_cover".

    \\*\\*kwargs : dict, optional
        Keyword arguments for XGBoost Booster object.  Full documentation of
        parameters can be found here:
        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.
        Attempting to set a parameter via the constructor args and \\*\\*kwargs
        dict simultaneously will result in a TypeError.

        Note: \\*\\*kwargs is unsupported by scikit-learn. We do not
        guarantee that parameters passed via this argument will interact
        properly with scikit-learn.

    Attributes
    ----------
    n_detector_ : int
        The number of unsupervised of detectors used.

    clf_ : object
        The XGBoost classifier.

    decision_scores_ : numpy array of shape (n_samples,)
        The outlier scores of the training data.
        The higher, the more abnormal. Outliers tend to have higher
        scores. This value is available once the detector is fitted.

    labels_ : int, either 0 or 1
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers/anomalies. It is generated by applying
        ``threshold_`` on ``decision_scores_``.

    """

    def __init__(self, estimator_list=None, standardization_flag_list=None, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, **kwargs):
        if False:
            print('Hello World!')
        super(XGBOD, self).__init__()
        self.estimator_list = estimator_list
        self.standardization_flag_list = standardization_flag_list
        self.max_depth = max_depth
        self.learning_rate = learning_rate
        self.n_estimators = n_estimators
        self.silent = silent
        self.objective = objective
        self.booster = booster
        self.n_jobs = n_jobs
        self.nthread = nthread
        self.gamma = gamma
        self.min_child_weight = min_child_weight
        self.max_delta_step = max_delta_step
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.colsample_bylevel = colsample_bylevel
        self.reg_alpha = reg_alpha
        self.reg_lambda = reg_lambda
        self.scale_pos_weight = scale_pos_weight
        self.base_score = base_score
        self.random_state = random_state
        self.kwargs = kwargs

    def _init_detectors(self, X):
        if False:
            i = 10
            return i + 15
        'initialize unsupervised detectors if no predefined detectors is\n        provided.\n\n        Parameters\n        ----------\n        X : numpy array of shape (n_samples, n_features)\n            The train data\n\n        Returns\n        -------\n        estimator_list : list of object\n            The initialized list of detectors\n\n        standardization_flag_list : list of boolean\n            The list of bool flag to indicate whether standardization is needed\n\n        '
        estimator_list = []
        standardization_flag_list = []
        k_range = [1, 3, 5, 10, 20, 30, 40, 50]
        k_range = [k for k in k_range if k < X.shape[0]]
        for k in k_range:
            estimator_list.append(KNN(n_neighbors=k, method='largest'))
            estimator_list.append(LOF(n_neighbors=k))
            standardization_flag_list.append(True)
            standardization_flag_list.append(True)
        n_bins_range = [5, 10, 15, 20, 25, 30, 50]
        for n_bins in n_bins_range:
            estimator_list.append(HBOS(n_bins=n_bins))
            standardization_flag_list.append(False)
        nu_range = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]
        for nu in nu_range:
            estimator_list.append(OCSVM(nu=nu))
            standardization_flag_list.append(True)
        n_range = [10, 20, 50, 70, 100, 150, 200]
        for n in n_range:
            estimator_list.append(IForest(n_estimators=n, random_state=self.random_state))
            standardization_flag_list.append(False)
        return (estimator_list, standardization_flag_list)

    def _validate_estimator(self, X):
        if False:
            for i in range(10):
                print('nop')
        if self.estimator_list is None:
            (self.estimator_list, self.standardization_flag_list) = self._init_detectors(X)
        if self.standardization_flag_list is None:
            self.standardization_flag_list = [True] * len(self.estimator_list)
        if len(self.estimator_list) != len(self.standardization_flag_list):
            raise ValueError('estimator_list length ({0}) is not equal to standardization_flag_list length ({1})'.format(len(self.estimator_list), len(self.standardization_flag_list)))
        check_parameter(len(self.estimator_list), low=1, param_name='number of estimators', include_left=True, include_right=True)
        for estimator in self.estimator_list:
            check_detector(estimator)
        return len(self.estimator_list)

    def _generate_new_features(self, X):
        if False:
            i = 10
            return i + 15
        X_add = np.zeros([X.shape[0], self.n_detector_])
        X_norm = self._scalar.transform(X)
        for (ind, estimator) in enumerate(self.estimator_list):
            if self.standardization_flag_list[ind]:
                X_add[:, ind] = estimator.decision_function(X_norm)
            else:
                X_add[:, ind] = estimator.decision_function(X)
        return X_add

    def fit(self, X, y):
        if False:
            i = 10
            return i + 15
        'Fit the model using X and y as training data.\n\n        Parameters\n        ----------\n        X : numpy array of shape (n_samples, n_features)\n            Training data.\n\n        y : numpy array of shape (n_samples,)\n            The ground truth (binary label)\n\n            - 0 : inliers\n            - 1 : outliers\n\n        Returns\n        -------\n        self : object\n        '
        (X, y) = check_X_y(X, y)
        X = check_array(X)
        self._set_n_classes(y)
        self.n_detector_ = self._validate_estimator(X)
        self.X_train_add_ = np.zeros([X.shape[0], self.n_detector_])
        (X_norm, self._scalar) = standardizer(X, keep_scalar=True)
        for (ind, estimator) in enumerate(self.estimator_list):
            if self.standardization_flag_list[ind]:
                estimator.fit(X_norm)
                self.X_train_add_[:, ind] = estimator.decision_scores_
            else:
                estimator.fit(X)
                self.X_train_add_[:, ind] = estimator.decision_scores_
        self.X_train_new_ = np.concatenate((X, self.X_train_add_), axis=1)
        self.clf_ = clf = XGBClassifier(max_depth=self.max_depth, learning_rate=self.learning_rate, n_estimators=self.n_estimators, silent=self.silent, objective=self.objective, booster=self.booster, n_jobs=self.n_jobs, nthread=self.nthread, gamma=self.gamma, min_child_weight=self.min_child_weight, max_delta_step=self.max_delta_step, subsample=self.subsample, colsample_bytree=self.colsample_bytree, colsample_bylevel=self.colsample_bylevel, reg_alpha=self.reg_alpha, reg_lambda=self.reg_lambda, scale_pos_weight=self.scale_pos_weight, base_score=self.base_score, random_state=self.random_state, **self.kwargs)
        self.clf_.fit(self.X_train_new_, y)
        self.decision_scores_ = self.clf_.predict_proba(self.X_train_new_)[:, 1]
        self.labels_ = self.clf_.predict(self.X_train_new_).ravel()
        return self

    def decision_function(self, X):
        if False:
            while True:
                i = 10
        check_is_fitted(self, ['clf_', 'decision_scores_', 'labels_', '_scalar'])
        X = check_array(X)
        X_add = self._generate_new_features(X)
        X_new = np.concatenate((X, X_add), axis=1)
        pred_scores = self.clf_.predict_proba(X_new)[:, 1]
        return pred_scores.ravel()

    def predict(self, X):
        if False:
            print('Hello World!')
        'Predict if a particular sample is an outlier or not.\n        Calling xgboost `predict` function.\n\n        Parameters\n        ----------\n        X : numpy array of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        outlier_labels : numpy array of shape (n_samples,)\n            For each observation, tells whether or not\n            it should be considered as an outlier according to the\n            fitted model. 0 stands for inliers and 1 for outliers.\n        '
        check_is_fitted(self, ['clf_', 'decision_scores_', 'labels_', '_scalar'])
        X = check_array(X)
        X_add = self._generate_new_features(X)
        X_new = np.concatenate((X, X_add), axis=1)
        pred_scores = self.clf_.predict(X_new)
        return pred_scores.ravel()

    def predict_proba(self, X):
        if False:
            while True:
                i = 10
        'Predict the probability of a sample being outlier.\n        Calling xgboost `predict_proba` function.\n\n        Parameters\n        ----------\n        X : numpy array of shape (n_samples, n_features)\n            The input samples.\n\n\n        Returns\n        -------\n        outlier_labels : numpy array of shape (n_samples,)\n            For each observation, tells whether or not\n            it should be considered as an outlier according to the\n            fitted model. Return the outlier probability, ranging\n            in [0,1].\n        '
        return self.decision_function(X)

    def fit_predict(self, X, y):
        if False:
            i = 10
            return i + 15
        self.fit(X, y)
        return self.labels_

    def fit_predict_score(self, X, y, scoring='roc_auc_score'):
        if False:
            print('Hello World!')
        "Fit the detector, predict on samples, and evaluate the model by\n        predefined metrics, e.g., ROC.\n\n        Parameters\n        ----------\n        X : numpy array of shape (n_samples, n_features)\n            The input samples.\n\n        y : Ignored\n            Not used, present for API consistency by convention.\n\n        scoring : str, optional (default='roc_auc_score')\n            Evaluation metric:\n\n            - 'roc_auc_score': ROC score\n            - 'prc_n_score': Precision @ rank n score\n\n        Returns\n        -------\n        score : float\n        "
        self.fit(X, y)
        if scoring == 'roc_auc_score':
            score = roc_auc_score(y, self.decision_scores_)
        elif scoring == 'prc_n_score':
            score = precision_n_scores(y, self.decision_scores_)
        else:
            raise NotImplementedError('PyOD built-in scoring only supports ROC and Precision @ rank n')
        print('{metric}: {score}'.format(metric=scoring, score=score))
        return score