[
    {
        "func_name": "ftest",
        "original": "def ftest(y_target, *y_model_predictions):\n    \"\"\"\n    F-Test test to compare 2 or more models.\n\n    Parameters\n    -----------\n    y_target : array-like, shape=[n_samples]\n        True class labels as 1D NumPy array.\n\n    *y_model_predictions : array-likes, shape=[n_samples]\n        Variable number of 2 or more arrays that\n        contain the predicted class labels\n        from models as 1D NumPy array.\n\n    Returns\n    -----------\n\n    f, p : float or None, float\n        Returns the F-value and the p-value\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\n\n    \"\"\"\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)",
        "mutated": [
            "def ftest(y_target, *y_model_predictions):\n    if False:\n        i = 10\n    '\\n    F-Test test to compare 2 or more models.\\n\\n    Parameters\\n    -----------\\n    y_target : array-like, shape=[n_samples]\\n        True class labels as 1D NumPy array.\\n\\n    *y_model_predictions : array-likes, shape=[n_samples]\\n        Variable number of 2 or more arrays that\\n        contain the predicted class labels\\n        from models as 1D NumPy array.\\n\\n    Returns\\n    -----------\\n\\n    f, p : float or None, float\\n        Returns the F-value and the p-value\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\\n\\n    '\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)",
            "def ftest(y_target, *y_model_predictions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    F-Test test to compare 2 or more models.\\n\\n    Parameters\\n    -----------\\n    y_target : array-like, shape=[n_samples]\\n        True class labels as 1D NumPy array.\\n\\n    *y_model_predictions : array-likes, shape=[n_samples]\\n        Variable number of 2 or more arrays that\\n        contain the predicted class labels\\n        from models as 1D NumPy array.\\n\\n    Returns\\n    -----------\\n\\n    f, p : float or None, float\\n        Returns the F-value and the p-value\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\\n\\n    '\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)",
            "def ftest(y_target, *y_model_predictions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    F-Test test to compare 2 or more models.\\n\\n    Parameters\\n    -----------\\n    y_target : array-like, shape=[n_samples]\\n        True class labels as 1D NumPy array.\\n\\n    *y_model_predictions : array-likes, shape=[n_samples]\\n        Variable number of 2 or more arrays that\\n        contain the predicted class labels\\n        from models as 1D NumPy array.\\n\\n    Returns\\n    -----------\\n\\n    f, p : float or None, float\\n        Returns the F-value and the p-value\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\\n\\n    '\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)",
            "def ftest(y_target, *y_model_predictions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    F-Test test to compare 2 or more models.\\n\\n    Parameters\\n    -----------\\n    y_target : array-like, shape=[n_samples]\\n        True class labels as 1D NumPy array.\\n\\n    *y_model_predictions : array-likes, shape=[n_samples]\\n        Variable number of 2 or more arrays that\\n        contain the predicted class labels\\n        from models as 1D NumPy array.\\n\\n    Returns\\n    -----------\\n\\n    f, p : float or None, float\\n        Returns the F-value and the p-value\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\\n\\n    '\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)",
            "def ftest(y_target, *y_model_predictions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    F-Test test to compare 2 or more models.\\n\\n    Parameters\\n    -----------\\n    y_target : array-like, shape=[n_samples]\\n        True class labels as 1D NumPy array.\\n\\n    *y_model_predictions : array-likes, shape=[n_samples]\\n        Variable number of 2 or more arrays that\\n        contain the predicted class labels\\n        from models as 1D NumPy array.\\n\\n    Returns\\n    -----------\\n\\n    f, p : float or None, float\\n        Returns the F-value and the p-value\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/\\n\\n    '\n    num_models = len(y_model_predictions)\n    model_lens = set()\n    y_model_predictions = list(y_model_predictions)\n    for ary in [y_target] + y_model_predictions:\n        if len(ary.shape) != 1:\n            raise ValueError('One or more input arrays are not 1-dimensional.')\n        model_lens.add(ary.shape[0])\n    if len(model_lens) > 1:\n        raise ValueError('Each prediction array must have the same number of samples.')\n    if num_models < 2:\n        raise ValueError('Provide at least 2 model prediction arrays.')\n    num_examples = len(y_target)\n    accuracies = []\n    correctly_classified_all_models = 0\n    correctly_classified_collection = []\n    for pred in y_model_predictions:\n        correctly_classified = (y_target == pred).sum()\n        acc = correctly_classified / num_examples\n        accuracies.append(acc)\n        correctly_classified_all_models += correctly_classified\n        correctly_classified_collection.append(correctly_classified)\n    avg_acc = sum(accuracies) / len(accuracies)\n    ssa = num_examples * sum([acc ** 2 for acc in accuracies]) - num_examples * num_models * avg_acc ** 2\n    binary_combin = list(itertools.product([0, 1], repeat=num_models))\n    ary = np.hstack([(y_target == mod).reshape(-1, 1) for mod in y_model_predictions]).astype(int)\n    correctly_classified_objects = 0\n    binary_combin_totals = np.zeros(len(binary_combin))\n    for (i, c) in enumerate(binary_combin):\n        binary_combin_totals[i] = ((ary == c).sum(axis=1) == num_models).sum()\n        correctly_classified_objects += sum(c) ** 2 * binary_combin_totals[i]\n    ssb = 1.0 / num_models * correctly_classified_objects - num_examples * num_models * avg_acc ** 2\n    sst = num_examples * num_models * avg_acc * (1 - avg_acc)\n    ssab = sst - ssa - ssb\n    mean_ssa = ssa / (num_models - 1)\n    mean_ssab = ssab / ((num_models - 1) * (num_examples - 1))\n    f = mean_ssa / mean_ssab\n    degrees_of_freedom_1 = num_models - 1\n    degrees_of_freedom_2 = degrees_of_freedom_1 * num_examples\n    p_value = scipy.stats.f.sf(f, degrees_of_freedom_1, degrees_of_freedom_2)\n    return (f, p_value)"
        ]
    },
    {
        "func_name": "score_diff",
        "original": "def score_diff(X_1, X_2, y_1, y_2):\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
        "mutated": [
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff",
            "def score_diff(X_1, X_2, y_1, y_2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    estimator1.fit(X_1, y_1)\n    estimator2.fit(X_1, y_1)\n    est1_score = scorer(estimator1, X_2, y_2)\n    est2_score = scorer(estimator2, X_2, y_2)\n    score_diff = est1_score - est2_score\n    return score_diff"
        ]
    },
    {
        "func_name": "combined_ftest_5x2cv",
        "original": "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    \"\"\"\n    Implements the 5x2cv combined F test proposed\n    by Alpaydin 1999,\n    to compare the performance of two models.\n\n    Parameters\n    ----------\n    estimator1 : scikit-learn classifier or regressor\n\n    estimator2 : scikit-learn classifier or regressor\n\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n        Training vectors, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape = [n_samples]\n        Target values.\n\n    scoring : str, callable, or None (default: None)\n        If None (default), uses 'accuracy' for sklearn classifiers\n        and 'r2' for sklearn regressors.\n        If str, uses a sklearn scoring metric string identifier, for example\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n        'median_absolute_error', 'r2'} for regressors.\n        If a callable object or function is provided, it has to be conform with\n        sklearn's signature ``scorer(estimator, X, y)``; see\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n        for more information.\n\n    random_seed : int or None (default: None)\n        Random seed for creating the test/train splits.\n\n    Returns\n    ----------\n    f : float\n        The F-statistic\n\n    pvalue : float\n        Two-tailed p-value.\n        If the chosen significance level is larger\n        than the p-value, we reject the null hypothesis\n        and accept that there are significant differences\n        in the two compared models.\n\n    Examples\n    -----------\n    For usage examples, please see\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\n\n    \"\"\"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))",
        "mutated": [
            "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n    \"\\n    Implements the 5x2cv combined F test proposed\\n    by Alpaydin 1999,\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    f : float\\n        The F-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))",
            "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Implements the 5x2cv combined F test proposed\\n    by Alpaydin 1999,\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    f : float\\n        The F-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))",
            "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Implements the 5x2cv combined F test proposed\\n    by Alpaydin 1999,\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    f : float\\n        The F-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))",
            "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Implements the 5x2cv combined F test proposed\\n    by Alpaydin 1999,\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    f : float\\n        The F-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))",
            "def combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Implements the 5x2cv combined F test proposed\\n    by Alpaydin 1999,\\n    to compare the performance of two models.\\n\\n    Parameters\\n    ----------\\n    estimator1 : scikit-learn classifier or regressor\\n\\n    estimator2 : scikit-learn classifier or regressor\\n\\n    X : {array-like, sparse matrix}, shape = [n_samples, n_features]\\n        Training vectors, where n_samples is the number of samples and\\n        n_features is the number of features.\\n\\n    y : array-like, shape = [n_samples]\\n        Target values.\\n\\n    scoring : str, callable, or None (default: None)\\n        If None (default), uses 'accuracy' for sklearn classifiers\\n        and 'r2' for sklearn regressors.\\n        If str, uses a sklearn scoring metric string identifier, for example\\n        {accuracy, f1, precision, recall, roc_auc} for classifiers,\\n        {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\\n        'median_absolute_error', 'r2'} for regressors.\\n        If a callable object or function is provided, it has to be conform with\\n        sklearn's signature ``scorer(estimator, X, y)``; see\\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\\n        for more information.\\n\\n    random_seed : int or None (default: None)\\n        Random seed for creating the test/train splits.\\n\\n    Returns\\n    ----------\\n    f : float\\n        The F-statistic\\n\\n    pvalue : float\\n        Two-tailed p-value.\\n        If the chosen significance level is larger\\n        than the p-value, we reject the null hypothesis\\n        and accept that there are significant differences\\n        in the two compared models.\\n\\n    Examples\\n    -----------\\n    For usage examples, please see\\n    https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/\\n\\n    \"\n    rng = np.random.RandomState(random_seed)\n    if scoring is None:\n        if estimator1._estimator_type == 'classifier':\n            scoring = 'accuracy'\n        elif estimator1._estimator_type == 'regressor':\n            scoring = 'r2'\n        else:\n            raise AttributeError('Estimator must be a Classifier or Regressor.')\n    if isinstance(scoring, str):\n        scorer = get_scorer(scoring)\n    else:\n        scorer = scoring\n    variances = []\n    differences = []\n\n    def score_diff(X_1, X_2, y_1, y_2):\n        estimator1.fit(X_1, y_1)\n        estimator2.fit(X_1, y_1)\n        est1_score = scorer(estimator1, X_2, y_2)\n        est2_score = scorer(estimator2, X_2, y_2)\n        score_diff = est1_score - est2_score\n        return score_diff\n    for i in range(5):\n        randint = rng.randint(low=0, high=32767)\n        (X_1, X_2, y_1, y_2) = train_test_split(X, y, test_size=0.5, random_state=randint)\n        score_diff_1 = score_diff(X_1, X_2, y_1, y_2)\n        score_diff_2 = score_diff(X_2, X_1, y_2, y_1)\n        score_mean = (score_diff_1 + score_diff_2) / 2.0\n        score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n        differences.extend([score_diff_1 ** 2, score_diff_2 ** 2])\n        variances.append(score_var)\n    numerator = sum(differences)\n    denominator = 2 * sum(variances)\n    f_stat = numerator / denominator\n    p_value = scipy.stats.f.sf(f_stat, 10, 5)\n    return (float(f_stat), float(p_value))"
        ]
    }
]