[
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Forward and backward function of learn mode.\n        Arguments:\n            - data (:obj:`dict`): Dict type data\n        Returns:\n            - info_dict (:obj:`Dict[str, Any]`):\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\n        \"\"\"\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos",
        "mutated": [
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\\n        '\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\\n        '\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\\n        '\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\\n        '\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_max, adv_mean, value_max, value_mean, approx_kl, clipfrac\\n        '\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=False)\n    if self._cuda:\n        data = to_device(data, self._device)\n    return_infos = []\n    self._learn_model.train()\n    for epoch in range(self._cfg.learn.epoch_per_collect):\n        if self._recompute_adv:\n            with torch.no_grad():\n                value = self._learn_model.forward(data['obs'], mode='compute_critic')['value']\n                next_value = self._learn_model.forward(data['next_obs'], mode='compute_critic')['value']\n                if self._value_norm:\n                    value *= self._running_mean_std.std\n                    next_value *= self._running_mean_std.std\n                compute_adv_data = gae_data(value, next_value, data['reward'], data['done'], data['traj_flag'])\n                data['adv'] = gae(compute_adv_data, self._gamma, self._gae_lambda)\n                unnormalized_returns = value + data['adv']\n                if self._value_norm:\n                    data['value'] = value / self._running_mean_std.std\n                    data['return'] = unnormalized_returns / self._running_mean_std.std\n                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())\n                else:\n                    data['value'] = value\n                    data['return'] = unnormalized_returns\n        elif self._value_norm:\n            unnormalized_return = data['adv'] + data['value'] * self._running_mean_std.std\n            data['return'] = unnormalized_return / self._running_mean_std.std\n            self._running_mean_std.update(unnormalized_return.cpu().numpy())\n        else:\n            data['return'] = data['adv'] + data['value']\n        for batch in split_data_generator(data, self._cfg.learn.batch_size, shuffle=True):\n            output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')\n            adv = batch['adv']\n            if self._adv_norm:\n                adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n            loss_list = []\n            info_list = []\n            action_num = len(batch['action'])\n            for i in range(action_num):\n                ppo_batch = ppo_data(output['logit'][i], batch['logit'][i], batch['action'][i], output['value'], batch['value'], adv, batch['return'], batch['weight'])\n                (ppo_loss, ppo_info) = ppo_error(ppo_batch, self._clip_ratio)\n                loss_list.append(ppo_loss)\n                info_list.append(ppo_info)\n            avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n            avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n            avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n            avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n            avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n            (wv, we) = (self._value_weight, self._entropy_weight)\n            total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n            self._optimizer.zero_grad()\n            total_loss.backward()\n            self._optimizer.step()\n            return_info = {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss.item(), 'value_loss': avg_value_loss.item(), 'entropy_loss': avg_entropy_loss.item(), 'adv_max': adv.max().item(), 'adv_mean': adv.mean().item(), 'value_mean': output['value'].mean().item(), 'value_max': output['value'].max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}\n            return_infos.append(return_info)\n    return return_infos"
        ]
    },
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Forward and backward function of learn mode.\n        Arguments:\n            - data (:obj:`dict`): Dict type data\n        Returns:\n            - info_dict (:obj:`Dict[str, Any]`):\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\n                        adv_abs_max, approx_kl, clipfrac\n        \"\"\"\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}",
        "mutated": [
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_abs_max, approx_kl, clipfrac\\n        '\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_abs_max, approx_kl, clipfrac\\n        '\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_abs_max, approx_kl, clipfrac\\n        '\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_abs_max, approx_kl, clipfrac\\n        '\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`dict`): Dict type data\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`):\\n              Including current lr, total_loss, policy_loss, value_loss, entropy_loss, \\\\\\n                        adv_abs_max, approx_kl, clipfrac\\n        '\n    assert not self._nstep_return\n    data = default_preprocess_learn(data, ignore_done=self._cfg.learn.ignore_done, use_nstep=self._nstep_return)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    output = self._learn_model.forward(data['obs'], mode='compute_actor_critic')\n    adv = data['adv']\n    return_ = data['value'] + adv\n    if self._adv_norm:\n        adv = (adv - adv.mean()) / (adv.std() + 1e-08)\n    loss_list = []\n    info_list = []\n    action_num = len(data['action'])\n    for i in range(action_num):\n        ppodata = ppo_data(output['logit'][i], data['logit'][i], data['action'][i], output['value'], data['value'], adv, return_, data['weight'])\n        (ppo_loss, ppo_info) = ppo_error(ppodata, self._clip_ratio)\n        loss_list.append(ppo_loss)\n        info_list.append(ppo_info)\n    avg_policy_loss = sum([item.policy_loss for item in loss_list]) / action_num\n    avg_value_loss = sum([item.value_loss for item in loss_list]) / action_num\n    avg_entropy_loss = sum([item.entropy_loss for item in loss_list]) / action_num\n    avg_approx_kl = sum([item.approx_kl for item in info_list]) / action_num\n    avg_clipfrac = sum([item.clipfrac for item in info_list]) / action_num\n    (wv, we) = (self._value_weight, self._entropy_weight)\n    total_loss = avg_policy_loss + wv * avg_value_loss - we * avg_entropy_loss\n    self._optimizer.zero_grad()\n    total_loss.backward()\n    self._optimizer.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': total_loss.item(), 'policy_loss': avg_policy_loss, 'value_loss': avg_value_loss, 'entropy_loss': avg_entropy_loss, 'adv_abs_max': adv.abs().max().item(), 'approx_kl': avg_approx_kl, 'clipfrac': avg_clipfrac}"
        ]
    }
]