[
    {
        "func_name": "__init__",
        "original": "def __init__(self, _j_statement_set, t_env):\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env",
        "mutated": [
            "def __init__(self, _j_statement_set, t_env):\n    if False:\n        i = 10\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env",
            "def __init__(self, _j_statement_set, t_env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env",
            "def __init__(self, _j_statement_set, t_env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env",
            "def __init__(self, _j_statement_set, t_env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env",
            "def __init__(self, _j_statement_set, t_env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._j_statement_set = _j_statement_set\n    self._t_env = t_env"
        ]
    },
    {
        "func_name": "add_insert_sql",
        "original": "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    \"\"\"\n        add insert statement to the set.\n\n        :param stmt: The statement to be added.\n        :return: current StatementSet instance.\n\n        .. versionadded:: 1.11.0\n        \"\"\"\n    self._j_statement_set.addInsertSql(stmt)\n    return self",
        "mutated": [
            "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    if False:\n        i = 10\n    '\\n        add insert statement to the set.\\n\\n        :param stmt: The statement to be added.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._j_statement_set.addInsertSql(stmt)\n    return self",
            "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        add insert statement to the set.\\n\\n        :param stmt: The statement to be added.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._j_statement_set.addInsertSql(stmt)\n    return self",
            "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        add insert statement to the set.\\n\\n        :param stmt: The statement to be added.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._j_statement_set.addInsertSql(stmt)\n    return self",
            "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        add insert statement to the set.\\n\\n        :param stmt: The statement to be added.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._j_statement_set.addInsertSql(stmt)\n    return self",
            "def add_insert_sql(self, stmt: str) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        add insert statement to the set.\\n\\n        :param stmt: The statement to be added.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._j_statement_set.addInsertSql(stmt)\n    return self"
        ]
    },
    {
        "func_name": "attach_as_datastream",
        "original": "def attach_as_datastream(self):\n    \"\"\"\n        Optimizes all statements as one entity and adds them as transformations to the underlying\n        StreamExecutionEnvironment.\n\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\n\n        The added statements will be cleared after calling this method.\n\n        .. versionadded:: 1.16.0\n        \"\"\"\n    self._j_statement_set.attachAsDataStream()",
        "mutated": [
            "def attach_as_datastream(self):\n    if False:\n        i = 10\n    '\\n        Optimizes all statements as one entity and adds them as transformations to the underlying\\n        StreamExecutionEnvironment.\\n\\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\\n\\n        The added statements will be cleared after calling this method.\\n\\n        .. versionadded:: 1.16.0\\n        '\n    self._j_statement_set.attachAsDataStream()",
            "def attach_as_datastream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Optimizes all statements as one entity and adds them as transformations to the underlying\\n        StreamExecutionEnvironment.\\n\\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\\n\\n        The added statements will be cleared after calling this method.\\n\\n        .. versionadded:: 1.16.0\\n        '\n    self._j_statement_set.attachAsDataStream()",
            "def attach_as_datastream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Optimizes all statements as one entity and adds them as transformations to the underlying\\n        StreamExecutionEnvironment.\\n\\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\\n\\n        The added statements will be cleared after calling this method.\\n\\n        .. versionadded:: 1.16.0\\n        '\n    self._j_statement_set.attachAsDataStream()",
            "def attach_as_datastream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Optimizes all statements as one entity and adds them as transformations to the underlying\\n        StreamExecutionEnvironment.\\n\\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\\n\\n        The added statements will be cleared after calling this method.\\n\\n        .. versionadded:: 1.16.0\\n        '\n    self._j_statement_set.attachAsDataStream()",
            "def attach_as_datastream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Optimizes all statements as one entity and adds them as transformations to the underlying\\n        StreamExecutionEnvironment.\\n\\n        Use :func:`~pyflink.datastream.StreamExecutionEnvironment.execute` to execute them.\\n\\n        The added statements will be cleared after calling this method.\\n\\n        .. versionadded:: 1.16.0\\n        '\n    self._j_statement_set.attachAsDataStream()"
        ]
    },
    {
        "func_name": "add_insert",
        "original": "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    \"\"\"\n        Adds a statement that the pipeline defined by the given Table object should be written to a\n        table (backed by a DynamicTableSink) that was registered under the specified path or\n        expressed via the given TableDescriptor.\n\n        1. When target_path_or_descriptor is a tale path:\n\n            See the documentation of :func:`~TableEnvironment.use_database` or\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\n\n        2. When target_path_or_descriptor is a table descriptor:\n\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\n            table (see :func:`~TableEnvironment.create_temporary_table`).\n\n            Then a statement is added to the statement set that inserts the Table object's pipeline\n            into that temporary table.\n\n            This method allows to declare a Schema for the sink descriptor. The declaration is\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\n\n                1. overwrite automatically derived columns with a custom DataType\n                2. add metadata columns next to the physical columns\n                3. declare a primary key\n\n            It is possible to declare a schema without physical/regular columns. In this case, those\n            columns will be automatically derived and implicitly put at the beginning of the schema\n            declaration.\n\n            Examples:\n            ::\n\n                >>> stmt_set = table_env.create_statement_set()\n                >>> source_table = table_env.from_path(\"SourceTable\")\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\n                ...     .schema(Schema.new_builder()\n                ...         .build()) \\\\\n                ...     .build()\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\n\n            .. note:: add_insert for a table descriptor (case 2.) was added from\n                flink 1.14.0.\n\n        :param target_path_or_descriptor: The path of the registered\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\n        :param table: The Table to add.\n        :type table: pyflink.table.Table\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\n        :return: current StatementSet instance.\n\n        .. versionadded:: 1.11.0\n        \"\"\"\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self",
        "mutated": [
            "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    if False:\n        i = 10\n    '\\n        Adds a statement that the pipeline defined by the given Table object should be written to a\\n        table (backed by a DynamicTableSink) that was registered under the specified path or\\n        expressed via the given TableDescriptor.\\n\\n        1. When target_path_or_descriptor is a tale path:\\n\\n            See the documentation of :func:`~TableEnvironment.use_database` or\\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\\n\\n        2. When target_path_or_descriptor is a table descriptor:\\n\\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\\n            table (see :func:`~TableEnvironment.create_temporary_table`).\\n\\n            Then a statement is added to the statement set that inserts the Table object\\'s pipeline\\n            into that temporary table.\\n\\n            This method allows to declare a Schema for the sink descriptor. The declaration is\\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\\n\\n                1. overwrite automatically derived columns with a custom DataType\\n                2. add metadata columns next to the physical columns\\n                3. declare a primary key\\n\\n            It is possible to declare a schema without physical/regular columns. In this case, those\\n            columns will be automatically derived and implicitly put at the beginning of the schema\\n            declaration.\\n\\n            Examples:\\n            ::\\n\\n                >>> stmt_set = table_env.create_statement_set()\\n                >>> source_table = table_env.from_path(\"SourceTable\")\\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\\n                ...     .schema(Schema.new_builder()\\n                ...         .build()) \\\\\\n                ...     .build()\\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\\n\\n            .. note:: add_insert for a table descriptor (case 2.) was added from\\n                flink 1.14.0.\\n\\n        :param target_path_or_descriptor: The path of the registered\\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\\n        :param table: The Table to add.\\n        :type table: pyflink.table.Table\\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self",
            "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Adds a statement that the pipeline defined by the given Table object should be written to a\\n        table (backed by a DynamicTableSink) that was registered under the specified path or\\n        expressed via the given TableDescriptor.\\n\\n        1. When target_path_or_descriptor is a tale path:\\n\\n            See the documentation of :func:`~TableEnvironment.use_database` or\\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\\n\\n        2. When target_path_or_descriptor is a table descriptor:\\n\\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\\n            table (see :func:`~TableEnvironment.create_temporary_table`).\\n\\n            Then a statement is added to the statement set that inserts the Table object\\'s pipeline\\n            into that temporary table.\\n\\n            This method allows to declare a Schema for the sink descriptor. The declaration is\\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\\n\\n                1. overwrite automatically derived columns with a custom DataType\\n                2. add metadata columns next to the physical columns\\n                3. declare a primary key\\n\\n            It is possible to declare a schema without physical/regular columns. In this case, those\\n            columns will be automatically derived and implicitly put at the beginning of the schema\\n            declaration.\\n\\n            Examples:\\n            ::\\n\\n                >>> stmt_set = table_env.create_statement_set()\\n                >>> source_table = table_env.from_path(\"SourceTable\")\\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\\n                ...     .schema(Schema.new_builder()\\n                ...         .build()) \\\\\\n                ...     .build()\\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\\n\\n            .. note:: add_insert for a table descriptor (case 2.) was added from\\n                flink 1.14.0.\\n\\n        :param target_path_or_descriptor: The path of the registered\\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\\n        :param table: The Table to add.\\n        :type table: pyflink.table.Table\\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self",
            "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Adds a statement that the pipeline defined by the given Table object should be written to a\\n        table (backed by a DynamicTableSink) that was registered under the specified path or\\n        expressed via the given TableDescriptor.\\n\\n        1. When target_path_or_descriptor is a tale path:\\n\\n            See the documentation of :func:`~TableEnvironment.use_database` or\\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\\n\\n        2. When target_path_or_descriptor is a table descriptor:\\n\\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\\n            table (see :func:`~TableEnvironment.create_temporary_table`).\\n\\n            Then a statement is added to the statement set that inserts the Table object\\'s pipeline\\n            into that temporary table.\\n\\n            This method allows to declare a Schema for the sink descriptor. The declaration is\\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\\n\\n                1. overwrite automatically derived columns with a custom DataType\\n                2. add metadata columns next to the physical columns\\n                3. declare a primary key\\n\\n            It is possible to declare a schema without physical/regular columns. In this case, those\\n            columns will be automatically derived and implicitly put at the beginning of the schema\\n            declaration.\\n\\n            Examples:\\n            ::\\n\\n                >>> stmt_set = table_env.create_statement_set()\\n                >>> source_table = table_env.from_path(\"SourceTable\")\\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\\n                ...     .schema(Schema.new_builder()\\n                ...         .build()) \\\\\\n                ...     .build()\\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\\n\\n            .. note:: add_insert for a table descriptor (case 2.) was added from\\n                flink 1.14.0.\\n\\n        :param target_path_or_descriptor: The path of the registered\\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\\n        :param table: The Table to add.\\n        :type table: pyflink.table.Table\\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self",
            "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Adds a statement that the pipeline defined by the given Table object should be written to a\\n        table (backed by a DynamicTableSink) that was registered under the specified path or\\n        expressed via the given TableDescriptor.\\n\\n        1. When target_path_or_descriptor is a tale path:\\n\\n            See the documentation of :func:`~TableEnvironment.use_database` or\\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\\n\\n        2. When target_path_or_descriptor is a table descriptor:\\n\\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\\n            table (see :func:`~TableEnvironment.create_temporary_table`).\\n\\n            Then a statement is added to the statement set that inserts the Table object\\'s pipeline\\n            into that temporary table.\\n\\n            This method allows to declare a Schema for the sink descriptor. The declaration is\\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\\n\\n                1. overwrite automatically derived columns with a custom DataType\\n                2. add metadata columns next to the physical columns\\n                3. declare a primary key\\n\\n            It is possible to declare a schema without physical/regular columns. In this case, those\\n            columns will be automatically derived and implicitly put at the beginning of the schema\\n            declaration.\\n\\n            Examples:\\n            ::\\n\\n                >>> stmt_set = table_env.create_statement_set()\\n                >>> source_table = table_env.from_path(\"SourceTable\")\\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\\n                ...     .schema(Schema.new_builder()\\n                ...         .build()) \\\\\\n                ...     .build()\\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\\n\\n            .. note:: add_insert for a table descriptor (case 2.) was added from\\n                flink 1.14.0.\\n\\n        :param target_path_or_descriptor: The path of the registered\\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\\n        :param table: The Table to add.\\n        :type table: pyflink.table.Table\\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self",
            "def add_insert(self, target_path_or_descriptor: Union[str, TableDescriptor], table, overwrite: bool=False) -> 'StatementSet':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Adds a statement that the pipeline defined by the given Table object should be written to a\\n        table (backed by a DynamicTableSink) that was registered under the specified path or\\n        expressed via the given TableDescriptor.\\n\\n        1. When target_path_or_descriptor is a tale path:\\n\\n            See the documentation of :func:`~TableEnvironment.use_database` or\\n            :func:`~TableEnvironment.use_catalog` for the rules on the path resolution.\\n\\n        2. When target_path_or_descriptor is a table descriptor:\\n\\n            The given TableDescriptor is registered as an inline (i.e. anonymous) temporary catalog\\n            table (see :func:`~TableEnvironment.create_temporary_table`).\\n\\n            Then a statement is added to the statement set that inserts the Table object\\'s pipeline\\n            into that temporary table.\\n\\n            This method allows to declare a Schema for the sink descriptor. The declaration is\\n            similar to a {@code CREATE TABLE} DDL in SQL and allows to:\\n\\n                1. overwrite automatically derived columns with a custom DataType\\n                2. add metadata columns next to the physical columns\\n                3. declare a primary key\\n\\n            It is possible to declare a schema without physical/regular columns. In this case, those\\n            columns will be automatically derived and implicitly put at the beginning of the schema\\n            declaration.\\n\\n            Examples:\\n            ::\\n\\n                >>> stmt_set = table_env.create_statement_set()\\n                >>> source_table = table_env.from_path(\"SourceTable\")\\n                >>> sink_descriptor = TableDescriptor.for_connector(\"blackhole\") \\\\\\n                ...     .schema(Schema.new_builder()\\n                ...         .build()) \\\\\\n                ...     .build()\\n                >>> stmt_set.add_insert(sink_descriptor, source_table)\\n\\n            .. note:: add_insert for a table descriptor (case 2.) was added from\\n                flink 1.14.0.\\n\\n        :param target_path_or_descriptor: The path of the registered\\n            :class:`~pyflink.table.TableSink` or the descriptor describing the sink table into which\\n            data should be inserted to which the :class:`~pyflink.table.Table` is written.\\n        :param table: The Table to add.\\n        :type table: pyflink.table.Table\\n        :param overwrite: Indicates whether the insert should overwrite existing data or not.\\n        :return: current StatementSet instance.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    if isinstance(target_path_or_descriptor, str):\n        self._j_statement_set.addInsert(target_path_or_descriptor, table._j_table, overwrite)\n    else:\n        self._j_statement_set.addInsert(target_path_or_descriptor._j_table_descriptor, table._j_table, overwrite)\n    return self"
        ]
    },
    {
        "func_name": "explain",
        "original": "def explain(self, *extra_details: ExplainDetail) -> str:\n    \"\"\"\n        returns the AST and the execution plan of all statements and Tables.\n\n        :param extra_details: The extra explain details which the explain result should include,\n                              e.g. estimated cost, changelog mode for streaming\n        :return: All statements and Tables for which the AST and execution plan will be returned.\n\n        .. versionadded:: 1.11.0\n        \"\"\"\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)",
        "mutated": [
            "def explain(self, *extra_details: ExplainDetail) -> str:\n    if False:\n        i = 10\n    '\\n        returns the AST and the execution plan of all statements and Tables.\\n\\n        :param extra_details: The extra explain details which the explain result should include,\\n                              e.g. estimated cost, changelog mode for streaming\\n        :return: All statements and Tables for which the AST and execution plan will be returned.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)",
            "def explain(self, *extra_details: ExplainDetail) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        returns the AST and the execution plan of all statements and Tables.\\n\\n        :param extra_details: The extra explain details which the explain result should include,\\n                              e.g. estimated cost, changelog mode for streaming\\n        :return: All statements and Tables for which the AST and execution plan will be returned.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)",
            "def explain(self, *extra_details: ExplainDetail) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        returns the AST and the execution plan of all statements and Tables.\\n\\n        :param extra_details: The extra explain details which the explain result should include,\\n                              e.g. estimated cost, changelog mode for streaming\\n        :return: All statements and Tables for which the AST and execution plan will be returned.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)",
            "def explain(self, *extra_details: ExplainDetail) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        returns the AST and the execution plan of all statements and Tables.\\n\\n        :param extra_details: The extra explain details which the explain result should include,\\n                              e.g. estimated cost, changelog mode for streaming\\n        :return: All statements and Tables for which the AST and execution plan will be returned.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)",
            "def explain(self, *extra_details: ExplainDetail) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        returns the AST and the execution plan of all statements and Tables.\\n\\n        :param extra_details: The extra explain details which the explain result should include,\\n                              e.g. estimated cost, changelog mode for streaming\\n        :return: All statements and Tables for which the AST and execution plan will be returned.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    TEXT = get_gateway().jvm.org.apache.flink.table.api.ExplainFormat.TEXT\n    j_extra_details = to_j_explain_detail_arr(extra_details)\n    return self._j_statement_set.explain(TEXT, j_extra_details)"
        ]
    },
    {
        "func_name": "execute",
        "original": "def execute(self) -> TableResult:\n    \"\"\"\n        execute all statements and Tables as a batch.\n\n        .. note::\n            The added statements and Tables will be cleared when executing this method.\n\n        :return: execution result.\n\n        .. versionadded:: 1.11.0\n        \"\"\"\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())",
        "mutated": [
            "def execute(self) -> TableResult:\n    if False:\n        i = 10\n    '\\n        execute all statements and Tables as a batch.\\n\\n        .. note::\\n            The added statements and Tables will be cleared when executing this method.\\n\\n        :return: execution result.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())",
            "def execute(self) -> TableResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        execute all statements and Tables as a batch.\\n\\n        .. note::\\n            The added statements and Tables will be cleared when executing this method.\\n\\n        :return: execution result.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())",
            "def execute(self) -> TableResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        execute all statements and Tables as a batch.\\n\\n        .. note::\\n            The added statements and Tables will be cleared when executing this method.\\n\\n        :return: execution result.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())",
            "def execute(self) -> TableResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        execute all statements and Tables as a batch.\\n\\n        .. note::\\n            The added statements and Tables will be cleared when executing this method.\\n\\n        :return: execution result.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())",
            "def execute(self) -> TableResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        execute all statements and Tables as a batch.\\n\\n        .. note::\\n            The added statements and Tables will be cleared when executing this method.\\n\\n        :return: execution result.\\n\\n        .. versionadded:: 1.11.0\\n        '\n    self._t_env._before_execute()\n    return TableResult(self._j_statement_set.execute())"
        ]
    }
]