[
    {
        "func_name": "test_regex_matching_valid",
        "original": "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)",
        "mutated": [
            "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    if False:\n        i = 10\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)",
            "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)",
            "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)",
            "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)",
            "@parameterized.expand(TEST_CASES)\ndef test_regex_matching_valid(self, key, target_modules, layers_to_transform, layers_pattern, expected_result):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_id = 'peft-internal-testing/tiny-OPTForCausalLM-lora'\n    config = LoraConfig(base_model_name_or_path=model_id, target_modules=target_modules, layers_pattern=layers_pattern, layers_to_transform=layers_to_transform)\n    actual_result = bool(check_target_module_exists(config, key))\n    self.assertEqual(actual_result, expected_result)"
        ]
    },
    {
        "func_name": "test_module_matching_lora",
        "original": "def test_module_matching_lora(self):\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)",
        "mutated": [
            "def test_module_matching_lora(self):\n    if False:\n        i = 10\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)",
            "def test_module_matching_lora(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)",
            "def test_module_matching_lora(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)",
            "def test_module_matching_lora(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)",
            "def test_module_matching_lora(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_id = 'hf-internal-testing/tiny-random-BloomForCausalLM'\n    model = self.transformers_class.from_pretrained(model_id)\n    config = LoraConfig()\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['h.0.self_attention.query_key_value', 'h.1.self_attention.query_key_value', 'h.2.self_attention.query_key_value', 'h.3.self_attention.query_key_value', 'h.4.self_attention.query_key_value']\n    self.assertEqual(matched, expected)\n    unmatched = output['unmatched']\n    for key in expected:\n        self.assertFalse(key in unmatched)"
        ]
    },
    {
        "func_name": "test_feedforward_matching_ia3",
        "original": "def test_feedforward_matching_ia3(self):\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)",
        "mutated": [
            "def test_feedforward_matching_ia3(self):\n    if False:\n        i = 10\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)",
            "def test_feedforward_matching_ia3(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)",
            "def test_feedforward_matching_ia3(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)",
            "def test_feedforward_matching_ia3(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)",
            "def test_feedforward_matching_ia3(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_id = 'hf-internal-testing/tiny-random-T5ForConditionalGeneration'\n    model = self.transformers_class.from_pretrained(model_id)\n    config_kwargs = {'target_modules': '.*encoder.*block.0.*(SelfAttention|EncDecAttention|DenseReluDense).(k|q|v|wo|wi)$', 'feedforward_modules': ['wo', 'wi']}\n    config = IA3Config(base_model_name_or_path=model_id, **config_kwargs)\n    peft_model = get_peft_model(model, config)\n    output = inspect_matched_modules(peft_model)\n    matched = output['matched']\n    expected = ['encoder.block.0.layer.0.SelfAttention.q', 'encoder.block.0.layer.0.SelfAttention.k', 'encoder.block.0.layer.0.SelfAttention.v', 'encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    expected_feedforward = ['encoder.block.0.layer.1.DenseReluDense.wi', 'encoder.block.0.layer.1.DenseReluDense.wo']\n    self.assertEqual(matched, expected)\n    module_dict = dict(model.named_modules())\n    for key in matched:\n        module = module_dict[key]\n        if key in expected_feedforward:\n            self.assertTrue(module.is_feedforward)\n        else:\n            self.assertFalse(module.is_feedforward)"
        ]
    }
]