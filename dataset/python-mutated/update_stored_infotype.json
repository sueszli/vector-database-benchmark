[
    {
        "func_name": "update_stored_infotype",
        "original": "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    \"\"\"Uses the Data Loss Prevention API to update stored infoType\n    detector by changing the source term list from one stored in Bigquery\n    to one stored in Cloud Storage.\n    Args:\n        project: The Google Cloud project id to use as a parent resource.\n        stored_info_type_id: The identifier of stored infoType which is to\n            be updated.\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\n            for the location of the source term list.\n        output_bucket_name: The name of the bucket in Google Cloud Storage\n            where large dictionary is stored.\n    \"\"\"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')",
        "mutated": [
            "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    if False:\n        i = 10\n    'Uses the Data Loss Prevention API to update stored infoType\\n    detector by changing the source term list from one stored in Bigquery\\n    to one stored in Cloud Storage.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        stored_info_type_id: The identifier of stored infoType which is to\\n            be updated.\\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\\n            for the location of the source term list.\\n        output_bucket_name: The name of the bucket in Google Cloud Storage\\n            where large dictionary is stored.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')",
            "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Uses the Data Loss Prevention API to update stored infoType\\n    detector by changing the source term list from one stored in Bigquery\\n    to one stored in Cloud Storage.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        stored_info_type_id: The identifier of stored infoType which is to\\n            be updated.\\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\\n            for the location of the source term list.\\n        output_bucket_name: The name of the bucket in Google Cloud Storage\\n            where large dictionary is stored.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')",
            "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Uses the Data Loss Prevention API to update stored infoType\\n    detector by changing the source term list from one stored in Bigquery\\n    to one stored in Cloud Storage.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        stored_info_type_id: The identifier of stored infoType which is to\\n            be updated.\\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\\n            for the location of the source term list.\\n        output_bucket_name: The name of the bucket in Google Cloud Storage\\n            where large dictionary is stored.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')",
            "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Uses the Data Loss Prevention API to update stored infoType\\n    detector by changing the source term list from one stored in Bigquery\\n    to one stored in Cloud Storage.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        stored_info_type_id: The identifier of stored infoType which is to\\n            be updated.\\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\\n            for the location of the source term list.\\n        output_bucket_name: The name of the bucket in Google Cloud Storage\\n            where large dictionary is stored.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')",
            "def update_stored_infotype(project: str, stored_info_type_id: str, gcs_input_file_path: str, output_bucket_name: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Uses the Data Loss Prevention API to update stored infoType\\n    detector by changing the source term list from one stored in Bigquery\\n    to one stored in Cloud Storage.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        stored_info_type_id: The identifier of stored infoType which is to\\n            be updated.\\n        gcs_input_file_path: The url in the format <bucket>/<path_to_file>\\n            for the location of the source term list.\\n        output_bucket_name: The name of the bucket in Google Cloud Storage\\n            where large dictionary is stored.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    stored_info_type_config = {'large_custom_dictionary': {'output_path': {'path': f'gs://{output_bucket_name}'}, 'cloud_storage_file_set': {'url': f'gs://{gcs_input_file_path}'}}}\n    field_mask = {'paths': ['large_custom_dictionary.cloud_storage_file_set.url']}\n    stored_info_type_name = f'projects/{project}/locations/global/storedInfoTypes/{stored_info_type_id}'\n    response = dlp.update_stored_info_type(request={'name': stored_info_type_name, 'config': stored_info_type_config, 'update_mask': field_mask})\n    print(f'Updated stored infoType successfully: {response.name}')"
        ]
    }
]