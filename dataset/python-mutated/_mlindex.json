[
    {
        "func_name": "__init__",
        "original": "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    \"\"\"\n        Initialize MLIndex from a URI or AzureML Data Asset.\n\n        Args:\n        ----\n            uri: URI to MLIndex asset folder (remote or local)\n            mlindex_config: MLIndex config dictionary\n            credential: Credential to use for talking to Azure resources\n        \"\"\"\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')",
        "mutated": [
            "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    if False:\n        i = 10\n    '\\n        Initialize MLIndex from a URI or AzureML Data Asset.\\n\\n        Args:\\n        ----\\n            uri: URI to MLIndex asset folder (remote or local)\\n            mlindex_config: MLIndex config dictionary\\n            credential: Credential to use for talking to Azure resources\\n        '\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')",
            "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Initialize MLIndex from a URI or AzureML Data Asset.\\n\\n        Args:\\n        ----\\n            uri: URI to MLIndex asset folder (remote or local)\\n            mlindex_config: MLIndex config dictionary\\n            credential: Credential to use for talking to Azure resources\\n        '\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')",
            "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Initialize MLIndex from a URI or AzureML Data Asset.\\n\\n        Args:\\n        ----\\n            uri: URI to MLIndex asset folder (remote or local)\\n            mlindex_config: MLIndex config dictionary\\n            credential: Credential to use for talking to Azure resources\\n        '\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')",
            "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Initialize MLIndex from a URI or AzureML Data Asset.\\n\\n        Args:\\n        ----\\n            uri: URI to MLIndex asset folder (remote or local)\\n            mlindex_config: MLIndex config dictionary\\n            credential: Credential to use for talking to Azure resources\\n        '\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')",
            "def __init__(self, uri: Optional[Union[str, Path, object]]=None, mlindex_config: Optional[dict]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Initialize MLIndex from a URI or AzureML Data Asset.\\n\\n        Args:\\n        ----\\n            uri: URI to MLIndex asset folder (remote or local)\\n            mlindex_config: MLIndex config dictionary\\n            credential: Credential to use for talking to Azure resources\\n        '\n    with track_activity(logger, 'MLIndex.__init__') as activity_logger:\n        if uri is not None:\n            if isinstance(uri, str):\n                uri = str(uri)\n            elif isinstance(uri, Path):\n                uri = str(uri)\n            else:\n                uri = uri.path\n            if find_spec('fsspec') is None:\n                raise ValueError('fsspec python package not installed. Please install it with `pip install fsspec`.')\n            if find_spec('azureml.fsspec') is None:\n                logger.warning('azureml-fsspec python package not installed. Loading from remote filesystems supported by AzureML will not work. Please install it with `pip install azureml-fsspec`.')\n            self.base_uri = uri\n            mlindex_config = None\n            uri = uri.rstrip('/')\n            mlindex_uri = f'{uri}/MLIndex' if not uri.endswith('MLIndex') else uri\n            try:\n                import fsspec\n                mlindex_file = fsspec.open(mlindex_uri, 'r')\n                if hasattr(mlindex_file.fs, '_path'):\n                    self.base_uri = mlindex_file.fs._path.split('/MLIndex')[0]\n                else:\n                    self.base_uri = mlindex_file.path.split('/MLIndex')[0]\n                with mlindex_file as f:\n                    mlindex_config = yaml.safe_load(f)\n            except Exception as e:\n                raise ValueError(f'Could not find MLIndex: {e}') from e\n        elif mlindex_config is None:\n            raise ValueError('Must provide either uri or mlindex_config')\n        self.index_config = mlindex_config.get('index', {})\n        if self.index_config is None:\n            raise ValueError('Could not find index config in MLIndex yaml')\n        activity_logger.activity_info['index_kind'] = self.index_config.get('kind', 'none')\n        self.embeddings_config = mlindex_config.get('embeddings', {})\n        if self.embeddings_config is None:\n            raise ValueError('Could not find embeddings config in MLIndex yaml')\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')"
        ]
    },
    {
        "func_name": "name",
        "original": "@property\ndef name(self) -> str:\n    \"\"\"Returns the name of the MLIndex.\"\"\"\n    return self.index_config.get('name', '')",
        "mutated": [
            "@property\ndef name(self) -> str:\n    if False:\n        i = 10\n    'Returns the name of the MLIndex.'\n    return self.index_config.get('name', '')",
            "@property\ndef name(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the name of the MLIndex.'\n    return self.index_config.get('name', '')",
            "@property\ndef name(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the name of the MLIndex.'\n    return self.index_config.get('name', '')",
            "@property\ndef name(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the name of the MLIndex.'\n    return self.index_config.get('name', '')",
            "@property\ndef name(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the name of the MLIndex.'\n    return self.index_config.get('name', '')"
        ]
    },
    {
        "func_name": "name",
        "original": "@name.setter\ndef name(self, value: str):\n    \"\"\"Sets the name of the MLIndex.\"\"\"\n    self.index_config['name'] = value",
        "mutated": [
            "@name.setter\ndef name(self, value: str):\n    if False:\n        i = 10\n    'Sets the name of the MLIndex.'\n    self.index_config['name'] = value",
            "@name.setter\ndef name(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Sets the name of the MLIndex.'\n    self.index_config['name'] = value",
            "@name.setter\ndef name(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Sets the name of the MLIndex.'\n    self.index_config['name'] = value",
            "@name.setter\ndef name(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Sets the name of the MLIndex.'\n    self.index_config['name'] = value",
            "@name.setter\ndef name(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Sets the name of the MLIndex.'\n    self.index_config['name'] = value"
        ]
    },
    {
        "func_name": "description",
        "original": "@property\ndef description(self) -> str:\n    \"\"\"Returns the description of the MLIndex.\"\"\"\n    return self.index_config.get('description', '')",
        "mutated": [
            "@property\ndef description(self) -> str:\n    if False:\n        i = 10\n    'Returns the description of the MLIndex.'\n    return self.index_config.get('description', '')",
            "@property\ndef description(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the description of the MLIndex.'\n    return self.index_config.get('description', '')",
            "@property\ndef description(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the description of the MLIndex.'\n    return self.index_config.get('description', '')",
            "@property\ndef description(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the description of the MLIndex.'\n    return self.index_config.get('description', '')",
            "@property\ndef description(self) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the description of the MLIndex.'\n    return self.index_config.get('description', '')"
        ]
    },
    {
        "func_name": "description",
        "original": "@description.setter\ndef description(self, value: str):\n    \"\"\"Sets the description of the MLIndex.\"\"\"\n    self.index_config['description'] = value",
        "mutated": [
            "@description.setter\ndef description(self, value: str):\n    if False:\n        i = 10\n    'Sets the description of the MLIndex.'\n    self.index_config['description'] = value",
            "@description.setter\ndef description(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Sets the description of the MLIndex.'\n    self.index_config['description'] = value",
            "@description.setter\ndef description(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Sets the description of the MLIndex.'\n    self.index_config['description'] = value",
            "@description.setter\ndef description(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Sets the description of the MLIndex.'\n    self.index_config['description'] = value",
            "@description.setter\ndef description(self, value: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Sets the description of the MLIndex.'\n    self.index_config['description'] = value"
        ]
    },
    {
        "func_name": "get_langchain_embeddings",
        "original": "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    \"\"\"Get the LangChainEmbeddings from the MLIndex.\"\"\"\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)",
        "mutated": [
            "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n    'Get the LangChainEmbeddings from the MLIndex.'\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)",
            "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the LangChainEmbeddings from the MLIndex.'\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)",
            "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the LangChainEmbeddings from the MLIndex.'\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)",
            "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the LangChainEmbeddings from the MLIndex.'\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)",
            "def get_langchain_embeddings(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the LangChainEmbeddings from the MLIndex.'\n    embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy())\n    return embeddings.as_langchain_embeddings(credential=credential)"
        ]
    },
    {
        "func_name": "as_langchain_vectorstore",
        "original": "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    \"\"\"Converts MLIndex to a retriever object that can be used with langchain, may download files.\"\"\"\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')",
        "mutated": [
            "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_vectorstore(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    with track_activity(logger, 'MLIndex.as_langchain_vectorstore') as activity_logger:\n        index_kind = self.index_config.get('kind', 'none')\n        activity_logger.activity_info['index_kind'] = index_kind\n        activity_logger.activity_info['embeddings_kind'] = self.embeddings_config.get('kind', 'none')\n        activity_logger.activity_info['embeddings_api_type'] = self.embeddings_config.get('api_type', 'none')\n        if index_kind == 'acs':\n            from azure.ai.generative.index._indexes.azure_search import import_azure_search_or_so_help_me\n            import_azure_search_or_so_help_me()\n            if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n                raise ValueError('field_mapping.embedding must be set in MLIndex config for acs index, try `.as_langchain_retriever()` instead.')\n            try:\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n            except Exception as e:\n                if 'AZURE_AI_SEARCH_KEY' in os.environ or 'AZURE_COGNITIVE_SEARCH_KEY' in os.environ:\n                    from azure.core.credentials import AzureKeyCredential\n                    logger.warning(f'Failed to get credential for ACS with {e}, falling back to env vars.')\n                    connection_credential = AzureKeyCredential(os.environ['AZURE_AI_SEARCH_KEY'] if 'AZURE_AI_SEARCH_KEY' in os.environ else os.environ['AZURE_COGNITIVE_SEARCH_KEY'])\n                else:\n                    raise e\n            index_endpoint = self.index_config.get('endpoint', None)\n            if not index_endpoint:\n                index_endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            azure_search_documents_version = packages_versions_for_compatibility['azure-search-documents']\n            search_client_version = pkg_version.parse(azure_search_documents_version)\n            langchain_pkg_version = pkg_version.parse(langchain_version)\n            if search_client_version > pkg_version.parse('11.4.0b6') and search_client_version <= pkg_version.parse('11.4.0b8') and (langchain_pkg_version > pkg_version.parse('0.0.273')) or (search_client_version == pkg_version.parse('11.4.0b6') and langchain_pkg_version < pkg_version.parse('0.0.273') and (langchain_pkg_version >= pkg_version.parse('0.0.198'))):\n                from langchain.vectorstores import azuresearch\n                azuresearch.FIELDS_ID = self.index_config.get('field_mapping', {}).get('id', 'id')\n                azuresearch.FIELDS_CONTENT = self.index_config.get('field_mapping', {}).get('content', 'content')\n                azuresearch.FIELDS_CONTENT_VECTOR = self.index_config.get('field_mapping', {}).get('embedding', 'content_vector_open_ai')\n                azuresearch.FIELDS_METADATA = self.index_config.get('field_mapping', {}).get('metadata', 'meta_json_string')\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores.azuresearch import AzureSearch\n                return AzureSearch(azure_search_endpoint=index_endpoint, azure_search_key=connection_credential.key if isinstance(connection_credential, AzureKeyCredential) else None, index_name=self.index_config.get('index'), embedding_function=self.get_langchain_embeddings(credential=credential).embed_query, search_type='hybrid', semantic_configuration_name=self.index_config.get('semantic_configuration_name', 'azureml-default'), user_agent=f'azureml-rag=={version}/mlindex,langchain=={langchain_version}')\n            else:\n                from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n                logger.warning(f'azure-search-documents=={azure_search_documents_version} not compatible langchain.vectorstores.azuresearch yet, using REST client based VectorStore.')\n                return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=index_endpoint, embeddings=self.get_langchain_embeddings(credential=credential), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential)\n        elif index_kind == 'faiss':\n            from fsspec.core import url_to_fs\n            store = None\n            engine = self.index_config.get('engine')\n            if engine == 'langchain.vectorstores.FAISS':\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                (fs, uri) = url_to_fs(self.base_uri)\n                with tempfile.TemporaryDirectory() as tmpdir:\n                    fs.download(f\"{uri.rstrip('/')}/index.pkl\", f'{str(tmpdir)}')\n                    fs.download(f\"{uri.rstrip('/')}/index.faiss\", f'{str(tmpdir)}')\n                    try:\n                        from langchain.vectorstores import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n                    except Exception as e:\n                        logger.warning(f'Failed to load FAISS Index using installed version of langchain, retrying with vendored FAISS VectorStore.\\n{e}')\n                        from azure.ai.generative.index._langchain.vendor.vectorstores.faiss import FAISS\n                        store = FAISS.load_local(str(tmpdir), embeddings)\n            elif engine.endswith('indexes.faiss.FaissAndDocStore'):\n                from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n                error_fmt_str = 'Failed to import langchain faiss bridge module with: {e}\\n\"\\n                        This could be due to an incompatible change in langchain since this bridge was implemented.\\n                        If you understand what has changed you could implement your own wrapper of azure.ai.tools.mlindex._indexes.faiss.FaissAndDocStore.\\n                        '\n                try:\n                    from azure.ai.generative.index._langchain.faiss import azureml_faiss_as_langchain_faiss\n                except Exception as e:\n                    logger.warning(error_fmt_str.format(e=e))\n                    azureml_faiss_as_langchain_faiss = None\n                embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config.copy()).as_langchain_embeddings(credential=credential)\n                store = FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n                if azureml_faiss_as_langchain_faiss is not None:\n                    try:\n                        store = azureml_faiss_as_langchain_faiss(FaissAndDocStore.load(self.base_uri, embeddings.embed_query))\n                    except Exception as e:\n                        logger.error(error_fmt_str.format(e=e))\n                        raise\n            else:\n                raise ValueError(f'Unknown engine: {engine}')\n            return store\n        elif index_kind == 'pinecone':\n            try:\n                import pinecone\n                from azure.core.credentials import AzureKeyCredential\n                from langchain.vectorstores import Pinecone\n                connection_credential = get_connection_credential(self.index_config, credential=credential)\n                if not isinstance(connection_credential, AzureKeyCredential):\n                    raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n                environment = get_pinecone_environment(self.index_config, credential=credential)\n                pinecone.init(api_key=connection_credential.key, environment=environment)\n                pinecone_index = pinecone.Index(self.index_config.get('index'))\n                index_stats = pinecone_index.describe_index_stats()\n                logger.info(f\"Pinecone index {self.index_config.get('index')} with stats {index_stats}\")\n                activity_logger.info('Pinecone index', extra={'properties': {'stats': index_stats}})\n                try:\n                    logger.info('Get Pinecone vectorstore by passing in `embedding` as a Callable.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential).embed_query, text_key=self.index_config.get('field_mapping', {})['content'])\n                except Exception as e:\n                    logger.warn(f'Failed to get Pinecone vectorstore due to {e}, try again by passing in `embedding` as an Embeddings object.')\n                    return Pinecone.from_existing_index(self.index_config.get('index'), self.get_langchain_embeddings(credential=credential), text_key=self.index_config.get('field_mapping', {})['content'])\n            except Exception as e:\n                logger.error(f'Failed to create Pinecone vectorstore due to: {e}')\n                raise\n        else:\n            raise ValueError(f'Unknown index kind: {index_kind}')"
        ]
    },
    {
        "func_name": "as_langchain_retriever",
        "original": "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    \"\"\"Converts MLIndex to a retriever object that can be used with langchain, may download files.\"\"\"\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
        "mutated": [
            "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    if False:\n        i = 10\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_langchain_retriever(self, credential: Optional[TokenCredential]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts MLIndex to a retriever object that can be used with langchain, may download files.'\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        if self.index_config.get('field_mapping', {}).get('embedding', None) is None:\n            from azure.ai.generative.index._langchain.acs import AzureCognitiveSearchVectorStore\n            connection_credential = get_connection_credential(self.index_config, credential=credential)\n            endpoint = self.index_config.get('endpoint', None)\n            if not endpoint:\n                endpoint = get_target_from_connection(get_connection_by_id_v2(self.index_config['connection']['id'], credential=credential))\n            return AzureCognitiveSearchVectorStore(index_name=self.index_config.get('index'), endpoint=endpoint, embeddings=self.get_langchain_embeddings(), field_mapping=self.index_config.get('field_mapping', {}), credential=connection_credential).as_retriever(**kwargs)\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'faiss':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    elif index_kind == 'pinecone':\n        return self.as_langchain_vectorstore(credential=credential).as_retriever(**kwargs)\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')"
        ]
    },
    {
        "func_name": "as_native_index_client",
        "original": "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    \"\"\"\n        Converts MLIndex config into a client for the underlying Index, may download files.\n\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\n        \"\"\"\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
        "mutated": [
            "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n    '\\n        Converts MLIndex config into a client for the underlying Index, may download files.\\n\\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\\n        '\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Converts MLIndex config into a client for the underlying Index, may download files.\\n\\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\\n        '\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Converts MLIndex config into a client for the underlying Index, may download files.\\n\\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\\n        '\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Converts MLIndex config into a client for the underlying Index, may download files.\\n\\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\\n        '\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')",
            "def as_native_index_client(self, credential: Optional[TokenCredential]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Converts MLIndex config into a client for the underlying Index, may download files.\\n\\n        An azure.search.documents.SearchClient for acs indexes or an azure.ai.generative.index._indexes.indexFaissAndDocStore for faiss indexes.\\n        '\n    index_kind = self.index_config.get('kind', None)\n    if index_kind == 'acs':\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        from azure.search.documents import SearchClient\n        return SearchClient(endpoint=self.index_config.get('endpoint'), index_name=self.index_config.get('index'), credential=connection_credential, user_agent=f'azureml-rag=={version}/mlindex', api_version=self.index_config.get('api_version', '2023-07-01-preview'))\n    elif index_kind == 'faiss':\n        from azure.ai.generative.index._indexes.faiss import FaissAndDocStore\n        embeddings = self.get_langchain_embeddings(credential=credential)\n        return FaissAndDocStore.load(self.base_uri, embeddings.embed_query)\n    elif index_kind == 'pinecone':\n        import pinecone\n        from azure.core.credentials import AzureKeyCredential\n        connection_credential = get_connection_credential(self.index_config, credential=credential)\n        if not isinstance(connection_credential, AzureKeyCredential):\n            raise ValueError(f'Expected credential to Pinecone index to be an AzureKeyCredential, instead got: {type(connection_credential)}')\n        environment = get_pinecone_environment(self.index_config, credential=credential)\n        pinecone.init(api_key=connection_credential.key, environment=environment)\n        return pinecone.Index(self.index_config.get('index'))\n    else:\n        raise ValueError(f'Unknown index kind: {index_kind}')"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    \"\"\"Returns a string representation of the MLIndex object.\"\"\"\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    'Returns a string representation of the MLIndex object.'\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a string representation of the MLIndex object.'\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a string representation of the MLIndex object.'\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a string representation of the MLIndex object.'\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a string representation of the MLIndex object.'\n    return yaml.dump({'index': self.index_config, 'embeddings': self.embeddings_config})"
        ]
    },
    {
        "func_name": "override_connections",
        "original": "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    \"\"\"\n        Override the connections used by the MLIndex.\n\n        Args:\n        ----\n            embedding_connection: Optional connection to use for embeddings model\n            index_connection: Optional connection to use for index\n            credential: Optional credential to use when resolving connection information\n        \"\"\"\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self",
        "mutated": [
            "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n    '\\n        Override the connections used by the MLIndex.\\n\\n        Args:\\n        ----\\n            embedding_connection: Optional connection to use for embeddings model\\n            index_connection: Optional connection to use for index\\n            credential: Optional credential to use when resolving connection information\\n        '\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self",
            "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Override the connections used by the MLIndex.\\n\\n        Args:\\n        ----\\n            embedding_connection: Optional connection to use for embeddings model\\n            index_connection: Optional connection to use for index\\n            credential: Optional credential to use when resolving connection information\\n        '\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self",
            "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Override the connections used by the MLIndex.\\n\\n        Args:\\n        ----\\n            embedding_connection: Optional connection to use for embeddings model\\n            index_connection: Optional connection to use for index\\n            credential: Optional credential to use when resolving connection information\\n        '\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self",
            "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Override the connections used by the MLIndex.\\n\\n        Args:\\n        ----\\n            embedding_connection: Optional connection to use for embeddings model\\n            index_connection: Optional connection to use for index\\n            credential: Optional credential to use when resolving connection information\\n        '\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self",
            "def override_connections(self, embedding_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, index_connection: Optional[Union[str, BaseConnection, WorkspaceConnection]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Override the connections used by the MLIndex.\\n\\n        Args:\\n        ----\\n            embedding_connection: Optional connection to use for embeddings model\\n            index_connection: Optional connection to use for index\\n            credential: Optional credential to use when resolving connection information\\n        '\n    if embedding_connection:\n        if self.embeddings_config.get('key') is not None:\n            self.embeddings_config.pop('key')\n        if embedding_connection.__class__.__name__ == 'AzureOpenAIConnection':\n            self.embeddings_config['connection_type'] = 'inline'\n            self.embeddings_config['connection'] = {'key': embedding_connection.secrets.get('api_key'), 'api_base': embedding_connection.api_base, 'api_type': embedding_connection.api_type}\n        else:\n            self.embeddings_config['connection_type'] = 'workspace_connection'\n            if isinstance(embedding_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                embedding_connection = get_connection_by_id_v2(embedding_connection, credential=credential)\n            self.embeddings_config['connection'] = {'id': get_id_from_connection(embedding_connection)}\n    if index_connection:\n        if self.index_config['kind'] != 'acs':\n            print('Index kind is not acs, ignoring override for connection')\n        else:\n            self.index_config['connection_type'] = 'workspace_connection'\n            if isinstance(index_connection, str):\n                from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            self.index_config['connection'] = {'id': get_id_from_connection(index_connection)}\n    self.save(just_config=True)\n    return self"
        ]
    },
    {
        "func_name": "set_embeddings_connection",
        "original": "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    \"\"\"Set the embeddings connection used by the MLIndex.\"\"\"\n    return self.override_connections(embedding_connection=connection)",
        "mutated": [
            "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n    'Set the embeddings connection used by the MLIndex.'\n    return self.override_connections(embedding_connection=connection)",
            "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Set the embeddings connection used by the MLIndex.'\n    return self.override_connections(embedding_connection=connection)",
            "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Set the embeddings connection used by the MLIndex.'\n    return self.override_connections(embedding_connection=connection)",
            "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Set the embeddings connection used by the MLIndex.'\n    return self.override_connections(embedding_connection=connection)",
            "def set_embeddings_connection(self, connection: Optional[Union[str, BaseConnection, WorkspaceConnection]], credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Set the embeddings connection used by the MLIndex.'\n    return self.override_connections(embedding_connection=connection)"
        ]
    },
    {
        "func_name": "from_files",
        "original": "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    \"\"\"\n        Create a new MLIndex from a repo.\n\n        Args:\n        ----\n            source_uri: Iterator of documents to index\n            source_glob: Glob pattern to match files to index\n            chunk_size: Size of chunks to split documents into\n            chunk_overlap: Size of overlap between chunks\n            citation_url: Optional url to replace citation urls with\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\n            embeddings_connection: Optional connection to use for embeddings model\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\n            index_type: Type of index to use, e.g. faiss\n            index_connection: Optional connection to use for index\n            index_config: Config for index, e.g. index_name or field_mapping for acs\n\n        Returns:\n        -------\n            MLIndex\n        \"\"\"\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
        "mutated": [
            "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n    '\\n        Create a new MLIndex from a repo.\\n\\n        Args:\\n        ----\\n            source_uri: Iterator of documents to index\\n            source_glob: Glob pattern to match files to index\\n            chunk_size: Size of chunks to split documents into\\n            chunk_overlap: Size of overlap between chunks\\n            citation_url: Optional url to replace citation urls with\\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_connection: Optional connection to use for embeddings model\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a new MLIndex from a repo.\\n\\n        Args:\\n        ----\\n            source_uri: Iterator of documents to index\\n            source_glob: Glob pattern to match files to index\\n            chunk_size: Size of chunks to split documents into\\n            chunk_overlap: Size of overlap between chunks\\n            citation_url: Optional url to replace citation urls with\\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_connection: Optional connection to use for embeddings model\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a new MLIndex from a repo.\\n\\n        Args:\\n        ----\\n            source_uri: Iterator of documents to index\\n            source_glob: Glob pattern to match files to index\\n            chunk_size: Size of chunks to split documents into\\n            chunk_overlap: Size of overlap between chunks\\n            citation_url: Optional url to replace citation urls with\\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_connection: Optional connection to use for embeddings model\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a new MLIndex from a repo.\\n\\n        Args:\\n        ----\\n            source_uri: Iterator of documents to index\\n            source_glob: Glob pattern to match files to index\\n            chunk_size: Size of chunks to split documents into\\n            chunk_overlap: Size of overlap between chunks\\n            citation_url: Optional url to replace citation urls with\\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_connection: Optional connection to use for embeddings model\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_files(source_uri: str, source_glob: str='**/*', chunk_size: int=1000, chunk_overlap: int=0, citation_url: Optional[str]=None, citation_replacement_regex: Optional[Dict[str, str]]=None, embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a new MLIndex from a repo.\\n\\n        Args:\\n        ----\\n            source_uri: Iterator of documents to index\\n            source_glob: Glob pattern to match files to index\\n            chunk_size: Size of chunks to split documents into\\n            chunk_overlap: Size of overlap between chunks\\n            citation_url: Optional url to replace citation urls with\\n            citation_replacement_regex: Optional regex to use to replace citation urls, e.g. `{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}`\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_connection: Optional connection to use for embeddings model\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    from azure.ai.generative.index._documents import DocumentChunksIterator, split_documents\n    with track_activity(logger, 'MLIndex.from_files'):\n        chunked_documents = DocumentChunksIterator(files_source=source_uri, glob=source_glob, base_url=citation_url, document_path_replacement_regex=citation_replacement_regex, chunked_document_processors=[lambda docs: split_documents(docs, splitter_args={'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap, 'use_rcts': False})])\n        mlindex = MLIndex.from_documents(chunked_documents, embeddings_model=embeddings_model, embeddings_connection=embeddings_connection, embeddings_container=embeddings_container, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex"
        ]
    },
    {
        "func_name": "from_documents",
        "original": "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    \"\"\"\n        Create a new MLIndex from documents.\n\n        Args:\n        ----\n            documents: Iterator of documents to index\n            index_kind: Kind of index to use\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\n            index_type: Type of index to use, e.g. faiss\n            index_connection: Optional connection to use for index\n            index_config: Config for index, e.g. index_name or field_mapping for acs\n            output_path: Optional path to save index to\n\n        Returns:\n        -------\n            MLIndex\n        \"\"\"\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
        "mutated": [
            "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n    '\\n        Create a new MLIndex from documents.\\n\\n        Args:\\n        ----\\n            documents: Iterator of documents to index\\n            index_kind: Kind of index to use\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a new MLIndex from documents.\\n\\n        Args:\\n        ----\\n            documents: Iterator of documents to index\\n            index_kind: Kind of index to use\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a new MLIndex from documents.\\n\\n        Args:\\n        ----\\n            documents: Iterator of documents to index\\n            index_kind: Kind of index to use\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a new MLIndex from documents.\\n\\n        Args:\\n        ----\\n            documents: Iterator of documents to index\\n            index_kind: Kind of index to use\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex",
            "@staticmethod\ndef from_documents(documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator], embeddings_model: str='hugging_face://model/sentence-transformers/all-mpnet-base-v2', embeddings_connection: Optional[str]=None, embeddings_container: Optional[Union[str, Path]]=None, index_type: str='faiss', index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a new MLIndex from documents.\\n\\n        Args:\\n        ----\\n            documents: Iterator of documents to index\\n            index_kind: Kind of index to use\\n            embeddings_model: Name of embeddings model to use, expected format `azure_open_ai://deployment/.../model/text-embedding-ada-002` or `hugging_face://model/all-mpnet-base-v2`\\n            embeddings_container: Optional path to location where un-indexed embeddings can be saved/loaded.\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n\\n        Returns:\\n        -------\\n            MLIndex\\n        '\n    import time\n    embeddings = None\n    try:\n        if embeddings_container is not None:\n            if isinstance(embeddings_container, str) and '://' in embeddings_container:\n                from fsspec.core import url_to_fs\n                (fs, uri) = url_to_fs(embeddings_container)\n            else:\n                embeddings_container = Path(embeddings_container)\n                previous_embeddings_dir_name = None\n                try:\n                    previous_embeddings_dir_name = str(max([dir for dir in embeddings_container.glob('*') if dir.is_dir()], key=os.path.getmtime).name)\n                except Exception as e:\n                    logger.warning(f'failed to get latest folder from {embeddings_container} with {e}.', extra={'print': True})\n                    pass\n                if previous_embeddings_dir_name is not None:\n                    try:\n                        embeddings = EmbeddingsContainer.load(previous_embeddings_dir_name, embeddings_container)\n                    except Exception as e:\n                        logger.warning(f'failed to load embeddings from {embeddings_container} with {e}.', extra={'print': True})\n                        pass\n    finally:\n        if embeddings is None:\n            logger.info('Creating new EmbeddingsContainer')\n            if isinstance(embeddings_model, str):\n                connection_args = {}\n                if 'open_ai' in embeddings_model:\n                    from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n                    if embeddings_connection:\n                        if isinstance(embeddings_connection, str):\n                            embeddings_connection = get_connection_by_id_v2(embeddings_connection, credential=credential)\n                        connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(embeddings_connection)}, 'endpoint': embeddings_connection.target if hasattr(embeddings_connection, 'target') else embeddings_connection['properties']['target']}\n                    else:\n                        connection_args = {'connection_type': 'environment', 'connection': {'key': 'OPENAI_API_KEY'}, 'endpoint': os.getenv('OPENAI_API_BASE')}\n                        if os.getenv('OPENAI_API_TYPE'):\n                            connection_args['api_type'] = os.getenv('OPENAI_API_TYPE')\n                        if os.getenv('OPENAI_API_VERSION'):\n                            connection_args['api_version'] = os.getenv('OPENAI_API_VERSION')\n                embeddings = EmbeddingsContainer.from_uri(embeddings_model, credential=credential, **connection_args)\n            else:\n                raise ValueError(f'Unknown embeddings model: {embeddings_model}')\n    pre_embed = time.time()\n    embeddings = embeddings.embed(documents)\n    post_embed = time.time()\n    logger.info(f'Embedding took {post_embed - pre_embed} seconds')\n    if embeddings_container is not None:\n        now = datetime.now()\n        embeddings.save(str(embeddings_container / f\"{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}_{str(uuid.uuid4()).split('-')[0]}\"))\n    mlindex = MLIndex.from_embeddings_container(embeddings, index_type=index_type, index_connection=index_connection, index_config=index_config, output_path=output_path, credential=credential)\n    return mlindex"
        ]
    },
    {
        "func_name": "from_embeddings_container",
        "original": "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    \"\"\"\n        Create a new MLIndex from embeddings.\n\n        Args\n        ----\n            embeddings: EmbeddingsContainer to index\n            index_type: Type of index to use, e.g. faiss\n            index_connection: Optional connection to use for index\n            index_config: Config for index, e.g. index_name or field_mapping for acs\n            output_path: Optional path to save index to\n            credential: Optional credential to use when resolving connection information\n\n        Returns\n        -------\n            MLIndex\n        \"\"\"\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex",
        "mutated": [
            "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n    '\\n        Create a new MLIndex from embeddings.\\n\\n        Args\\n        ----\\n            embeddings: EmbeddingsContainer to index\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n            credential: Optional credential to use when resolving connection information\\n\\n        Returns\\n        -------\\n            MLIndex\\n        '\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex",
            "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a new MLIndex from embeddings.\\n\\n        Args\\n        ----\\n            embeddings: EmbeddingsContainer to index\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n            credential: Optional credential to use when resolving connection information\\n\\n        Returns\\n        -------\\n            MLIndex\\n        '\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex",
            "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a new MLIndex from embeddings.\\n\\n        Args\\n        ----\\n            embeddings: EmbeddingsContainer to index\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n            credential: Optional credential to use when resolving connection information\\n\\n        Returns\\n        -------\\n            MLIndex\\n        '\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex",
            "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a new MLIndex from embeddings.\\n\\n        Args\\n        ----\\n            embeddings: EmbeddingsContainer to index\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n            credential: Optional credential to use when resolving connection information\\n\\n        Returns\\n        -------\\n            MLIndex\\n        '\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex",
            "@staticmethod\ndef from_embeddings_container(embeddings: EmbeddingsContainer, index_type: str, index_connection: Optional[str]=None, index_config: Dict[str, Any]={}, output_path: Optional[Union[str, Path]]=None, credential: Optional[TokenCredential]=None) -> 'MLIndex':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a new MLIndex from embeddings.\\n\\n        Args\\n        ----\\n            embeddings: EmbeddingsContainer to index\\n            index_type: Type of index to use, e.g. faiss\\n            index_connection: Optional connection to use for index\\n            index_config: Config for index, e.g. index_name or field_mapping for acs\\n            output_path: Optional path to save index to\\n            credential: Optional credential to use when resolving connection information\\n\\n        Returns\\n        -------\\n            MLIndex\\n        '\n    if output_path is None:\n        output_path = Path.cwd() / f'{index_type}_{embeddings.kind}_index'\n    if index_type == 'faiss':\n        embeddings.write_as_faiss_mlindex(output_path=output_path, engine=index_config.get('engine', 'indexes.faiss.FaissAndDocStore'))\n        mlindex = MLIndex(uri=Path(output_path))\n    elif index_type == 'acs':\n        from azure.ai.generative.index._tasks.update_acs import create_index_from_raw_embeddings\n        from azure.ai.generative.index._utils.connections import get_connection_by_id_v2\n        if not index_connection:\n            index_config = {**index_config, **{'endpoint': os.getenv('AZURE_AI_SEARCH_ENDPOINT'), 'api_version': '2023-07-01-preview'}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'AZURE_AI_SEARCH_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'endpoint': index_connection.target if hasattr(index_connection, 'target') else index_connection['properties']['target'], 'api_version': index_connection.metadata.get('apiVersion', '2023-07-01-preview') if hasattr(index_connection, 'metadata') else index_connection['properties']['metadata'].get('apiVersion', '2023-07-01-preview')}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    elif index_type == 'pinecone':\n        from azure.ai.generative.index._tasks.update_pinecone import create_index_from_raw_embeddings\n        if not index_connection:\n            index_config = {**index_config, **{'environment': os.getenv('PINECONE_ENVIRONMENT')}}\n            connection_args = {'connection_type': 'environment', 'connection': {'key': 'PINECONE_API_KEY'}}\n        else:\n            if isinstance(index_connection, str):\n                index_connection = get_connection_by_id_v2(index_connection, credential=credential)\n            index_config = {**index_config, **{'environment': index_connection['properties']['metadata']['environment']}}\n            connection_args = {'connection_type': 'workspace_connection', 'connection': {'id': get_id_from_connection(index_connection)}}\n        mlindex = create_index_from_raw_embeddings(embeddings, index_config, connection=connection_args, output_path=str(output_path), credential=credential)\n    else:\n        raise ValueError(f'Unknown index type: {index_type}')\n    return mlindex"
        ]
    },
    {
        "func_name": "save",
        "original": "def save(self, output_uri: Optional[str], just_config: bool=False):\n    \"\"\"\n        Save the MLIndex to a uri.\n\n        Will use uri MLIndex was loaded from if `output_uri` not set.\n        \"\"\"\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e",
        "mutated": [
            "def save(self, output_uri: Optional[str], just_config: bool=False):\n    if False:\n        i = 10\n    '\\n        Save the MLIndex to a uri.\\n\\n        Will use uri MLIndex was loaded from if `output_uri` not set.\\n        '\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e",
            "def save(self, output_uri: Optional[str], just_config: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Save the MLIndex to a uri.\\n\\n        Will use uri MLIndex was loaded from if `output_uri` not set.\\n        '\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e",
            "def save(self, output_uri: Optional[str], just_config: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Save the MLIndex to a uri.\\n\\n        Will use uri MLIndex was loaded from if `output_uri` not set.\\n        '\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e",
            "def save(self, output_uri: Optional[str], just_config: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Save the MLIndex to a uri.\\n\\n        Will use uri MLIndex was loaded from if `output_uri` not set.\\n        '\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e",
            "def save(self, output_uri: Optional[str], just_config: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Save the MLIndex to a uri.\\n\\n        Will use uri MLIndex was loaded from if `output_uri` not set.\\n        '\n    try:\n        import fsspec\n        mlindex_file = fsspec.open(f\"{output_uri.rstrip('/')}/MLIndex\", 'w')\n        with mlindex_file as f:\n            yaml.safe_dump({'embeddings': self.embeddings_config, 'index': self.index_config}, f)\n        if not just_config:\n            files = fsspec.open_files(f'{self.base_uri}/*')\n            files += fsspec.open_files(f'{self.base_uri}/**/*')\n            for file in files:\n                if file.path.endswith('MLIndex'):\n                    continue\n                with file.open() as src:\n                    with fsspec.open(f\"{output_uri.rstrip('/')}/{file.path.replace(self.base_uri, '').lstrip('/')}\", 'wb') as dest:\n                        dest.write(src.read())\n    except Exception as e:\n        raise ValueError(f'Could not save MLIndex: {e}') from e"
        ]
    }
]