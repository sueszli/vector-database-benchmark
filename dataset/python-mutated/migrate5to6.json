[
    {
        "func_name": "_wrapper",
        "original": "def _wrapper(*args):\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise",
        "mutated": [
            "def _wrapper(*args):\n    if False:\n        i = 10\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise",
            "def _wrapper(*args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise",
            "def _wrapper(*args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise",
            "def _wrapper(*args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise",
            "def _wrapper(*args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cursor = db.cursor()\n    try:\n        result = fn(cursor, *args)\n        db.commit()\n        return result\n    except sqlite3.IntegrityError:\n        db.rollback()\n        raise"
        ]
    },
    {
        "func_name": "_decorate",
        "original": "def _decorate(fn):\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper",
        "mutated": [
            "def _decorate(fn):\n    if False:\n        i = 10\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper",
            "def _decorate(fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper",
            "def _decorate(fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper",
            "def _decorate(fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper",
            "def _decorate(fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _wrapper(*args):\n        cursor = db.cursor()\n        try:\n            result = fn(cursor, *args)\n            db.commit()\n            return result\n        except sqlite3.IntegrityError:\n            db.rollback()\n            raise\n    return _wrapper"
        ]
    },
    {
        "func_name": "run_operation",
        "original": "def run_operation(db):\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate",
        "mutated": [
            "def run_operation(db):\n    if False:\n        i = 10\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate",
            "def run_operation(db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate",
            "def run_operation(db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate",
            "def run_operation(db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate",
            "def run_operation(db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _decorate(fn):\n\n        def _wrapper(*args):\n            cursor = db.cursor()\n            try:\n                result = fn(cursor, *args)\n                db.commit()\n                return result\n            except sqlite3.IntegrityError:\n                db.rollback()\n                raise\n        return _wrapper\n    return _decorate"
        ]
    },
    {
        "func_name": "verify_sd_blob",
        "original": "def verify_sd_blob(sd_hash, blob_dir):\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)",
        "mutated": [
            "def verify_sd_blob(sd_hash, blob_dir):\n    if False:\n        i = 10\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)",
            "def verify_sd_blob(sd_hash, blob_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)",
            "def verify_sd_blob(sd_hash, blob_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)",
            "def verify_sd_blob(sd_hash, blob_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)",
            "def verify_sd_blob(sd_hash, blob_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(os.path.join(blob_dir, sd_hash), 'r') as sd_file:\n        data = sd_file.read()\n        sd_length = len(data)\n        decoded = json.loads(data)\n    assert set(decoded.keys()) == {'stream_name', 'blobs', 'stream_type', 'key', 'suggested_file_name', 'stream_hash'}, 'invalid sd blob'\n    for blob in sorted(decoded['blobs'], key=lambda x: int(x['blob_num']), reverse=True):\n        if blob['blob_num'] == len(decoded['blobs']) - 1:\n            assert {'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream terminator'\n            assert blob['length'] == 0, 'non zero length stream terminator'\n        else:\n            assert {'blob_hash', 'length', 'blob_num', 'iv'} == set(blob.keys()), 'invalid stream blob'\n            assert blob['length'] > 0, 'zero length stream blob'\n    return (decoded, sd_length)"
        ]
    },
    {
        "func_name": "_populate_blobs",
        "original": "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])",
        "mutated": [
            "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    if False:\n        i = 10\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])",
            "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])",
            "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])",
            "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])",
            "@run_operation(connection)\ndef _populate_blobs(transaction, blob_infos):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])"
        ]
    },
    {
        "func_name": "_import_file",
        "original": "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))",
        "mutated": [
            "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    if False:\n        i = 10\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))",
            "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))",
            "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))",
            "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))",
            "@run_operation(connection)\ndef _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    try:\n        transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n    except sqlite3.IntegrityError:\n        return sd_hash\n    transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n    for (blob_hash, length, position, iv) in stream_blobs:\n        transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n    download_dir = conf.download_dir\n    if not isinstance(download_dir, bytes):\n        download_dir = download_dir.encode()\n    transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))"
        ]
    },
    {
        "func_name": "_add_recovered_blobs",
        "original": "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))",
        "mutated": [
            "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    if False:\n        i = 10\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))",
            "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))",
            "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))",
            "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))",
            "@run_operation(connection)\ndef _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n    for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n        if blob['blob_num'] < len(blob_infos) - 1:\n            transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))"
        ]
    },
    {
        "func_name": "_make_db",
        "original": "@run_operation(connection)\ndef _make_db(new_db):\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])",
        "mutated": [
            "@run_operation(connection)\ndef _make_db(new_db):\n    if False:\n        i = 10\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])",
            "@run_operation(connection)\ndef _make_db(new_db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])",
            "@run_operation(connection)\ndef _make_db(new_db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])",
            "@run_operation(connection)\ndef _make_db(new_db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])",
            "@run_operation(connection)\ndef _make_db(new_db):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    new_db.executescript(CREATE_TABLES_QUERY)\n    blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n    _populate_blobs(blobs)\n    log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n    file_args = {}\n    file_outpoints = {}\n    for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n        if rowid in old_rowid_to_outpoint:\n            file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n        elif sd_hash in old_sd_hash_to_outpoint:\n            file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n        sd_hash_to_stream_hash[sd_hash] = stream_hash\n        if stream_hash in stream_hash_to_stream_blobs:\n            file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n    claim_queries = {}\n    for (outpoint, sd_hash) in file_outpoints.items():\n        if outpoint in claim_outpoint_queries:\n            claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n    new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n    log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n    damaged_stream_sds = []\n    for (sd_hash, file_query) in file_args.items():\n        failed_sd = _import_file(*file_query)\n        if failed_sd:\n            damaged_stream_sds.append(failed_sd)\n    if damaged_stream_sds:\n        blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n        damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n        for damaged_sd in damaged_sds_on_disk:\n            try:\n                (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                blobs = decoded['blobs']\n                _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                _import_file(*file_args[damaged_sd])\n                damaged_stream_sds.remove(damaged_sd)\n            except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                continue\n    log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n    for claim_arg_tup in claim_queries.values():\n        if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n            try:\n                new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n            except sqlite3.IntegrityError:\n                continue\n    log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])"
        ]
    },
    {
        "func_name": "do_migration",
        "original": "def do_migration(conf):\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()",
        "mutated": [
            "def do_migration(conf):\n    if False:\n        i = 10\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()",
            "def do_migration(conf):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()",
            "def do_migration(conf):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()",
            "def do_migration(conf):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()",
            "def do_migration(conf):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    new_db_path = os.path.join(conf.data_dir, 'lbrynet.sqlite')\n    connection = sqlite3.connect(new_db_path)\n    metadata_db = sqlite3.connect(os.path.join(conf.data_dir, 'blockchainname.db'))\n    lbryfile_db = sqlite3.connect(os.path.join(conf.data_dir, 'lbryfile_info.db'))\n    blobs_db = sqlite3.connect(os.path.join(conf.data_dir, 'blobs.db'))\n    name_metadata_cursor = metadata_db.cursor()\n    lbryfile_cursor = lbryfile_db.cursor()\n    blobs_db_cursor = blobs_db.cursor()\n    old_rowid_to_outpoint = {rowid: (txid, nout) for (rowid, txid, nout) in lbryfile_cursor.execute('select * from lbry_file_metadata').fetchall()}\n    old_sd_hash_to_outpoint = {sd_hash: (txid, nout) for (txid, nout, sd_hash) in name_metadata_cursor.execute('select txid, n, sd_hash from name_metadata').fetchall()}\n    sd_hash_to_stream_hash = dict(lbryfile_cursor.execute('select sd_blob_hash, stream_hash from lbry_file_descriptors').fetchall())\n    stream_hash_to_stream_blobs = {}\n    for (blob_hash, stream_hash, position, iv, length) in lbryfile_db.execute('select * from lbry_file_blobs').fetchall():\n        stream_blobs = stream_hash_to_stream_blobs.get(stream_hash, [])\n        stream_blobs.append((blob_hash, length, position, iv))\n        stream_hash_to_stream_blobs[stream_hash] = stream_blobs\n    claim_outpoint_queries = {}\n    for claim_query in metadata_db.execute('select distinct c.txid, c.n, c.claimId, c.name, claim_cache.claim_sequence, claim_cache.claim_address, claim_cache.height, claim_cache.amount, claim_cache.claim_pb from claim_cache inner join claim_ids c on claim_cache.claim_id=c.claimId'):\n        (txid, nout) = (claim_query[0], claim_query[1])\n        if (txid, nout) in claim_outpoint_queries:\n            continue\n        claim_outpoint_queries[txid, nout] = claim_query\n\n    @run_operation(connection)\n    def _populate_blobs(transaction, blob_infos):\n        transaction.executemany('insert into blob values (?, ?, ?, ?, ?)', [(blob_hash, blob_length, int(next_announce_time), should_announce, 'finished') for (blob_hash, blob_length, _, next_announce_time, should_announce) in blob_infos])\n\n    @run_operation(connection)\n    def _import_file(transaction, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status, stream_blobs):\n        try:\n            transaction.execute('insert or ignore into stream values (?, ?, ?, ?, ?)', (stream_hash, sd_hash, key, stream_name, suggested_file_name))\n        except sqlite3.IntegrityError:\n            return sd_hash\n        transaction.executemany('insert or ignore into blob values (?, ?, ?, ?, ?)', [(blob_hash, length, 0, 0, 'pending') for (blob_hash, length, position, iv) in stream_blobs])\n        for (blob_hash, length, position, iv) in stream_blobs:\n            transaction.execute('insert or ignore into stream_blob values (?, ?, ?, ?)', (stream_hash, blob_hash, position, iv))\n        download_dir = conf.download_dir\n        if not isinstance(download_dir, bytes):\n            download_dir = download_dir.encode()\n        transaction.execute('insert or ignore into file values (?, ?, ?, ?, ?)', (stream_hash, stream_name, hexlify(download_dir), data_rate, status))\n\n    @run_operation(connection)\n    def _add_recovered_blobs(transaction, blob_infos, sd_hash, sd_length):\n        transaction.execute('insert or replace into blob values (?, ?, ?, ?, ?)', (sd_hash, sd_length, 0, 1, 'finished'))\n        for blob in sorted(blob_infos, key=lambda x: x['blob_num'], reverse=True):\n            if blob['blob_num'] < len(blob_infos) - 1:\n                transaction.execute('insert or ignore into blob values (?, ?, ?, ?, ?)', (blob['blob_hash'], blob['length'], 0, 0, 'pending'))\n\n    @run_operation(connection)\n    def _make_db(new_db):\n        new_db.executescript(CREATE_TABLES_QUERY)\n        blobs = blobs_db_cursor.execute('select * from blobs').fetchall()\n        _populate_blobs(blobs)\n        log.info('migrated %i blobs', new_db.execute('select count(*) from blob').fetchone()[0])\n        file_args = {}\n        file_outpoints = {}\n        for (rowid, sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate, status) in lbryfile_db.execute('select distinct lbry_files.rowid, d.sd_blob_hash, lbry_files.*, o.blob_data_rate, o.status from lbry_files inner join lbry_file_descriptors d on lbry_files.stream_hash=d.stream_hash inner join lbry_file_options o on lbry_files.stream_hash=o.stream_hash'):\n            if rowid in old_rowid_to_outpoint:\n                file_outpoints[old_rowid_to_outpoint[rowid]] = sd_hash\n            elif sd_hash in old_sd_hash_to_outpoint:\n                file_outpoints[old_sd_hash_to_outpoint[sd_hash]] = sd_hash\n            sd_hash_to_stream_hash[sd_hash] = stream_hash\n            if stream_hash in stream_hash_to_stream_blobs:\n                file_args[sd_hash] = (sd_hash, stream_hash, key, stream_name, suggested_file_name, data_rate or 0.0, status, stream_hash_to_stream_blobs.pop(stream_hash))\n        claim_queries = {}\n        for (outpoint, sd_hash) in file_outpoints.items():\n            if outpoint in claim_outpoint_queries:\n                claim_queries[sd_hash] = claim_outpoint_queries[outpoint]\n        new_db.executemany('insert or ignore into claim values (?, ?, ?, ?, ?, ?, ?, ?, ?)', [('%s:%i' % (claim_arg_tup[0], claim_arg_tup[1]), claim_arg_tup[2], claim_arg_tup[3], claim_arg_tup[7], claim_arg_tup[6], claim_arg_tup[8], Claim.from_bytes(claim_arg_tup[8]).signing_channel_id, claim_arg_tup[5], claim_arg_tup[4]) for (sd_hash, claim_arg_tup) in claim_queries.items() if claim_arg_tup])\n        log.info('migrated %i claims', new_db.execute('select count(*) from claim').fetchone()[0])\n        damaged_stream_sds = []\n        for (sd_hash, file_query) in file_args.items():\n            failed_sd = _import_file(*file_query)\n            if failed_sd:\n                damaged_stream_sds.append(failed_sd)\n        if damaged_stream_sds:\n            blob_dir = os.path.join(conf.data_dir, 'blobfiles')\n            damaged_sds_on_disk = [] if not os.path.isdir(blob_dir) else list({p for p in os.listdir(blob_dir) if p in damaged_stream_sds})\n            for damaged_sd in damaged_sds_on_disk:\n                try:\n                    (decoded, sd_length) = verify_sd_blob(damaged_sd, blob_dir)\n                    blobs = decoded['blobs']\n                    _add_recovered_blobs(blobs, damaged_sd, sd_length)\n                    _import_file(*file_args[damaged_sd])\n                    damaged_stream_sds.remove(damaged_sd)\n                except (OSError, ValueError, TypeError, AssertionError, sqlite3.IntegrityError):\n                    continue\n        log.info('migrated %i files', new_db.execute('select count(*) from file').fetchone()[0])\n        for claim_arg_tup in claim_queries.values():\n            if claim_arg_tup and (claim_arg_tup[0], claim_arg_tup[1]) in file_outpoints and (file_outpoints[claim_arg_tup[0], claim_arg_tup[1]] in sd_hash_to_stream_hash):\n                try:\n                    new_db.execute('insert or ignore into content_claim values (?, ?)', (sd_hash_to_stream_hash.get(file_outpoints.get((claim_arg_tup[0], claim_arg_tup[1]))), '%s:%i' % (claim_arg_tup[0], claim_arg_tup[1])))\n                except sqlite3.IntegrityError:\n                    continue\n        log.info('migrated %i content claims', new_db.execute('select count(*) from content_claim').fetchone()[0])\n    try:\n        _make_db()\n    except sqlite3.OperationalError as err:\n        if err.message == 'table blob has 7 columns but 5 values were supplied':\n            log.warning('detected a failed previous migration to revision 6, repairing it')\n            connection.close()\n            os.remove(new_db_path)\n            return do_migration(conf)\n        raise err\n    connection.close()\n    blobs_db.close()\n    lbryfile_db.close()\n    metadata_db.close()"
        ]
    }
]