[
    {
        "func_name": "model_error_contribution",
        "original": "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Calculate features contributing to model error.\"\"\"\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)",
        "mutated": [
            "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    if False:\n        i = 10\n    'Calculate features contributing to model error.'\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)",
            "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate features contributing to model error.'\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)",
            "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate features contributing to model error.'\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)",
            "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate features contributing to model error.'\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)",
            "def model_error_contribution(train_dataset: pd.DataFrame, train_scores: pd.Series, test_dataset: pd.DataFrame, test_scores: pd.Series, numeric_features: List, categorical_features: List, min_error_model_score=0.5, random_state=42) -> Tuple[pd.Series, pd.Series]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate features contributing to model error.'\n    (error_model, new_feature_order) = create_error_regression_model(numeric_features, categorical_features, random_state=random_state)\n    error_model.fit(train_dataset, y=train_scores)\n    error_model_predicted = error_model.predict(test_dataset)\n    error_model_score = r2_score(test_scores, error_model_predicted)\n    if error_model_score < min_error_model_score:\n        raise DeepchecksProcessError(f'Unable to train meaningful error model (r^2 score: {format_number(error_model_score)})')\n    (error_fi, _) = _calculate_feature_importance(error_model, Dataset(test_dataset, test_scores, cat_features=categorical_features), model_classes=None, observed_classes=None, task_type=TaskType.REGRESSION, permutation_kwargs={'random_state': random_state, 'skip_messages': True})\n    error_fi.index = new_feature_order\n    error_fi.sort_values(ascending=False, inplace=True)\n    return (error_fi, error_model_predicted)"
        ]
    },
    {
        "func_name": "create_error_regression_model",
        "original": "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    \"\"\"Create regression model to calculate error.\"\"\"\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)",
        "mutated": [
            "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    if False:\n        i = 10\n    'Create regression model to calculate error.'\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)",
            "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create regression model to calculate error.'\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)",
            "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create regression model to calculate error.'\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)",
            "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create regression model to calculate error.'\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)",
            "def create_error_regression_model(numeric_features, cat_features, random_state=42) -> Tuple[Pipeline, List[Hashable]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create regression model to calculate error.'\n    numeric_transformer = SimpleImputer()\n    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', TargetEncoder())])\n    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, cat_features)])\n    return (Pipeline(steps=[('preprocessing', preprocessor), ('model', RandomForestRegressor(max_depth=4, n_jobs=-1, random_state=random_state))]), numeric_features + cat_features)"
        ]
    },
    {
        "func_name": "error_model_display_dataframe",
        "original": "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    \"\"\"Wrap dataframe with tabular.Dataset for error_model_display with no scorer.\"\"\"\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)",
        "mutated": [
            "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    if False:\n        i = 10\n    'Wrap dataframe with tabular.Dataset for error_model_display with no scorer.'\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)",
            "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Wrap dataframe with tabular.Dataset for error_model_display with no scorer.'\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)",
            "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Wrap dataframe with tabular.Dataset for error_model_display with no scorer.'\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)",
            "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Wrap dataframe with tabular.Dataset for error_model_display with no scorer.'\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)",
            "def error_model_display_dataframe(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: pd.DataFrame, cat_features: List, max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Wrap dataframe with tabular.Dataset for error_model_display with no scorer.'\n    return error_model_display(error_fi, error_model_predicted, tabular.Dataset(dataset, cat_features=cat_features), None, None, max_features_to_show, min_feature_contribution, n_display_samples, min_segment_size, random_state, with_display)"
        ]
    },
    {
        "func_name": "error_model_display",
        "original": "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    \"\"\"Calculate and display segments with large error discrepancies.\n\n    Parameters\n    ----------\n    error_fi : pd.Series\n        Feature Importances of the error model\n    error_model_predicted : pd.Series\n        Predictions of the values of the error model\n    dataset : tabular.Dataset\n        Dataset to create display from\n    model : Optional[Any]\n        Original model for calculating the score on tabular data (Must come with scorer)\n    scorer : Optional[Callable]\n        Scorer to calculate the output values of the segments (Must come with model)\n    max_features_to_show : int\n        Maximum number of features to output.\n    min_feature_contribution : float\n        Minimum value to consider a feature to output.\n    n_display_samples : int\n        Maximum number of values to represent in the display\n    min_segment_size : float\n        Minimum segment size to consider.\n    random_state: int\n        Random seed\n\n    Returns\n    -------\n    Tuple[List, Dict]:\n        List of display elements and Dict of segment description\n    \"\"\"\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)",
        "mutated": [
            "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    if False:\n        i = 10\n    'Calculate and display segments with large error discrepancies.\\n\\n    Parameters\\n    ----------\\n    error_fi : pd.Series\\n        Feature Importances of the error model\\n    error_model_predicted : pd.Series\\n        Predictions of the values of the error model\\n    dataset : tabular.Dataset\\n        Dataset to create display from\\n    model : Optional[Any]\\n        Original model for calculating the score on tabular data (Must come with scorer)\\n    scorer : Optional[Callable]\\n        Scorer to calculate the output values of the segments (Must come with model)\\n    max_features_to_show : int\\n        Maximum number of features to output.\\n    min_feature_contribution : float\\n        Minimum value to consider a feature to output.\\n    n_display_samples : int\\n        Maximum number of values to represent in the display\\n    min_segment_size : float\\n        Minimum segment size to consider.\\n    random_state: int\\n        Random seed\\n\\n    Returns\\n    -------\\n    Tuple[List, Dict]:\\n        List of display elements and Dict of segment description\\n    '\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)",
            "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate and display segments with large error discrepancies.\\n\\n    Parameters\\n    ----------\\n    error_fi : pd.Series\\n        Feature Importances of the error model\\n    error_model_predicted : pd.Series\\n        Predictions of the values of the error model\\n    dataset : tabular.Dataset\\n        Dataset to create display from\\n    model : Optional[Any]\\n        Original model for calculating the score on tabular data (Must come with scorer)\\n    scorer : Optional[Callable]\\n        Scorer to calculate the output values of the segments (Must come with model)\\n    max_features_to_show : int\\n        Maximum number of features to output.\\n    min_feature_contribution : float\\n        Minimum value to consider a feature to output.\\n    n_display_samples : int\\n        Maximum number of values to represent in the display\\n    min_segment_size : float\\n        Minimum segment size to consider.\\n    random_state: int\\n        Random seed\\n\\n    Returns\\n    -------\\n    Tuple[List, Dict]:\\n        List of display elements and Dict of segment description\\n    '\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)",
            "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate and display segments with large error discrepancies.\\n\\n    Parameters\\n    ----------\\n    error_fi : pd.Series\\n        Feature Importances of the error model\\n    error_model_predicted : pd.Series\\n        Predictions of the values of the error model\\n    dataset : tabular.Dataset\\n        Dataset to create display from\\n    model : Optional[Any]\\n        Original model for calculating the score on tabular data (Must come with scorer)\\n    scorer : Optional[Callable]\\n        Scorer to calculate the output values of the segments (Must come with model)\\n    max_features_to_show : int\\n        Maximum number of features to output.\\n    min_feature_contribution : float\\n        Minimum value to consider a feature to output.\\n    n_display_samples : int\\n        Maximum number of values to represent in the display\\n    min_segment_size : float\\n        Minimum segment size to consider.\\n    random_state: int\\n        Random seed\\n\\n    Returns\\n    -------\\n    Tuple[List, Dict]:\\n        List of display elements and Dict of segment description\\n    '\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)",
            "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate and display segments with large error discrepancies.\\n\\n    Parameters\\n    ----------\\n    error_fi : pd.Series\\n        Feature Importances of the error model\\n    error_model_predicted : pd.Series\\n        Predictions of the values of the error model\\n    dataset : tabular.Dataset\\n        Dataset to create display from\\n    model : Optional[Any]\\n        Original model for calculating the score on tabular data (Must come with scorer)\\n    scorer : Optional[Callable]\\n        Scorer to calculate the output values of the segments (Must come with model)\\n    max_features_to_show : int\\n        Maximum number of features to output.\\n    min_feature_contribution : float\\n        Minimum value to consider a feature to output.\\n    n_display_samples : int\\n        Maximum number of values to represent in the display\\n    min_segment_size : float\\n        Minimum segment size to consider.\\n    random_state: int\\n        Random seed\\n\\n    Returns\\n    -------\\n    Tuple[List, Dict]:\\n        List of display elements and Dict of segment description\\n    '\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)",
            "def error_model_display(error_fi: pd.Series, error_model_predicted: pd.Series, dataset: tabular.Dataset, model: Optional[Any], scorer: Optional[Callable], max_features_to_show: int, min_feature_contribution: float, n_display_samples: int, min_segment_size: float, random_state: int, with_display: bool) -> Tuple[List, Dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate and display segments with large error discrepancies.\\n\\n    Parameters\\n    ----------\\n    error_fi : pd.Series\\n        Feature Importances of the error model\\n    error_model_predicted : pd.Series\\n        Predictions of the values of the error model\\n    dataset : tabular.Dataset\\n        Dataset to create display from\\n    model : Optional[Any]\\n        Original model for calculating the score on tabular data (Must come with scorer)\\n    scorer : Optional[Callable]\\n        Scorer to calculate the output values of the segments (Must come with model)\\n    max_features_to_show : int\\n        Maximum number of features to output.\\n    min_feature_contribution : float\\n        Minimum value to consider a feature to output.\\n    n_display_samples : int\\n        Maximum number of values to represent in the display\\n    min_segment_size : float\\n        Minimum segment size to consider.\\n    random_state: int\\n        Random seed\\n\\n    Returns\\n    -------\\n    Tuple[List, Dict]:\\n        List of display elements and Dict of segment description\\n    '\n    n_samples_display = min(n_display_samples, len(dataset))\n    error_col_name = 'Deepchecks model error'\n    display_error = pd.Series(error_model_predicted, name=error_col_name, index=dataset.data.index)\n    display = []\n    value = {'scorer_name': scorer.name if scorer else None, 'feature_segments': {}}\n    weak_color = '#d74949'\n    ok_color = colors['Test']\n    for feature in error_fi.keys()[:max_features_to_show]:\n        if error_fi[feature] < min_feature_contribution:\n            break\n        data = pd.concat([dataset.data[feature], display_error], axis=1)\n        value['feature_segments'][feature] = {}\n        segment1_details = {}\n        segment2_details = {}\n        if feature in dataset.cat_features:\n            error_per_segment_ser = data.groupby(feature).agg(['mean', 'count'])[error_col_name].sort_values('mean', ascending=not scorer)\n            cum_sum_ratio = error_per_segment_ser['count'].cumsum() / error_per_segment_ser['count'].sum()\n            first_weakest_category_to_pass_min_segment_size = np.where(cum_sum_ratio.values >= min_segment_size)[0][0]\n            in_segment_indices = np.arange(len(cum_sum_ratio)) <= first_weakest_category_to_pass_min_segment_size\n            weak_categories = error_per_segment_ser.index[in_segment_indices]\n            ok_categories = error_per_segment_ser.index[~in_segment_indices]\n            if scorer:\n                (ok_name_feature, segment1_details) = get_segment_details(model, scorer, dataset, data[feature].isin(ok_categories))\n            else:\n                (ok_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(ok_categories))\n            if with_display:\n                color_map = {ok_name_feature: ok_color}\n                if len(weak_categories) >= 1:\n                    if scorer:\n                        (weak_name_feature, segment2_details) = get_segment_details(model, scorer, dataset, data[feature].isin(weak_categories))\n                    else:\n                        (weak_name_feature, segment1_details) = get_segment_details_using_error(error_col_name, data, data[feature].isin(weak_categories))\n                    color_map[weak_name_feature] = weak_color\n                else:\n                    weak_name_feature = None\n                replace_dict = {x: weak_name_feature if x in weak_categories else ok_name_feature for x in error_per_segment_ser.index}\n                color_col = data[feature].replace(replace_dict)\n                display.append(px.violin(data, y=error_col_name, x=feature, title=f'Segmentation of error by feature: {feature}', box=False, labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, color=color_col, color_discrete_map=color_map))\n        elif feature in dataset.numerical_features:\n            np.random.seed(random_state)\n            sampling_idx = np.random.choice(range(len(data)), size=n_samples_display, replace=False)\n            data = data.iloc[sampling_idx]\n            tree_partitioner = DecisionTreeRegressor(max_depth=1, min_samples_leaf=min_segment_size + np.finfo(float).eps, random_state=random_state).fit(data[[feature]], data[error_col_name])\n            if len(tree_partitioner.tree_.threshold) > 1:\n                threshold = tree_partitioner.tree_.threshold[0]\n                color_col = data[feature].ge(threshold)\n                sampled_dataset = dataset.data.iloc[sampling_idx]\n                if scorer:\n                    (segment1_text, segment1_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), color_col)\n                    (segment2_text, segment2_details) = get_segment_details(model, scorer, dataset.copy(sampled_dataset), ~color_col)\n                    segment1_ok = segment1_details['score'] >= segment2_details['score']\n                    color_col = color_col.replace([True, False], [segment1_text, segment2_text])\n                else:\n                    (segment1_text, segment1_details) = get_segment_details_using_error(error_col_name, data, ~color_col)\n                    (segment2_text, segment2_details) = get_segment_details_using_error(error_col_name, data, color_col)\n                    segment1_ok = segment1_details['score'] < segment2_details['score']\n                    color_col = color_col.replace([False, True], [segment1_text, segment2_text])\n                if segment1_ok:\n                    color_map = {segment1_text: ok_color, segment2_text: weak_color}\n                    category_order = [segment2_text, segment1_text]\n                else:\n                    color_map = {segment1_text: weak_color, segment2_text: ok_color}\n                    category_order = [segment1_text, segment2_text]\n            else:\n                color_col = data[error_col_name]\n                color_map = None\n                category_order = None\n            if with_display:\n                display.append(px.scatter(data, x=feature, y=error_col_name, color=color_col, title=f'Segmentation of error by the feature: {feature}', labels={error_col_name: 'model error', 'color': 'Weak & OK Segments'}, category_orders={'color': category_order}, color_discrete_map=color_map))\n        if segment1_details:\n            value['feature_segments'][feature]['segment1'] = segment1_details\n        if segment2_details:\n            value['feature_segments'][feature]['segment2'] = segment2_details\n    return (display if with_display else None, value)"
        ]
    },
    {
        "func_name": "get_segment_details",
        "original": "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    \"\"\"Return details about the data segment using the scorer and model.\"\"\"\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
        "mutated": [
            "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n    'Return details about the data segment using the scorer and model.'\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return details about the data segment using the scorer and model.'\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return details about the data segment using the scorer and model.'\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return details about the data segment using the scorer and model.'\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details(model: Any, scorer: Callable, dataset: tabular.Dataset, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return details about the data segment using the scorer and model.'\n    performance = scorer(model, dataset.copy(dataset.data[segment_condition_col.values]))\n    n_samples = dataset.data[segment_condition_col].shape[0]\n    segment_label = f'{scorer.name}: {format_number(performance)}, Samples: {n_samples} ({format_percent(n_samples / len(dataset))})'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)"
        ]
    },
    {
        "func_name": "get_segment_details_using_error",
        "original": "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    \"\"\"Return details about the data segment using the error column.\"\"\"\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
        "mutated": [
            "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n    'Return details about the data segment using the error column.'\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return details about the data segment using the error column.'\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return details about the data segment using the error column.'\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return details about the data segment using the error column.'\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)",
            "def get_segment_details_using_error(error_column_name, dataset: pd.DataFrame, segment_condition_col: pd.Series) -> Tuple[str, Dict[str, float]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return details about the data segment using the error column.'\n    n_samples = dataset[segment_condition_col].shape[0]\n    performance = dataset[segment_condition_col][error_column_name].sum() / n_samples\n    segment_label = f'Error: {format_number(performance)}, ({n_samples} samples - {format_percent(n_samples / len(dataset))} of the dataset)'\n    segment_details = {'score': performance, 'n_samples': n_samples, 'frac_samples': n_samples / len(dataset)}\n    return (segment_label, segment_details)"
        ]
    }
]