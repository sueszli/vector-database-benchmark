[
    {
        "func_name": "__init__",
        "original": "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    \"\"\"Ctor.\"\"\"\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
        "mutated": [
            "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n    'Ctor.'\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Ctor.'\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Ctor.'\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Ctor.'\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)",
            "def __init__(self, temperature=1.0, proj_grad=True, euclidean=False, cheap=False, lrs=(0.01,), exp_thresh=-1.0, vr=True, rnd_init=False, seed=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Ctor.'\n    del kwargs\n    if temperature < 0.0:\n        raise ValueError('temperature must be non-negative')\n    self.num_players = None\n    self.temperature = temperature\n    self.proj_grad = proj_grad\n    self.cheap = cheap\n    self.vr = vr\n    self.pm_vr = None\n    self.rnd_init = rnd_init\n    self.lrs = lrs\n    self.exp_thresh = exp_thresh\n    self.has_aux = False\n    self.euclidean = euclidean\n    if euclidean:\n        self.update = self.euc_descent_step\n    else:\n        self.update = self.mirror_descent_step\n    self.seed = seed\n    self.random = np.random.RandomState(seed)"
        ]
    },
    {
        "func_name": "init_vars",
        "original": "def init_vars(self, num_strats, num_players):\n    \"\"\"Initialize solver parameters.\"\"\"\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)",
        "mutated": [
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)",
            "def init_vars(self, num_strats, num_players):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize solver parameters.'\n    self.num_players = num_players\n    if self.rnd_init:\n        init_dist = self.random.rand(num_strats)\n    else:\n        init_dist = np.ones(num_strats)\n    init_dist /= init_dist.sum()\n    init_anneal_steps = 0\n    if self.cheap and self.vr:\n        self.pm_vr = np.zeros((num_strats, num_strats))\n    return (init_dist, init_anneal_steps)"
        ]
    },
    {
        "func_name": "compute_gradients",
        "original": "def compute_gradients(self, params, payoff_matrices):\n    \"\"\"Compute and return gradients (and exploitabilities) for all parameters.\n\n    Args:\n      params: tuple of params (dist, anneal_steps), see gradients\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n    Returns:\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)",
        "mutated": [
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)",
            "def compute_gradients(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute and return gradients (and exploitabilities) for all parameters.\\n\\n    Args:\\n      params: tuple of params (dist, anneal_steps), see gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      tuple of gradients (grad_dist, grad_anneal_steps), see gradients\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    if self.cheap and self.vr:\n        (grads, pm_vr, exp_sto, exp_solver_sto) = self.cheap_gradients_vr(self.random, *params, payoff_matrices, self.num_players, self.pm_vr, self.temperature, self.proj_grad)\n        self.pm_vr = pm_vr\n        return (grads, exp_sto, exp_solver_sto)\n    elif self.cheap and (not self.vr):\n        return self.cheap_gradients(self.random, *params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)\n    else:\n        return self.gradients(*params, payoff_matrices, self.num_players, self.temperature, self.proj_grad)"
        ]
    },
    {
        "func_name": "exploitability",
        "original": "def exploitability(self, params, payoff_matrices):\n    \"\"\"Compute and return shannon entropy regularized exploitability.\n\n    Args:\n      params: tuple of params (dist, y), see qre.gradients\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n    Returns:\n      float, exploitability of current dist\n    \"\"\"\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)",
        "mutated": [
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n    'Compute and return shannon entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see qre.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute and return shannon entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see qre.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute and return shannon entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see qre.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute and return shannon entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see qre.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)",
            "def exploitability(self, params, payoff_matrices):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute and return shannon entropy regularized exploitability.\\n\\n    Args:\\n      params: tuple of params (dist, y), see qre.gradients\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n    Returns:\\n      float, exploitability of current dist\\n    '\n    return exp.qre_exploitability(params, payoff_matrices, self.temperature)"
        ]
    },
    {
        "func_name": "gradients",
        "original": "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    \"\"\"Computes exploitablity gradient and aux variable gradients.\n\n    Args:\n      dist: 1-d np.array, current estimate of nash distribution\n      anneal_steps: int, elapsed num steps since last anneal\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n      num_players: int, number of players, in case payoff_matrices is\n        abbreviated\n      temperature: non-negative float, default 0.\n      proj_grad: bool, if True, projects dist gradient onto simplex\n    Returns:\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)",
        "mutated": [
            "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n    'Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)",
            "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)",
            "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)",
            "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)",
            "def gradients(self, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes exploitablity gradient and aux variable gradients.\\n\\n    Args:\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    y = nabla = payoff_matrices[0].dot(dist)\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        log_br_safe = np.clip(np.log(br), -100000.0, 0)\n        br_policy_gradient = nabla - temperature * (log_br_safe + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = np.array(nabla)\n    if temperature > 0:\n        log_dist_safe = np.clip(np.log(dist), -100000.0, 0)\n        policy_gradient -= temperature * (log_dist_safe + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    other_player_fx_translated = payoff_matrices[1].dot(other_player_fx)\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if reg_exp < self.exp_thresh:\n        self.temperature = np.clip(temperature / 2.0, 0.0, np.inf)\n        if self.temperature < 0.001:\n            self.temperature = 0.0\n        grad_anneal_steps = -anneal_steps\n    else:\n        grad_anneal_steps = 1\n    return ((grad_dist, grad_anneal_steps), unreg_exp, reg_exp)"
        ]
    },
    {
        "func_name": "cheap_gradients",
        "original": "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    \"\"\"Computes exploitablity gradient and aux variable gradients with samples.\n\n    This implementation takes payoff_matrices as input so technically uses\n    O(d^2) compute but only a single column of payoff_matrices is used to\n    perform the update so can be re-implemented in O(d) if needed.\n\n    Args:\n      random: random number generator, np.random.RandomState(seed)\n      dist: 1-d np.array, current estimate of nash distribution\n      anneal_steps: int, elapsed num steps since last anneal\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n      num_players: int, number of players, in case payoff_matrices is\n        abbreviated\n      temperature: non-negative float, default 0.\n      proj_grad: bool, if True, projects dist gradient onto simplex\n    Returns:\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)",
        "mutated": [
            "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)",
            "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)",
            "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)",
            "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)",
            "def cheap_gradients(self, random, dist, anneal_steps, payoff_matrices, num_players, temperature=0.0, proj_grad=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    action_u = random.choice(dist.size)\n    other_player_fx = dist.size * other_player_fx[action_u]\n    other_player_fx_translat = payoff_matrices[1, :, action_u] * other_player_fx\n    grad_dist = -policy_gradient + (num_players - 1) * other_player_fx_translat\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    return ((grad_dist, None), unreg_exp, reg_exp)"
        ]
    },
    {
        "func_name": "cheap_gradients_vr",
        "original": "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    \"\"\"Computes exploitablity gradient and aux variable gradients with samples.\n\n    This implementation takes payoff_matrices as input so technically uses\n    O(d^2) compute but only a single column of payoff_matrices is used to\n    perform the update so can be re-implemented in O(d) if needed.\n\n    Args:\n      random: random number generator, np.random.RandomState(seed)\n      dist: 1-d np.array, current estimate of nash distribution\n      anneal_steps: int, elapsed num steps since last anneal\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\n      num_players: int, number of players, in case payoff_matrices is\n        abbreviated\n      pm_vr: approximate payoff_matrix for variance reduction\n      temperature: non-negative float, default 0.\n      proj_grad: bool, if True, projects dist gradient onto simplex\n      version: int, default 0, two options for variance reduction\n    Returns:\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\n      unregularized exploitability (stochastic estimate)\n      tsallis regularized exploitability (stochastic estimate)\n    \"\"\"\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)",
        "mutated": [
            "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    if False:\n        i = 10\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      pm_vr: approximate payoff_matrix for variance reduction\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n      version: int, default 0, two options for variance reduction\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)",
            "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      pm_vr: approximate payoff_matrix for variance reduction\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n      version: int, default 0, two options for variance reduction\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)",
            "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      pm_vr: approximate payoff_matrix for variance reduction\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n      version: int, default 0, two options for variance reduction\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)",
            "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      pm_vr: approximate payoff_matrix for variance reduction\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n      version: int, default 0, two options for variance reduction\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)",
            "def cheap_gradients_vr(self, random, dist, anneal_steps, payoff_matrices, num_players, pm_vr, temperature=0.0, proj_grad=True, version=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes exploitablity gradient and aux variable gradients with samples.\\n\\n    This implementation takes payoff_matrices as input so technically uses\\n    O(d^2) compute but only a single column of payoff_matrices is used to\\n    perform the update so can be re-implemented in O(d) if needed.\\n\\n    Args:\\n      random: random number generator, np.random.RandomState(seed)\\n      dist: 1-d np.array, current estimate of nash distribution\\n      anneal_steps: int, elapsed num steps since last anneal\\n      payoff_matrices: (>=2 x A x A) np.array, payoffs for each joint action\\n      num_players: int, number of players, in case payoff_matrices is\\n        abbreviated\\n      pm_vr: approximate payoff_matrix for variance reduction\\n      temperature: non-negative float, default 0.\\n      proj_grad: bool, if True, projects dist gradient onto simplex\\n      version: int, default 0, two options for variance reduction\\n    Returns:\\n      gradient of exploitability w.r.t. (dist, anneal_steps) as tuple\\n      unregularized exploitability (stochastic estimate)\\n      tsallis regularized exploitability (stochastic estimate)\\n    '\n    del anneal_steps\n    if pm_vr is None:\n        raise ValueError('pm_vr must be np.array of shape (num_strats,) * 2')\n    if not isinstance(version, int) or version < 0 or version > 1:\n        raise ValueError('version must be non-negative int < 2')\n    action_1 = random.choice(dist.size, p=dist)\n    y = nabla = payoff_matrices[0][:, action_1]\n    if temperature >= 0.001:\n        br = special.softmax(y / temperature)\n        br_mat = (np.diag(br) - np.outer(br, br)) / temperature\n        br_policy_gradient = nabla - temperature * (np.log(br) + 1)\n    else:\n        power = np.inf\n        s = np.linalg.norm(y, ord=power)\n        br = np.zeros_like(dist)\n        maxima = y == s\n        br[maxima] = 1.0 / maxima.sum()\n        br_mat = np.zeros((br.size, br.size))\n        br_policy_gradient = np.zeros_like(br)\n    unreg_exp = np.max(y) - y.dot(dist)\n    entr_br = temperature * special.entr(br).sum()\n    entr_dist = temperature * special.entr(dist).sum()\n    reg_exp = y.dot(br - dist) + entr_br - entr_dist\n    policy_gradient = nabla - temperature * (np.log(dist) + 1)\n    other_player_fx = br - dist + br_mat.dot(br_policy_gradient)\n    if version == 0:\n        other_player_fx_translated = pm_vr.dot(other_player_fx)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        m = dist.size\n        pm_mod = m * (payoff_matrices[1, :, action_u] - pm_vr[:, action_u])\n        other_player_fx_translated += pm_mod * other_player_fx\n    elif version == 1:\n        other_player_fx_translated = np.sum(pm_vr, axis=1)\n        action_u = random.choice(dist.size)\n        other_player_fx = other_player_fx[action_u]\n        pm_mod = dist.size * payoff_matrices[1, :, action_u]\n        r = dist.size * pm_vr[:, action_u]\n        other_player_fx_translated += pm_mod * other_player_fx - r\n    grad_dist = -policy_gradient\n    grad_dist += (num_players - 1) * other_player_fx_translated\n    if proj_grad:\n        grad_dist = simplex.project_grad(grad_dist)\n    if version == 0:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u]\n    elif version == 1:\n        pm_vr[:, action_u] = payoff_matrices[1, :, action_u] * other_player_fx\n    return ((grad_dist, None), pm_vr, unreg_exp, reg_exp)"
        ]
    },
    {
        "func_name": "euc_descent_step",
        "original": "def euc_descent_step(self, params, grads, t):\n    \"\"\"Projected gradient descent on exploitability using Euclidean projection.\n\n    Args:\n      params: tuple of variables to be updated (dist, anneal_steps)\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\n      t: int, solver iteration\n    Returns:\n      new_params: tuple of update params (new_dist, new_anneal_steps)\n    \"\"\"\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
        "mutated": [
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def euc_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Projected gradient descent on exploitability using Euclidean projection.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [params[0] - lr_dist * grads[0]]\n    new_params = euc_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params"
        ]
    },
    {
        "func_name": "mirror_descent_step",
        "original": "def mirror_descent_step(self, params, grads, t):\n    \"\"\"Entropic mirror descent on exploitability.\n\n    Args:\n      params: tuple of variables to be updated (dist, anneal_steps)\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\n      t: int, solver iteration\n    Returns:\n      new_params: tuple of update params (new_dist, new_anneal_steps)\n    \"\"\"\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
        "mutated": [
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params",
            "def mirror_descent_step(self, params, grads, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Entropic mirror descent on exploitability.\\n\\n    Args:\\n      params: tuple of variables to be updated (dist, anneal_steps)\\n      grads: tuple of variable gradients (grad_dist, grad_anneal_steps)\\n      t: int, solver iteration\\n    Returns:\\n      new_params: tuple of update params (new_dist, new_anneal_steps)\\n    '\n    del t\n    lr_dist = self.lrs[0]\n    new_params = [np.log(np.clip(params[0], 0, np.inf)) - lr_dist * grads[0]]\n    new_params = mirror_project(*new_params)\n    new_params += (params[1] + grads[1],)\n    return new_params"
        ]
    },
    {
        "func_name": "euc_project",
        "original": "def euc_project(dist):\n    \"\"\"Project variables onto their feasible sets (euclidean proj for dist).\n\n  Args:\n    dist: 1-d np.array, current estimate of nash distribution\n  Returns:\n    projected variables (dist,) as tuple\n  \"\"\"\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)",
        "mutated": [
            "def euc_project(dist):\n    if False:\n        i = 10\n    'Project variables onto their feasible sets (euclidean proj for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)",
            "def euc_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Project variables onto their feasible sets (euclidean proj for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)",
            "def euc_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Project variables onto their feasible sets (euclidean proj for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)",
            "def euc_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Project variables onto their feasible sets (euclidean proj for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)",
            "def euc_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Project variables onto their feasible sets (euclidean proj for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = simplex.euclidean_projection_onto_simplex(dist)\n    return (dist,)"
        ]
    },
    {
        "func_name": "mirror_project",
        "original": "def mirror_project(dist):\n    \"\"\"Project variables onto their feasible sets (softmax for dist).\n\n  Args:\n    dist: 1-d np.array, current estimate of nash distribution\n  Returns:\n    projected variables (dist,) as tuple\n  \"\"\"\n    dist = special.softmax(dist)\n    return (dist,)",
        "mutated": [
            "def mirror_project(dist):\n    if False:\n        i = 10\n    'Project variables onto their feasible sets (softmax for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = special.softmax(dist)\n    return (dist,)",
            "def mirror_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Project variables onto their feasible sets (softmax for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = special.softmax(dist)\n    return (dist,)",
            "def mirror_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Project variables onto their feasible sets (softmax for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = special.softmax(dist)\n    return (dist,)",
            "def mirror_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Project variables onto their feasible sets (softmax for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = special.softmax(dist)\n    return (dist,)",
            "def mirror_project(dist):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Project variables onto their feasible sets (softmax for dist).\\n\\n  Args:\\n    dist: 1-d np.array, current estimate of nash distribution\\n  Returns:\\n    projected variables (dist,) as tuple\\n  '\n    dist = special.softmax(dist)\n    return (dist,)"
        ]
    }
]