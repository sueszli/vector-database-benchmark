[
    {
        "func_name": "map_fields",
        "original": "def map_fields(field: str) -> dict:\n    return {'name': field}",
        "mutated": [
            "def map_fields(field: str) -> dict:\n    if False:\n        i = 10\n    return {'name': field}",
            "def map_fields(field: str) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'name': field}",
            "def map_fields(field: str) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'name': field}",
            "def map_fields(field: str) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'name': field}",
            "def map_fields(field: str) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'name': field}"
        ]
    },
    {
        "func_name": "get_values",
        "original": "def get_values(obj: types.Value) -> str:\n    return str(obj.string_value)",
        "mutated": [
            "def get_values(obj: types.Value) -> str:\n    if False:\n        i = 10\n    return str(obj.string_value)",
            "def get_values(obj: types.Value) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return str(obj.string_value)",
            "def get_values(obj: types.Value) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return str(obj.string_value)",
            "def get_values(obj: types.Value) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return str(obj.string_value)",
            "def get_values(obj: types.Value) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return str(obj.string_value)"
        ]
    },
    {
        "func_name": "k_anonymity_with_entity_id",
        "original": "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    \"\"\"Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\n        of a column set in a Google BigQuery table.\n    Args:\n        project: The Google Cloud project id to use as a parent resource.\n        source_table_project_id: The Google Cloud project id where the BigQuery table\n            is stored.\n        source_dataset_id: The id of the dataset to inspect.\n        source_table_id: The id of the table to inspect.\n        entity_id: The column name of the table that enables accurately determining k-anonymity\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\n         information.\n        quasi_ids: A set of columns that form a composite key.\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\n            is stored.\n        output_dataset_id: The id of the output BigQuery dataset.\n        output_table_id: The id of the output BigQuery table.\n    \"\"\"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')",
        "mutated": [
            "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    if False:\n        i = 10\n    'Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\\n        of a column set in a Google BigQuery table.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        source_table_project_id: The Google Cloud project id where the BigQuery table\\n            is stored.\\n        source_dataset_id: The id of the dataset to inspect.\\n        source_table_id: The id of the table to inspect.\\n        entity_id: The column name of the table that enables accurately determining k-anonymity\\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\\n         information.\\n        quasi_ids: A set of columns that form a composite key.\\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\\n            is stored.\\n        output_dataset_id: The id of the output BigQuery dataset.\\n        output_table_id: The id of the output BigQuery table.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')",
            "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\\n        of a column set in a Google BigQuery table.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        source_table_project_id: The Google Cloud project id where the BigQuery table\\n            is stored.\\n        source_dataset_id: The id of the dataset to inspect.\\n        source_table_id: The id of the table to inspect.\\n        entity_id: The column name of the table that enables accurately determining k-anonymity\\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\\n         information.\\n        quasi_ids: A set of columns that form a composite key.\\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\\n            is stored.\\n        output_dataset_id: The id of the output BigQuery dataset.\\n        output_table_id: The id of the output BigQuery table.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')",
            "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\\n        of a column set in a Google BigQuery table.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        source_table_project_id: The Google Cloud project id where the BigQuery table\\n            is stored.\\n        source_dataset_id: The id of the dataset to inspect.\\n        source_table_id: The id of the table to inspect.\\n        entity_id: The column name of the table that enables accurately determining k-anonymity\\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\\n         information.\\n        quasi_ids: A set of columns that form a composite key.\\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\\n            is stored.\\n        output_dataset_id: The id of the output BigQuery dataset.\\n        output_table_id: The id of the output BigQuery table.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')",
            "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\\n        of a column set in a Google BigQuery table.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        source_table_project_id: The Google Cloud project id where the BigQuery table\\n            is stored.\\n        source_dataset_id: The id of the dataset to inspect.\\n        source_table_id: The id of the table to inspect.\\n        entity_id: The column name of the table that enables accurately determining k-anonymity\\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\\n         information.\\n        quasi_ids: A set of columns that form a composite key.\\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\\n            is stored.\\n        output_dataset_id: The id of the output BigQuery dataset.\\n        output_table_id: The id of the output BigQuery table.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')",
            "def k_anonymity_with_entity_id(project: str, source_table_project_id: str, source_dataset_id: str, source_table_id: str, entity_id: str, quasi_ids: List[str], output_table_project_id: str, output_dataset_id: str, output_table_id: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Uses the Data Loss Prevention API to compute the k-anonymity using entity_id\\n        of a column set in a Google BigQuery table.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        source_table_project_id: The Google Cloud project id where the BigQuery table\\n            is stored.\\n        source_dataset_id: The id of the dataset to inspect.\\n        source_table_id: The id of the table to inspect.\\n        entity_id: The column name of the table that enables accurately determining k-anonymity\\n         in the common scenario wherein several rows of dataset correspond to the same sensitive\\n         information.\\n        quasi_ids: A set of columns that form a composite key.\\n        output_table_project_id: The Google Cloud project id where the output BigQuery table\\n            is stored.\\n        output_dataset_id: The id of the output BigQuery dataset.\\n        output_table_id: The id of the output BigQuery table.\\n    '\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    source_table = {'project_id': source_table_project_id, 'dataset_id': source_dataset_id, 'table_id': source_table_id}\n    dest_table = {'project_id': output_table_project_id, 'dataset_id': output_dataset_id, 'table_id': output_table_id}\n\n    def map_fields(field: str) -> dict:\n        return {'name': field}\n    quasi_ids = map(map_fields, quasi_ids)\n    actions = [{'save_findings': {'output_config': {'table': dest_table}}}]\n    privacy_metric = {'k_anonymity_config': {'entity_id': {'field': {'name': entity_id}}, 'quasi_ids': quasi_ids}}\n    risk_job = {'privacy_metric': privacy_metric, 'source_table': source_table, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    response = dlp.create_dlp_job(request={'parent': parent, 'risk_job': risk_job})\n    job_name = response.name\n    print(f'Inspection Job started : {job_name}')\n    job = dlp.get_dlp_job(request={'name': job_name})\n    no_of_attempts = 30\n    while no_of_attempts > 0:\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.DONE:\n            break\n        if job.state == google.cloud.dlp_v2.DlpJob.JobState.FAILED:\n            print('Job Failed, Please check the configuration.')\n            return\n        time.sleep(30)\n        no_of_attempts -= 1\n        job = dlp.get_dlp_job(request={'name': job_name})\n    if job.state != google.cloud.dlp_v2.DlpJob.JobState.DONE:\n        print('Job did not complete within 15 minutes.')\n        return\n\n    def get_values(obj: types.Value) -> str:\n        return str(obj.string_value)\n    print(f'Job name: {job.name}')\n    histogram_buckets = job.risk_details.k_anonymity_result.equivalence_class_histogram_buckets\n    for (i, bucket) in enumerate(histogram_buckets):\n        print(f'Bucket {i}:')\n        if bucket.equivalence_class_size_lower_bound:\n            print(f'Bucket size range: [{bucket.equivalence_class_size_lower_bound}, {bucket.equivalence_class_size_upper_bound}]')\n            for value_bucket in bucket.bucket_values:\n                print(f'Quasi-ID values: {get_values(value_bucket.quasi_ids_values[0])}')\n                print(f'Class size: {value_bucket.equivalence_class_size}')\n        else:\n            print('No findings.')"
        ]
    }
]