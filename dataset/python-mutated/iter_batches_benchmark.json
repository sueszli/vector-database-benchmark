[
    {
        "func_name": "iter_batches",
        "original": "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
        "mutated": [
            "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_batches(ds: Dataset, block_format: Literal['pandas', 'pyarrow', 'simple']='pyarrow', batch_size: Optional[int]=None, batch_format: Literal['default', 'pandas', 'pyarrow', 'numpy']='default', local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_batches(batch_format=batch_format, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size):\n            num_batches += 1\n    print('iter_batches done, block_format:', block_format, 'batch_format:', batch_format, 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds"
        ]
    },
    {
        "func_name": "run_iter_batches_benchmark",
        "original": "def run_iter_batches_benchmark(benchmark: Benchmark):\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)",
        "mutated": [
            "def run_iter_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)",
            "def run_iter_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)",
            "def run_iter_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)",
            "def run_iter_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)",
            "def run_iter_batches_benchmark(benchmark: Benchmark):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ds = ray.data.read_parquet('s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01').repartition(12).materialize()\n    batch_formats = ['pandas', 'numpy']\n    batch_sizes = [4 * 1024, 16 * 1024, 64 * 1024]\n    test_name = 'iter-batches-default'\n    benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, use_default_params=True)\n    for current_format in ['pyarrow', 'pandas']:\n        new_ds = ds.map_batches(lambda ds: ds, batch_format=current_format, batch_size=None).materialize()\n        for new_format in ['pyarrow', 'pandas', 'numpy']:\n            for batch_size in batch_sizes:\n                test_name = f'iter-batches-conversion-{current_format}-to-{new_format}-{batch_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format=new_format, block_format=current_format, batch_size=batch_size)\n    for batch_format in batch_formats:\n        for batch_size in batch_sizes:\n            for shuffle_buffer_size in [batch_size, 2 * batch_size, 4 * batch_size]:\n                test_name = f'iter-batches-shuffle-{batch_format}-{batch_size}-{shuffle_buffer_size}'\n                benchmark.run_materialize_ds(test_name, iter_batches, ds=ds, batch_size=batch_size, batch_format=batch_format, local_shuffle_buffer_size=shuffle_buffer_size)\n    new_ds = ds.repartition(512)\n    new_ds = new_ds.map_batches(lambda ds: ds, batch_format='pandas', batch_size=None).materialize()\n    for batch_size in [32 * 1024, 64 * 1024, 256 * 1024]:\n        test_name = f'iter-batches-block-concat-to-batch-{batch_size}'\n        benchmark.run_materialize_ds(test_name, iter_batches, ds=new_ds, batch_format='pandas', block_format='pandas', batch_size=batch_size)"
        ]
    }
]