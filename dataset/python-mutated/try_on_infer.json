[
    {
        "func_name": "load_checkpoint",
        "original": "def load_checkpoint(model, checkpoint_path, device):\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model",
        "mutated": [
            "def load_checkpoint(model, checkpoint_path, device):\n    if False:\n        i = 10\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model",
            "def load_checkpoint(model, checkpoint_path, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model",
            "def load_checkpoint(model, checkpoint_path, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model",
            "def load_checkpoint(model, checkpoint_path, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model",
            "def load_checkpoint(model, checkpoint_path, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    params = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(params, strict=False)\n    model.to(device)\n    model.eval()\n    return model"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)",
        "mutated": [
            "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    if False:\n        i = 10\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)",
            "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)",
            "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)",
            "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)",
            "def __init__(self, model_dir, device_id=0, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(*args, model_dir=model_dir, device_id=device_id, **kwargs)\n    if torch.cuda.is_available():\n        self.device = 'cuda'\n        logger.info('Use GPU')\n    else:\n        self.device = 'cpu'\n        logger.info('Use CPU')\n    self.model = VTONGenerator(12, 3, 5, ngf=96, norm_layer=nn.BatchNorm2d)\n    self.model = load_checkpoint(self.model, model_dir + '/' + ModelFile.TORCH_MODEL_BIN_FILE, self.device)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x, y):\n    pred_result = self.model(x, y)\n    return pred_result",
        "mutated": [
            "def forward(self, x, y):\n    if False:\n        i = 10\n    pred_result = self.model(x, y)\n    return pred_result",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pred_result = self.model(x, y)\n    return pred_result",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pred_result = self.model(x, y)\n    return pred_result",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pred_result = self.model(x, y)\n    return pred_result",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pred_result = self.model(x, y)\n    return pred_result"
        ]
    },
    {
        "func_name": "infer",
        "original": "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr",
        "mutated": [
            "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    if False:\n        i = 10\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr",
            "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr",
            "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr",
            "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr",
            "def infer(ourgen_model, model_path, person_img, garment_img, mask_img, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ourwarp_model = Warping()\n    landmark_model = VTONLandmark()\n    ourwarp_model = load_checkpoint(ourwarp_model, model_path + '/warp.pth', device)\n    landmark_model.load_state_dict(torch.load(model_path + '/landmark.pth', map_location=device))\n    landmark_model.to(device).eval()\n    input_scale = 4\n    with torch.no_grad():\n        garment_img = cv2.imread(garment_img)\n        garment_img = cv2.cvtColor(garment_img, cv2.COLOR_BGR2RGB)\n        clothes = cv2.resize(garment_img, (768, 1024))\n        mask_img = cv2.imread(mask_img)\n        person_img = cv2.imread(person_img)\n        person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n        cm = mask_img[:, :, 0]\n        input_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        clothes = input_transform(clothes).unsqueeze(0).to(device)\n        cm_array = np.array(cm)\n        cm_array = (cm_array >= 128).astype(np.float32)\n        cm = torch.from_numpy(cm_array)\n        cm = cm.unsqueeze(0).unsqueeze(0)\n        cm = torch.FloatTensor((cm.numpy() > 0.5).astype(float)).to(device)\n        im = person_img\n        (h_ori, w_ori) = im.shape[0:2]\n        im = cv2.resize(im, (768, 1024))\n        im = input_transform(im).unsqueeze(0).to(device)\n        (h, w) = (512, 384)\n        p_down = F.interpolate(im, size=(h, w), mode='bilinear')\n        c_down = F.interpolate(clothes, size=(h, w), mode='bilinear')\n        (c_heatmap, c_property, p_heatmap, p_property) = landmark_model(c_down, p_down)\n        N = c_heatmap.shape[0]\n        paired_cloth = clothes[0].cpu()\n        color_map = {'1': (0, 0, 255), '0': (255, 0, 0)}\n        c_im = (np.array(paired_cloth.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        c_im = cv2.cvtColor(c_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(c_property, dim=1)\n        point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(c_im, (x, y), 2, point_color, 4)\n            cv2.putText(c_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        paired_im = im[0].cpu()\n        color_map = {'2': (0, 0, 255), '1': (0, 255, 0), '0': (255, 0, 0)}\n        p_im = (np.array(paired_im.permute(1, 2, 0)).copy() + 1) / 2 * 255\n        p_im = cv2.cvtColor(p_im, cv2.COLOR_RGB2BGR)\n        pred_class = torch.argmax(p_property, dim=1)\n        point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (pred_y, pred_x) = (8 * (point_ind // 96), 8 * (point_ind % 96))\n        for ind in range(32):\n            point_class = int(pred_class[0, ind])\n            if point_class < 0.9:\n                continue\n            point_color = color_map[str(point_class)]\n            (y, x) = (pred_y[0][ind], pred_x[0][ind])\n            cv2.circle(p_im, (x, y), 2, point_color, 4)\n            cv2.putText(p_im, str(ind), (x + 4, y + 4), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.75, color=point_color, thickness=1)\n        valid_c_point = np.zeros((32, 2)).astype(np.float32)\n        valid_p_point = np.zeros((32, 2)).astype(np.float32)\n        c_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        p_point_heatmap = -1 * torch.ones(32, 1024, 768)\n        (cloth_property, person_property) = (torch.argmax(c_property, dim=1), torch.argmax(p_property, dim=1))\n        cloth_point_ind = torch.argmax(c_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (cloth_y, cloth_x) = (8 * (cloth_point_ind // 96), 8 * (cloth_point_ind % 96))\n        person_point_ind = torch.argmax(p_heatmap.view(N, 32, -1), dim=2).cpu().numpy()\n        (person_y, person_x) = (8 * (person_point_ind // 96), 8 * (person_point_ind % 96))\n        r = 20\n        for k in range(32):\n            (property_c, property_p) = (cloth_property[0, k], person_property[0, k] - 1)\n            if property_c > 0.1:\n                (c_x, c_y) = (cloth_x[0, k], cloth_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(c_x - r - 1, 0), max(c_y - r - 1, 0), min(c_x + r, 768), min(c_y + r, 1024))\n                c_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_c)\n                (valid_c_point[k, 0], valid_c_point[k, 1]) = (c_x, c_y)\n            if property_p > -0.99:\n                (p_x, p_y) = (person_x[0, k], person_y[0, k])\n                (x_min, y_min, x_max, y_max) = (max(p_x - r - 1, 0), max(p_y - r - 1, 0), min(p_x + r, 768), min(p_y + r, 1024))\n                p_point_heatmap[k, y_min:y_max, x_min:x_max] = torch.tensor(property_p)\n                if property_p > 0:\n                    (valid_p_point[k, 0], valid_p_point[k, 1]) = (p_x, p_y)\n        c_point_plane = torch.tensor(valid_c_point).unsqueeze(0).to(device)\n        p_point_plane = torch.tensor(valid_p_point).unsqueeze(0).to(device)\n        c_point_heatmap = c_point_heatmap.unsqueeze(0).to(device)\n        p_point_heatmap = p_point_heatmap.unsqueeze(0).to(device)\n        if input_scale > 1:\n            (h, w) = (1024 // input_scale, 768 // input_scale)\n            c_point_plane = c_point_plane // input_scale\n            p_point_plane = p_point_plane // input_scale\n            c_point_heatmap = F.interpolate(c_point_heatmap, size=(h, w), mode='nearest')\n            p_point_heatmap = F.interpolate(p_point_heatmap, size=(h, w), mode='nearest')\n            im_down = F.interpolate(im, size=(h, w), mode='bilinear')\n            c_down = F.interpolate(cm * clothes, size=(h, w), mode='bilinear')\n            cm_down = F.interpolate(cm, size=(h, w), mode='nearest')\n        warping_input = [c_down, im_down, c_point_heatmap, p_point_heatmap, c_point_plane, p_point_plane, cm_down, cm * clothes, device]\n        (final_warped_cloth, last_flow, last_flow_all, flow_all, delta_list, x_all, x_edge_all, delta_x_all, delta_y_all, local_warped_cloth_list, fuse_cloth, globalmap, up_cloth) = ourwarp_model(warping_input)\n        gen_inputs = torch.cat([im, up_cloth], 1)\n        gen_outputs = ourgen_model(gen_inputs, p_point_heatmap)\n        combine = torch.cat([gen_outputs[0]], 2).squeeze()\n        cv_img = (combine.permute(1, 2, 0).detach().cpu().numpy() + 1) / 2\n        rgb = (cv_img * 255).astype(np.uint8)\n        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n        bgr = cv2.resize(bgr, (w_ori, h_ori))\n    return bgr"
        ]
    }
]