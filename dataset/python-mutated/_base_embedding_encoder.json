[
    {
        "func_name": "embed_queries",
        "original": "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    \"\"\"\n        Create embeddings for a list of queries.\n\n        :param queries: List of queries to embed.\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\n        \"\"\"\n    pass",
        "mutated": [
            "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Create embeddings for a list of queries.\\n\\n        :param queries: List of queries to embed.\\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create embeddings for a list of queries.\\n\\n        :param queries: List of queries to embed.\\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create embeddings for a list of queries.\\n\\n        :param queries: List of queries to embed.\\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create embeddings for a list of queries.\\n\\n        :param queries: List of queries to embed.\\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_queries(self, queries: List[str]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create embeddings for a list of queries.\\n\\n        :param queries: List of queries to embed.\\n        :return: Embeddings, one per input query, shape: (queries, embedding_dim)\\n        '\n    pass"
        ]
    },
    {
        "func_name": "embed_documents",
        "original": "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    \"\"\"\n        Create embeddings for a list of documents.\n\n        :param docs: List of documents to embed.\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\n        \"\"\"\n    pass",
        "mutated": [
            "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Create embeddings for a list of documents.\\n\\n        :param docs: List of documents to embed.\\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create embeddings for a list of documents.\\n\\n        :param docs: List of documents to embed.\\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create embeddings for a list of documents.\\n\\n        :param docs: List of documents to embed.\\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create embeddings for a list of documents.\\n\\n        :param docs: List of documents to embed.\\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\\n        '\n    pass",
            "@abstractmethod\ndef embed_documents(self, docs: List[Document]) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create embeddings for a list of documents.\\n\\n        :param docs: List of documents to embed.\\n        :return: Embeddings, one per input document, shape: (documents, embedding_dim)\\n        '\n    pass"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    \"\"\"\n        Trains or adapts the underlying embedding model.\n\n        Each training data example is a dictionary with the following keys:\n\n        * question: The question string.\n        * pos_doc: Positive document string (the document containing the answer).\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\n        * score: The score margin the answer must fall within.\n\n\n        :param training_data: The training data in a dictionary format. Required.\n        :type training_data: List[Dict[str, Any]]\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\n        :type learning_rate: float\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\n        :type n_epochs: int\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\n        :type num_warmup_steps: int\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\n        :type batch_size: int (optional)\n        \"\"\"\n    pass",
        "mutated": [
            "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    if False:\n        i = 10\n    \"\\n        Trains or adapts the underlying embedding model.\\n\\n        Each training data example is a dictionary with the following keys:\\n\\n        * question: The question string.\\n        * pos_doc: Positive document string (the document containing the answer).\\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\\n        * score: The score margin the answer must fall within.\\n\\n\\n        :param training_data: The training data in a dictionary format. Required.\\n        :type training_data: List[Dict[str, Any]]\\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\\n        :type learning_rate: float\\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\\n        :type n_epochs: int\\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\\n        :type num_warmup_steps: int\\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\\n        :type batch_size: int (optional)\\n        \"\n    pass",
            "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Trains or adapts the underlying embedding model.\\n\\n        Each training data example is a dictionary with the following keys:\\n\\n        * question: The question string.\\n        * pos_doc: Positive document string (the document containing the answer).\\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\\n        * score: The score margin the answer must fall within.\\n\\n\\n        :param training_data: The training data in a dictionary format. Required.\\n        :type training_data: List[Dict[str, Any]]\\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\\n        :type learning_rate: float\\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\\n        :type n_epochs: int\\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\\n        :type num_warmup_steps: int\\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\\n        :type batch_size: int (optional)\\n        \"\n    pass",
            "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Trains or adapts the underlying embedding model.\\n\\n        Each training data example is a dictionary with the following keys:\\n\\n        * question: The question string.\\n        * pos_doc: Positive document string (the document containing the answer).\\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\\n        * score: The score margin the answer must fall within.\\n\\n\\n        :param training_data: The training data in a dictionary format. Required.\\n        :type training_data: List[Dict[str, Any]]\\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\\n        :type learning_rate: float\\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\\n        :type n_epochs: int\\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\\n        :type num_warmup_steps: int\\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\\n        :type batch_size: int (optional)\\n        \"\n    pass",
            "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Trains or adapts the underlying embedding model.\\n\\n        Each training data example is a dictionary with the following keys:\\n\\n        * question: The question string.\\n        * pos_doc: Positive document string (the document containing the answer).\\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\\n        * score: The score margin the answer must fall within.\\n\\n\\n        :param training_data: The training data in a dictionary format. Required.\\n        :type training_data: List[Dict[str, Any]]\\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\\n        :type learning_rate: float\\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\\n        :type n_epochs: int\\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\\n        :type num_warmup_steps: int\\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\\n        :type batch_size: int (optional)\\n        \"\n    pass",
            "def train(self, training_data: List[Dict[str, Any]], learning_rate: float=2e-05, n_epochs: int=1, num_warmup_steps: Optional[int]=None, batch_size: int=16):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Trains or adapts the underlying embedding model.\\n\\n        Each training data example is a dictionary with the following keys:\\n\\n        * question: The question string.\\n        * pos_doc: Positive document string (the document containing the answer).\\n        * neg_doc: Negative document string (the document that doesn't contain the answer).\\n        * score: The score margin the answer must fall within.\\n\\n\\n        :param training_data: The training data in a dictionary format. Required.\\n        :type training_data: List[Dict[str, Any]]\\n        :param learning_rate: The speed at which the model learns. Required. We recommend that you leave the default `2e-5` value.\\n        :type learning_rate: float\\n        :param n_epochs: The number of epochs (complete passes of the training data through the algorithm) that you want the model to go through. Required.\\n        :type n_epochs: int\\n        :param num_warmup_steps: The number of warmup steps for the model. Warmup steps are epochs when the learning rate is very low. You can use them at the beginning of the training to prevent early overfitting of your model. Required.\\n        :type num_warmup_steps: int\\n        :param batch_size: The batch size to use for the training. Optional. The default values is 16.\\n        :type batch_size: int (optional)\\n        \"\n    pass"
        ]
    },
    {
        "func_name": "save",
        "original": "def save(self, save_dir: Union[Path, str]):\n    \"\"\"\n        Save the model to the directory you specify.\n\n        :param save_dir: The directory where the model is saved. Required.\n        :type save_dir: Union[Path, str]\n        \"\"\"\n    pass",
        "mutated": [
            "def save(self, save_dir: Union[Path, str]):\n    if False:\n        i = 10\n    '\\n        Save the model to the directory you specify.\\n\\n        :param save_dir: The directory where the model is saved. Required.\\n        :type save_dir: Union[Path, str]\\n        '\n    pass",
            "def save(self, save_dir: Union[Path, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Save the model to the directory you specify.\\n\\n        :param save_dir: The directory where the model is saved. Required.\\n        :type save_dir: Union[Path, str]\\n        '\n    pass",
            "def save(self, save_dir: Union[Path, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Save the model to the directory you specify.\\n\\n        :param save_dir: The directory where the model is saved. Required.\\n        :type save_dir: Union[Path, str]\\n        '\n    pass",
            "def save(self, save_dir: Union[Path, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Save the model to the directory you specify.\\n\\n        :param save_dir: The directory where the model is saved. Required.\\n        :type save_dir: Union[Path, str]\\n        '\n    pass",
            "def save(self, save_dir: Union[Path, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Save the model to the directory you specify.\\n\\n        :param save_dir: The directory where the model is saved. Required.\\n        :type save_dir: Union[Path, str]\\n        '\n    pass"
        ]
    },
    {
        "func_name": "_check_docstore_similarity_function",
        "original": "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    \"\"\"\n        Check that document_store uses a similarity function\n        compatible with the embedding model\n        \"\"\"\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)",
        "mutated": [
            "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    if False:\n        i = 10\n    '\\n        Check that document_store uses a similarity function\\n        compatible with the embedding model\\n        '\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)",
            "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Check that document_store uses a similarity function\\n        compatible with the embedding model\\n        '\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)",
            "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Check that document_store uses a similarity function\\n        compatible with the embedding model\\n        '\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)",
            "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Check that document_store uses a similarity function\\n        compatible with the embedding model\\n        '\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)",
            "def _check_docstore_similarity_function(self, document_store: BaseDocumentStore, model_name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Check that document_store uses a similarity function\\n        compatible with the embedding model\\n        '\n    if 'sentence-transformers' in model_name.lower():\n        model_similarity = None\n        if '-cos-' in model_name.lower():\n            model_similarity = 'cosine'\n        elif '-dot-' in model_name.lower():\n            model_similarity = 'dot_product'\n        if model_similarity is not None and document_store.similarity != model_similarity:\n            logger.warning('You seem to be using %s model with the %s function instead of the recommended %s. This can be set when initializing the DocumentStore', model_name, document_store.similarity, model_similarity)\n    elif 'dpr' in model_name.lower() and document_store.similarity != 'dot_product':\n        logger.warning('You seem to be using a DPR model with the %s function. We recommend using dot_product instead. This can be set when initializing the DocumentStore', document_store.similarity)"
        ]
    }
]