[
    {
        "func_name": "raise_if_duplicated",
        "original": "def raise_if_duplicated(input_list):\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))",
        "mutated": [
            "def raise_if_duplicated(input_list):\n    if False:\n        i = 10\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))",
            "def raise_if_duplicated(input_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))",
            "def raise_if_duplicated(input_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))",
            "def raise_if_duplicated(input_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))",
            "def raise_if_duplicated(input_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_names = [t.name for t in input_list if t.name is not None]\n    dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n    if len(dups) > 0:\n        raise ValueError('Duplicated inputs: {}'.format(dups))"
        ]
    },
    {
        "func_name": "_flatten_list",
        "original": "def _flatten_list(_inputs):\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret",
        "mutated": [
            "def _flatten_list(_inputs):\n    if False:\n        i = 10\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret",
            "def _flatten_list(_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret",
            "def _flatten_list(_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret",
            "def _flatten_list(_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret",
            "def _flatten_list(_inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ret = []\n    for _input in _inputs:\n        if isinstance(_input, (list, tuple)):\n            ret.extend(_flatten_list(_input))\n        elif isinstance(_input, InputType):\n            ret.append(_input)\n        else:\n            raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n    return ret"
        ]
    },
    {
        "func_name": "convert",
        "original": "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    \"\"\"\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\n    parameter is required may differ between frameworks (see below). Note that\n    this function is aliased as `ct.convert` in the tutorials.\n\n    Parameters\n    ----------\n    model:\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\n        format:\n\n        For TensorFlow versions 1.x:\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\n            - Frozen graph (`.pb`) file path\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\n        For TensorFlow versions 2.x:\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\n        For Pytorch:\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\n            - Path to a `.pt` file\n\n    source: str (optional)\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\n        framework automatically for most cases. Raise ValueError if it fails\n        to determine the source framework.\n\n    inputs: list of `TensorType` or `ImageType`\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\n        - For PyTorch models, the inputs may be nested list or tuple, but for\n          TensorFlow models it must be a flat list.\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\n          nodes in the model (if model is frozen graph) or function inputs (if\n          model is tf function).\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\n          subset of all Placeholder in the TF model.\n\n    outputs: list[str] (optional)\n\n        TensorFlow 1 and 2:\n            - `outputs` are optional.\n\n            - If specified, `outputs` is a list of string representing node\n              names.\n\n            - If `outputs` are not specified, converter infers outputs as all\n              terminal identity nodes.\n\n        PyTorch:\n            - `outputs` must not be specified.\n\n    classifier_config: ClassifierConfig class (optional)\n        The configuration if the mlmodel is intended to be a classifier.\n\n    minimum_deployment_target: coremltools.target enumeration (optional)\n        - one of the members of enum \"coremltools.target.\"\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\n\n    Returns\n    -------\n    model: MLModel\n        A Core ML MLModel object\n\n    Examples\n    --------\n    TensorFlow 1, 2 (`model` is a frozen graph):\n\n        >>> with tf.Graph().as_default() as graph:\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\n        >>>     y = tf.nn.relu(x, name=\"output\")\n\n        # Automatically infer inputs and outputs\n        >>> mlmodel = ct.convert(graph)\n\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\n        >>> results = mlmodel.predict({\"input\": test_input})\n        >>> print(results['output'])\n\n    TensorFlow 2 (`model` is tf.Keras model path):\n\n        >>> x = tf.keras.Input(shape=(32,), name='input')\n        >>> y = tf.keras.layers.Dense(16, activation='softmax')(x)\n        >>> keras_model = tf.keras.Model(x, y)\n\n        >>> keras_model.save(h5_path)\n        >>> mlmodel = ct.convert(h5_path)\n\n        >>> test_input = np.random.rand(2, 32)\n        >>> results = mlmodel.predict({'input': test_input})\n        >>> print(results['Identity'])\n\n    Pytorch:\n\n        >>> model = torchvision.models.mobilenet_v2()\n        >>> model.eval()\n        >>> example_input = torch.rand(1, 3, 256, 256)\n        >>> traced_model = torch.jit.trace(model, example_input)\n\n        >>> input = ct.TensorType(name='input_name', shape=(1, 3, 256, 256))\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\n        >>> print(results['1651']) # 1651 is the node name given by PyTorch's JIT\n\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\n    more advanced options\n    \"\"\"\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model",
        "mutated": [
            "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    if False:\n        i = 10\n    '\\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\\n    parameter is required may differ between frameworks (see below). Note that\\n    this function is aliased as `ct.convert` in the tutorials.\\n\\n    Parameters\\n    ----------\\n    model:\\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\\n        format:\\n\\n        For TensorFlow versions 1.x:\\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\\n            - Frozen graph (`.pb`) file path\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n        For TensorFlow versions 2.x:\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\\n        For Pytorch:\\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\\n            - Path to a `.pt` file\\n\\n    source: str (optional)\\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\\n        framework automatically for most cases. Raise ValueError if it fails\\n        to determine the source framework.\\n\\n    inputs: list of `TensorType` or `ImageType`\\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\\n        - For PyTorch models, the inputs may be nested list or tuple, but for\\n          TensorFlow models it must be a flat list.\\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\\n          nodes in the model (if model is frozen graph) or function inputs (if\\n          model is tf function).\\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\\n          subset of all Placeholder in the TF model.\\n\\n    outputs: list[str] (optional)\\n\\n        TensorFlow 1 and 2:\\n            - `outputs` are optional.\\n\\n            - If specified, `outputs` is a list of string representing node\\n              names.\\n\\n            - If `outputs` are not specified, converter infers outputs as all\\n              terminal identity nodes.\\n\\n        PyTorch:\\n            - `outputs` must not be specified.\\n\\n    classifier_config: ClassifierConfig class (optional)\\n        The configuration if the mlmodel is intended to be a classifier.\\n\\n    minimum_deployment_target: coremltools.target enumeration (optional)\\n        - one of the members of enum \"coremltools.target.\"\\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\\n\\n    Returns\\n    -------\\n    model: MLModel\\n        A Core ML MLModel object\\n\\n    Examples\\n    --------\\n    TensorFlow 1, 2 (`model` is a frozen graph):\\n\\n        >>> with tf.Graph().as_default() as graph:\\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\\n        >>>     y = tf.nn.relu(x, name=\"output\")\\n\\n        # Automatically infer inputs and outputs\\n        >>> mlmodel = ct.convert(graph)\\n\\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\\n        >>> results = mlmodel.predict({\"input\": test_input})\\n        >>> print(results[\\'output\\'])\\n\\n    TensorFlow 2 (`model` is tf.Keras model path):\\n\\n        >>> x = tf.keras.Input(shape=(32,), name=\\'input\\')\\n        >>> y = tf.keras.layers.Dense(16, activation=\\'softmax\\')(x)\\n        >>> keras_model = tf.keras.Model(x, y)\\n\\n        >>> keras_model.save(h5_path)\\n        >>> mlmodel = ct.convert(h5_path)\\n\\n        >>> test_input = np.random.rand(2, 32)\\n        >>> results = mlmodel.predict({\\'input\\': test_input})\\n        >>> print(results[\\'Identity\\'])\\n\\n    Pytorch:\\n\\n        >>> model = torchvision.models.mobilenet_v2()\\n        >>> model.eval()\\n        >>> example_input = torch.rand(1, 3, 256, 256)\\n        >>> traced_model = torch.jit.trace(model, example_input)\\n\\n        >>> input = ct.TensorType(name=\\'input_name\\', shape=(1, 3, 256, 256))\\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\\n        >>> print(results[\\'1651\\']) # 1651 is the node name given by PyTorch\\'s JIT\\n\\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\\n    more advanced options\\n    '\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model",
            "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\\n    parameter is required may differ between frameworks (see below). Note that\\n    this function is aliased as `ct.convert` in the tutorials.\\n\\n    Parameters\\n    ----------\\n    model:\\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\\n        format:\\n\\n        For TensorFlow versions 1.x:\\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\\n            - Frozen graph (`.pb`) file path\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n        For TensorFlow versions 2.x:\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\\n        For Pytorch:\\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\\n            - Path to a `.pt` file\\n\\n    source: str (optional)\\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\\n        framework automatically for most cases. Raise ValueError if it fails\\n        to determine the source framework.\\n\\n    inputs: list of `TensorType` or `ImageType`\\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\\n        - For PyTorch models, the inputs may be nested list or tuple, but for\\n          TensorFlow models it must be a flat list.\\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\\n          nodes in the model (if model is frozen graph) or function inputs (if\\n          model is tf function).\\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\\n          subset of all Placeholder in the TF model.\\n\\n    outputs: list[str] (optional)\\n\\n        TensorFlow 1 and 2:\\n            - `outputs` are optional.\\n\\n            - If specified, `outputs` is a list of string representing node\\n              names.\\n\\n            - If `outputs` are not specified, converter infers outputs as all\\n              terminal identity nodes.\\n\\n        PyTorch:\\n            - `outputs` must not be specified.\\n\\n    classifier_config: ClassifierConfig class (optional)\\n        The configuration if the mlmodel is intended to be a classifier.\\n\\n    minimum_deployment_target: coremltools.target enumeration (optional)\\n        - one of the members of enum \"coremltools.target.\"\\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\\n\\n    Returns\\n    -------\\n    model: MLModel\\n        A Core ML MLModel object\\n\\n    Examples\\n    --------\\n    TensorFlow 1, 2 (`model` is a frozen graph):\\n\\n        >>> with tf.Graph().as_default() as graph:\\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\\n        >>>     y = tf.nn.relu(x, name=\"output\")\\n\\n        # Automatically infer inputs and outputs\\n        >>> mlmodel = ct.convert(graph)\\n\\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\\n        >>> results = mlmodel.predict({\"input\": test_input})\\n        >>> print(results[\\'output\\'])\\n\\n    TensorFlow 2 (`model` is tf.Keras model path):\\n\\n        >>> x = tf.keras.Input(shape=(32,), name=\\'input\\')\\n        >>> y = tf.keras.layers.Dense(16, activation=\\'softmax\\')(x)\\n        >>> keras_model = tf.keras.Model(x, y)\\n\\n        >>> keras_model.save(h5_path)\\n        >>> mlmodel = ct.convert(h5_path)\\n\\n        >>> test_input = np.random.rand(2, 32)\\n        >>> results = mlmodel.predict({\\'input\\': test_input})\\n        >>> print(results[\\'Identity\\'])\\n\\n    Pytorch:\\n\\n        >>> model = torchvision.models.mobilenet_v2()\\n        >>> model.eval()\\n        >>> example_input = torch.rand(1, 3, 256, 256)\\n        >>> traced_model = torch.jit.trace(model, example_input)\\n\\n        >>> input = ct.TensorType(name=\\'input_name\\', shape=(1, 3, 256, 256))\\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\\n        >>> print(results[\\'1651\\']) # 1651 is the node name given by PyTorch\\'s JIT\\n\\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\\n    more advanced options\\n    '\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model",
            "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\\n    parameter is required may differ between frameworks (see below). Note that\\n    this function is aliased as `ct.convert` in the tutorials.\\n\\n    Parameters\\n    ----------\\n    model:\\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\\n        format:\\n\\n        For TensorFlow versions 1.x:\\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\\n            - Frozen graph (`.pb`) file path\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n        For TensorFlow versions 2.x:\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\\n        For Pytorch:\\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\\n            - Path to a `.pt` file\\n\\n    source: str (optional)\\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\\n        framework automatically for most cases. Raise ValueError if it fails\\n        to determine the source framework.\\n\\n    inputs: list of `TensorType` or `ImageType`\\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\\n        - For PyTorch models, the inputs may be nested list or tuple, but for\\n          TensorFlow models it must be a flat list.\\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\\n          nodes in the model (if model is frozen graph) or function inputs (if\\n          model is tf function).\\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\\n          subset of all Placeholder in the TF model.\\n\\n    outputs: list[str] (optional)\\n\\n        TensorFlow 1 and 2:\\n            - `outputs` are optional.\\n\\n            - If specified, `outputs` is a list of string representing node\\n              names.\\n\\n            - If `outputs` are not specified, converter infers outputs as all\\n              terminal identity nodes.\\n\\n        PyTorch:\\n            - `outputs` must not be specified.\\n\\n    classifier_config: ClassifierConfig class (optional)\\n        The configuration if the mlmodel is intended to be a classifier.\\n\\n    minimum_deployment_target: coremltools.target enumeration (optional)\\n        - one of the members of enum \"coremltools.target.\"\\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\\n\\n    Returns\\n    -------\\n    model: MLModel\\n        A Core ML MLModel object\\n\\n    Examples\\n    --------\\n    TensorFlow 1, 2 (`model` is a frozen graph):\\n\\n        >>> with tf.Graph().as_default() as graph:\\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\\n        >>>     y = tf.nn.relu(x, name=\"output\")\\n\\n        # Automatically infer inputs and outputs\\n        >>> mlmodel = ct.convert(graph)\\n\\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\\n        >>> results = mlmodel.predict({\"input\": test_input})\\n        >>> print(results[\\'output\\'])\\n\\n    TensorFlow 2 (`model` is tf.Keras model path):\\n\\n        >>> x = tf.keras.Input(shape=(32,), name=\\'input\\')\\n        >>> y = tf.keras.layers.Dense(16, activation=\\'softmax\\')(x)\\n        >>> keras_model = tf.keras.Model(x, y)\\n\\n        >>> keras_model.save(h5_path)\\n        >>> mlmodel = ct.convert(h5_path)\\n\\n        >>> test_input = np.random.rand(2, 32)\\n        >>> results = mlmodel.predict({\\'input\\': test_input})\\n        >>> print(results[\\'Identity\\'])\\n\\n    Pytorch:\\n\\n        >>> model = torchvision.models.mobilenet_v2()\\n        >>> model.eval()\\n        >>> example_input = torch.rand(1, 3, 256, 256)\\n        >>> traced_model = torch.jit.trace(model, example_input)\\n\\n        >>> input = ct.TensorType(name=\\'input_name\\', shape=(1, 3, 256, 256))\\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\\n        >>> print(results[\\'1651\\']) # 1651 is the node name given by PyTorch\\'s JIT\\n\\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\\n    more advanced options\\n    '\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model",
            "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\\n    parameter is required may differ between frameworks (see below). Note that\\n    this function is aliased as `ct.convert` in the tutorials.\\n\\n    Parameters\\n    ----------\\n    model:\\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\\n        format:\\n\\n        For TensorFlow versions 1.x:\\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\\n            - Frozen graph (`.pb`) file path\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n        For TensorFlow versions 2.x:\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\\n        For Pytorch:\\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\\n            - Path to a `.pt` file\\n\\n    source: str (optional)\\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\\n        framework automatically for most cases. Raise ValueError if it fails\\n        to determine the source framework.\\n\\n    inputs: list of `TensorType` or `ImageType`\\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\\n        - For PyTorch models, the inputs may be nested list or tuple, but for\\n          TensorFlow models it must be a flat list.\\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\\n          nodes in the model (if model is frozen graph) or function inputs (if\\n          model is tf function).\\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\\n          subset of all Placeholder in the TF model.\\n\\n    outputs: list[str] (optional)\\n\\n        TensorFlow 1 and 2:\\n            - `outputs` are optional.\\n\\n            - If specified, `outputs` is a list of string representing node\\n              names.\\n\\n            - If `outputs` are not specified, converter infers outputs as all\\n              terminal identity nodes.\\n\\n        PyTorch:\\n            - `outputs` must not be specified.\\n\\n    classifier_config: ClassifierConfig class (optional)\\n        The configuration if the mlmodel is intended to be a classifier.\\n\\n    minimum_deployment_target: coremltools.target enumeration (optional)\\n        - one of the members of enum \"coremltools.target.\"\\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\\n\\n    Returns\\n    -------\\n    model: MLModel\\n        A Core ML MLModel object\\n\\n    Examples\\n    --------\\n    TensorFlow 1, 2 (`model` is a frozen graph):\\n\\n        >>> with tf.Graph().as_default() as graph:\\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\\n        >>>     y = tf.nn.relu(x, name=\"output\")\\n\\n        # Automatically infer inputs and outputs\\n        >>> mlmodel = ct.convert(graph)\\n\\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\\n        >>> results = mlmodel.predict({\"input\": test_input})\\n        >>> print(results[\\'output\\'])\\n\\n    TensorFlow 2 (`model` is tf.Keras model path):\\n\\n        >>> x = tf.keras.Input(shape=(32,), name=\\'input\\')\\n        >>> y = tf.keras.layers.Dense(16, activation=\\'softmax\\')(x)\\n        >>> keras_model = tf.keras.Model(x, y)\\n\\n        >>> keras_model.save(h5_path)\\n        >>> mlmodel = ct.convert(h5_path)\\n\\n        >>> test_input = np.random.rand(2, 32)\\n        >>> results = mlmodel.predict({\\'input\\': test_input})\\n        >>> print(results[\\'Identity\\'])\\n\\n    Pytorch:\\n\\n        >>> model = torchvision.models.mobilenet_v2()\\n        >>> model.eval()\\n        >>> example_input = torch.rand(1, 3, 256, 256)\\n        >>> traced_model = torch.jit.trace(model, example_input)\\n\\n        >>> input = ct.TensorType(name=\\'input_name\\', shape=(1, 3, 256, 256))\\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\\n        >>> print(results[\\'1651\\']) # 1651 is the node name given by PyTorch\\'s JIT\\n\\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\\n    more advanced options\\n    '\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model",
            "@_profile\ndef convert(model, source='auto', inputs=None, outputs=None, classifier_config=None, minimum_deployment_target=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Convert TensorFlow or Pytorch models to Core ML model format. Whether a\\n    parameter is required may differ between frameworks (see below). Note that\\n    this function is aliased as `ct.convert` in the tutorials.\\n\\n    Parameters\\n    ----------\\n    model:\\n        TensorFlow 1, TensorFlow 2 or Pytorch model in one of the following\\n        format:\\n\\n        For TensorFlow versions 1.x:\\n            - Frozen `tf.Graph <https://www.tensorflow.org/api_docs/python/tf/Graph>`_\\n            - Frozen graph (`.pb`) file path\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            -  `HDF5 <https://keras.io/api/models/model_saving_apis/>`_ file path (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n        For TensorFlow versions 2.x:\\n            - `tf.keras.Model <https://www.tensorflow.org/api_docs/python/tf/keras>`_\\n            - `HDF5 file path <https://keras.io/api/models/model_saving_apis/>`_ (`.h5`)\\n            - `SavedModel <https://www.tensorflow.org/guide/saved_model>`_ directory path\\n            - A `concrete function <https://www.tensorflow.org/guide/concrete_function>`_\\n        For Pytorch:\\n            - A `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ object\\n            - Path to a `.pt` file\\n\\n    source: str (optional)\\n        One of `auto`, `tensorflow`, or `pytorch`. `auto` determines the\\n        framework automatically for most cases. Raise ValueError if it fails\\n        to determine the source framework.\\n\\n    inputs: list of `TensorType` or `ImageType`\\n        - Inputs are required for PyTorch model, but optional for TensorFlow.\\n        - For PyTorch models, the inputs may be nested list or tuple, but for\\n          TensorFlow models it must be a flat list.\\n        - For TensorFlow, if inputs is `None`, the inputs are `Placeholder`\\n          nodes in the model (if model is frozen graph) or function inputs (if\\n          model is tf function).\\n        - For TensorFlow, if inputs is not `None`, inputs may contain only a\\n          subset of all Placeholder in the TF model.\\n\\n    outputs: list[str] (optional)\\n\\n        TensorFlow 1 and 2:\\n            - `outputs` are optional.\\n\\n            - If specified, `outputs` is a list of string representing node\\n              names.\\n\\n            - If `outputs` are not specified, converter infers outputs as all\\n              terminal identity nodes.\\n\\n        PyTorch:\\n            - `outputs` must not be specified.\\n\\n    classifier_config: ClassifierConfig class (optional)\\n        The configuration if the mlmodel is intended to be a classifier.\\n\\n    minimum_deployment_target: coremltools.target enumeration (optional)\\n        - one of the members of enum \"coremltools.target.\"\\n        - When not-specified or None, converter aims for as minimum of a deployment target as possible\\n\\n    Returns\\n    -------\\n    model: MLModel\\n        A Core ML MLModel object\\n\\n    Examples\\n    --------\\n    TensorFlow 1, 2 (`model` is a frozen graph):\\n\\n        >>> with tf.Graph().as_default() as graph:\\n        >>>     x = tf.placeholder(tf.float32, shape=(1, 2, 3), name=\"input\")\\n        >>>     y = tf.nn.relu(x, name=\"output\")\\n\\n        # Automatically infer inputs and outputs\\n        >>> mlmodel = ct.convert(graph)\\n\\t    >>> test_input = np.random.rand(1, 2, 3) - 0.5\\n        >>> results = mlmodel.predict({\"input\": test_input})\\n        >>> print(results[\\'output\\'])\\n\\n    TensorFlow 2 (`model` is tf.Keras model path):\\n\\n        >>> x = tf.keras.Input(shape=(32,), name=\\'input\\')\\n        >>> y = tf.keras.layers.Dense(16, activation=\\'softmax\\')(x)\\n        >>> keras_model = tf.keras.Model(x, y)\\n\\n        >>> keras_model.save(h5_path)\\n        >>> mlmodel = ct.convert(h5_path)\\n\\n        >>> test_input = np.random.rand(2, 32)\\n        >>> results = mlmodel.predict({\\'input\\': test_input})\\n        >>> print(results[\\'Identity\\'])\\n\\n    Pytorch:\\n\\n        >>> model = torchvision.models.mobilenet_v2()\\n        >>> model.eval()\\n        >>> example_input = torch.rand(1, 3, 256, 256)\\n        >>> traced_model = torch.jit.trace(model, example_input)\\n\\n        >>> input = ct.TensorType(name=\\'input_name\\', shape=(1, 3, 256, 256))\\n        >>> mlmodel = ct.convert(traced_model, inputs=[input])\\n        >>> results = mlmodel.predict({\"input\": example_input.numpy()})\\n        >>> print(results[\\'1651\\']) # 1651 is the node name given by PyTorch\\'s JIT\\n\\n    See `here <https://coremltools.readme.io/docs/neural-network-conversion>`_ for\\n    more advanced options\\n    '\n    if minimum_deployment_target is not None and (not isinstance(minimum_deployment_target, AvailableTarget)):\n        msg = \"Unrecognized value of argument 'minimum_deployment_target': {}. It needs to be a member of 'coremltools.target' enumeration. For example, coremltools.target.iOS13\"\n        raise TypeError(msg.format(minimum_deployment_target))\n    source = source.lower()\n    if source not in {'auto', 'tensorflow', 'pytorch'}:\n        msg = 'Unrecognized value of argument \"source\": {}. It must be one of [\"auto\", \"tensorflow\", \"pytorch\"].'\n        raise ValueError(msg.format(source))\n\n    def raise_if_duplicated(input_list):\n        input_names = [t.name for t in input_list if t.name is not None]\n        dups = [item for (item, count) in collections.Counter(input_names).items() if count > 1]\n        if len(dups) > 0:\n            raise ValueError('Duplicated inputs: {}'.format(dups))\n    if inputs is not None:\n        if not isinstance(inputs, list):\n            msg = '\"inputs\" must be of type list'\n            raise ValueError(msg)\n    if classifier_config is not None:\n        if not isinstance(classifier_config, ClassifierConfig):\n            msg = '\"classifier_config\" must be of type ClassifierConfig'\n            raise ValueError(msg)\n    if source == 'tensorflow' and _HAS_TF_2:\n        source = 'tensorflow2'\n    if source == 'auto' and _HAS_TF_1:\n        try:\n            loader = TF1Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow'\n        except:\n            pass\n    if source == 'auto' and _HAS_TF_2:\n        try:\n            loader = TF2Loader(model, outputs=outputs)\n            loader._graph_def_from_model(outputs=outputs)\n            source = 'tensorflow2'\n        except:\n            pass\n    if source == 'auto' and _HAS_TORCH:\n        try:\n            pytorch_load(model)\n            source = 'pytorch'\n        except:\n            pass\n    if source == 'auto' and isinstance(model, Program):\n        source = 'mil'\n    convert_to = kwargs.get('convert_to', 'nn_proto')\n    kwargs.pop('convert_to', None)\n    if source == 'auto':\n        msg = 'Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you\\'re converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.'\n        raise ValueError(msg)\n    elif source in {'tensorflow', 'tensorflow2'}:\n        if source == 'tensorflow' and (not _HAS_TF_1):\n            raise ValueError('Converter was called with source=\"tensorflow\", but missing tensorflow package')\n        if inputs is not None:\n            raise_if_duplicated(inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in inputs])):\n            raise ValueError('Input should be a list of TensorType or ImageType')\n        proto_spec = _convert(model, convert_from=source, convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'pytorch':\n        if 'example_inputs' in kwargs:\n            msg = 'Unexpected argument \"example_inputs\" found'\n            raise ValueError(msg)\n\n        def _flatten_list(_inputs):\n            ret = []\n            for _input in _inputs:\n                if isinstance(_input, (list, tuple)):\n                    ret.extend(_flatten_list(_input))\n                elif isinstance(_input, InputType):\n                    ret.append(_input)\n                else:\n                    raise ValueError('Unknown type {} for flattening into InputType.'.format(type(_input)))\n            return ret\n        flat_inputs = _flatten_list(inputs)\n        raise_if_duplicated(flat_inputs)\n        if inputs is not None and (not all([isinstance(_input, InputType) for _input in flat_inputs])):\n            raise ValueError('Input should be a list/tuple (or nested lists/tuples) of TensorType or ImageType')\n        if outputs is not None:\n            raise ValueError('outputs must not be specified for PyTorch')\n        proto_spec = _convert(model, convert_from='torch', convert_to=convert_to, inputs=inputs, outputs=outputs, classifier_config=classifier_config, **kwargs)\n    elif source == 'mil':\n        if not isinstance(model, Program):\n            msg = 'Converter was asked to convert MIL input, but input is not a MIL program!'\n            raise ValueError(msg)\n        proto_spec = _convert(model, convert_from='mil', convert_to=convert_to, example_inputs=inputs, classifier_config=classifier_config, **kwargs)\n    model = coremltools.models.MLModel(proto_spec, useCPUOnly=True)\n    if minimum_deployment_target is not None:\n        check_deployment_compatibility(spec=proto_spec, representation=convert_to, deployment_target=minimum_deployment_target)\n    del proto_spec\n    gc.collect()\n    if source in {'tensorflow', 'tensorflow2'} and (_HAS_TF_1 or _HAS_TF_2):\n        src_pkg_version = 'tensorflow=={0}'.format(tf.__version__)\n    elif source == 'pytorch' and _HAS_TORCH:\n        src_pkg_version = 'torch=={0}'.format(torch.__version__)\n    else:\n        src_pkg_version = 'unknown'\n    model.user_defined_metadata[_METADATA_VERSION] = ct_version\n    model.user_defined_metadata[_METADATA_SOURCE] = src_pkg_version\n    return model"
        ]
    }
]