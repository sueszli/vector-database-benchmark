[
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
        "mutated": [
            "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    if False:\n        i = 10\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, use_edge_fusion, edge_fusion_inds, edge_heatmap_ratio, filter_outside_objs=True, loss_cls=dict(type='GaussianFocalLoss', loss_weight=1.0), loss_bbox=dict(type='IoULoss', loss_weight=0.1), loss_dir=dict(type='MultiBinLoss', loss_weight=0.1), loss_keypoints=dict(type='L1Loss', loss_weight=0.1), loss_dims=dict(type='L1Loss', loss_weight=0.1), loss_offsets2d=dict(type='L1Loss', loss_weight=0.1), loss_direct_depth=dict(type='L1Loss', loss_weight=0.1), loss_keypoints_depth=dict(type='L1Loss', loss_weight=0.1), loss_combined_depth=dict(type='L1Loss', loss_weight=0.1), loss_attr=None, bbox_coder=dict(type='MonoFlexCoder', code_size=7), norm_cfg=dict(type='BN'), init_cfg=None, init_bias=-2.19, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.use_edge_fusion = use_edge_fusion\n    self.edge_fusion_inds = edge_fusion_inds\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.filter_outside_objs = filter_outside_objs\n    self.edge_heatmap_ratio = edge_heatmap_ratio\n    self.init_bias = init_bias\n    self.loss_dir = build_loss(loss_dir)\n    self.loss_keypoints = build_loss(loss_keypoints)\n    self.loss_dims = build_loss(loss_dims)\n    self.loss_offsets2d = build_loss(loss_offsets2d)\n    self.loss_direct_depth = build_loss(loss_direct_depth)\n    self.loss_keypoints_depth = build_loss(loss_keypoints_depth)\n    self.loss_combined_depth = build_loss(loss_combined_depth)\n    self.bbox_coder = build_bbox_coder(bbox_coder)"
        ]
    },
    {
        "func_name": "_init_edge_module",
        "original": "def _init_edge_module(self):\n    \"\"\"Initialize edge fusion module for feature extraction.\"\"\"\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)",
        "mutated": [
            "def _init_edge_module(self):\n    if False:\n        i = 10\n    'Initialize edge fusion module for feature extraction.'\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)",
            "def _init_edge_module(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize edge fusion module for feature extraction.'\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)",
            "def _init_edge_module(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize edge fusion module for feature extraction.'\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)",
            "def _init_edge_module(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize edge fusion module for feature extraction.'\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)",
            "def _init_edge_module(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize edge fusion module for feature extraction.'\n    self.edge_fuse_cls = EdgeFusionModule(self.num_classes, 256)\n    for i in range(len(self.edge_fusion_inds)):\n        (reg_inds, out_inds) = self.edge_fusion_inds[i]\n        out_channels = self.group_reg_dims[reg_inds][out_inds]\n        fusion_layer = EdgeFusionModule(out_channels, 256)\n        layer_name = f'edge_fuse_reg_{reg_inds}_{out_inds}'\n        self.add_module(layer_name, fusion_layer)"
        ]
    },
    {
        "func_name": "init_weights",
        "original": "def init_weights(self):\n    \"\"\"Initialize weights.\"\"\"\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)",
        "mutated": [
            "def init_weights(self):\n    if False:\n        i = 10\n    'Initialize weights.'\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize weights.'\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize weights.'\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize weights.'\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize weights.'\n    super().init_weights()\n    self.conv_cls.bias.data.fill_(self.init_bias)\n    xavier_init(self.conv_regs[4][0], gain=0.01)\n    xavier_init(self.conv_regs[7][0], gain=0.01)\n    for m in self.conv_regs.modules():\n        if isinstance(m, nn.Conv2d):\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)"
        ]
    },
    {
        "func_name": "_init_predictor",
        "original": "def _init_predictor(self):\n    \"\"\"Initialize predictor layers of the head.\"\"\"\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)",
        "mutated": [
            "def _init_predictor(self):\n    if False:\n        i = 10\n    'Initialize predictor layers of the head.'\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize predictor layers of the head.'\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize predictor layers of the head.'\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize predictor layers of the head.'\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)",
            "def _init_predictor(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize predictor layers of the head.'\n    self.conv_cls_prev = self._init_branch(conv_channels=self.cls_branch, conv_strides=(1,) * len(self.cls_branch))\n    self.conv_cls = nn.Conv2d(self.cls_branch[-1], self.cls_out_channels, 1)\n    self.conv_reg_prevs = nn.ModuleList()\n    self.conv_regs = nn.ModuleList()\n    for i in range(len(self.group_reg_dims)):\n        reg_dims = self.group_reg_dims[i]\n        reg_branch_channels = self.reg_branch[i]\n        out_channel = self.out_channels[i]\n        reg_list = nn.ModuleList()\n        if len(reg_branch_channels) > 0:\n            self.conv_reg_prevs.append(self._init_branch(conv_channels=reg_branch_channels, conv_strides=(1,) * len(reg_branch_channels)))\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(out_channel, reg_dim, 1))\n            self.conv_regs.append(reg_list)\n        else:\n            self.conv_reg_prevs.append(None)\n            for reg_dim in reg_dims:\n                reg_list.append(nn.Conv2d(self.feat_channels, reg_dim, 1))\n            self.conv_regs.append(reg_list)"
        ]
    },
    {
        "func_name": "_init_layers",
        "original": "def _init_layers(self):\n    \"\"\"Initialize layers of the head.\"\"\"\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()",
        "mutated": [
            "def _init_layers(self):\n    if False:\n        i = 10\n    'Initialize layers of the head.'\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize layers of the head.'\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize layers of the head.'\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize layers of the head.'\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize layers of the head.'\n    self._init_predictor()\n    if self.use_edge_fusion:\n        self._init_edge_module()"
        ]
    },
    {
        "func_name": "forward_train",
        "original": "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    \"\"\"\n        Args:\n            x (list[Tensor]): Features from FPN.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            gt_labels (list[Tensor]): Ground truth labels of each box,\n                shape (num_gts,).\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\n                shape (num_gts, self.bbox_code_size).\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\n                shape (num_gts,).\n            centers2d (list[Tensor]): Projected 3D center of each box,\n                shape (num_gts, 2).\n            depths (list[Tensor]): Depth of projected 3D center of each box,\n                shape (num_gts,).\n            attr_labels (list[Tensor]): Attribute labels of each box,\n                shape (num_gts,).\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used\n        Returns:\n            tuple:\n                losses: (dict[str, Tensor]): A dictionary of loss components.\n                proposal_list (list[Tensor]): Proposals of each image.\n        \"\"\"\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)",
        "mutated": [
            "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    if False:\n        i = 10\n    '\\n        Args:\\n            x (list[Tensor]): Features from FPN.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\\n                shape (num_gts, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\\n                shape (num_gts, self.bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\\n                shape (num_gts,).\\n            centers2d (list[Tensor]): Projected 3D center of each box,\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth of projected 3D center of each box,\\n                shape (num_gts,).\\n            attr_labels (list[Tensor]): Attribute labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\\n                ignored, shape (num_ignored_gts, 4).\\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used\\n        Returns:\\n            tuple:\\n                losses: (dict[str, Tensor]): A dictionary of loss components.\\n                proposal_list (list[Tensor]): Proposals of each image.\\n        '\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)",
            "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Args:\\n            x (list[Tensor]): Features from FPN.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\\n                shape (num_gts, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\\n                shape (num_gts, self.bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\\n                shape (num_gts,).\\n            centers2d (list[Tensor]): Projected 3D center of each box,\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth of projected 3D center of each box,\\n                shape (num_gts,).\\n            attr_labels (list[Tensor]): Attribute labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\\n                ignored, shape (num_ignored_gts, 4).\\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used\\n        Returns:\\n            tuple:\\n                losses: (dict[str, Tensor]): A dictionary of loss components.\\n                proposal_list (list[Tensor]): Proposals of each image.\\n        '\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)",
            "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Args:\\n            x (list[Tensor]): Features from FPN.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\\n                shape (num_gts, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\\n                shape (num_gts, self.bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\\n                shape (num_gts,).\\n            centers2d (list[Tensor]): Projected 3D center of each box,\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth of projected 3D center of each box,\\n                shape (num_gts,).\\n            attr_labels (list[Tensor]): Attribute labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\\n                ignored, shape (num_ignored_gts, 4).\\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used\\n        Returns:\\n            tuple:\\n                losses: (dict[str, Tensor]): A dictionary of loss components.\\n                proposal_list (list[Tensor]): Proposals of each image.\\n        '\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)",
            "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Args:\\n            x (list[Tensor]): Features from FPN.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\\n                shape (num_gts, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\\n                shape (num_gts, self.bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\\n                shape (num_gts,).\\n            centers2d (list[Tensor]): Projected 3D center of each box,\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth of projected 3D center of each box,\\n                shape (num_gts,).\\n            attr_labels (list[Tensor]): Attribute labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\\n                ignored, shape (num_ignored_gts, 4).\\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used\\n        Returns:\\n            tuple:\\n                losses: (dict[str, Tensor]): A dictionary of loss components.\\n                proposal_list (list[Tensor]): Proposals of each image.\\n        '\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)",
            "def forward_train(self, x, input_metas, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, gt_bboxes_ignore, proposal_cfg, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Args:\\n            x (list[Tensor]): Features from FPN.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of the image,\\n                shape (num_gts, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_3d (list[Tensor]): 3D ground truth bboxes of the image,\\n                shape (num_gts, self.bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D ground truth labels of each box,\\n                shape (num_gts,).\\n            centers2d (list[Tensor]): Projected 3D center of each box,\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth of projected 3D center of each box,\\n                shape (num_gts,).\\n            attr_labels (list[Tensor]): Attribute labels of each box,\\n                shape (num_gts,).\\n            gt_bboxes_ignore (list[Tensor]): Ground truth bboxes to be\\n                ignored, shape (num_ignored_gts, 4).\\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\\n                if None, test_cfg would be used\\n        Returns:\\n            tuple:\\n                losses: (dict[str, Tensor]): A dictionary of loss components.\\n                proposal_list (list[Tensor]): Proposals of each image.\\n        '\n    outs = self(x, input_metas)\n    if gt_labels is None:\n        loss_inputs = outs + (gt_bboxes, gt_bboxes_3d, centers2d, depths, attr_labels, input_metas)\n    else:\n        loss_inputs = outs + (gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas)\n    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n    if proposal_cfg is None:\n        return losses\n    else:\n        proposal_list = self.get_bboxes(*outs, input_metas, cfg=proposal_cfg)\n        return (losses, proposal_list)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, feats, input_metas):\n    \"\"\"Forward features from the upstream network.\n\n        Args:\n            feats (list[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            tuple:\n                cls_scores (list[Tensor]): Box scores for each scale level,\n                    each is a 4D-tensor, the channel number is\n                    num_points * num_classes.\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                    level, each is a 4D-tensor, the channel number is\n                    num_points * bbox_code_size.\n        \"\"\"\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)",
        "mutated": [
            "def forward(self, feats, input_metas):\n    if False:\n        i = 10\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (list[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)",
            "def forward(self, feats, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (list[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)",
            "def forward(self, feats, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (list[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)",
            "def forward(self, feats, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (list[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)",
            "def forward(self, feats, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (list[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    mlvl_input_metas = [input_metas for i in range(len(feats))]\n    return multi_apply(self.forward_single, feats, mlvl_input_metas)"
        ]
    },
    {
        "func_name": "forward_single",
        "original": "def forward_single(self, x, input_metas):\n    \"\"\"Forward features of a single scale level.\n\n        Args:\n            x (Tensor): Feature maps from a specific FPN feature level.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            tuple: Scores for each class, bbox predictions.\n        \"\"\"\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)",
        "mutated": [
            "def forward_single(self, x, input_metas):\n    if False:\n        i = 10\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Feature maps from a specific FPN feature level.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox predictions.\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Feature maps from a specific FPN feature level.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox predictions.\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Feature maps from a specific FPN feature level.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox predictions.\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Feature maps from a specific FPN feature level.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox predictions.\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Feature maps from a specific FPN feature level.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox predictions.\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = x.shape\n    downsample_ratio = img_h / feat_h\n    for conv_cls_prev_layer in self.conv_cls_prev:\n        cls_feat = conv_cls_prev_layer(x)\n    out_cls = self.conv_cls(cls_feat)\n    if self.use_edge_fusion:\n        edge_indices_list = get_edge_indices(input_metas, downsample_ratio, device=x.device)\n        edge_lens = [edge_indices.shape[0] for edge_indices in edge_indices_list]\n        max_edge_len = max(edge_lens)\n        edge_indices = x.new_zeros((batch_size, max_edge_len, 2), dtype=torch.long)\n        for i in range(batch_size):\n            edge_indices[i, :edge_lens[i]] = edge_indices_list[i]\n        out_cls = self.edge_fuse_cls(cls_feat, out_cls, edge_indices, edge_lens, feat_h, feat_w)\n    bbox_pred = []\n    for i in range(len(self.group_reg_dims)):\n        reg_feat = x.clone()\n        if len(self.reg_branch[i]) > 0:\n            for conv_reg_prev_layer in self.conv_reg_prevs[i]:\n                reg_feat = conv_reg_prev_layer(reg_feat)\n        for (j, conv_reg) in enumerate(self.conv_regs[i]):\n            out_reg = conv_reg(reg_feat)\n            if self.use_edge_fusion and (i, j) in self.edge_fusion_inds:\n                out_reg = getattr(self, 'edge_fuse_reg_{}_{}'.format(i, j))(reg_feat, out_reg, edge_indices, edge_lens, feat_h, feat_w)\n            bbox_pred.append(out_reg)\n    bbox_pred = torch.cat(bbox_pred, dim=1)\n    cls_score = out_cls.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    return (cls_score, bbox_pred)"
        ]
    },
    {
        "func_name": "get_bboxes",
        "original": "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    \"\"\"Generate bboxes from bbox head predictions.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level.\n            bbox_preds (list[Tensor]): Box regression for each scale.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            rescale (bool): If True, return boxes in original image space.\n        Returns:\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\n                Each item in result_list is 4-tuple.\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
        "mutated": [
            "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    if False:\n        i = 10\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], input_metas, cam2imgs=cam2imgs, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(input_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = input_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list"
        ]
    },
    {
        "func_name": "decode_heatmap",
        "original": "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    \"\"\"Transform outputs into detections raw bbox predictions.\n\n        Args:\n            class_score (Tensor): Center predict heatmap,\n                shape (B, num_classes, H, W).\n            reg_pred (Tensor): Box regression map.\n                shape (B, channel, H , W).\n            input_metas (List[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            cam2imgs (Tensor): Camera intrinsic matrix.\n                shape (N, 4, 4)\n            topk (int, optional): Get top k center keypoints from heatmap.\n                Default 100.\n            kernel (int, optional): Max pooling kernel for extract local\n                maximum pixels. Default 3.\n\n        Returns:\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\n               the following Tensors:\n              - batch_bboxes (Tensor): Coords of each 3D box.\n                    shape (B, k, 7)\n              - batch_scores (Tensor): Scores of each 3D box.\n                    shape (B, k)\n              - batch_topk_labels (Tensor): Categories of each 3D box.\n                    shape (B, k)\n        \"\"\"\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
        "mutated": [
            "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    if False:\n        i = 10\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            input_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrix.\\n                shape (N, 4, 4)\\n            topk (int, optional): Get top k center keypoints from heatmap.\\n                Default 100.\\n            kernel (int, optional): Max pooling kernel for extract local\\n                maximum pixels. Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            input_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrix.\\n                shape (N, 4, 4)\\n            topk (int, optional): Get top k center keypoints from heatmap.\\n                Default 100.\\n            kernel (int, optional): Max pooling kernel for extract local\\n                maximum pixels. Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            input_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrix.\\n                shape (N, 4, 4)\\n            topk (int, optional): Get top k center keypoints from heatmap.\\n                Default 100.\\n            kernel (int, optional): Max pooling kernel for extract local\\n                maximum pixels. Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            input_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrix.\\n                shape (N, 4, 4)\\n            topk (int, optional): Get top k center keypoints from heatmap.\\n                Default 100.\\n            kernel (int, optional): Max pooling kernel for extract local\\n                maximum pixels. Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, input_metas, cam2imgs, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            input_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrix.\\n                shape (N, 4, 4)\\n            topk (int, optional): Get top k center keypoints from heatmap.\\n                Default 100.\\n            kernel (int, optional): Max pooling kernel for extract local\\n                maximum pixels. Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = input_metas[0]['pad_shape'][:2]\n    (batch_size, _, feat_h, feat_w) = cls_score.shape\n    downsample_ratio = img_h / feat_h\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    pred_base_centers2d = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    preds = self.bbox_coder.decode(regression, batch_topk_labels, downsample_ratio, cam2imgs)\n    pred_locations = self.bbox_coder.decode_location(pred_base_centers2d, preds['offsets2d'], preds['combined_depth'], cam2imgs, downsample_ratio)\n    pred_yaws = self.bbox_coder.decode_orientation(preds['orientations']).unsqueeze(-1)\n    pred_dims = preds['dimensions']\n    batch_bboxes = torch.cat((pred_locations, pred_dims, pred_yaws), dim=1)\n    batch_bboxes = batch_bboxes.view(batch_size, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)"
        ]
    },
    {
        "func_name": "get_predictions",
        "original": "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    \"\"\"Prepare predictions for computing loss.\n\n        Args:\n            pred_reg (Tensor): Box regression map.\n                shape (B, channel, H , W).\n            labels3d (Tensor): Labels of each 3D box.\n                shape (B * max_objs, )\n            centers2d (Tensor): Coords of each projected 3D box\n                center on image. shape (N, 2)\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\n                shape (B * max_objs, )\n            batch_indices (Tenosr): Batch indices of the 3D box.\n                shape (N, 3)\n            input_metas (list[dict]): Meta information of each image,\n                e.g., image size, scaling factor, etc.\n            downsample_ratio (int): The stride of feature map.\n\n        Returns:\n            dict: The predictions for computing loss.\n        \"\"\"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds",
        "mutated": [
            "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    if False:\n        i = 10\n    'Prepare predictions for computing loss.\\n\\n        Args:\\n            pred_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B * max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (N, 2)\\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            batch_indices (Tenosr): Batch indices of the 3D box.\\n                shape (N, 3)\\n            input_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            downsample_ratio (int): The stride of feature map.\\n\\n        Returns:\\n            dict: The predictions for computing loss.\\n        '\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds",
            "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Prepare predictions for computing loss.\\n\\n        Args:\\n            pred_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B * max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (N, 2)\\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            batch_indices (Tenosr): Batch indices of the 3D box.\\n                shape (N, 3)\\n            input_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            downsample_ratio (int): The stride of feature map.\\n\\n        Returns:\\n            dict: The predictions for computing loss.\\n        '\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds",
            "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Prepare predictions for computing loss.\\n\\n        Args:\\n            pred_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B * max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (N, 2)\\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            batch_indices (Tenosr): Batch indices of the 3D box.\\n                shape (N, 3)\\n            input_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            downsample_ratio (int): The stride of feature map.\\n\\n        Returns:\\n            dict: The predictions for computing loss.\\n        '\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds",
            "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Prepare predictions for computing loss.\\n\\n        Args:\\n            pred_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B * max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (N, 2)\\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            batch_indices (Tenosr): Batch indices of the 3D box.\\n                shape (N, 3)\\n            input_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            downsample_ratio (int): The stride of feature map.\\n\\n        Returns:\\n            dict: The predictions for computing loss.\\n        '\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds",
            "def get_predictions(self, pred_reg, labels3d, centers2d, reg_mask, batch_indices, input_metas, downsample_ratio):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Prepare predictions for computing loss.\\n\\n        Args:\\n            pred_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B * max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (N, 2)\\n            reg_mask (Tensor): Indexes of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            batch_indices (Tenosr): Batch indices of the 3D box.\\n                shape (N, 3)\\n            input_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            downsample_ratio (int): The stride of feature map.\\n\\n        Returns:\\n            dict: The predictions for computing loss.\\n        '\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([centers2d.new_tensor(input_meta['cam2img']) for input_meta in input_metas])\n    cam2imgs = cam2imgs[batch_indices, :, :]\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)[reg_mask]\n    preds = self.bbox_coder.decode(pred_regression_pois, labels3d, downsample_ratio, cam2imgs)\n    return preds"
        ]
    },
    {
        "func_name": "get_targets",
        "original": "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    \"\"\"Get training targets for batch images.\n``\n        Args:\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\n                image, shape (num_gt, 4).\n            gt_labels_list (list[Tensor]): Ground truth labels of each\n                box, shape (num_gt,).\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\n                Ground truth bboxes of each image,\n                shape (num_gt, bbox_code_size).\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\n                each box, shape (num_gt,).\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\n                image, shape (num_gt, 2).\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\n                image, each has shape (num_gt, 1).\n            feat_shape (tuple[int]): Feature map shape with value,\n                shape (B, _, H, W).\n            img_shape (tuple[int]): Image shape in [h, w] format.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            tuple[Tensor, dict]: The Tensor value is the targets of\n                center heatmap, the dict has components below:\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\n                    center on image. shape (B * max_objs, 2), [dtype: int]\n              - labels3d (Tensor): Labels of each 3D box.\n                    shape (N, )\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\n                    shape (B * max_objs, )\n              - batch_indices (Tensor): Batch id of the 3D box.\n                    shape (N, )\n              - depth_target (Tensor): Depth target of each 3D box.\n                    shape (N, )\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\n                    on image. shape (N, 10, 2)\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\n                    box on image. shape (N, 10)\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\n                    of each 3D box. shape (N, 3)\n              - orientations_target (Tensor): Orientation (encoded local yaw)\n                    target of each 3D box. shape (N, )\n              - offsets2d_target (Tensor): Offsets target of each projected\n                    3D box. shape (N, 2)\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\n                    shape (N, 3)\n              - downsample_ratio (int): The stride of feature map.\n        \"\"\"\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)",
        "mutated": [
            "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    if False:\n        i = 10\n    'Get training targets for batch images.\\n``\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\\n                image, shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each\\n                box, shape (num_gt,).\\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\\n                Ground truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\\n                each box, shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\\n                image, shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2), [dtype: int]\\n              - labels3d (Tensor): Labels of each 3D box.\\n                    shape (N, )\\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - batch_indices (Tensor): Batch id of the 3D box.\\n                    shape (N, )\\n              - depth_target (Tensor): Depth target of each 3D box.\\n                    shape (N, )\\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\\n                    on image. shape (N, 10, 2)\\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\\n                    box on image. shape (N, 10)\\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\\n                    of each 3D box. shape (N, 3)\\n              - orientations_target (Tensor): Orientation (encoded local yaw)\\n                    target of each 3D box. shape (N, )\\n              - offsets2d_target (Tensor): Offsets target of each projected\\n                    3D box. shape (N, 2)\\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\\n                    shape (N, 3)\\n              - downsample_ratio (int): The stride of feature map.\\n        '\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get training targets for batch images.\\n``\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\\n                image, shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each\\n                box, shape (num_gt,).\\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\\n                Ground truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\\n                each box, shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\\n                image, shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2), [dtype: int]\\n              - labels3d (Tensor): Labels of each 3D box.\\n                    shape (N, )\\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - batch_indices (Tensor): Batch id of the 3D box.\\n                    shape (N, )\\n              - depth_target (Tensor): Depth target of each 3D box.\\n                    shape (N, )\\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\\n                    on image. shape (N, 10, 2)\\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\\n                    box on image. shape (N, 10)\\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\\n                    of each 3D box. shape (N, 3)\\n              - orientations_target (Tensor): Orientation (encoded local yaw)\\n                    target of each 3D box. shape (N, )\\n              - offsets2d_target (Tensor): Offsets target of each projected\\n                    3D box. shape (N, 2)\\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\\n                    shape (N, 3)\\n              - downsample_ratio (int): The stride of feature map.\\n        '\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get training targets for batch images.\\n``\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\\n                image, shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each\\n                box, shape (num_gt,).\\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\\n                Ground truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\\n                each box, shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\\n                image, shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2), [dtype: int]\\n              - labels3d (Tensor): Labels of each 3D box.\\n                    shape (N, )\\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - batch_indices (Tensor): Batch id of the 3D box.\\n                    shape (N, )\\n              - depth_target (Tensor): Depth target of each 3D box.\\n                    shape (N, )\\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\\n                    on image. shape (N, 10, 2)\\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\\n                    box on image. shape (N, 10)\\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\\n                    of each 3D box. shape (N, 3)\\n              - orientations_target (Tensor): Orientation (encoded local yaw)\\n                    target of each 3D box. shape (N, )\\n              - offsets2d_target (Tensor): Offsets target of each projected\\n                    3D box. shape (N, 2)\\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\\n                    shape (N, 3)\\n              - downsample_ratio (int): The stride of feature map.\\n        '\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get training targets for batch images.\\n``\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\\n                image, shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each\\n                box, shape (num_gt,).\\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\\n                Ground truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\\n                each box, shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\\n                image, shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2), [dtype: int]\\n              - labels3d (Tensor): Labels of each 3D box.\\n                    shape (N, )\\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - batch_indices (Tensor): Batch id of the 3D box.\\n                    shape (N, )\\n              - depth_target (Tensor): Depth target of each 3D box.\\n                    shape (N, )\\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\\n                    on image. shape (N, 10, 2)\\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\\n                    box on image. shape (N, 10)\\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\\n                    of each 3D box. shape (N, 3)\\n              - orientations_target (Tensor): Orientation (encoded local yaw)\\n                    target of each 3D box. shape (N, )\\n              - offsets2d_target (Tensor): Offsets target of each projected\\n                    3D box. shape (N, 2)\\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\\n                    shape (N, 3)\\n              - downsample_ratio (int): The stride of feature map.\\n        '\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, depths_list, feat_shape, img_shape, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get training targets for batch images.\\n``\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each\\n                image, shape (num_gt, 4).\\n            gt_labels_list (list[Tensor]): Ground truth labels of each\\n                box, shape (num_gt,).\\n            gt_bboxes_3d_list (list[:obj:`CameraInstance3DBoxes`]): 3D\\n                Ground truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of\\n                each box, shape (num_gt,).\\n            centers2d_list (list[Tensor]): Projected 3D centers onto 2D\\n                image, shape (num_gt, 2).\\n            depths_list (list[Tensor]): Depth of projected 3D centers onto 2D\\n                image, each has shape (num_gt, 1).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - base_centers2d_target (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2), [dtype: int]\\n              - labels3d (Tensor): Labels of each 3D box.\\n                    shape (N, )\\n              - reg_mask (Tensor): Mask of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - batch_indices (Tensor): Batch id of the 3D box.\\n                    shape (N, )\\n              - depth_target (Tensor): Depth target of each 3D box.\\n                    shape (N, )\\n              - keypoints2d_target (Tensor): Keypoints of each projected 3D box\\n                    on image. shape (N, 10, 2)\\n              - keypoints_mask (Tensor): Keypoints mask of each projected 3D\\n                    box on image. shape (N, 10)\\n              - keypoints_depth_mask (Tensor): Depths decoded from keypoints\\n                    of each 3D box. shape (N, 3)\\n              - orientations_target (Tensor): Orientation (encoded local yaw)\\n                    target of each 3D box. shape (N, )\\n              - offsets2d_target (Tensor): Offsets target of each projected\\n                    3D box. shape (N, 2)\\n              - dimensions_target (Tensor): Dimensions target of each 3D box.\\n                    shape (N, 3)\\n              - downsample_ratio (int): The stride of feature map.\\n        '\n    (img_h, img_w) = img_shape[:2]\n    (batch_size, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    if self.filter_outside_objs:\n        filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list, gt_labels_3d_list, centers2d_list, input_metas)\n    (base_centers2d_list, offsets2d_list, trunc_mask_list) = handle_proj_objs(centers2d_list, gt_bboxes_list, input_metas)\n    (keypoints2d_list, keypoints_mask_list, keypoints_depth_mask_list) = get_keypoints(gt_bboxes_3d_list, centers2d_list, input_metas)\n    center_heatmap_target = gt_bboxes_list[-1].new_zeros([batch_size, self.num_classes, feat_h, feat_w])\n    for batch_id in range(batch_size):\n        gt_bboxes = gt_bboxes_list[batch_id] * width_ratio\n        gt_labels = gt_labels_list[batch_id]\n        gt_base_centers2d = base_centers2d_list[batch_id] * width_ratio\n        trunc_masks = trunc_mask_list[batch_id]\n        for (j, base_center2d) in enumerate(gt_base_centers2d):\n            if trunc_masks[j]:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_w = min(base_center2d_x_int - gt_bboxes[j][0], gt_bboxes[j][2] - base_center2d_x_int)\n                scale_box_h = min(base_center2d_y_int - gt_bboxes[j][1], gt_bboxes[j][3] - base_center2d_y_int)\n                radius_x = scale_box_w * self.edge_heatmap_ratio\n                radius_y = scale_box_h * self.edge_heatmap_ratio\n                (radius_x, radius_y) = (max(0, int(radius_x)), max(0, int(radius_y)))\n                assert min(radius_x, radius_y) == 0\n                ind = gt_labels[j]\n                get_ellip_gaussian_2D(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius_x, radius_y)\n            else:\n                (base_center2d_x_int, base_center2d_y_int) = base_center2d.int()\n                scale_box_h = gt_bboxes[j][3] - gt_bboxes[j][1]\n                scale_box_w = gt_bboxes[j][2] - gt_bboxes[j][0]\n                radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n                radius = max(0, int(radius))\n                ind = gt_labels[j]\n                gen_gaussian_target(center_heatmap_target[batch_id, ind], [base_center2d_x_int, base_center2d_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [centers2d.shape[0] for centers2d in centers2d_list]\n    max_objs = max(num_ctrs)\n    batch_indices = [centers2d_list[0].new_full((num_ctrs[i],), i) for i in range(batch_size)]\n    batch_indices = torch.cat(batch_indices, dim=0)\n    reg_mask = torch.zeros((batch_size, max_objs), dtype=torch.bool).to(base_centers2d_list[0].device)\n    gt_bboxes_3d = input_metas['box_type_3d'].cat(gt_bboxes_3d_list)\n    gt_bboxes_3d = gt_bboxes_3d.to(base_centers2d_list[0].device)\n    orienations_target = self.bbox_coder.encode(gt_bboxes_3d)\n    batch_base_centers2d = base_centers2d_list[0].new_zeros((batch_size, max_objs, 2))\n    for i in range(batch_size):\n        reg_mask[i, :num_ctrs[i]] = 1\n        batch_base_centers2d[i, :num_ctrs[i]] = base_centers2d_list[i]\n    flatten_reg_mask = reg_mask.flatten()\n    batch_base_centers2d = batch_base_centers2d.view(-1, 2) * width_ratio\n    dimensions_target = gt_bboxes_3d.tensor[:, 3:6]\n    labels_3d = torch.cat(gt_labels_3d_list)\n    keypoints2d_target = torch.cat(keypoints2d_list)\n    keypoints_mask = torch.cat(keypoints_mask_list)\n    keypoints_depth_mask = torch.cat(keypoints_depth_mask_list)\n    offsets2d_target = torch.cat(offsets2d_list)\n    bboxes2d = torch.cat(gt_bboxes_list)\n    bboxes2d_target = torch.cat([bboxes2d[:, 0:2] * -1, bboxes2d[:, 2:]], dim=-1)\n    depths = torch.cat(depths_list)\n    target_labels = dict(base_centers2d_target=batch_base_centers2d.int(), labels3d=labels_3d, reg_mask=flatten_reg_mask, batch_indices=batch_indices, bboxes2d_target=bboxes2d_target, depth_target=depths, keypoints2d_target=keypoints2d_target, keypoints_mask=keypoints_mask, keypoints_depth_mask=keypoints_depth_mask, orienations_target=orienations_target, offsets2d_target=offsets2d_target, dimensions_target=dimensions_target, downsample_ratio=1 / width_ratio)\n    return (center_heatmap_target, avg_factor, target_labels)"
        ]
    },
    {
        "func_name": "loss",
        "original": "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    \"\"\"Compute loss of the head.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level.\n                shape (num_gt, 4).\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\n                number is bbox_code_size.\n                shape (B, 7, H, W).\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\n                shape (num_gts, ).\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\n                truth. it is the flipped gt_bboxes\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\n            centers2d (list[Tensor]): 2D centers on the image.\n                shape (num_gts, 2).\n            depths (list[Tensor]): Depth ground truth.\n                shape (num_gts, ).\n            attr_labels (list[Tensor]): Attributes indices of each box.\n                In kitti it's None.\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n                Default: None.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict",
        "mutated": [
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, input_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, center2d_heatmap.shape, input_metas[0]['pad_shape'], input_metas)\n    preds = self.get_predictions(pred_reg=pred_reg, labels3d=target_labels['labels3d'], centers2d=target_labels['base_centers2d_target'], reg_mask=target_labels['reg_mask'], batch_indices=target_labels['batch_indices'], input_metas=input_metas, downsample_ratio=target_labels['downsample_ratio'])\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    loss_bbox = self.loss_bbox(preds['bboxes2d'], target_labels['bboxes2d_target'])\n    keypoints2d_mask = target_labels['keypoints2d_mask']\n    loss_keypoints = self.loss_keypoints(preds['keypoints2d'][keypoints2d_mask], target_labels['keypoints2d_target'][keypoints2d_mask])\n    loss_dir = self.loss_dir(preds['orientations'], target_labels['orientations_target'])\n    loss_dims = self.loss_dims(preds['dimensions'], target_labels['dimensions_target'])\n    loss_offsets2d = self.loss_offsets2d(preds['offsets2d'], target_labels['offsets2d_target'])\n    direct_depth_weights = torch.exp(-preds['direct_depth_uncertainty'])\n    loss_weight_1 = self.loss_direct_depth.loss_weight\n    loss_direct_depth = self.loss_direct_depth(preds['direct_depth'], target_labels['depth_target'], direct_depth_weights)\n    loss_uncertainty_1 = preds['direct_depth_uncertainty'] * loss_weight_1\n    loss_direct_depth = loss_direct_depth + loss_uncertainty_1.mean()\n    depth_mask = target_labels['keypoints_depth_mask']\n    depth_target = target_labels['depth_target'].unsqueeze(-1).repeat(1, 3)\n    valid_keypoints_depth_uncertainty = preds['keypoints_depth_uncertainty'][depth_mask]\n    valid_keypoints_depth_weights = torch.exp(-valid_keypoints_depth_uncertainty)\n    loss_keypoints_depth = self.loss_keypoint_depth(preds['keypoints_depth'][depth_mask], depth_target[depth_mask], valid_keypoints_depth_weights)\n    loss_weight_2 = self.loss_keypoints_depth.loss_weight\n    loss_uncertainty_2 = valid_keypoints_depth_uncertainty * loss_weight_2\n    loss_keypoints_depth = loss_keypoints_depth + loss_uncertainty_2.mean()\n    loss_combined_depth = self.loss_combined_depth(preds['combined_depth'], target_labels['depth_target'])\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox, loss_keypoints=loss_keypoints, loss_dir=loss_dir, loss_dims=loss_dims, loss_offsets2d=loss_offsets2d, loss_direct_depth=loss_direct_depth, loss_keypoints_depth=loss_keypoints_depth, loss_combined_depth=loss_combined_depth)\n    return loss_dict"
        ]
    }
]