[
    {
        "func_name": "_get_conv_kernel",
        "original": "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    \"\"\"\n    Builds the convolution kernel for a specified filter size.\n\n    Arguments:\n        dtype (np.dtype): The data type which the kernel will operate on.\n        filter_size (int): Total number of elements per filter (R * S)\n        bsum (boolean): If set to true, kernel will include code to compute\n            batch sum during fprop\n        operation (string): Determines which kernel to build. options follow:\n            'fprop': Forward propagation of activations.\n            'bprop': Backward propagation of error.\n            'update': Computes gradients for filter weights based on error and inputs.\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\n            not a multiple of 32.\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\n    \"\"\"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel",
        "mutated": [
            "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    if False:\n        i = 10\n    \"\\n    Builds the convolution kernel for a specified filter size.\\n\\n    Arguments:\\n        dtype (np.dtype): The data type which the kernel will operate on.\\n        filter_size (int): Total number of elements per filter (R * S)\\n        bsum (boolean): If set to true, kernel will include code to compute\\n            batch sum during fprop\\n        operation (string): Determines which kernel to build. options follow:\\n            'fprop': Forward propagation of activations.\\n            'bprop': Backward propagation of error.\\n            'update': Computes gradients for filter weights based on error and inputs.\\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\\n            not a multiple of 32.\\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\\n    \"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel",
            "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Builds the convolution kernel for a specified filter size.\\n\\n    Arguments:\\n        dtype (np.dtype): The data type which the kernel will operate on.\\n        filter_size (int): Total number of elements per filter (R * S)\\n        bsum (boolean): If set to true, kernel will include code to compute\\n            batch sum during fprop\\n        operation (string): Determines which kernel to build. options follow:\\n            'fprop': Forward propagation of activations.\\n            'bprop': Backward propagation of error.\\n            'update': Computes gradients for filter weights based on error and inputs.\\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\\n            not a multiple of 32.\\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\\n    \"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel",
            "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Builds the convolution kernel for a specified filter size.\\n\\n    Arguments:\\n        dtype (np.dtype): The data type which the kernel will operate on.\\n        filter_size (int): Total number of elements per filter (R * S)\\n        bsum (boolean): If set to true, kernel will include code to compute\\n            batch sum during fprop\\n        operation (string): Determines which kernel to build. options follow:\\n            'fprop': Forward propagation of activations.\\n            'bprop': Backward propagation of error.\\n            'update': Computes gradients for filter weights based on error and inputs.\\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\\n            not a multiple of 32.\\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\\n    \"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel",
            "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Builds the convolution kernel for a specified filter size.\\n\\n    Arguments:\\n        dtype (np.dtype): The data type which the kernel will operate on.\\n        filter_size (int): Total number of elements per filter (R * S)\\n        bsum (boolean): If set to true, kernel will include code to compute\\n            batch sum during fprop\\n        operation (string): Determines which kernel to build. options follow:\\n            'fprop': Forward propagation of activations.\\n            'bprop': Backward propagation of error.\\n            'update': Computes gradients for filter weights based on error and inputs.\\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\\n            not a multiple of 32.\\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\\n    \"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel",
            "@context_dependent_memoize\ndef _get_conv_kernel(dtype, filter_size, bsum, operation, filter_bounds_check=False, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Builds the convolution kernel for a specified filter size.\\n\\n    Arguments:\\n        dtype (np.dtype): The data type which the kernel will operate on.\\n        filter_size (int): Total number of elements per filter (R * S)\\n        bsum (boolean): If set to true, kernel will include code to compute\\n            batch sum during fprop\\n        operation (string): Determines which kernel to build. options follow:\\n            'fprop': Forward propagation of activations.\\n            'bprop': Backward propagation of error.\\n            'update': Computes gradients for filter weights based on error and inputs.\\n        filter_bounds_check (boolean): Checks if filter weight is in bounds when K is\\n            not a multiple of 32.\\n        debug (boolean): When set to true, kernels will be compiled with debug symbols.\\n    \"\n    assert operation in ['fprop', 'bprop', 'update']\n    if operation == 'fprop' or operation == 'update':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_x, base_y;\\n\\n        base_x = output_pixel_x * stride_w - padding_w;\\n        base_y = output_pixel_y * stride_h - padding_h;\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_x = base_x + filter_x * dilation_w;\\n            int index_y = base_y + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (index_x >= 0 && index_x < W && index_y >= 0 && index_y < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = ((index_y * W + index_x) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    elif operation == 'bprop':\n        lut_code = '\\n    if(tid < 32)\\n    {\\n        int rs = tid;\\n        int base_q, base_p;\\n\\n        base_q = output_pixel_x - ((S - 1) * dilation_w - padding_w);\\n        base_p = output_pixel_y - ((R - 1) * dilation_h - padding_h);\\n\\n        unsigned int mask = (1 << tid) - 1;\\n\\n        while(rs < FILTER_SIZE)\\n        {\\n            int filter_x, filter_y;\\n            _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n            int index_q = base_q + filter_x * dilation_w;\\n            int index_p = base_p + filter_y * dilation_h;\\n\\n            //Check if the index is valid\\n            int in_bounds = (((index_q % stride_w) | (index_p % stride_h)) == 0);\\n            index_q /= stride_w;\\n            index_p /= stride_h;\\n            in_bounds = in_bounds && (index_q >= 0 && index_q < W\\n                                      && index_p >= 0 && index_p < H);\\n            unsigned int threads_in_bounds = __ballot(in_bounds);\\n\\n            //Store lookup table entry\\n            if(in_bounds)\\n            {\\n                int2 lut_entry;\\n                lut_entry.x = (((index_p * W) + index_q) * N) >> 2;\\n                lut_entry.y = (rs * K) >> 2;\\n\\n                int index = lut_size_local + __popc(threads_in_bounds & mask);\\n                lookup_table[index] = lut_entry;\\n            }\\n\\n            lut_size_local += __popc(threads_in_bounds);\\n\\n            rs += 32;\\n        }\\n    }\\n'\n    if bsum:\n        bsum_code = '\\n            float local_bsum = result[q_offset].f[0] + result[q_offset].f[1] +\\n                               result[q_offset].f[2] + result[q_offset].f[3];\\n            atomicAdd(&bsum[filter_id], local_bsum);\\n'\n    else:\n        bsum_code = ''\n    if operation == 'update':\n        a_name = 'image'\n        b_name = 'error'\n    elif operation == 'fprop':\n        a_name = 'image'\n        b_name = 'filter'\n    elif operation == 'bprop':\n        a_name = 'error'\n        b_name = 'filter'\n    if filter_bounds_check:\n        filter_load_cond = 'int filter_load_in_bounds = (((filter_id + threadIdx.x) << 2) < K);'\n        check_filter_cond = '(!filter_load_in_bounds) ? make_float4(0, 0, 0, 0) :'\n    else:\n        filter_load_cond = ''\n        check_filter_cond = ''\n    header_code = '\\n#define TILE_DIM            32\\n#define ITEMS_PER_THREAD    4\\n#define THREADS_DIM         8\\n\\n#define REG_TILE_X          4\\n#define REG_TILE_Y          4\\n#define THREADS_DIM_X       8\\n#define THREADS_DIM_Y       8\\n#define SM_TILE_X           (REG_TILE_X * THREADS_DIM_X)\\n#define SM_TILE_Y           (REG_TILE_Y * THREADS_DIM_Y)\\n\\n#define NUM_ROWS            8\\n#define FILTER_SIZE         %(filter_size)s\\n#define MAGIC_FILTER_SIZE   %(magic_filter_size)s\\n#define SHIFT_FILTER_SIZE   %(shift_filter_size)s\\n\\ntypedef union Matrix {\\n    %(type)s4 f4;\\n    %(type)s f[4];\\n} Matrix;\\n\\n__device__ inline void _idiv_fast(int numerator, int denominator, float rcp,\\n                                 int& result, int& remainder)\\n{\\n    result = (int)((float)numerator * rcp);\\n    remainder = numerator - (result * denominator);\\n    result = (remainder >= denominator) ? (result + 1) : result;\\n    remainder = (remainder >= denominator) ? (remainder - denominator) : remainder;\\n}\\n\\n__device__ inline void _idiv_magic(int numerator, unsigned int magic, unsigned int shift,\\n                                   int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        unsigned long long res64 = numerator * (unsigned long long)magic;\\n        result = ((int)(res64 >> 32) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n__device__ inline void _idiv_magic32(int numerator, unsigned int magic, unsigned int shift,\\n                                     int denominator, int& result, int& remainder)\\n{\\n    if(magic == 1)\\n    {\\n        result = numerator >> shift;\\n    }\\n    else\\n    {\\n        result = ((numerator * magic) >> shift);\\n    }\\n    remainder = numerator - (result * denominator);\\n}\\n\\n//Note: N and K must be multiples of 4\\n//blockIdx.x is gemm tile id (K dimension) and output pixel id\\n//blockIdx.y is gemm tile id (N dimension)\\n//threadIdx.x is gemm tile offset (K dimension)\\n//threadIdx.y is gemm tile offset (N dimension)\\n__global__ void conv_%(operation)s(\\n                           %(type)s alpha, %(type)s beta,\\n                           Matrix *I, Matrix *F, Matrix *O, float* bsum,\\n                           int C, int D, int H, int W, int N,\\n                           int T, int R, int S, int K,\\n                           int M, int P, int Q,\\n                           int stride_w, int stride_h, int padding_w, int padding_h,\\n                           int dilation_w, int dilation_h,\\n                           int input_channel_size, int filter_channel_size,\\n                           int output_filter_size,\\n                           int output_pixels, int grid_p, int grid_q,\\n                           unsigned int magic_pq, unsigned int shift_pq,\\n                           unsigned int magic_q, unsigned int shift_q,\\n                           unsigned int magic_s, unsigned int shift_s)\\n\\n'\n    code = '\\n{\\n    __shared__ int2 lookup_table[FILTER_SIZE];\\n    __shared__ int lut_size;\\n    __shared__ Matrix %(a_name)s_data[NUM_ROWS][THREADS_DIM_X];\\n    __shared__ Matrix %(b_name)s_data[NUM_ROWS][THREADS_DIM_Y];\\n\\n    int lut_size_local = 0;\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, image_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, image_id, output_pixel);\\n    image_id = (image_id * blockDim.x);\\n\\n    //Zig zag along x axis to increase cache hits\\n    int temp_x, temp_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, Q, temp_y, temp_x);\\n    int output_pixel_x = (temp_y & 1) ? (Q - temp_x - 1) : temp_x;\\n    int output_pixel_y = temp_y;\\n    output_pixel = output_pixel_x + (output_pixel_y * Q);\\n\\n    int filter_id = blockIdx.y * blockDim.y;\\n    int tid = threadIdx.x + threadIdx.y * blockDim.x;\\n\\n    //Offset buffers based on thread id\\n    I = &(I[image_id  + threadIdx.x]);\\n    F = &(F[filter_id + threadIdx.x]);\\n\\n    %(filter_load_cond)s\\n\\n    //Compute lookup table for filter/image data\\n%(lut_code)s\\n\\n    if(tid == 0)\\n    {\\n        lut_size = lut_size_local;\\n    }\\n\\n    __syncthreads();\\n\\n    lut_size_local = lut_size;\\n    Matrix result[REG_TILE_Y] = {0};\\n    output_pixel = (output_pixel * N) >> 2;\\n    if(lut_size_local > 0)\\n    {\\n        //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n        int CRS = lut_size_local * C;\\n\\n        //Compute magic numbers for division by lut_size\\n        float reciprocal = 1.0f / (float)lut_size_local;\\n\\n        //Initialize shared mem for first block\\n        int crs = CRS %% NUM_ROWS;\\n        crs = (crs == 0) ? 8 : crs;\\n\\n        int c, rs;\\n        _idiv_fast(CRS - threadIdx.y - 1, lut_size_local, reciprocal, c, rs);\\n\\n        int2 lut_entry = ((threadIdx.y & 7) >= crs) ? make_int2(0, 0) : lookup_table[rs];\\n        %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            I[(c * input_channel_size)  + lut_entry.x].f4;\\n        %(b_name)s_data[threadIdx.y][threadIdx.x].f4 = %(check_filter_cond)s\\n            ((threadIdx.y & 7) >= crs) ? make_float4(0, 0, 0, 0) :\\n            F[(c * filter_channel_size) + lut_entry.y].f4;\\n\\n        //Iterate over entire filter\\n        for(crs = CRS - crs - 1; crs > 0; crs -= NUM_ROWS)\\n        {\\n            __syncthreads();\\n\\n            #pragma unroll\\n            for(int i = 0; i < NUM_ROWS; i++)\\n            {\\n                Matrix load_row;\\n                Matrix load_col;\\n\\n                load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n                load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                    {\\n                        result[q_offset].f[p_offset] += (load_row.f[p_offset] *\\n                                                         load_col.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            __syncthreads();\\n\\n            //Load new image data and filter weights\\n            _idiv_fast(crs - threadIdx.y, lut_size_local, reciprocal, c, rs);\\n\\n            lut_entry = lookup_table[rs];\\n            %(a_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                I[(c * input_channel_size)  + lut_entry.x].f4;\\n            %(b_name)s_data[threadIdx.y][threadIdx.x].f4 =\\n                %(check_filter_cond)s F[(c * filter_channel_size) + lut_entry.y].f4;\\n        }\\n\\n        __syncthreads();\\n\\n        //Accumulate product for last iteration\\n        #pragma unroll\\n        for(int i = 0; i < NUM_ROWS; i++)\\n        {\\n            Matrix load_row;\\n            Matrix load_col;\\n\\n            load_row.f4 = %(a_name)s_data[i][threadIdx.x].f4;\\n            load_col.f4 = %(b_name)s_data[i][threadIdx.y].f4;\\n\\n            //Accumulate product\\n            #pragma unroll\\n            for(int q_offset = 0; q_offset < REG_TILE_Y; q_offset++)\\n            {\\n                #pragma unroll\\n                for(int p_offset = 0; p_offset < REG_TILE_X; p_offset++)\\n                {\\n                    result[q_offset].f[p_offset] += (load_row.f[p_offset] * load_col.f[q_offset]);\\n                }\\n            }\\n        }\\n    }\\n\\n    //Store result\\n    filter_id = (filter_id + threadIdx.y) << 2;\\n    if(filter_id < K)\\n    {\\n        image_id += threadIdx.x;\\n\\n        #pragma unroll\\n        for(int q_offset = 0; q_offset < 4; q_offset++)\\n        {\\n            if(filter_id < K)\\n            {\\n                int out_index = (filter_id * output_filter_size) + output_pixel + image_id;\\n                %(bsum_code)s\\n\\n                Matrix cur_value = {0};\\n                if(beta > 0.0f)\\n                {\\n                    cur_value.f4 = O[out_index].f4;\\n                }\\n\\n                result[q_offset].f[0] = (result[q_offset].f[0] * alpha) + (cur_value.f[0] * beta);\\n                result[q_offset].f[1] = (result[q_offset].f[1] * alpha) + (cur_value.f[1] * beta);\\n                result[q_offset].f[2] = (result[q_offset].f[2] * alpha) + (cur_value.f[2] * beta);\\n                result[q_offset].f[3] = (result[q_offset].f[3] * alpha) + (cur_value.f[3] * beta);\\n\\n                O[out_index].f4 = result[q_offset].f4;\\n            }\\n            filter_id++;\\n        }\\n    }\\n}\\n'\n    update_code = '\\n{\\n    __shared__ Matrix %(a_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n    __shared__ Matrix %(b_name)s_data[TILE_DIM / 4][THREADS_DIM * 4 + 4];\\n\\n    //TODO: Use square access pattern to image data to increase cache hits\\n    int output_pixel, filter_id;\\n    _idiv_magic(blockIdx.x, magic_pq, shift_pq, output_pixels, filter_id, output_pixel);\\n    filter_id = filter_id * TILE_DIM;\\n    int load_filter_id = filter_id + threadIdx.y;\\n\\n    int filter_pixel_id = blockIdx.y * TILE_DIM;\\n\\n    //TODO: Zig zag along x axis to increase cache hits\\n    int output_pixel_x, output_pixel_y;\\n    _idiv_magic(output_pixel, magic_q, shift_q, grid_q, output_pixel_y, output_pixel_x);\\n\\n    //Compute input image and filter offsets for this pixel\\n    int c, rs;\\n    int crs = filter_pixel_id + threadIdx.y;\\n    _idiv_magic(crs, MAGIC_FILTER_SIZE, SHIFT_FILTER_SIZE, FILTER_SIZE, c, rs);\\n\\n    int filter_x, filter_y;\\n    _idiv_magic32(rs, magic_s, shift_s, S, filter_y, filter_x);\\n\\n    int output_pixel_x_save = output_pixel_x;\\n    for(; output_pixel_y < P; output_pixel_y += grid_p)\\n    {\\n        for(output_pixel_x = output_pixel_x_save; output_pixel_x < Q; output_pixel_x += grid_q)\\n        {\\n            int base_x = output_pixel_x * stride_w - padding_w + filter_x * dilation_w;\\n            int base_y = output_pixel_y * stride_h - padding_h + filter_y * dilation_h;\\n            int crs_in_bounds = (c < C) && (base_x >= 0) && (base_x < W) &&\\n                                (base_y >= 0) && (base_y < H);\\n            int input_pixel = W * base_y + base_x;\\n            output_pixel = output_pixel_x + (Q * output_pixel_y);\\n\\n            //Pre-multiply offset to simplify indexing\\n            input_pixel = (input_pixel * N) >> 2;\\n            output_pixel = (output_pixel * N) >> 2;\\n\\n            //Evaluate gemm with outer product dimensions N, K and inner product CRS\\n            Matrix result[ITEMS_PER_THREAD] = {0};\\n\\n            //Load image data and transpose into shared mem\\n            //TODO: pad shared memory to avoid bank conflicts\\n            Matrix buffer;\\n            buffer.f4 = crs_in_bounds ?\\n                        I[(c * input_channel_size) + input_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Load error data and transpose into shared mem\\n            buffer.f4 = (load_filter_id < K) ?\\n                        F[(load_filter_id * output_filter_size) + output_pixel + threadIdx.x].f4 :\\n                        make_float4(0, 0, 0, 0);\\n            %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[0];\\n            %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[1];\\n            %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[2];\\n            %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] = buffer.f[3];\\n\\n            //Iterate over entire minibatch\\n            for(int n = threadIdx.x + (TILE_DIM >> 2); n < (N >> 2); n += (TILE_DIM >> 2))\\n            {\\n                __syncthreads();\\n\\n                #pragma unroll\\n                for(int i = 0; i < (TILE_DIM >> 2); i++)\\n                {\\n                    Matrix row_image;\\n                    Matrix row_error;\\n\\n                    row_image.f4 =\\n                        %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                    row_error.f4 =\\n                        %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                    //Accumulate product\\n                    #pragma unroll\\n                    for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                    {\\n                        #pragma unroll\\n                        for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                        {\\n                            result[p_offset].f[q_offset] +=\\n                                (row_image.f[p_offset] * row_error.f[q_offset]);\\n                        }\\n                    }\\n                }\\n\\n                __syncthreads();\\n\\n                //Load image data and transpose into shared mem\\n                buffer.f4 = crs_in_bounds ?\\n                    I[(c * input_channel_size) + input_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(a_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(a_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(a_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(a_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n\\n                //Load error data and transpose into shared mem\\n                buffer.f4 = (load_filter_id < K) ?\\n                    F[(load_filter_id * output_filter_size) + output_pixel + n].f4 :\\n                    make_float4(0, 0, 0, 0);\\n                %(b_name)s_data[threadIdx.x][ 0 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[0];\\n                %(b_name)s_data[threadIdx.x][ 8 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[1];\\n                %(b_name)s_data[threadIdx.x][16 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[2];\\n                %(b_name)s_data[threadIdx.x][24 | threadIdx.y >> 2].f[threadIdx.y & 3] =\\n                    buffer.f[3];\\n            }\\n\\n            __syncthreads();\\n\\n            //Accumulate product for last iteration\\n            #pragma unroll\\n            for(int i = 0; i < (TILE_DIM >> 2); i++)\\n            {\\n                Matrix row_image;\\n                Matrix row_error;\\n\\n                row_image.f4 = %(a_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.y >> 2].f4;\\n                row_error.f4 = %(b_name)s_data[i][((threadIdx.y & 3) << 3) | threadIdx.x].f4;\\n\\n                //Accumulate product\\n                #pragma unroll\\n                for(int q_offset = 0; q_offset < ITEMS_PER_THREAD; q_offset++)\\n                {\\n                    #pragma unroll\\n                    for(int p_offset = 0; p_offset < ITEMS_PER_THREAD; p_offset++)\\n                    {\\n                        result[p_offset].f[q_offset] +=\\n                            (row_image.f[p_offset] * row_error.f[q_offset]);\\n                    }\\n                }\\n            }\\n\\n            //Reduce result between threads in warp\\n            Matrix outbound;\\n            int warp_y = threadIdx.y & 3;\\n            int warp_id = threadIdx.x + (threadIdx.y << 3);\\n            buffer.f4 = (warp_y == 0) ? result[0].f4 :\\n                        (warp_y == 1) ? result[1].f4 :\\n                        (warp_y == 2) ? result[2].f4 :\\n                        result[3].f4;\\n\\n            outbound.f4 = (warp_y == 0) ? result[3].f4 :\\n                          (warp_y == 1) ? result[0].f4 :\\n                          (warp_y == 2) ? result[1].f4 :\\n                          result[2].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 8);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 8);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 8);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 8);\\n\\n            outbound.f4 = (warp_y == 0) ? result[2].f4 :\\n                          (warp_y == 1) ? result[3].f4 :\\n                          (warp_y == 2) ? result[0].f4 :\\n                          result[1].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 16);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 16);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 16);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 16);\\n\\n            outbound.f4 = (warp_y == 0) ? result[1].f4 :\\n                          (warp_y == 1) ? result[2].f4 :\\n                          (warp_y == 2) ? result[3].f4 :\\n                          result[0].f4;\\n            buffer.f[0] += __shfl(outbound.f[0], warp_id + 24);\\n            buffer.f[1] += __shfl(outbound.f[1], warp_id + 24);\\n            buffer.f[2] += __shfl(outbound.f[2], warp_id + 24);\\n            buffer.f[3] += __shfl(outbound.f[3], warp_id + 24);\\n\\n            //Store result\\n            int idx_filter_id = filter_id + (threadIdx.x << 2);\\n            if(idx_filter_id < K && crs_in_bounds)\\n            {\\n                int out_index = (c * filter_channel_size) + (((rs * K) + (idx_filter_id)) >> 2);\\n\\n                atomicAdd(&O[out_index].f[0], buffer.f[0]);\\n                atomicAdd(&O[out_index].f[1], buffer.f[1]);\\n                atomicAdd(&O[out_index].f[2], buffer.f[2]);\\n                atomicAdd(&O[out_index].f[3], buffer.f[3]);\\n            }\\n        }\\n    }\\n}\\n'\n    if operation == 'update':\n        code = header_code + update_code\n    else:\n        code = header_code + code\n    magic = _magic64(filter_size)\n    code = code % {'filter_size': filter_size, 'magic_filter_size': magic[0], 'shift_filter_size': magic[1], 'type': _ew_types[dtype]['type'], 'lut_code': lut_code, 'bsum_code': bsum_code, 'operation': operation, 'a_name': a_name, 'b_name': b_name, 'filter_load_cond': filter_load_cond, 'check_filter_cond': check_filter_cond}\n    options = ['--use_fast_math']\n    if debug and operation == 'bprop':\n        options = options + ['-g', '-G']\n    module = SourceModule(code, options=options)\n    kernel = module.get_function('conv_' + operation)\n    kernel.prepare('ffPPPPIIIIIIIIIIIIIIIIIIIIIIIIIIIIII')\n    kernel.name = 'conv_' + operation\n    return kernel"
        ]
    }
]