[
    {
        "func_name": "smart_tokenizer_and_embedding_resize",
        "original": "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
        "mutated": [
            "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    if False:\n        i = 10\n    'Resize tokenizer and embedding.\\n\\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\\n    '\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
            "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resize tokenizer and embedding.\\n\\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\\n    '\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
            "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resize tokenizer and embedding.\\n\\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\\n    '\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
            "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resize tokenizer and embedding.\\n\\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\\n    '\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
            "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer, model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resize tokenizer and embedding.\\n\\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\\n    '\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
        ]
    },
    {
        "func_name": "make_same_shape",
        "original": "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')",
        "mutated": [
            "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if False:\n        i = 10\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')",
            "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')",
            "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')",
            "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')",
            "def make_same_shape(model_raw: Model, model_convert: Model, tokenizer_raw, tokenizer_convert):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if model_raw.__class__ != model_convert.__class__:\n        logger.error(f'weight diff: These two models should be of the same class. model_raw:{model_raw.__class__} vs model_convert: {model_convert.__class__}.')\n    special_tokens = {}\n    for (k, v) in tokenizer_convert.special_tokens_map_extended.items():\n        if k not in tokenizer_raw.special_tokens_map_extended:\n            special_tokens[k] = v\n    smart_tokenizer_and_embedding_resize(special_tokens_dict=special_tokens, model=model_raw, tokenizer=tokenizer_raw)\n    state_dict_tuned = model_convert.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        if state_dict_tuned[key].shape != state_dict_raw[key].shape:\n            logger.error(f'weight diff: shape mismatch. {key}, model_raw shape: {state_dict_raw[key].shape} vs model_convert shape: {state_dict_tuned[key].shape}.')"
        ]
    },
    {
        "func_name": "_weight_diff",
        "original": "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)",
        "mutated": [
            "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    if False:\n        i = 10\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)",
            "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)",
            "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)",
            "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)",
            "def _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=None, make_diff_or_recover='diff'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    make_same_shape(model_raw, model_convert, tokenizer_raw, tokenizer_convert)\n    state_dict_raw = model_raw.state_dict()\n    state_dict_convert = model_convert.state_dict()\n    if make_diff_or_recover == 'diff':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(-state_dict_raw[key])\n    elif make_diff_or_recover == 'recover':\n        for key in tqdm.tqdm(state_dict_convert):\n            state_dict_convert[key].add_(state_dict_raw[key])\n    if path_to_save:\n        model_convert.save_pretrained(path_to_save, 'pytorch_model.bin')\n        tokenizer_convert.save_pretrained(path_to_save)\n    return (model_convert, tokenizer_convert)"
        ]
    },
    {
        "func_name": "weight_diff",
        "original": "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    \"\"\"Make the weight diff.\n\n    This function is given to present full transparency of how the weight diff was created.\n    \"\"\"\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)",
        "mutated": [
            "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    if False:\n        i = 10\n    'Make the weight diff.\\n\\n    This function is given to present full transparency of how the weight diff was created.\\n    '\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)",
            "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Make the weight diff.\\n\\n    This function is given to present full transparency of how the weight diff was created.\\n    '\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)",
            "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Make the weight diff.\\n\\n    This function is given to present full transparency of how the weight diff was created.\\n    '\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)",
            "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Make the weight diff.\\n\\n    This function is given to present full transparency of how the weight diff was created.\\n    '\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)",
            "@torch.inference_mode()\ndef weight_diff(path_raw: str, path_convert: str, path_to_save: str, make_diff_or_recover, device='cpu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Make the weight diff.\\n\\n    This function is given to present full transparency of how the weight diff was created.\\n    '\n    if not os.path.exists(path_raw):\n        logger.info(f'Path `{path_raw}` not found. Try to load from cache or remote.')\n        path_raw = snapshot_download(path_raw)\n    if not os.path.exists(path_convert):\n        logger.info(f'Path `{path_convert}` not found. Try to load from cache or remote.')\n        path_convert = snapshot_download(path_convert)\n    model_raw = Model.from_pretrained(path_raw, device=device)\n    model_convert = Model.from_pretrained(path_convert, device=device)\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_raw)\n    tokenizer_convert: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(path_convert)\n    return _weight_diff(model_raw, model_convert, tokenizer_raw, tokenizer_convert, path_to_save=path_to_save, make_diff_or_recover=make_diff_or_recover)"
        ]
    }
]