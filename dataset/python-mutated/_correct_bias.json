[
    {
        "func_name": "get_module",
        "original": "def get_module(model, name):\n    \"\"\"Given name of submodule, this function grabs the submodule from given model.\"\"\"\n    return dict(model.named_modules())[name]",
        "mutated": [
            "def get_module(model, name):\n    if False:\n        i = 10\n    'Given name of submodule, this function grabs the submodule from given model.'\n    return dict(model.named_modules())[name]",
            "def get_module(model, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Given name of submodule, this function grabs the submodule from given model.'\n    return dict(model.named_modules())[name]",
            "def get_module(model, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Given name of submodule, this function grabs the submodule from given model.'\n    return dict(model.named_modules())[name]",
            "def get_module(model, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Given name of submodule, this function grabs the submodule from given model.'\n    return dict(model.named_modules())[name]",
            "def get_module(model, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Given name of submodule, this function grabs the submodule from given model.'\n    return dict(model.named_modules())[name]"
        ]
    },
    {
        "func_name": "parent_child_names",
        "original": "def parent_child_names(name):\n    \"\"\"Split full name of submodule into parent submodule's full name and submodule's name.\"\"\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])",
        "mutated": [
            "def parent_child_names(name):\n    if False:\n        i = 10\n    \"Split full name of submodule into parent submodule's full name and submodule's name.\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])",
            "def parent_child_names(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Split full name of submodule into parent submodule's full name and submodule's name.\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])",
            "def parent_child_names(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Split full name of submodule into parent submodule's full name and submodule's name.\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])",
            "def parent_child_names(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Split full name of submodule into parent submodule's full name and submodule's name.\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])",
            "def parent_child_names(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Split full name of submodule into parent submodule's full name and submodule's name.\"\n    split_name = name.rsplit('.', 1)\n    if len(split_name) == 1:\n        return ('', split_name[0])\n    else:\n        return (split_name[0], split_name[1])"
        ]
    },
    {
        "func_name": "get_param",
        "original": "def get_param(module, attr):\n    \"\"\"Get the parameter given a module and attribute.\n\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\n    gives a function that will give you the raw tensor, this function takes care of that logic\n    \"\"\"\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param",
        "mutated": [
            "def get_param(module, attr):\n    if False:\n        i = 10\n    'Get the parameter given a module and attribute.\\n\\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\\n    gives a function that will give you the raw tensor, this function takes care of that logic\\n    '\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param",
            "def get_param(module, attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the parameter given a module and attribute.\\n\\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\\n    gives a function that will give you the raw tensor, this function takes care of that logic\\n    '\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param",
            "def get_param(module, attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the parameter given a module and attribute.\\n\\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\\n    gives a function that will give you the raw tensor, this function takes care of that logic\\n    '\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param",
            "def get_param(module, attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the parameter given a module and attribute.\\n\\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\\n    gives a function that will give you the raw tensor, this function takes care of that logic\\n    '\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param",
            "def get_param(module, attr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the parameter given a module and attribute.\\n\\n    Sometimes the weights/bias attribute gives you the raw tensor, but sometimes\\n    gives a function that will give you the raw tensor, this function takes care of that logic\\n    '\n    param = getattr(module, attr, None)\n    if callable(param):\n        return param()\n    else:\n        return param"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    \"\"\"Set up initial values for float and quantized stats, count, float sum, and quant sum.\"\"\"\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    'Set up initial values for float and quantized stats, count, float sum, and quant sum.'\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Set up initial values for float and quantized stats, count, float sum, and quant sum.'\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Set up initial values for float and quantized stats, count, float sum, and quant sum.'\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Set up initial values for float and quantized stats, count, float sum, and quant sum.'\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Set up initial values for float and quantized stats, count, float sum, and quant sum.'\n    super().__init__()\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x, y):\n    \"\"\"Compute the average of quantized and floating-point data from modules.\n\n        The inputs x,y are output data from the quantized and floating-point modules.\n        x is for the quantized module, y is for the floating point module\n        \"\"\"\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count",
        "mutated": [
            "def forward(self, x, y):\n    if False:\n        i = 10\n    'Compute the average of quantized and floating-point data from modules.\\n\\n        The inputs x,y are output data from the quantized and floating-point modules.\\n        x is for the quantized module, y is for the floating point module\\n        '\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the average of quantized and floating-point data from modules.\\n\\n        The inputs x,y are output data from the quantized and floating-point modules.\\n        x is for the quantized module, y is for the floating point module\\n        '\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the average of quantized and floating-point data from modules.\\n\\n        The inputs x,y are output data from the quantized and floating-point modules.\\n        x is for the quantized module, y is for the floating point module\\n        '\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the average of quantized and floating-point data from modules.\\n\\n        The inputs x,y are output data from the quantized and floating-point modules.\\n        x is for the quantized module, y is for the floating point module\\n        '\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count",
            "def forward(self, x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the average of quantized and floating-point data from modules.\\n\\n        The inputs x,y are output data from the quantized and floating-point modules.\\n        x is for the quantized module, y is for the floating point module\\n        '\n    if x.is_quantized:\n        x = x.dequantize()\n    self.count += 1\n    if self.stats['quantized'] is None:\n        self.stats['quantized'] = x\n        self.quant_sum = x\n    else:\n        self.quant_sum += x\n        self.stats['quantized'] = self.quant_sum / self.count\n    if self.stats['float'] is None:\n        self.stats['float'] = y\n        self.float_sum = y\n    else:\n        self.float_sum += y\n        self.stats['float'] = self.float_sum / self.count"
        ]
    },
    {
        "func_name": "clear",
        "original": "def clear(self):\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
        "mutated": [
            "def clear(self):\n    if False:\n        i = 10\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None",
            "def clear(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.stats['float'] = None\n    self.stats['quantized'] = None\n    self.count = 0\n    self.float_sum = None\n    self.quant_sum = None"
        ]
    },
    {
        "func_name": "bias_correction",
        "original": "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    \"\"\"Perform bias correction on a module.\n\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\n    by quantization\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\n\n    Args:\n        float_model: a trained model that serves as a reference to what bias correction should aim for\n        quantized_model: quantized form of float_model that bias correction is to applied to\n        img_data: calibration data to estimate the expected output (used to find quantization error)\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\n                unquantized submodules)\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\n    \"\"\"\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()",
        "mutated": [
            "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    if False:\n        i = 10\n    'Perform bias correction on a module.\\n\\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\\n    by quantization\\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\\n\\n    Args:\\n        float_model: a trained model that serves as a reference to what bias correction should aim for\\n        quantized_model: quantized form of float_model that bias correction is to applied to\\n        img_data: calibration data to estimate the expected output (used to find quantization error)\\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\\n                unquantized submodules)\\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\\n    '\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()",
            "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Perform bias correction on a module.\\n\\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\\n    by quantization\\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\\n\\n    Args:\\n        float_model: a trained model that serves as a reference to what bias correction should aim for\\n        quantized_model: quantized form of float_model that bias correction is to applied to\\n        img_data: calibration data to estimate the expected output (used to find quantization error)\\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\\n                unquantized submodules)\\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\\n    '\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()",
            "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Perform bias correction on a module.\\n\\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\\n    by quantization\\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\\n\\n    Args:\\n        float_model: a trained model that serves as a reference to what bias correction should aim for\\n        quantized_model: quantized form of float_model that bias correction is to applied to\\n        img_data: calibration data to estimate the expected output (used to find quantization error)\\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\\n                unquantized submodules)\\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\\n    '\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()",
            "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Perform bias correction on a module.\\n\\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\\n    by quantization\\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\\n\\n    Args:\\n        float_model: a trained model that serves as a reference to what bias correction should aim for\\n        quantized_model: quantized form of float_model that bias correction is to applied to\\n        img_data: calibration data to estimate the expected output (used to find quantization error)\\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\\n                unquantized submodules)\\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\\n    '\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()",
            "def bias_correction(float_model, quantized_model, img_data, target_modules=_supported_modules_quantized, neval_batches=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Perform bias correction on a module.\\n\\n    Using numeric suite shadow module, the expected output of the floating point and quantized modules\\n    is recorded. Using that data the bias of supported modules is shifted to compensate for the drift caused\\n    by quantization\\n    Paper reference: https://arxiv.org/pdf/1906.04721.pdf (Section 4.2)\\n\\n    Args:\\n        float_model: a trained model that serves as a reference to what bias correction should aim for\\n        quantized_model: quantized form of float_model that bias correction is to applied to\\n        img_data: calibration data to estimate the expected output (used to find quantization error)\\n        target_modules: specifies what submodules in quantized_model need bias correction (can be extended to\\n                unquantized submodules)\\n        neval_batches: a cap to the number of batches you want to be used for estimating the expected output\\n    '\n    ns.prepare_model_with_stubs(float_model, quantized_model, _supported_modules, MeanShadowLogger)\n    uncorrected_modules = {}\n    for (name, submodule) in quantized_model.named_modules():\n        if type(submodule) in target_modules:\n            uncorrected_modules[name] = submodule\n    for uncorrected_module in uncorrected_modules:\n        quantized_submodule = get_module(quantized_model, uncorrected_module)\n        bias = get_param(quantized_submodule, 'bias')\n        if bias is not None:\n            count = 0\n            for data in img_data:\n                quantized_model(data[0])\n                count += 1\n                if count == neval_batches:\n                    break\n            ob_dict = ns.get_logger_dict(quantized_model)\n            (parent_name, _) = parent_child_names(uncorrected_module)\n            float_data = ob_dict[parent_name + '.stats']['float']\n            quant_data = ob_dict[parent_name + '.stats']['quantized']\n            quantization_error = quant_data - float_data\n            dims = list(range(quantization_error.dim()))\n            dims.remove(1)\n            expected_error = torch.mean(quantization_error, dims)\n            updated_bias = bias.data - expected_error\n            bias.data = updated_bias\n            for (name, submodule) in quantized_model.named_modules():\n                if isinstance(submodule, MeanShadowLogger):\n                    submodule.clear()"
        ]
    }
]