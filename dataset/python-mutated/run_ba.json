[
    {
        "func_name": "merge_reconstructions",
        "original": "def merge_reconstructions(reconstructions, tracks_manager):\n    \"\"\"Put all reconstruction points and shots in a single one.\n\n    No alignment is performed.  Just merging the lists of points and\n    shots as they are.\n\n    Point(track) names have to be unique in the merged reconstruction.\n    We add a prefix according to the source reconstruction index.\n    \"\"\"\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged",
        "mutated": [
            "def merge_reconstructions(reconstructions, tracks_manager):\n    if False:\n        i = 10\n    'Put all reconstruction points and shots in a single one.\\n\\n    No alignment is performed.  Just merging the lists of points and\\n    shots as they are.\\n\\n    Point(track) names have to be unique in the merged reconstruction.\\n    We add a prefix according to the source reconstruction index.\\n    '\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged",
            "def merge_reconstructions(reconstructions, tracks_manager):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Put all reconstruction points and shots in a single one.\\n\\n    No alignment is performed.  Just merging the lists of points and\\n    shots as they are.\\n\\n    Point(track) names have to be unique in the merged reconstruction.\\n    We add a prefix according to the source reconstruction index.\\n    '\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged",
            "def merge_reconstructions(reconstructions, tracks_manager):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Put all reconstruction points and shots in a single one.\\n\\n    No alignment is performed.  Just merging the lists of points and\\n    shots as they are.\\n\\n    Point(track) names have to be unique in the merged reconstruction.\\n    We add a prefix according to the source reconstruction index.\\n    '\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged",
            "def merge_reconstructions(reconstructions, tracks_manager):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Put all reconstruction points and shots in a single one.\\n\\n    No alignment is performed.  Just merging the lists of points and\\n    shots as they are.\\n\\n    Point(track) names have to be unique in the merged reconstruction.\\n    We add a prefix according to the source reconstruction index.\\n    '\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged",
            "def merge_reconstructions(reconstructions, tracks_manager):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Put all reconstruction points and shots in a single one.\\n\\n    No alignment is performed.  Just merging the lists of points and\\n    shots as they are.\\n\\n    Point(track) names have to be unique in the merged reconstruction.\\n    We add a prefix according to the source reconstruction index.\\n    '\n    merged = types.Reconstruction()\n    merged.set_reference(reconstructions[0].reference)\n    for (ix_r, reconstruction) in enumerate(reconstructions):\n        assert merged.reference == reconstruction.reference\n        for camera in reconstruction.cameras.values():\n            merged.add_camera(camera)\n        for point in reconstruction.points.values():\n            new_point = merged.create_point(f'R{ix_r}_{point.id}', point.coordinates)\n            new_point.color = point.color\n        for shot in reconstruction.shots.values():\n            merged.add_shot(shot)\n            try:\n                obsdict = tracks_manager.get_shot_observations(shot.id)\n            except RuntimeError:\n                logger.warning(f'Shot id {shot.id} missing from tracks_manager!')\n                continue\n            for (track_id, obs) in obsdict.items():\n                merged_track_id = f'R{ix_r}_{track_id}'\n                if merged_track_id in merged.points:\n                    merged.add_observation(shot.id, merged_track_id, obs)\n    return merged"
        ]
    },
    {
        "func_name": "resplit_reconstruction",
        "original": "def resplit_reconstruction(merged, original_reconstructions):\n    \"\"\"Resplit reconstructions that were previously merged.\n\n    The original_reconstructions are used to decide which point and shot\n    goes on which of the split reconstructions.\n    \"\"\"\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split",
        "mutated": [
            "def resplit_reconstruction(merged, original_reconstructions):\n    if False:\n        i = 10\n    'Resplit reconstructions that were previously merged.\\n\\n    The original_reconstructions are used to decide which point and shot\\n    goes on which of the split reconstructions.\\n    '\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split",
            "def resplit_reconstruction(merged, original_reconstructions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resplit reconstructions that were previously merged.\\n\\n    The original_reconstructions are used to decide which point and shot\\n    goes on which of the split reconstructions.\\n    '\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split",
            "def resplit_reconstruction(merged, original_reconstructions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resplit reconstructions that were previously merged.\\n\\n    The original_reconstructions are used to decide which point and shot\\n    goes on which of the split reconstructions.\\n    '\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split",
            "def resplit_reconstruction(merged, original_reconstructions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resplit reconstructions that were previously merged.\\n\\n    The original_reconstructions are used to decide which point and shot\\n    goes on which of the split reconstructions.\\n    '\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split",
            "def resplit_reconstruction(merged, original_reconstructions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resplit reconstructions that were previously merged.\\n\\n    The original_reconstructions are used to decide which point and shot\\n    goes on which of the split reconstructions.\\n    '\n    split = []\n    for (ix_r, original) in enumerate(original_reconstructions):\n        r = types.Reconstruction()\n        r.set_reference(merged.reference)\n        for shot_id in original.shots:\n            r.add_shot(merged.shots[shot_id])\n        for point_id in original.points:\n            merged_id = f'R{ix_r}_{point_id}'\n            if merged_id not in merged.points:\n                continue\n            merged_point = merged.points[merged_id]\n            new_point = r.create_point(point_id, merged_point.coordinates)\n            new_point.color = merged_point.color\n        for camera_id in original.cameras:\n            r.add_camera(merged.cameras[camera_id])\n        split.append(r)\n    return split"
        ]
    },
    {
        "func_name": "gcp_geopositional_error",
        "original": "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out",
        "mutated": [
            "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out",
            "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out",
            "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out",
            "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out",
            "def gcp_geopositional_error(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    coords_reconstruction = triangulate_gcps(gcps, reconstruction)\n    out = {}\n    for (ix, gcp) in enumerate(gcps):\n        expected = reconstruction.reference.to_lla(*gcp.lla_vec) if gcp.lla else None\n        triangulated = coords_reconstruction[ix] if coords_reconstruction[ix] is not None else None\n        if expected is not None and triangulated is not None:\n            error = np.linalg.norm(expected - triangulated)\n            out[gcp.id] = {'expected_xyz': list(expected), 'triangulated_xyz': list(triangulated), 'expected_lla': reconstruction.reference.to_lla(*expected), 'triangulated_lla': reconstruction.reference.to_lla(*triangulated), 'error': float(error)}\n            (lat, lon, _alt) = out[gcp.id]['expected_lla']\n            expected_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            (lat, lon, _alt) = out[gcp.id]['triangulated_lla']\n            triangulated_xy = reconstruction.reference.to_topocentric(lat, lon, 0)\n            out[gcp.id]['error_planar'] = np.linalg.norm(np.array(expected_xy) - np.array(triangulated_xy))\n        else:\n            out[gcp.id] = {'error': np.nan, 'error_planar': np.nan}\n    return out"
        ]
    },
    {
        "func_name": "triangulate_gcps",
        "original": "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords",
        "mutated": [
            "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords",
            "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords",
            "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords",
            "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords",
            "def triangulate_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    coords = []\n    for gcp in gcps:\n        res = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        coords.append(res)\n    return coords"
        ]
    },
    {
        "func_name": "reproject_gcps",
        "original": "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output",
        "mutated": [
            "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    if False:\n        i = 10\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output",
            "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output",
            "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output",
            "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output",
            "def reproject_gcps(gcps: List[pymap.GroundControlPoint], reconstruction: types.Reconstruction, reproj_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output = {}\n    for gcp in gcps:\n        point = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=reproj_threshold, min_ray_angle_degrees=0.1)\n        output[gcp.id] = {}\n        n_obs = len(gcp.observations)\n        if point is None:\n            logger.info(f'Could not triangulate {gcp.id} with {n_obs} annotations')\n            continue\n        for observation in gcp.observations:\n            (lat, lon, alt) = reconstruction.reference.to_lla(*point)\n            output[gcp.id][observation.shot_id] = {'lla': [lat, lon, alt], 'error': 0}\n            if observation.shot_id not in reconstruction.shots:\n                continue\n            shot = reconstruction.shots[observation.shot_id]\n            reproj = shot.project(point)\n            error = np.linalg.norm(reproj - observation.projection)\n            output[gcp.id][observation.shot_id].update({'error': error, 'reprojection': [reproj[0], reproj[1]]})\n    return output"
        ]
    },
    {
        "func_name": "get_sorted_reprojection_errors",
        "original": "def get_sorted_reprojection_errors(gcp_reprojections):\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])",
        "mutated": [
            "def get_sorted_reprojection_errors(gcp_reprojections):\n    if False:\n        i = 10\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])",
            "def get_sorted_reprojection_errors(gcp_reprojections):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])",
            "def get_sorted_reprojection_errors(gcp_reprojections):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])",
            "def get_sorted_reprojection_errors(gcp_reprojections):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])",
            "def get_sorted_reprojection_errors(gcp_reprojections):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output = []\n    for gcp_id in gcp_reprojections:\n        for shot_id in gcp_reprojections[gcp_id]:\n            e = gcp_reprojections[gcp_id][shot_id]['error']\n            output.append((gcp_id, shot_id, e))\n    return sorted(output, key=lambda t: -t[2])"
        ]
    },
    {
        "func_name": "get_number_of_wrong_annotations_per_gcp",
        "original": "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output",
        "mutated": [
            "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    if False:\n        i = 10\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output",
            "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output",
            "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output",
            "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output",
            "def get_number_of_wrong_annotations_per_gcp(gcp_reprojections, wrong_threshold):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    output = {}\n    for (gcp_id, reprojections) in gcp_reprojections.items():\n        errors = [reprojections[shot_id]['error'] for shot_id in reprojections]\n        output[gcp_id] = sum((e > wrong_threshold for e in errors))\n    return output"
        ]
    },
    {
        "func_name": "compute_gcp_std",
        "original": "def compute_gcp_std(gcp_errors):\n    \"\"\"Compute the standard deviation of reprojection errors of all gcps.\"\"\"\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))",
        "mutated": [
            "def compute_gcp_std(gcp_errors):\n    if False:\n        i = 10\n    'Compute the standard deviation of reprojection errors of all gcps.'\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))",
            "def compute_gcp_std(gcp_errors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the standard deviation of reprojection errors of all gcps.'\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))",
            "def compute_gcp_std(gcp_errors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the standard deviation of reprojection errors of all gcps.'\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))",
            "def compute_gcp_std(gcp_errors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the standard deviation of reprojection errors of all gcps.'\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))",
            "def compute_gcp_std(gcp_errors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the standard deviation of reprojection errors of all gcps.'\n    all_errors = []\n    for gcp_id in gcp_errors:\n        errors = [e['error'] for e in gcp_errors[gcp_id].values()]\n        logger.info(f'gcp {gcp_id} mean reprojection error = {np.mean(errors)}')\n        all_errors.extend(errors)\n    all_errors = [e for e in all_errors if not np.isnan(e)]\n    return np.sqrt(np.mean(np.array(all_errors) ** 2))"
        ]
    },
    {
        "func_name": "find_alignment",
        "original": "def find_alignment(points0, points1):\n    \"\"\"Compute similarity transform between point sets.\n\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\n    \"\"\"\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)",
        "mutated": [
            "def find_alignment(points0, points1):\n    if False:\n        i = 10\n    'Compute similarity transform between point sets.\\n\\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\\n    '\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)",
            "def find_alignment(points0, points1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute similarity transform between point sets.\\n\\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\\n    '\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)",
            "def find_alignment(points0, points1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute similarity transform between point sets.\\n\\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\\n    '\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)",
            "def find_alignment(points0, points1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute similarity transform between point sets.\\n\\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\\n    '\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)",
            "def find_alignment(points0, points1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute similarity transform between point sets.\\n\\n    Returns (s, A, b) such that ``points0 = s * A * points1 + b``\\n    '\n    (v0, v1) = ([], [])\n    for (p0, p1) in zip(points0, points1):\n        if p0 is not None and p1 is not None:\n            v0.append(p0)\n            v1.append(p1)\n    v0 = np.array(v0).T\n    v1 = np.array(v1).T\n    M = tf.affine_matrix_from_points(v0, v1, shear=False)\n    s = np.linalg.det(M[:3, :3]) ** (1.0 / 3.0)\n    A = M[:3, :3] / s\n    b = M[:3, 3]\n    return (s, A, b)"
        ]
    },
    {
        "func_name": "add_gcp_to_bundle",
        "original": "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    \"\"\"Add Ground Control Points constraints to the bundle problem.\"\"\"\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)",
        "mutated": [
            "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    if False:\n        i = 10\n    'Add Ground Control Points constraints to the bundle problem.'\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)",
            "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Add Ground Control Points constraints to the bundle problem.'\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)",
            "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Add Ground Control Points constraints to the bundle problem.'\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)",
            "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Add Ground Control Points constraints to the bundle problem.'\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)",
            "def add_gcp_to_bundle(ba: orec.pybundle.BundleAdjuster, reference: types.TopocentricConverter, gcp: List[pymap.GroundControlPoint], gcp_std, shots):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Add Ground Control Points constraints to the bundle problem.'\n    for point in gcp:\n        point_id = 'gcp-' + point.id\n        coordinates = multiview.triangulate_gcp(point, shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if coordinates is None:\n            if point.lla:\n                enu = reference.to_topocentric(*point.lla_vec)\n                logger.warning(f\"Could not triangulate GCP '{point.id}'.Using {enu} (derived from lat,lon)\")\n                coordinates = enu\n            else:\n                logger.warning(\"Cannot initialize GCP '{}'.  Ignoring it\".format(point.id))\n                continue\n        ba.add_point(point_id, coordinates, False)\n        for observation in point.observations:\n            if observation.shot_id in shots:\n                ba.add_point_projection_observation(observation.shot_id, point_id, observation.projection, gcp_std)"
        ]
    },
    {
        "func_name": "bundle_with_fixed_images",
        "original": "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    \"\"\"Bundle adjust a reconstruction while keeping some images fixed.\"\"\"\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid",
        "mutated": [
            "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    if False:\n        i = 10\n    'Bundle adjust a reconstruction while keeping some images fixed.'\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid",
            "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Bundle adjust a reconstruction while keeping some images fixed.'\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid",
            "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Bundle adjust a reconstruction while keeping some images fixed.'\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid",
            "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Bundle adjust a reconstruction while keeping some images fixed.'\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid",
            "def bundle_with_fixed_images(reconstruction, camera_priors, gcp, gcp_std, fixed_images, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Bundle adjust a reconstruction while keeping some images fixed.'\n    fix_cameras = True\n    chrono = orec.Chronometer()\n    ba = orec.pybundle.BundleAdjuster()\n    for camera in reconstruction.cameras.values():\n        camera_prior = camera_priors[camera.id]\n        ba.add_camera(camera.id, camera, camera_prior, fix_cameras)\n    for shot in reconstruction.shots.values():\n        r = shot.pose.rotation\n        t = shot.pose.translation\n        ba.add_shot(shot.id, shot.camera.id, r, t, shot.id in fixed_images)\n    for point in reconstruction.points.values():\n        ba.add_point(point.id, point.coordinates, False)\n        ba.add_point_prior(point.id, point.coordinates, np.array([100.0, 100.0, 100.0]), False)\n    for shot_id in reconstruction.shots:\n        shot = reconstruction.get_shot(shot_id)\n        for point in shot.get_valid_landmarks():\n            obs = shot.get_landmark_observation(point)\n            ba.add_point_projection_observation(shot.id, point.id, obs.point, obs.scale)\n    add_gcp_to_bundle(ba, reconstruction.reference, gcp, gcp_std, reconstruction.shots)\n    ba.set_point_projection_loss_function(config['loss_function'], config['loss_function_threshold'])\n    ba.set_internal_parameters_prior_sd(config['exif_focal_sd'], config['principal_point_sd'], config['radial_distortion_k1_sd'], config['radial_distortion_k2_sd'], config['tangential_distortion_p1_sd'], config['tangential_distortion_p2_sd'], config['radial_distortion_k3_sd'], config['radial_distortion_k4_sd'])\n    ba.set_num_threads(config['processes'])\n    ba.set_max_num_iterations(config['bundle_max_iterations'])\n    ba.set_linear_solver_type('SPARSE_SCHUR')\n    ba.set_compute_covariances(True)\n    chrono.lap('setup')\n    ba.run()\n    chrono.lap('run')\n    cov_valid = ba.get_covariance_estimation_valid()\n    if not cov_valid:\n        logger.warning('Could not compute covariance')\n    for camera in reconstruction.cameras.values():\n        orec._get_camera_from_bundle(ba, camera)\n    for shot in reconstruction.shots.values():\n        s = ba.get_shot(shot.id)\n        shot.pose.rotation = [s.r[0], s.r[1], s.r[2]]\n        shot.pose.translation = [s.t[0], s.t[1], s.t[2]]\n        shot.covariance = s.get_covariance_inv_param()\n    for point in reconstruction.points.values():\n        p = ba.get_point(point.id)\n        point.coordinates = [p.p[0], p.p[1], p.p[2]]\n        point.reprojection_errors = p.reprojection_errors\n    chrono.lap('teardown')\n    logger.info(ba.full_report())\n    logger.info(ba.brief_report())\n    return cov_valid"
        ]
    },
    {
        "func_name": "decompose_covariance",
        "original": "def decompose_covariance(covariance):\n    \"\"\"Decompose covariance into a rotation and diagonal scaling.\"\"\"\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))",
        "mutated": [
            "def decompose_covariance(covariance):\n    if False:\n        i = 10\n    'Decompose covariance into a rotation and diagonal scaling.'\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))",
            "def decompose_covariance(covariance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Decompose covariance into a rotation and diagonal scaling.'\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))",
            "def decompose_covariance(covariance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Decompose covariance into a rotation and diagonal scaling.'\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))",
            "def decompose_covariance(covariance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Decompose covariance into a rotation and diagonal scaling.'\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))",
            "def decompose_covariance(covariance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Decompose covariance into a rotation and diagonal scaling.'\n    (u, s, vh) = np.linalg.svd(covariance)\n    return (u, np.sqrt(s))"
        ]
    },
    {
        "func_name": "resect_annotated_single_images",
        "original": "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    \"\"\"Resect images that do not belong to reconstruction but have enough GCPs annotated.\n\n    Returns:\n        A reconstruction with all the resected images.\n    \"\"\"\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected",
        "mutated": [
            "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    if False:\n        i = 10\n    'Resect images that do not belong to reconstruction but have enough GCPs annotated.\\n\\n    Returns:\\n        A reconstruction with all the resected images.\\n    '\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected",
            "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resect images that do not belong to reconstruction but have enough GCPs annotated.\\n\\n    Returns:\\n        A reconstruction with all the resected images.\\n    '\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected",
            "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resect images that do not belong to reconstruction but have enough GCPs annotated.\\n\\n    Returns:\\n        A reconstruction with all the resected images.\\n    '\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected",
            "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resect images that do not belong to reconstruction but have enough GCPs annotated.\\n\\n    Returns:\\n        A reconstruction with all the resected images.\\n    '\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected",
            "def resect_annotated_single_images(reconstruction, gcps, camera_models, data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resect images that do not belong to reconstruction but have enough GCPs annotated.\\n\\n    Returns:\\n        A reconstruction with all the resected images.\\n    '\n    not_in_rec = set()\n    for gcp in gcps:\n        for obs in gcp.observations:\n            im = obs.shot_id\n            if im not in reconstruction.shots and im in data.images():\n                not_in_rec.add(im)\n    resected = types.Reconstruction()\n    resected.reference = reconstruction.reference\n    for im in not_in_rec:\n        exif = data.load_exif(im)\n        camera = camera_models[exif['camera']]\n        resect_image(im, camera, gcps, reconstruction, data, resected)\n    logger.info(f'Resected: {len(resected.shots)} shots and {len(resected.cameras)} cameras')\n    return resected"
        ]
    },
    {
        "func_name": "_gcp_image_observation",
        "original": "def _gcp_image_observation(gcp, image):\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None",
        "mutated": [
            "def _gcp_image_observation(gcp, image):\n    if False:\n        i = 10\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None",
            "def _gcp_image_observation(gcp, image):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None",
            "def _gcp_image_observation(gcp, image):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None",
            "def _gcp_image_observation(gcp, image):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None",
            "def _gcp_image_observation(gcp, image):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for obs in gcp.observations:\n        if image == obs.shot_id:\n            return obs\n    return None"
        ]
    },
    {
        "func_name": "resect_image",
        "original": "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    \"\"\"\n    Resect an image into a reconstruction based only on GCPs annotations.\n    Pass another reconstruction to dst_reconstruction\n    if you want the resected points to be added there instead\n\n    Returns:\n        The resected shot.\n    \"\"\"\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None",
        "mutated": [
            "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    if False:\n        i = 10\n    '\\n    Resect an image into a reconstruction based only on GCPs annotations.\\n    Pass another reconstruction to dst_reconstruction\\n    if you want the resected points to be added there instead\\n\\n    Returns:\\n        The resected shot.\\n    '\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None",
            "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Resect an image into a reconstruction based only on GCPs annotations.\\n    Pass another reconstruction to dst_reconstruction\\n    if you want the resected points to be added there instead\\n\\n    Returns:\\n        The resected shot.\\n    '\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None",
            "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Resect an image into a reconstruction based only on GCPs annotations.\\n    Pass another reconstruction to dst_reconstruction\\n    if you want the resected points to be added there instead\\n\\n    Returns:\\n        The resected shot.\\n    '\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None",
            "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Resect an image into a reconstruction based only on GCPs annotations.\\n    Pass another reconstruction to dst_reconstruction\\n    if you want the resected points to be added there instead\\n\\n    Returns:\\n        The resected shot.\\n    '\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None",
            "def resect_image(im, camera, gcps, reconstruction, data, dst_reconstruction=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Resect an image into a reconstruction based only on GCPs annotations.\\n    Pass another reconstruction to dst_reconstruction\\n    if you want the resected points to be added there instead\\n\\n    Returns:\\n        The resected shot.\\n    '\n    threshold = 0.01\n    min_inliers = 3\n    (bs, Xs) = ([], [])\n    for gcp in gcps:\n        obs = _gcp_image_observation(gcp, im)\n        if not obs:\n            continue\n        gcp_3d_coords = multiview.triangulate_gcp(gcp, reconstruction.shots, reproj_threshold=1, min_ray_angle_degrees=0.1)\n        if gcp_3d_coords is None:\n            continue\n        b = camera.pixel_bearing(obs.projection)\n        bs.append(b)\n        Xs.append(gcp_3d_coords)\n    bs = np.array(bs)\n    Xs = np.array(Xs)\n    if len(bs) < min_inliers:\n        logger.info(f'Not enough annotations to resect image {im}')\n        return None\n    T = multiview.absolute_pose_ransac(bs, Xs, threshold, 1000, 0.999)\n    R = T[:, :3]\n    t = T[:, 3]\n    reprojected_bs = R.T.dot((Xs - t).T).T\n    reprojected_bs /= np.linalg.norm(reprojected_bs, axis=1)[:, np.newaxis]\n    inliers = np.linalg.norm(reprojected_bs - bs, axis=1) < threshold\n    ninliers = int(sum(inliers))\n    logger.info(f'{im} resection inliers: {ninliers} / {len(bs)}')\n    if dst_reconstruction is None:\n        dst_reconstruction = reconstruction\n    if ninliers >= min_inliers:\n        R = T[:, :3].T\n        t = -R.dot(T[:, 3])\n        dst_reconstruction.add_camera(camera)\n        shot = dst_reconstruction.create_shot(im, camera.id, pygeometry.Pose(R, t))\n        shot.metadata = helpers.get_image_metadata(data, im)\n        return shot\n    else:\n        logger.info(f'Not enough inliers to resect image {im}')\n        return None"
        ]
    },
    {
        "func_name": "parse_args",
        "original": "def parse_args():\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args",
        "mutated": [
            "def parse_args():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='Merge reconstructions and run BA with GCPs')\n    parser.add_argument('path_dataset', help='dataset to process')\n    parser.add_argument('--rec_a', default=0, type=int, help='Index of reconstruction A.\\nIf rec_b is set to None, the rec_a is used as a fixed reference: all annotated images not belonging to it are resected into it using the GCPs.\\nIf reconstruction B is set to a number, the pair of reconstructions (A,B) will be aligned to each other using the ground control points.')\n    parser.add_argument('--rec_b', default=1, type=int, help='Index of reconstruction B. Read the help for rec_a')\n    parser.add_argument('--std-threshold', default=0.3, help='Positional threshold (m) to classify images as well-localized')\n    parser.add_argument('--px-threshold', default=0.016, help='threshold in normalized pixels to classify a GCP annotation as correct', type=float)\n    parser.add_argument('--rigid', action='store_true', help='Skip BA entirely')\n    parser.add_argument('--covariance', action='store_true', help='Run BA with fixed images to obtain pose covariances')\n    args = parser.parse_args()\n    if args.covariance:\n        assert not args.rigid, 'rigid and covariance are mutually exclusive'\n    return args"
        ]
    },
    {
        "func_name": "align_3d_annotations_to_reconstruction",
        "original": "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})",
        "mutated": [
            "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    if False:\n        i = 10\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})",
            "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})",
            "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})",
            "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})",
            "def align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    coords_triangulated_gcps = triangulate_gcps(gcps_this_model, reconstruction)\n    n_triangulated = sum((x is not None for x in coords_triangulated_gcps))\n    if n_triangulated < 3:\n        logger.info(f'{model_id} has {n_triangulated} gcps, not aligning')\n        return\n    coords_annotated_gcps = []\n    for gcp in gcps_this_model:\n        for (annotated_point_3D, gcp_id) in gcps_3d[model_id]:\n            if gcp_id == gcp.id:\n                coords_annotated_gcps.append(annotated_point_3D)\n                break\n    gcp_reprojections = {}\n    try:\n        (s, A, b) = find_alignment(coords_triangulated_gcps, coords_annotated_gcps)\n        for (gcp, coords) in zip(gcps_this_model, coords_triangulated_gcps):\n            gcp_reprojections[gcp.id] = (s * A.dot(coords) + b).tolist() if coords is not None else None\n    except ValueError:\n        logger.warning(f'Could not align reconstruction with {model_id}')\n    return (gcp_reprojections, {'s': s.tolist(), 'A': A.tolist(), 'b': b.tolist()})"
        ]
    },
    {
        "func_name": "align_external_3d_models_to_reconstruction",
        "original": "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    \"\"\"\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\n\n    gcps only contains the 2D observations\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\n    \"\"\"\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)",
        "mutated": [
            "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    if False:\n        i = 10\n    '\\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\\n\\n    gcps only contains the 2D observations\\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\\n    '\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)",
            "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\\n\\n    gcps only contains the 2D observations\\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\\n    '\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)",
            "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\\n\\n    gcps only contains the 2D observations\\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\\n    '\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)",
            "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\\n\\n    gcps only contains the 2D observations\\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\\n    '\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)",
            "def align_external_3d_models_to_reconstruction(data, gcps, reconstruction, ix_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Triangulates all the control points in common with 3D models (those that have at least one 3D annotation)\\n    and saves them in the coordinate space of the 3D models so that they can be visualized in the annotation tool.\\n\\n    gcps only contains the 2D observations\\n    We re-load the gcps file here manually to also get the 3D observations (hacky but will do for now)\\n    '\n    map_filename_to_gcp_ids = defaultdict(set)\n    gcps_3d = load_3d_annotations_from_gcp_file(data)\n    for (shot_id, observations) in gcps_3d.items():\n        for (_coords, gcp_id) in observations:\n            map_filename_to_gcp_ids[shot_id].add(gcp_id)\n    gcps_per_source_file = defaultdict(list)\n    for gcp in gcps:\n        for (shot_id, gcp_ids_annotated_in_3d_for_this_file) in map_filename_to_gcp_ids.items():\n            if gcp.id in gcp_ids_annotated_in_3d_for_this_file:\n                gcps_per_source_file[shot_id].append(gcp)\n    for (model_id, gcps_this_model) in gcps_per_source_file.items():\n        (gcp_reprojections, alignment) = align_3d_annotations_to_reconstruction(gcps_this_model, gcps_3d, model_id, reconstruction)\n        p_out = f'{data.data_path}/gcp_reprojections_3D_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving reprojected 3D points to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n        p_out = f'{data.data_path}/affine_alignment_{ix_rec}x{model_id}.json'\n        logger.info(f'Saving affine parameters to {p_out}')\n        with open(p_out, 'w') as f:\n            json.dump(alignment, f, indent=4, sort_keys=True)"
        ]
    },
    {
        "func_name": "load_3d_annotations_from_gcp_file",
        "original": "def load_3d_annotations_from_gcp_file(data):\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps",
        "mutated": [
            "def load_3d_annotations_from_gcp_file(data):\n    if False:\n        i = 10\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps",
            "def load_3d_annotations_from_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps",
            "def load_3d_annotations_from_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps",
            "def load_3d_annotations_from_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps",
            "def load_3d_annotations_from_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    dict_gcps = defaultdict(list)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'point' in obs:\n                dict_gcps[obs['shot_id']].append((obs['point'], gcp['id']))\n    return dict_gcps"
        ]
    },
    {
        "func_name": "fix_3d_annotations_in_gcp_file",
        "original": "def fix_3d_annotations_in_gcp_file(data):\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)",
        "mutated": [
            "def fix_3d_annotations_in_gcp_file(data):\n    if False:\n        i = 10\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)",
            "def fix_3d_annotations_in_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)",
            "def fix_3d_annotations_in_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)",
            "def fix_3d_annotations_in_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)",
            "def fix_3d_annotations_in_gcp_file(data):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(data._ground_control_points_file()) as f:\n        raw_gcps = json.load(f)\n    for gcp in raw_gcps['points']:\n        for obs in gcp['observations']:\n            if 'projection' not in obs:\n                continue\n            if len(obs['projection']) == 3:\n                obs['point'] = obs['projection']\n                del obs['projection']\n    with open(data._ground_control_points_file(), 'w') as f:\n        json.dump(raw_gcps, f, indent=4, sort_keys=True)"
        ]
    },
    {
        "func_name": "align",
        "original": "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')",
        "mutated": [
            "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    if False:\n        i = 10\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')",
            "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')",
            "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')",
            "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')",
            "def align(path: str, rec_a: int, rec_b: int, rigid: bool, covariance: bool, px_threshold: float, std_threshold: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data = dataset.DataSet(path)\n    for fn in ('reconstruction.json', 'ground_control_points.json', 'tracks.csv'):\n        if not os.path.exists(os.path.join(path, fn)):\n            logger.error(f'Missing file: {fn}')\n            return\n    camera_models = data.load_camera_models()\n    tracks_manager = data.load_tracks_manager()\n    fix_3d_annotations_in_gcp_file(data)\n    gcps = data.load_ground_control_points()\n    fn_resplit = f'reconstruction_gcp_ba_resplit_{rec_a}x{rec_b}.json'\n    fn_rigid = f'reconstruction_gcp_rigid_{rec_a}x{rec_b}.json'\n    reconstructions = data.load_reconstruction()\n    if len(reconstructions) > 1:\n        if rec_b is not None:\n            if rigid and os.path.exists(data._reconstruction_file(fn_resplit)):\n                reconstructions = data.load_reconstruction(fn_resplit)\n            else:\n                reconstructions = data.load_reconstruction()\n                reconstructions = [reconstructions[rec_a], reconstructions[rec_b]]\n            coords0 = triangulate_gcps(gcps, reconstructions[0])\n            coords1 = triangulate_gcps(gcps, reconstructions[1])\n            n_valid_0 = sum((c is not None for c in coords0))\n            logger.debug(f'Triangulated {n_valid_0}/{len(gcps)} gcps for rec #{rec_a}')\n            n_valid_1 = sum((c is not None for c in coords1))\n            logger.debug(f'Triangulated {n_valid_1}/{len(gcps)} gcps for rec #{rec_b}')\n            try:\n                (s, A, b) = find_alignment(coords1, coords0)\n                apply_similarity(reconstructions[1], s, A, b)\n            except ValueError:\n                logger.warning(f'Could not rigidly align rec #{rec_b} to rec #{rec_a}')\n                return\n            logger.info(f'Rigidly aligned rec #{rec_b} to rec #{rec_a}')\n        else:\n            reconstructions = data.load_reconstruction()\n            base = reconstructions[rec_a]\n            resected = resect_annotated_single_images(base, gcps, camera_models, data)\n            reconstructions = [base, resected]\n    else:\n        logger.debug('Only one reconstruction in reconstruction.json. Will only to 3d-3d alignment if any')\n        align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n        return\n    logger.debug(f'Aligning annotations, if any, to rec #{rec_a}')\n    align_external_3d_models_to_reconstruction(data, gcps, reconstructions[0], rec_a)\n    for shot in reconstructions[1].shots.values():\n        shot.metadata.gps_position.value = shot.pose.get_origin()\n    data.save_reconstruction(reconstructions, fn_rigid)\n    logger.info('Merging reconstructions')\n    merged = merge_reconstructions(reconstructions, tracks_manager)\n    for shot in merged.shots.values():\n        shot.metadata.gps_accuracy.value = 0.5 * len(merged.shots)\n    gcp_alignment = {'after_rigid': gcp_geopositional_error(gcps, merged)}\n    logger.info('GCP errors after rigid alignment:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_rigid'].items())))\n    if not rigid:\n        data.config['bundle_max_iterations'] = 200\n        data.config['bundle_use_gcp'] = True\n        logger.info('Running BA on merged reconstructions')\n        orec.bundle(merged, camera_models, {}, gcp=gcps, config=data.config)\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_{rec_a}x{rec_b}.json')\n        gcp_alignment['after_bundle'] = gcp_geopositional_error(gcps, merged)\n        logger.info('GCP errors after bundle:\\n' + '\\n'.join(('[{}]: {:.2f} m / {:.2f} m (planar)'.format(k, v['error'], v['error_planar']) for (k, v) in gcp_alignment['after_bundle'].items())))\n        with open(f'{data.data_path}/gcp_alignment_{rec_a}x{rec_b}.json', 'w') as f:\n            json.dump(gcp_alignment, f, indent=4, sort_keys=True)\n    gcp_reprojections = reproject_gcps(gcps, merged, reproj_threshold=10)\n    reprojection_errors = get_sorted_reprojection_errors(gcp_reprojections)\n    err_values = [t[2] for t in reprojection_errors]\n    max_reprojection_error = np.max(err_values)\n    median_reprojection_error = np.median(err_values)\n    with open(f'{data.data_path}/gcp_reprojections_{rec_a}x{rec_b}.json', 'w') as f:\n        json.dump(gcp_reprojections, f, indent=4, sort_keys=True)\n    n_bad_gcp_annotations = int(sum((t[2] > px_threshold for t in reprojection_errors)))\n    if n_bad_gcp_annotations > 0:\n        logger.info(f'{n_bad_gcp_annotations} large reprojection errors:')\n        for t in reprojection_errors:\n            if t[2] > px_threshold:\n                logger.info(t)\n    gcp_std = compute_gcp_std(gcp_reprojections)\n    logger.info(f'GCP reprojection error STD: {gcp_std}')\n    resplit = resplit_reconstruction(merged, reconstructions)\n    data.save_reconstruction(resplit, fn_resplit)\n    if covariance:\n        n_points = len(merged.points)\n        logger.info('Re-triangulating...')\n        backup = data.config['triangulation_min_ray_angle']\n        data.config['triangulation_min_ray_angle'] = 2.0\n        orec.retriangulate(tracks_manager, merged, data.config)\n        orec.paint_reconstruction(data, tracks_manager, merged)\n        data.config['triangulation_min_ray_angle'] = backup\n        logger.info(f'Re-triangulated. Removed {n_points - len(merged.points)}. Kept {int(100 * len(merged.points) / n_points)}%')\n        data.save_reconstruction([merged], f'reconstruction_gcp_ba_retriangulated_{rec_a}x{rec_b}.json')\n        all_shots_std = []\n        _rec_ixs = [(0, 1), (1, 0)] if rec_b is not None else [(0, 1)]\n        for rec_ixs in _rec_ixs:\n            logger.info(f'Running BA with fixed images. Fixing rec #{rec_ixs[0]}')\n            fixed_images = set(reconstructions[rec_ixs[0]].shots.keys())\n            covariance_estimation_valid = bundle_with_fixed_images(merged, camera_models, gcp=gcps, gcp_std=gcp_std, fixed_images=fixed_images, config=data.config)\n            if not covariance_estimation_valid:\n                logger.info(f'Could not get positional uncertainty for pair {rec_ixs} It could be because:\\na) there are not enough GCPs.\\nb) they are badly distributed in 3D.\\nc) there are some wrong annotations')\n                shots_std_this_pair = [(shot, np.nan) for shot in reconstructions[rec_ixs[1]].shots]\n            else:\n                shots_std_this_pair = []\n                for shot in merged.shots.values():\n                    if shot.id in reconstructions[rec_ixs[1]].shots:\n                        (u, std_v) = decompose_covariance(shot.covariance[3:, 3:])\n                        std = np.linalg.norm(std_v)\n                        shots_std_this_pair.append((shot.id, std))\n                        logger.debug(f'{shot.id} std: {std}')\n            all_shots_std.extend(shots_std_this_pair)\n        std_values = [x[1] for x in all_shots_std]\n        n_nan_std = sum((np.isnan(std) for std in std_values))\n        n_good_std = sum((std <= std_threshold for std in std_values if not np.isnan(std)))\n        n_bad_std = len(std_values) - n_good_std - n_nan_std\n        median_shot_std = np.median(std_values)\n        with open(f'{data.data_path}/shots_std_{rec_a}x{rec_b}.csv', 'w') as f:\n            s = sorted(all_shots_std, key=lambda t: -t[-1])\n            for t in s:\n                line = '{}, {}'.format(*t)\n                f.write(line + '\\n')\n            max_shot_std = s[0][1]\n    else:\n        n_nan_std = -1\n        n_bad_std = -1\n        n_good_std = -1\n        median_shot_std = -1\n        max_shot_std = -1\n        std_values = []\n    metrics = {'n_reconstructions': len(data.load_reconstruction()), 'median_shot_std': median_shot_std, 'max_shot_std': max_shot_std, 'max_reprojection_error': max_reprojection_error, 'median_reprojection_error': median_reprojection_error, 'n_gcp': len(gcps), 'n_bad_gcp_annotations': n_bad_gcp_annotations, 'n_bad_position_std': int(n_bad_std), 'n_good_position_std': int(n_good_std), 'n_nan_position_std': int(n_nan_std), 'rec_a': rec_a, 'rec_b': rec_b}\n    logger.info(metrics)\n    p_metrics = f'{data.data_path}/run_ba_metrics_{rec_a}x{rec_b}.json'\n    with open(p_metrics, 'w') as f:\n        json.dump(metrics, f, indent=4, sort_keys=True)\n    logger.info(f'Saved metrics to {p_metrics}')\n    logger.info('========================================')\n    logger.info('=============== Summary ================')\n    logger.info('========================================')\n    if n_bad_std == 0 and n_bad_gcp_annotations == 0:\n        logger.info(f'No issues. All gcp reprojections are under {px_threshold} and all frames are localized within {std_threshold}m')\n    if n_bad_std != 0 or n_bad_gcp_annotations != 0:\n        if rigid:\n            logger.info('Positional uncertainty was not calculated. (--rigid was set).')\n        elif not covariance:\n            logger.info('Positional uncertainty was not calculated (--covariance not set).')\n        else:\n            logger.info(f'{n_nan_std}/{len(std_values)} images with unknown error.\\n{n_good_std}/{len(std_values)} well-localized images.\\n{n_bad_std}/{len(std_values)} badly localized images.')\n        if n_bad_gcp_annotations > 0:\n            logger.info(f'{n_bad_gcp_annotations} annotations with large reprojection error. Worst offenders:')\n            stats_bad_reprojections = get_number_of_wrong_annotations_per_gcp(gcp_reprojections, px_threshold)\n            gcps_sorted = sorted(stats_bad_reprojections, key=lambda k: -stats_bad_reprojections[k])\n            for (ix, gcp_id) in enumerate(gcps_sorted[:5]):\n                n = stats_bad_reprojections[gcp_id]\n                if n > 0:\n                    logger.info(f'#{ix + 1} - {gcp_id}: {n} bad annotations')\n        else:\n            logger.info('No annotations with large reprojection errors')"
        ]
    },
    {
        "func_name": "main",
        "original": "def main() -> None:\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))",
        "mutated": [
            "def main() -> None:\n    if False:\n        i = 10\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))",
            "def main() -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))",
            "def main() -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))",
            "def main() -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))",
            "def main() -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    log.setup()\n    args = parse_args()\n    sys.exit(align(args.path_dataset, args.rec_a, args.rec_b, args.rigid, args.covariance, args.px_threshold, args.std_threshold))"
        ]
    }
]