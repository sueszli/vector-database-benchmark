[
    {
        "func_name": "write_model_card",
        "original": "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)",
        "mutated": [
            "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    if False:\n        i = 10\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)",
            "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)",
            "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)",
            "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)",
            "def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    texts = {'en': \"Machine learning is great, isn't it?\", 'ru': '\u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 - \u044d\u0442\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u043d\u0435 \u0442\u0430\u043a \u043b\u0438?', 'de': 'Maschinelles Lernen ist gro\u00dfartig, nicht wahr?'}\n    scores = {'wmt19-de-en-6-6-base': [0, 38.37], 'wmt19-de-en-6-6-big': [0, 39.9]}\n    pair = f'{src_lang}-{tgt_lang}'\n    readme = f'\\n---\\n\\nlanguage:\\n- {src_lang}\\n- {tgt_lang}\\nthumbnail:\\ntags:\\n- translation\\n- wmt19\\n- allenai\\nlicense: apache-2.0\\ndatasets:\\n- wmt19\\nmetrics:\\n- bleu\\n---\\n\\n# FSMT\\n\\n## Model description\\n\\nThis is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\\n\\nFor more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\\n\\n2 models are available:\\n\\n* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\\n* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\\nmname = \"allenai/{model_name}\"\\ntokenizer = FSMTTokenizer.from_pretrained(mname)\\nmodel = FSMTForConditionalGeneration.from_pretrained(mname)\\n\\ninput = \"{texts[src_lang]}\"\\ninput_ids = tokenizer.encode(input, return_tensors=\"pt\")\\noutputs = model.generate(input_ids)\\ndecoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(decoded) # {texts[tgt_lang]}\\n\\n```\\n\\n#### Limitations and bias\\n\\n\\n## Training data\\n\\nPretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\\n\\n## Eval results\\n\\nHere are the BLEU scores:\\n\\nmodel   |  transformers\\n-------|---------\\n{model_name}  |  {scores[model_name][1]}\\n\\nThe score was calculated using this code:\\n\\n```bash\\ngit clone https://github.com/huggingface/transformers\\ncd transformers\\nexport PAIR={pair}\\nexport DATA_DIR=data/$PAIR\\nexport SAVE_DIR=data/$PAIR\\nexport BS=8\\nexport NUM_BEAMS=5\\nmkdir -p $DATA_DIR\\nsacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\\nsacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\\necho $PAIR\\nPYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\\n```\\n\\n## Data Sources\\n\\n- [training, etc.](http://www.statmt.org/wmt19/)\\n- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\\n\\n\\n### BibTeX entry and citation info\\n\\n```\\n@misc{{kasai2020deep,\\n    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\\n    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\\n    year={{2020}},\\n    eprint={{2006.10369}},\\n    archivePrefix={{arXiv}},\\n    primaryClass={{cs.CL}}\\n}}\\n```\\n\\n'\n    model_card_dir.mkdir(parents=True, exist_ok=True)\n    path = os.path.join(model_card_dir, 'README.md')\n    print(f'Generating {path}')\n    with open(path, 'w', encoding='utf-8') as f:\n        f.write(readme)"
        ]
    }
]