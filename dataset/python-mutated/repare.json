[
    {
        "func_name": "build_data_train",
        "original": "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    \"\"\"\n    Loads the data file and spits out a h5 file with record of\n    {y, review_text, review_int}\n    Typically two passes over the data.\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\n    from thereafter.\n\n    WARNING: we use h5 just as proof of concept for handling large datasets\n    Datasets may fit entirely in memory as numpy as array\n\n    \"\"\"\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)",
        "mutated": [
            "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    if False:\n        i = 10\n    '\\n    Loads the data file and spits out a h5 file with record of\\n    {y, review_text, review_int}\\n    Typically two passes over the data.\\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\\n    from thereafter.\\n\\n    WARNING: we use h5 just as proof of concept for handling large datasets\\n    Datasets may fit entirely in memory as numpy as array\\n\\n    '\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)",
            "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Loads the data file and spits out a h5 file with record of\\n    {y, review_text, review_int}\\n    Typically two passes over the data.\\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\\n    from thereafter.\\n\\n    WARNING: we use h5 just as proof of concept for handling large datasets\\n    Datasets may fit entirely in memory as numpy as array\\n\\n    '\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)",
            "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Loads the data file and spits out a h5 file with record of\\n    {y, review_text, review_int}\\n    Typically two passes over the data.\\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\\n    from thereafter.\\n\\n    WARNING: we use h5 just as proof of concept for handling large datasets\\n    Datasets may fit entirely in memory as numpy as array\\n\\n    '\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)",
            "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Loads the data file and spits out a h5 file with record of\\n    {y, review_text, review_int}\\n    Typically two passes over the data.\\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\\n    from thereafter.\\n\\n    WARNING: we use h5 just as proof of concept for handling large datasets\\n    Datasets may fit entirely in memory as numpy as array\\n\\n    '\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)",
            "def build_data_train(path='.', filepath='labeledTrainData.tsv', vocab_file=None, vocab=None, skip_headers=True, train_ratio=0.8):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Loads the data file and spits out a h5 file with record of\\n    {y, review_text, review_int}\\n    Typically two passes over the data.\\n    1st pass is for vocab and pre-processing. (WARNING: to get phrases, we need to go\\n    though multiple passes). 2nd pass is converting text into integers. We will deal with integers\\n    from thereafter.\\n\\n    WARNING: we use h5 just as proof of concept for handling large datasets\\n    Datasets may fit entirely in memory as numpy as array\\n\\n    '\n    fname_h5 = filepath + '.h5'\n    if vocab_file is None:\n        fname_vocab = filepath + '.vocab'\n    else:\n        fname_vocab = vocab_file\n    if not os.path.exists(fname_h5) or not os.path.exists(fname_vocab):\n        h5f = h5py.File(fname_h5, 'w')\n        (shape, maxshape) = ((2 ** 16,), (None,))\n        dt = np.dtype([('y', np.uint8), ('split', np.bool), ('num_words', np.uint16), ('text', h5py.special_dtype(vlen=str))])\n        reviews_text = h5f.create_dataset('reviews', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        reviews_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        reviews_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        wdata = np.zeros((1,), dtype=dt)\n        build_vocab = False\n        if vocab is None:\n            vocab = defaultdict(int)\n            build_vocab = True\n        nsamples = 0\n        f = open(filepath, 'r')\n        if skip_headers:\n            f.readline()\n        for (i, line) in enumerate(f):\n            (_, rating, review) = line.strip().split('\\t')\n            review = clean_string(review)\n            review_words = review.strip().split()\n            num_words = len(review_words)\n            split = int(np.random.rand() < train_ratio)\n            wdata['y'] = int(float(rating))\n            wdata['text'] = review\n            wdata['num_words'] = num_words\n            wdata['split'] = split\n            reviews_text[i] = wdata\n            if build_vocab:\n                for word in review_words:\n                    vocab[word] += 1\n            nsamples += 1\n        (ratings, counts) = np.unique(reviews_text['y'][:nsamples], return_counts=True)\n        (sen_len, sen_len_counts) = np.unique(reviews_text['num_words'][:nsamples], return_counts=True)\n        vocab_size = len(vocab)\n        nclass = len(ratings)\n        reviews_text.attrs['vocab_size'] = vocab_size\n        reviews_text.attrs['nrows'] = nsamples\n        reviews_text.attrs['nclass'] = nclass\n        reviews_text.attrs['class_distribution'] = counts\n        neon_logger.display('vocabulary size - {}'.format(vocab_size))\n        neon_logger.display('# of samples - {}'.format(nsamples))\n        neon_logger.display('# of classes {}'.format(nclass))\n        neon_logger.display('class distribution - {} {}'.format(ratings, counts))\n        sen_counts = list(zip(sen_len, sen_len_counts))\n        sen_counts = sorted(sen_counts, key=lambda kv: kv[1], reverse=True)\n        neon_logger.display('sentence length - {} {} {}'.format(len(sen_len), sen_len, sen_len_counts))\n        if build_vocab:\n            vocab_sorted = sorted(list(vocab.items()), key=lambda kv: kv[1], reverse=True)\n            vocab = {}\n            for (i, t) in enumerate(list(zip(*vocab_sorted))[0]):\n                vocab[t] = i\n        ntrain = 0\n        nvalid = 0\n        for i in range(nsamples):\n            text = reviews_text[i]['text']\n            y = int(reviews_text[i]['y'])\n            split = reviews_text[i]['split']\n            text_int = [y] + [vocab[t] for t in text.strip().split()]\n            if split:\n                reviews_train[ntrain] = text_int\n                ntrain += 1\n            else:\n                reviews_valid[nvalid] = text_int\n                nvalid += 1\n        reviews_text.attrs['ntrain'] = ntrain\n        reviews_text.attrs['nvalid'] = nvalid\n        neon_logger.display('# of train - {0}, # of valid - {1}'.format(reviews_text.attrs['ntrain'], reviews_text.attrs['nvalid']))\n        h5f.close()\n        f.close()\n    if not os.path.exists(fname_vocab):\n        rev_vocab = {}\n        for (wrd, wrd_id) in vocab.items():\n            rev_vocab[wrd_id] = wrd\n        neon_logger.display('vocabulary from IMDB dataset is saved into {}'.format(fname_vocab))\n        pickle.dump((vocab, rev_vocab), open(fname_vocab, 'wb'), 2)\n    return (fname_h5, fname_vocab)"
        ]
    }
]