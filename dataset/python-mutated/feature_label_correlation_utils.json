[
    {
        "func_name": "get_pps_figure",
        "original": "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    \"\"\"If per_class is True, then no title is defined on the figure.\"\"\"\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig",
        "mutated": [
            "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    if False:\n        i = 10\n    'If per_class is True, then no title is defined on the figure.'\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig",
            "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'If per_class is True, then no title is defined on the figure.'\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig",
            "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'If per_class is True, then no title is defined on the figure.'\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig",
            "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'If per_class is True, then no title is defined on the figure.'\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig",
            "def get_pps_figure(per_class: bool, n_of_features: int, x_name: str='feature', xaxis_title: str='Column'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'If per_class is True, then no title is defined on the figure.'\n    fig = go.Figure()\n    fig.update_layout(yaxis_title='Predictive Power Score (PPS)', yaxis_range=(0, 1.05), xaxis_range=(-3, n_of_features + 2), legend=dict(x=1.0, y=1.0), barmode='group', height=500, xaxis_type='category')\n    if per_class:\n        fig.update_layout(xaxis_title='Class')\n    else:\n        fig.update_layout(title=f'Predictive Power Score (PPS) - Can a {x_name} predict the label by itself?', xaxis_title=xaxis_title)\n    return fig"
        ]
    },
    {
        "func_name": "pd_series_to_trace",
        "original": "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    \"\"\"Create bar plotly bar trace out of pandas Series.\"\"\"\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')",
        "mutated": [
            "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    if False:\n        i = 10\n    'Create bar plotly bar trace out of pandas Series.'\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')",
            "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create bar plotly bar trace out of pandas Series.'\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')",
            "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create bar plotly bar trace out of pandas Series.'\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')",
            "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create bar plotly bar trace out of pandas Series.'\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')",
            "def pd_series_to_trace(s_pps: pd.Series, train_or_test: str, name: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create bar plotly bar trace out of pandas Series.'\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + s_pps.round(2).astype(str) + '</b>', textposition='outside')"
        ]
    },
    {
        "func_name": "pd_series_to_trace_with_diff",
        "original": "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    \"\"\"Create bar plotly bar trace out of pandas Series, with difference shown in percentages.\"\"\"\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')",
        "mutated": [
            "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    if False:\n        i = 10\n    'Create bar plotly bar trace out of pandas Series, with difference shown in percentages.'\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')",
            "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create bar plotly bar trace out of pandas Series, with difference shown in percentages.'\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')",
            "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create bar plotly bar trace out of pandas Series, with difference shown in percentages.'\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')",
            "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create bar plotly bar trace out of pandas Series, with difference shown in percentages.'\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')",
            "def pd_series_to_trace_with_diff(s_pps: pd.Series, train_or_test: str, name: str, diffs: pd.Series):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create bar plotly bar trace out of pandas Series, with difference shown in percentages.'\n    diffs_text = '(' + diffs.apply(format_percent, floating_point=0, add_positive_prefix=True) + ')'\n    text = diffs_text + '<br>' + s_pps.round(2).astype(str)\n    return go.Bar(x=s_pps.index, y=s_pps, name=name, marker_color=colors.get(train_or_test), text='<b>' + text + '</b>', textposition='outside')"
        ]
    },
    {
        "func_name": "get_feature_label_correlation",
        "original": "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    \"\"\"\n    Calculate the PPS for train, test and difference for feature label correlation checks.\n\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\n\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\n\n    Args:\n        train_df: pd.DataFrame\n            DataFrame of all train features and label\n        train_label_name:: str\n            name of label column in train dataframe\n        test_df:\n            DataFrame of all test features and label\n        test_label_name: str\n            name of label column in test dataframe\n        ppscore_params: dict\n            dictionary of additional parameters for the ppscore predictor function\n        n_show_top: int\n            Number of features to show, sorted by the magnitude of difference in PPS\n        min_pps_to_show: float, default 0.05\n            Minimum PPS to show a class in the graph\n        random_state: int, default None\n            Random state for the ppscore.predictors function\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\n            The names to show in the display for the first and second datasets.\n\n    Returns:\n        CheckResult\n            value: dictionaries of PPS values for train, test and train-test difference.\n            display: bar graph of the PPS of each feature.\n    \"\"\"\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)",
        "mutated": [
            "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of PPS values for train, test and train-test difference.\\n            display: bar graph of the PPS of each feature.\\n    '\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)",
            "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of PPS values for train, test and train-test difference.\\n            display: bar graph of the PPS of each feature.\\n    '\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)",
            "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of PPS values for train, test and train-test difference.\\n            display: bar graph of the PPS of each feature.\\n    '\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)",
            "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of PPS values for train, test and train-test difference.\\n            display: bar graph of the PPS of each feature.\\n    '\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)",
            "def get_feature_label_correlation(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of PPS values for train, test and train-test difference.\\n            display: bar graph of the PPS of each feature.\\n    '\n    df_pps_train = pps.predictors(df=train_df, y=train_label_name, random_seed=random_state, **ppscore_params)\n    df_pps_test = pps.predictors(df=test_df, y=test_label_name, random_seed=random_state, **ppscore_params)\n    s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n    s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n    s_difference = s_pps_train - s_pps_test\n    ret_value = {'train': s_pps_train.to_dict(), 'test': s_pps_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n    if not with_display:\n        return (ret_value, None)\n    sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n    s_pps_train_to_display = s_pps_train[sorted_order_for_display]\n    s_pps_test_to_display = s_pps_test[sorted_order_for_display]\n    fig = get_pps_figure(per_class=False, n_of_features=len(sorted_order_for_display))\n    fig.add_trace(pd_series_to_trace(s_pps_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n    fig.add_trace(pd_series_to_trace(s_pps_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n    display = [fig] if any(s_pps_train > min_pps_to_show) or any(s_pps_test > min_pps_to_show) else None\n    return (ret_value, display)"
        ]
    },
    {
        "func_name": "get_feature_label_correlation_per_class",
        "original": "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    \"\"\"\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\n\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\n\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\n\n    Args:\n        train_df: pd.DataFrame\n            DataFrame of all train features and label\n        train_label_name:: str\n            name of label column in train dataframe\n        test_df:\n            DataFrame of all test features and label\n        test_label_name: str\n            name of label column in test dataframe\n        ppscore_params: dict\n            dictionary of additional parameters for the ppscore predictor function\n        n_show_top: int\n            Number of features to show, sorted by the magnitude of difference in PPS\n        min_pps_to_show: float, default 0.05\n            Minimum PPS to show a class in the graph\n        random_state: int, default None\n            Random state for the ppscore.predictors function\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\n            The names to show in the display for the first and second datasets.\n\n    Returns:\n        CheckResult\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\n            train-test difference.\n            display: bar graphs of the PPS for each feature.\n    \"\"\"\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)",
        "mutated": [
            "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\\n            train-test difference.\\n            display: bar graphs of the PPS for each feature.\\n    '\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)",
            "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\\n            train-test difference.\\n            display: bar graphs of the PPS for each feature.\\n    '\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)",
            "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\\n            train-test difference.\\n            display: bar graphs of the PPS for each feature.\\n    '\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)",
            "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\\n            train-test difference.\\n            display: bar graphs of the PPS for each feature.\\n    '\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)",
            "def get_feature_label_correlation_per_class(train_df: pd.DataFrame, train_label_name: Optional[Hashable], test_df: pd.DataFrame, test_label_name: Optional[Hashable], ppscore_params: dict, n_show_top: int, min_pps_to_show: float=0.05, random_state: int=None, with_display: bool=True, dataset_names: Tuple[str]=DEFAULT_DATASET_NAMES):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the PPS for train, test and difference for feature label correlation checks per class.\\n\\n    The PPS represents the ability of a feature to single-handedly predict another feature or label.\\n    This function calculates the PPS per feature for both train and test, and returns the data and display graph.\\n\\n    Uses the ppscore package - for more info, see https://github.com/8080labs/ppscore\\n\\n    Args:\\n        train_df: pd.DataFrame\\n            DataFrame of all train features and label\\n        train_label_name:: str\\n            name of label column in train dataframe\\n        test_df:\\n            DataFrame of all test features and label\\n        test_label_name: str\\n            name of label column in test dataframe\\n        ppscore_params: dict\\n            dictionary of additional parameters for the ppscore predictor function\\n        n_show_top: int\\n            Number of features to show, sorted by the magnitude of difference in PPS\\n        min_pps_to_show: float, default 0.05\\n            Minimum PPS to show a class in the graph\\n        random_state: int, default None\\n            Random state for the ppscore.predictors function\\n        dataset_names: tuple, default: DEFAULT_DATASET_NAMES\\n            The names to show in the display for the first and second datasets.\\n\\n    Returns:\\n        CheckResult\\n            value: dictionaries of features, each value is 3 dictionaries of PPS values for train, test and\\n            train-test difference.\\n            display: bar graphs of the PPS for each feature.\\n    '\n    df_pps_train_all = pd.DataFrame()\n    df_pps_test_all = pd.DataFrame()\n    df_pps_difference_all = pd.DataFrame()\n    display = []\n    ret_value = {}\n    for c in train_df[train_label_name].unique():\n        train_df_all_vs_one = train_df.copy()\n        test_df_all_vs_one = test_df.copy()\n        train_df_all_vs_one[train_label_name] = train_df_all_vs_one[train_label_name].apply(lambda x: 1 if x == c else 0)\n        test_df_all_vs_one[test_label_name] = test_df_all_vs_one[test_label_name].apply(lambda x: 1 if x == c else 0)\n        df_pps_train = pps.predictors(df=train_df_all_vs_one, y=train_label_name, random_seed=random_state, **ppscore_params)\n        df_pps_test = pps.predictors(df=test_df_all_vs_one, y=test_label_name, random_seed=random_state, **ppscore_params)\n        s_pps_train = df_pps_train.set_index('x', drop=True)['ppscore']\n        s_pps_test = df_pps_test.set_index('x', drop=True)['ppscore']\n        s_difference = s_pps_train - s_pps_test\n        df_pps_train_all[c] = s_pps_train\n        df_pps_test_all[c] = s_pps_test\n        df_pps_difference_all[c] = s_difference\n    for feature in df_pps_train_all.index:\n        s_train = df_pps_train_all.loc[feature]\n        s_test = df_pps_test_all.loc[feature]\n        s_difference = df_pps_difference_all.loc[feature]\n        ret_value[feature] = {'train': s_train.to_dict(), 'test': s_test.to_dict(), 'train-test difference': s_difference.to_dict()}\n        if with_display and any(s_train > min_pps_to_show) or any(s_test > min_pps_to_show):\n            sorted_order_for_display = np.abs(s_difference).sort_values(ascending=False).head(n_show_top).index\n            s_train_to_display = s_train[sorted_order_for_display]\n            s_test_to_display = s_test[sorted_order_for_display]\n            fig = get_pps_figure(per_class=True, n_of_features=len(sorted_order_for_display))\n            fig.update_layout(title=f'{feature}: Predictive Power Score (PPS) Per Class')\n            fig.add_trace(pd_series_to_trace(s_train_to_display, DEFAULT_DATASET_NAMES[0], dataset_names[0]))\n            fig.add_trace(pd_series_to_trace(s_test_to_display, DEFAULT_DATASET_NAMES[1], dataset_names[1]))\n            display.append(fig)\n    return (ret_value, display)"
        ]
    }
]