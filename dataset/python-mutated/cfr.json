[
    {
        "func_name": "_apply_regret_matching_plus_reset",
        "original": "def _apply_regret_matching_plus_reset(info_state_nodes):\n    \"\"\"Resets negative cumulative regrets to 0.\n\n  Regret Matching+ corresponds to the following cumulative regrets update:\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\n\n  This must be done at the level of the information set, and thus cannot be\n  done during the tree traversal (which is done on histories). It is thus\n  performed as an additional step.\n\n  This function is a module level function to be reused by both CFRSolver and\n  CFRBRSolver.\n\n  Args:\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\n  \"\"\"\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0",
        "mutated": [
            "def _apply_regret_matching_plus_reset(info_state_nodes):\n    if False:\n        i = 10\n    'Resets negative cumulative regrets to 0.\\n\\n  Regret Matching+ corresponds to the following cumulative regrets update:\\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n\\n  This must be done at the level of the information set, and thus cannot be\\n  done during the tree traversal (which is done on histories). It is thus\\n  performed as an additional step.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0",
            "def _apply_regret_matching_plus_reset(info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Resets negative cumulative regrets to 0.\\n\\n  Regret Matching+ corresponds to the following cumulative regrets update:\\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n\\n  This must be done at the level of the information set, and thus cannot be\\n  done during the tree traversal (which is done on histories). It is thus\\n  performed as an additional step.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0",
            "def _apply_regret_matching_plus_reset(info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Resets negative cumulative regrets to 0.\\n\\n  Regret Matching+ corresponds to the following cumulative regrets update:\\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n\\n  This must be done at the level of the information set, and thus cannot be\\n  done during the tree traversal (which is done on histories). It is thus\\n  performed as an additional step.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0",
            "def _apply_regret_matching_plus_reset(info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Resets negative cumulative regrets to 0.\\n\\n  Regret Matching+ corresponds to the following cumulative regrets update:\\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n\\n  This must be done at the level of the information set, and thus cannot be\\n  done during the tree traversal (which is done on histories). It is thus\\n  performed as an additional step.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0",
            "def _apply_regret_matching_plus_reset(info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Resets negative cumulative regrets to 0.\\n\\n  Regret Matching+ corresponds to the following cumulative regrets update:\\n  cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n\\n  This must be done at the level of the information set, and thus cannot be\\n  done during the tree traversal (which is done on histories). It is thus\\n  performed as an additional step.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for info_state_node in info_state_nodes.values():\n        action_to_cum_regret = info_state_node.cumulative_regret\n        for (action, cumulative_regret) in action_to_cum_regret.items():\n            if cumulative_regret < 0:\n                action_to_cum_regret[action] = 0"
        ]
    },
    {
        "func_name": "_update_current_policy",
        "original": "def _update_current_policy(current_policy, info_state_nodes):\n    \"\"\"Updates in place `current_policy` from the cumulative regrets.\n\n  This function is a module level function to be reused by both CFRSolver and\n  CFRBRSolver.\n\n  Args:\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\n  \"\"\"\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value",
        "mutated": [
            "def _update_current_policy(current_policy, info_state_nodes):\n    if False:\n        i = 10\n    'Updates in place `current_policy` from the cumulative regrets.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value",
            "def _update_current_policy(current_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Updates in place `current_policy` from the cumulative regrets.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value",
            "def _update_current_policy(current_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Updates in place `current_policy` from the cumulative regrets.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value",
            "def _update_current_policy(current_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Updates in place `current_policy` from the cumulative regrets.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value",
            "def _update_current_policy(current_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Updates in place `current_policy` from the cumulative regrets.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    current_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        state_policy = current_policy.policy_for_key(info_state)\n        for (action, value) in _regret_matching(info_state_node.cumulative_regret, info_state_node.legal_actions).items():\n            state_policy[action] = value"
        ]
    },
    {
        "func_name": "_update_average_policy",
        "original": "def _update_average_policy(average_policy, info_state_nodes):\n    \"\"\"Updates in place `average_policy` to the average of all policies iterated.\n\n  This function is a module level function to be reused by both CFRSolver and\n  CFRBRSolver.\n\n  Args:\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\n  \"\"\"\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum",
        "mutated": [
            "def _update_average_policy(average_policy, info_state_nodes):\n    if False:\n        i = 10\n    'Updates in place `average_policy` to the average of all policies iterated.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum",
            "def _update_average_policy(average_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Updates in place `average_policy` to the average of all policies iterated.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum",
            "def _update_average_policy(average_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Updates in place `average_policy` to the average of all policies iterated.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum",
            "def _update_average_policy(average_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Updates in place `average_policy` to the average of all policies iterated.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum",
            "def _update_average_policy(average_policy, info_state_nodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Updates in place `average_policy` to the average of all policies iterated.\\n\\n  This function is a module level function to be reused by both CFRSolver and\\n  CFRBRSolver.\\n\\n  Args:\\n    average_policy: A `policy.TabularPolicy` to be updated in-place.\\n    info_state_nodes: A dictionary {`info_state_str` -> `_InfoStateNode`}.\\n  '\n    for (info_state, info_state_node) in info_state_nodes.items():\n        info_state_policies_sum = info_state_node.cumulative_policy\n        state_policy = average_policy.policy_for_key(info_state)\n        probabilities_sum = sum(info_state_policies_sum.values())\n        if probabilities_sum == 0:\n            num_actions = len(info_state_node.legal_actions)\n            for action in info_state_node.legal_actions:\n                state_policy[action] = 1 / num_actions\n        else:\n            for (action, action_prob_sum) in info_state_policies_sum.items():\n                state_policy[action] = action_prob_sum / probabilities_sum"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    \"\"\"Initializer.\n\n    Args:\n      game: The `pyspiel.Game` to run on.\n      alternating_updates: If `True`, alternating updates are performed: for\n        each player, we compute and update the cumulative regrets and policies.\n        In that case, and when the policy is frozen during tree traversal, the\n        cache is reset after each update for one player.\n        Otherwise, the update is simultaneous.\n      linear_averaging: Whether to use linear averaging, i.e.\n        cumulative_policy[info_state][action] += (\n          iteration_number * reach_prob * action_prob)\n\n        or not:\n\n        cumulative_policy[info_state][action] += reach_prob * action_prob\n      regret_matching_plus: Whether to use Regret Matching+:\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\n        or simply regret matching:\n        cumulative_regrets = cumulative_regrets + regrets\n    \"\"\"\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus",
        "mutated": [
            "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    if False:\n        i = 10\n    'Initializer.\\n\\n    Args:\\n      game: The `pyspiel.Game` to run on.\\n      alternating_updates: If `True`, alternating updates are performed: for\\n        each player, we compute and update the cumulative regrets and policies.\\n        In that case, and when the policy is frozen during tree traversal, the\\n        cache is reset after each update for one player.\\n        Otherwise, the update is simultaneous.\\n      linear_averaging: Whether to use linear averaging, i.e.\\n        cumulative_policy[info_state][action] += (\\n          iteration_number * reach_prob * action_prob)\\n\\n        or not:\\n\\n        cumulative_policy[info_state][action] += reach_prob * action_prob\\n      regret_matching_plus: Whether to use Regret Matching+:\\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n        or simply regret matching:\\n        cumulative_regrets = cumulative_regrets + regrets\\n    '\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus",
            "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializer.\\n\\n    Args:\\n      game: The `pyspiel.Game` to run on.\\n      alternating_updates: If `True`, alternating updates are performed: for\\n        each player, we compute and update the cumulative regrets and policies.\\n        In that case, and when the policy is frozen during tree traversal, the\\n        cache is reset after each update for one player.\\n        Otherwise, the update is simultaneous.\\n      linear_averaging: Whether to use linear averaging, i.e.\\n        cumulative_policy[info_state][action] += (\\n          iteration_number * reach_prob * action_prob)\\n\\n        or not:\\n\\n        cumulative_policy[info_state][action] += reach_prob * action_prob\\n      regret_matching_plus: Whether to use Regret Matching+:\\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n        or simply regret matching:\\n        cumulative_regrets = cumulative_regrets + regrets\\n    '\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus",
            "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializer.\\n\\n    Args:\\n      game: The `pyspiel.Game` to run on.\\n      alternating_updates: If `True`, alternating updates are performed: for\\n        each player, we compute and update the cumulative regrets and policies.\\n        In that case, and when the policy is frozen during tree traversal, the\\n        cache is reset after each update for one player.\\n        Otherwise, the update is simultaneous.\\n      linear_averaging: Whether to use linear averaging, i.e.\\n        cumulative_policy[info_state][action] += (\\n          iteration_number * reach_prob * action_prob)\\n\\n        or not:\\n\\n        cumulative_policy[info_state][action] += reach_prob * action_prob\\n      regret_matching_plus: Whether to use Regret Matching+:\\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n        or simply regret matching:\\n        cumulative_regrets = cumulative_regrets + regrets\\n    '\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus",
            "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializer.\\n\\n    Args:\\n      game: The `pyspiel.Game` to run on.\\n      alternating_updates: If `True`, alternating updates are performed: for\\n        each player, we compute and update the cumulative regrets and policies.\\n        In that case, and when the policy is frozen during tree traversal, the\\n        cache is reset after each update for one player.\\n        Otherwise, the update is simultaneous.\\n      linear_averaging: Whether to use linear averaging, i.e.\\n        cumulative_policy[info_state][action] += (\\n          iteration_number * reach_prob * action_prob)\\n\\n        or not:\\n\\n        cumulative_policy[info_state][action] += reach_prob * action_prob\\n      regret_matching_plus: Whether to use Regret Matching+:\\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n        or simply regret matching:\\n        cumulative_regrets = cumulative_regrets + regrets\\n    '\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus",
            "def __init__(self, game, alternating_updates, linear_averaging, regret_matching_plus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializer.\\n\\n    Args:\\n      game: The `pyspiel.Game` to run on.\\n      alternating_updates: If `True`, alternating updates are performed: for\\n        each player, we compute and update the cumulative regrets and policies.\\n        In that case, and when the policy is frozen during tree traversal, the\\n        cache is reset after each update for one player.\\n        Otherwise, the update is simultaneous.\\n      linear_averaging: Whether to use linear averaging, i.e.\\n        cumulative_policy[info_state][action] += (\\n          iteration_number * reach_prob * action_prob)\\n\\n        or not:\\n\\n        cumulative_policy[info_state][action] += reach_prob * action_prob\\n      regret_matching_plus: Whether to use Regret Matching+:\\n        cumulative_regrets = max(cumulative_regrets + regrets, 0)\\n        or simply regret matching:\\n        cumulative_regrets = cumulative_regrets + regrets\\n    '\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"CFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'\n    self._game = game\n    self._num_players = game.num_players()\n    self._root_node = self._game.new_initial_state()\n    self._current_policy = policy.TabularPolicy(game)\n    self._average_policy = self._current_policy.__copy__()\n    self._info_state_nodes = {}\n    self._initialize_info_state_nodes(self._root_node)\n    self._iteration = 0\n    self._linear_averaging = linear_averaging\n    self._alternating_updates = alternating_updates\n    self._regret_matching_plus = regret_matching_plus"
        ]
    },
    {
        "func_name": "_initialize_info_state_nodes",
        "original": "def _initialize_info_state_nodes(self, state):\n    \"\"\"Initializes info_state_nodes.\n\n    Create one _InfoStateNode per infoset. We could also initialize the node\n    when we try to access it and it does not exist.\n\n    Args:\n      state: The current state in the tree walk. This should be the root node\n        when we call this function from a CFR solver.\n    \"\"\"\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))",
        "mutated": [
            "def _initialize_info_state_nodes(self, state):\n    if False:\n        i = 10\n    'Initializes info_state_nodes.\\n\\n    Create one _InfoStateNode per infoset. We could also initialize the node\\n    when we try to access it and it does not exist.\\n\\n    Args:\\n      state: The current state in the tree walk. This should be the root node\\n        when we call this function from a CFR solver.\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))",
            "def _initialize_info_state_nodes(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes info_state_nodes.\\n\\n    Create one _InfoStateNode per infoset. We could also initialize the node\\n    when we try to access it and it does not exist.\\n\\n    Args:\\n      state: The current state in the tree walk. This should be the root node\\n        when we call this function from a CFR solver.\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))",
            "def _initialize_info_state_nodes(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes info_state_nodes.\\n\\n    Create one _InfoStateNode per infoset. We could also initialize the node\\n    when we try to access it and it does not exist.\\n\\n    Args:\\n      state: The current state in the tree walk. This should be the root node\\n        when we call this function from a CFR solver.\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))",
            "def _initialize_info_state_nodes(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes info_state_nodes.\\n\\n    Create one _InfoStateNode per infoset. We could also initialize the node\\n    when we try to access it and it does not exist.\\n\\n    Args:\\n      state: The current state in the tree walk. This should be the root node\\n        when we call this function from a CFR solver.\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))",
            "def _initialize_info_state_nodes(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes info_state_nodes.\\n\\n    Create one _InfoStateNode per infoset. We could also initialize the node\\n    when we try to access it and it does not exist.\\n\\n    Args:\\n      state: The current state in the tree walk. This should be the root node\\n        when we call this function from a CFR solver.\\n    '\n    if state.is_terminal():\n        return\n    if state.is_chance_node():\n        for (action, unused_action_prob) in state.chance_outcomes():\n            self._initialize_info_state_nodes(state.child(action))\n        return\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    info_state_node = self._info_state_nodes.get(info_state)\n    if info_state_node is None:\n        legal_actions = state.legal_actions(current_player)\n        info_state_node = _InfoStateNode(legal_actions=legal_actions, index_in_tabular_policy=self._current_policy.state_lookup[info_state])\n        self._info_state_nodes[info_state] = info_state_node\n    for action in info_state_node.legal_actions:\n        self._initialize_info_state_nodes(state.child(action))"
        ]
    },
    {
        "func_name": "current_policy",
        "original": "def current_policy(self):\n    \"\"\"Returns the current policy as a TabularPolicy.\n\n    WARNING: The same object, updated in-place will be returned! You can copy\n    it (or its `action_probability_array` field).\n\n    For CFR/CFR+, this policy does not necessarily have to converge. It\n    converges with high probability for CFR-BR.\n    \"\"\"\n    return self._current_policy",
        "mutated": [
            "def current_policy(self):\n    if False:\n        i = 10\n    'Returns the current policy as a TabularPolicy.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    For CFR/CFR+, this policy does not necessarily have to converge. It\\n    converges with high probability for CFR-BR.\\n    '\n    return self._current_policy",
            "def current_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the current policy as a TabularPolicy.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    For CFR/CFR+, this policy does not necessarily have to converge. It\\n    converges with high probability for CFR-BR.\\n    '\n    return self._current_policy",
            "def current_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the current policy as a TabularPolicy.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    For CFR/CFR+, this policy does not necessarily have to converge. It\\n    converges with high probability for CFR-BR.\\n    '\n    return self._current_policy",
            "def current_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the current policy as a TabularPolicy.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    For CFR/CFR+, this policy does not necessarily have to converge. It\\n    converges with high probability for CFR-BR.\\n    '\n    return self._current_policy",
            "def current_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the current policy as a TabularPolicy.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    For CFR/CFR+, this policy does not necessarily have to converge. It\\n    converges with high probability for CFR-BR.\\n    '\n    return self._current_policy"
        ]
    },
    {
        "func_name": "average_policy",
        "original": "def average_policy(self):\n    \"\"\"Returns the average of all policies iterated.\n\n    WARNING: The same object, updated in-place will be returned! You can copy\n    it (or its `action_probability_array` field).\n\n    This average policy converges to a Nash policy as the number of iterations\n    increases.\n\n    The policy is computed using the accumulated policy probabilities computed\n    using `evaluate_and_update_policy`.\n\n    Returns:\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\n      time averaged policy (weighted by player reach probabilities) for both\n      players.\n    \"\"\"\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy",
        "mutated": [
            "def average_policy(self):\n    if False:\n        i = 10\n    'Returns the average of all policies iterated.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    This average policy converges to a Nash policy as the number of iterations\\n    increases.\\n\\n    The policy is computed using the accumulated policy probabilities computed\\n    using `evaluate_and_update_policy`.\\n\\n    Returns:\\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\\n      time averaged policy (weighted by player reach probabilities) for both\\n      players.\\n    '\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy",
            "def average_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the average of all policies iterated.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    This average policy converges to a Nash policy as the number of iterations\\n    increases.\\n\\n    The policy is computed using the accumulated policy probabilities computed\\n    using `evaluate_and_update_policy`.\\n\\n    Returns:\\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\\n      time averaged policy (weighted by player reach probabilities) for both\\n      players.\\n    '\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy",
            "def average_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the average of all policies iterated.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    This average policy converges to a Nash policy as the number of iterations\\n    increases.\\n\\n    The policy is computed using the accumulated policy probabilities computed\\n    using `evaluate_and_update_policy`.\\n\\n    Returns:\\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\\n      time averaged policy (weighted by player reach probabilities) for both\\n      players.\\n    '\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy",
            "def average_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the average of all policies iterated.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    This average policy converges to a Nash policy as the number of iterations\\n    increases.\\n\\n    The policy is computed using the accumulated policy probabilities computed\\n    using `evaluate_and_update_policy`.\\n\\n    Returns:\\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\\n      time averaged policy (weighted by player reach probabilities) for both\\n      players.\\n    '\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy",
            "def average_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the average of all policies iterated.\\n\\n    WARNING: The same object, updated in-place will be returned! You can copy\\n    it (or its `action_probability_array` field).\\n\\n    This average policy converges to a Nash policy as the number of iterations\\n    increases.\\n\\n    The policy is computed using the accumulated policy probabilities computed\\n    using `evaluate_and_update_policy`.\\n\\n    Returns:\\n      A `policy.TabularPolicy` object (shared between calls) giving the (linear)\\n      time averaged policy (weighted by player reach probabilities) for both\\n      players.\\n    '\n    _update_average_policy(self._average_policy, self._info_state_nodes)\n    return self._average_policy"
        ]
    },
    {
        "func_name": "_compute_counterfactual_regret_for_player",
        "original": "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    \"\"\"Increments the cumulative regrets and policy for `player`.\n\n    Args:\n      state: The initial game state to analyze from.\n      policies: A list of `num_players` callables taking as input an\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\n          this is simply returning the current policy, but this can be used in\n          the CFR-BR solver, to prevent code duplication. If None,\n          `_get_infostate_policy` is used.\n      reach_probabilities: The probability for each player of reaching `state`\n        as a numpy array [prob for player 0, for player 1,..., for chance].\n        `player_reach_probabilities[player]` will work in all cases.\n      player: The 0-indexed player to update the values for. If `None`, the\n        update for all players will be performed.\n\n    Returns:\n      The utility of `state` for all players, assuming all players follow the\n      current policy defined by `self.Policy`.\n    \"\"\"\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value",
        "mutated": [
            "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    if False:\n        i = 10\n    'Increments the cumulative regrets and policy for `player`.\\n\\n    Args:\\n      state: The initial game state to analyze from.\\n      policies: A list of `num_players` callables taking as input an\\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\\n          this is simply returning the current policy, but this can be used in\\n          the CFR-BR solver, to prevent code duplication. If None,\\n          `_get_infostate_policy` is used.\\n      reach_probabilities: The probability for each player of reaching `state`\\n        as a numpy array [prob for player 0, for player 1,..., for chance].\\n        `player_reach_probabilities[player]` will work in all cases.\\n      player: The 0-indexed player to update the values for. If `None`, the\\n        update for all players will be performed.\\n\\n    Returns:\\n      The utility of `state` for all players, assuming all players follow the\\n      current policy defined by `self.Policy`.\\n    '\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value",
            "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Increments the cumulative regrets and policy for `player`.\\n\\n    Args:\\n      state: The initial game state to analyze from.\\n      policies: A list of `num_players` callables taking as input an\\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\\n          this is simply returning the current policy, but this can be used in\\n          the CFR-BR solver, to prevent code duplication. If None,\\n          `_get_infostate_policy` is used.\\n      reach_probabilities: The probability for each player of reaching `state`\\n        as a numpy array [prob for player 0, for player 1,..., for chance].\\n        `player_reach_probabilities[player]` will work in all cases.\\n      player: The 0-indexed player to update the values for. If `None`, the\\n        update for all players will be performed.\\n\\n    Returns:\\n      The utility of `state` for all players, assuming all players follow the\\n      current policy defined by `self.Policy`.\\n    '\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value",
            "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Increments the cumulative regrets and policy for `player`.\\n\\n    Args:\\n      state: The initial game state to analyze from.\\n      policies: A list of `num_players` callables taking as input an\\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\\n          this is simply returning the current policy, but this can be used in\\n          the CFR-BR solver, to prevent code duplication. If None,\\n          `_get_infostate_policy` is used.\\n      reach_probabilities: The probability for each player of reaching `state`\\n        as a numpy array [prob for player 0, for player 1,..., for chance].\\n        `player_reach_probabilities[player]` will work in all cases.\\n      player: The 0-indexed player to update the values for. If `None`, the\\n        update for all players will be performed.\\n\\n    Returns:\\n      The utility of `state` for all players, assuming all players follow the\\n      current policy defined by `self.Policy`.\\n    '\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value",
            "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Increments the cumulative regrets and policy for `player`.\\n\\n    Args:\\n      state: The initial game state to analyze from.\\n      policies: A list of `num_players` callables taking as input an\\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\\n          this is simply returning the current policy, but this can be used in\\n          the CFR-BR solver, to prevent code duplication. If None,\\n          `_get_infostate_policy` is used.\\n      reach_probabilities: The probability for each player of reaching `state`\\n        as a numpy array [prob for player 0, for player 1,..., for chance].\\n        `player_reach_probabilities[player]` will work in all cases.\\n      player: The 0-indexed player to update the values for. If `None`, the\\n        update for all players will be performed.\\n\\n    Returns:\\n      The utility of `state` for all players, assuming all players follow the\\n      current policy defined by `self.Policy`.\\n    '\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value",
            "def _compute_counterfactual_regret_for_player(self, state, policies, reach_probabilities, player):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Increments the cumulative regrets and policy for `player`.\\n\\n    Args:\\n      state: The initial game state to analyze from.\\n      policies: A list of `num_players` callables taking as input an\\n        `info_state_node` and returning a {action: prob} dictionary. For CFR,\\n          this is simply returning the current policy, but this can be used in\\n          the CFR-BR solver, to prevent code duplication. If None,\\n          `_get_infostate_policy` is used.\\n      reach_probabilities: The probability for each player of reaching `state`\\n        as a numpy array [prob for player 0, for player 1,..., for chance].\\n        `player_reach_probabilities[player]` will work in all cases.\\n      player: The 0-indexed player to update the values for. If `None`, the\\n        update for all players will be performed.\\n\\n    Returns:\\n      The utility of `state` for all players, assuming all players follow the\\n      current policy defined by `self.Policy`.\\n    '\n    if state.is_terminal():\n        return np.asarray(state.returns())\n    if state.is_chance_node():\n        state_value = 0.0\n        for (action, action_prob) in state.chance_outcomes():\n            assert action_prob > 0\n            new_state = state.child(action)\n            new_reach_probabilities = reach_probabilities.copy()\n            new_reach_probabilities[-1] *= action_prob\n            state_value += action_prob * self._compute_counterfactual_regret_for_player(new_state, policies, new_reach_probabilities, player)\n        return state_value\n    current_player = state.current_player()\n    info_state = state.information_state_string(current_player)\n    if all(reach_probabilities[:-1] == 0):\n        return np.zeros(self._num_players)\n    state_value = np.zeros(self._num_players)\n    children_utilities = {}\n    info_state_node = self._info_state_nodes[info_state]\n    if policies is None:\n        info_state_policy = self._get_infostate_policy(info_state)\n    else:\n        info_state_policy = policies[current_player](info_state)\n    for action in state.legal_actions():\n        action_prob = info_state_policy.get(action, 0.0)\n        new_state = state.child(action)\n        new_reach_probabilities = reach_probabilities.copy()\n        new_reach_probabilities[current_player] *= action_prob\n        child_utility = self._compute_counterfactual_regret_for_player(new_state, policies=policies, reach_probabilities=new_reach_probabilities, player=player)\n        state_value += action_prob * child_utility\n        children_utilities[action] = child_utility\n    simulatenous_updates = player is None\n    if not simulatenous_updates and current_player != player:\n        return state_value\n    reach_prob = reach_probabilities[current_player]\n    counterfactual_reach_prob = np.prod(reach_probabilities[:current_player]) * np.prod(reach_probabilities[current_player + 1:])\n    state_value_for_player = state_value[current_player]\n    for (action, action_prob) in info_state_policy.items():\n        cfr_regret = counterfactual_reach_prob * (children_utilities[action][current_player] - state_value_for_player)\n        info_state_node.cumulative_regret[action] += cfr_regret\n        if self._linear_averaging:\n            info_state_node.cumulative_policy[action] += self._iteration * reach_prob * action_prob\n        else:\n            info_state_node.cumulative_policy[action] += reach_prob * action_prob\n    return state_value"
        ]
    },
    {
        "func_name": "_get_infostate_policy",
        "original": "def _get_infostate_policy(self, info_state_str):\n    \"\"\"Returns an {action: prob} dictionary for the policy on `info_state`.\"\"\"\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}",
        "mutated": [
            "def _get_infostate_policy(self, info_state_str):\n    if False:\n        i = 10\n    'Returns an {action: prob} dictionary for the policy on `info_state`.'\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}",
            "def _get_infostate_policy(self, info_state_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns an {action: prob} dictionary for the policy on `info_state`.'\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}",
            "def _get_infostate_policy(self, info_state_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns an {action: prob} dictionary for the policy on `info_state`.'\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}",
            "def _get_infostate_policy(self, info_state_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns an {action: prob} dictionary for the policy on `info_state`.'\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}",
            "def _get_infostate_policy(self, info_state_str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns an {action: prob} dictionary for the policy on `info_state`.'\n    info_state_node = self._info_state_nodes[info_state_str]\n    prob_vec = self._current_policy.action_probability_array[info_state_node.index_in_tabular_policy]\n    return {action: prob_vec[action] for action in info_state_node.legal_actions}"
        ]
    },
    {
        "func_name": "_regret_matching",
        "original": "def _regret_matching(cumulative_regrets, legal_actions):\n    \"\"\"Returns an info state policy by applying regret-matching.\n\n  Args:\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\n    legal_actions: the list of legal actions at this state.\n\n  Returns:\n    A dict of action -> prob for all legal actions.\n  \"\"\"\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy",
        "mutated": [
            "def _regret_matching(cumulative_regrets, legal_actions):\n    if False:\n        i = 10\n    'Returns an info state policy by applying regret-matching.\\n\\n  Args:\\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\\n    legal_actions: the list of legal actions at this state.\\n\\n  Returns:\\n    A dict of action -> prob for all legal actions.\\n  '\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy",
            "def _regret_matching(cumulative_regrets, legal_actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns an info state policy by applying regret-matching.\\n\\n  Args:\\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\\n    legal_actions: the list of legal actions at this state.\\n\\n  Returns:\\n    A dict of action -> prob for all legal actions.\\n  '\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy",
            "def _regret_matching(cumulative_regrets, legal_actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns an info state policy by applying regret-matching.\\n\\n  Args:\\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\\n    legal_actions: the list of legal actions at this state.\\n\\n  Returns:\\n    A dict of action -> prob for all legal actions.\\n  '\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy",
            "def _regret_matching(cumulative_regrets, legal_actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns an info state policy by applying regret-matching.\\n\\n  Args:\\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\\n    legal_actions: the list of legal actions at this state.\\n\\n  Returns:\\n    A dict of action -> prob for all legal actions.\\n  '\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy",
            "def _regret_matching(cumulative_regrets, legal_actions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns an info state policy by applying regret-matching.\\n\\n  Args:\\n    cumulative_regrets: A {action: cumulative_regret} dictionary.\\n    legal_actions: the list of legal actions at this state.\\n\\n  Returns:\\n    A dict of action -> prob for all legal actions.\\n  '\n    regrets = cumulative_regrets.values()\n    sum_positive_regrets = sum((regret for regret in regrets if regret > 0))\n    info_state_policy = {}\n    if sum_positive_regrets > 0:\n        for action in legal_actions:\n            positive_action_regret = max(0.0, cumulative_regrets[action])\n            info_state_policy[action] = positive_action_regret / sum_positive_regrets\n    else:\n        for action in legal_actions:\n            info_state_policy[action] = 1.0 / len(legal_actions)\n    return info_state_policy"
        ]
    },
    {
        "func_name": "evaluate_and_update_policy",
        "original": "def evaluate_and_update_policy(self):\n    \"\"\"Performs a single step of policy evaluation and policy improvement.\"\"\"\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)",
        "mutated": [
            "def evaluate_and_update_policy(self):\n    if False:\n        i = 10\n    'Performs a single step of policy evaluation and policy improvement.'\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)",
            "def evaluate_and_update_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Performs a single step of policy evaluation and policy improvement.'\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)",
            "def evaluate_and_update_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Performs a single step of policy evaluation and policy improvement.'\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)",
            "def evaluate_and_update_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Performs a single step of policy evaluation and policy improvement.'\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)",
            "def evaluate_and_update_policy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Performs a single step of policy evaluation and policy improvement.'\n    self._iteration += 1\n    if self._alternating_updates:\n        for player in range(self._game.num_players()):\n            self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=player)\n            if self._regret_matching_plus:\n                _apply_regret_matching_plus_reset(self._info_state_nodes)\n            _update_current_policy(self._current_policy, self._info_state_nodes)\n    else:\n        self._compute_counterfactual_regret_for_player(self._root_node, policies=None, reach_probabilities=np.ones(self._game.num_players() + 1), player=None)\n        if self._regret_matching_plus:\n            _apply_regret_matching_plus_reset(self._info_state_nodes)\n        _update_current_policy(self._current_policy, self._info_state_nodes)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game):\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)",
        "mutated": [
            "def __init__(self, game):\n    if False:\n        i = 10\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(CFRPlusSolver, self).__init__(game, regret_matching_plus=True, alternating_updates=True, linear_averaging=True)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, game):\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)",
        "mutated": [
            "def __init__(self, game):\n    if False:\n        i = 10\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(CFRSolver, self).__init__(game, regret_matching_plus=False, alternating_updates=True, linear_averaging=False)"
        ]
    }
]