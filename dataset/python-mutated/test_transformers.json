[
    {
        "func_name": "use_deterministic_algorithims",
        "original": "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    \"\"\"\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\n    Upon exiting the context manager, the previous state of the flag will be restored.\n    \"\"\"\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)",
        "mutated": [
            "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    if False:\n        i = 10\n    '\\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\\n    Upon exiting the context manager, the previous state of the flag will be restored.\\n    '\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)",
            "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\\n    Upon exiting the context manager, the previous state of the flag will be restored.\\n    '\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)",
            "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\\n    Upon exiting the context manager, the previous state of the flag will be restored.\\n    '\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)",
            "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\\n    Upon exiting the context manager, the previous state of the flag will be restored.\\n    '\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)",
            "@contextlib.contextmanager\ndef use_deterministic_algorithims(mode: bool, warn_only: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This context manager can be used to temporarily enable or disable deterministic algorithms.\\n    Upon exiting the context manager, the previous state of the flag will be restored.\\n    '\n    previous_mode: bool = torch.are_deterministic_algorithms_enabled()\n    previous_warn_only: bool = torch.is_deterministic_algorithms_warn_only_enabled()\n    try:\n        torch.use_deterministic_algorithms(mode, warn_only=warn_only)\n        yield {}\n    finally:\n        torch.use_deterministic_algorithms(previous_mode, warn_only=previous_warn_only)"
        ]
    },
    {
        "func_name": "get_rtol",
        "original": "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()",
        "mutated": [
            "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()",
            "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()",
            "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()",
            "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()",
            "def get_rtol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    deviation = true_value - computed_value\n    deviation = torch.abs(deviation / true_value)\n    torch.nan_to_num_(deviation, nan=default_rtol[computed_value.dtype])\n    return deviation.max().item()"
        ]
    },
    {
        "func_name": "get_atol",
        "original": "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol",
        "mutated": [
            "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol",
            "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol",
            "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol",
            "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol",
            "def get_atol(true_value: torch.Tensor, computed_value: torch.Tensor) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    deviation = true_value - computed_value\n    atol = torch.abs(deviation).max().item()\n    return atol"
        ]
    },
    {
        "func_name": "get_tolerances",
        "original": "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    \"\"\"Returns the absolute and relative tolerances for comparing two tensors.\"\"\"\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)",
        "mutated": [
            "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    if False:\n        i = 10\n    'Returns the absolute and relative tolerances for comparing two tensors.'\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)",
            "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the absolute and relative tolerances for comparing two tensors.'\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)",
            "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the absolute and relative tolerances for comparing two tensors.'\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)",
            "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the absolute and relative tolerances for comparing two tensors.'\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)",
            "def get_tolerances(true_value: torch.Tensor, computed_value: torch.Tensor, fudge_factor: Optional[float]=None) -> Tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the absolute and relative tolerances for comparing two tensors.'\n    fudge_factor = fudge_factor if fudge_factor is not None else 1.0\n    atol = get_atol(true_value, computed_value)\n    rtol = get_rtol(true_value, computed_value)\n    atol = fudge_factor * max(atol, default_atol[computed_value.dtype])\n    rtol = fudge_factor * max(rtol, default_rtol[computed_value.dtype])\n    if rtol > 1e+30:\n        rtol = default_rtol[computed_value.dtype]\n    return (atol, rtol)"
        ]
    },
    {
        "func_name": "_size",
        "original": "def _size(i):\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)",
        "mutated": [
            "def _size(i):\n    if False:\n        i = 10\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)",
            "def _size(i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)",
            "def _size(i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)",
            "def _size(i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)",
            "def _size(i):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)"
        ]
    },
    {
        "func_name": "rand_sdpa_tensor",
        "original": "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    \"\"\"Creates rand dense or nested tensor with given shape and type.\n\n    Args:\n        shape (Tuple[int]): Shape of Tensor to construct\n        device (str): which device to create tensor on\n        dtype (torch.dtype): Tensors' dtype\n        type (str): Nested or Dense\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\n\n    Returns:\n        torch.Tensor: A new tensor\n    \"\"\"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)",
        "mutated": [
            "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    if False:\n        i = 10\n    \"Creates rand dense or nested tensor with given shape and type.\\n\\n    Args:\\n        shape (Tuple[int]): Shape of Tensor to construct\\n        device (str): which device to create tensor on\\n        dtype (torch.dtype): Tensors' dtype\\n        type (str): Nested or Dense\\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\\n\\n    Returns:\\n        torch.Tensor: A new tensor\\n    \"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)",
            "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Creates rand dense or nested tensor with given shape and type.\\n\\n    Args:\\n        shape (Tuple[int]): Shape of Tensor to construct\\n        device (str): which device to create tensor on\\n        dtype (torch.dtype): Tensors' dtype\\n        type (str): Nested or Dense\\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\\n\\n    Returns:\\n        torch.Tensor: A new tensor\\n    \"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)",
            "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Creates rand dense or nested tensor with given shape and type.\\n\\n    Args:\\n        shape (Tuple[int]): Shape of Tensor to construct\\n        device (str): which device to create tensor on\\n        dtype (torch.dtype): Tensors' dtype\\n        type (str): Nested or Dense\\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\\n\\n    Returns:\\n        torch.Tensor: A new tensor\\n    \"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)",
            "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Creates rand dense or nested tensor with given shape and type.\\n\\n    Args:\\n        shape (Tuple[int]): Shape of Tensor to construct\\n        device (str): which device to create tensor on\\n        dtype (torch.dtype): Tensors' dtype\\n        type (str): Nested or Dense\\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\\n\\n    Returns:\\n        torch.Tensor: A new tensor\\n    \"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)",
            "def rand_sdpa_tensor(shape: SdpaShape, device: str, dtype: torch.dtype, type: str, requires_grad: bool=False, packed: bool=False) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Creates rand dense or nested tensor with given shape and type.\\n\\n    Args:\\n        shape (Tuple[int]): Shape of Tensor to construct\\n        device (str): which device to create tensor on\\n        dtype (torch.dtype): Tensors' dtype\\n        type (str): Nested or Dense\\n        requires_grad (bool, optional): Tensors grad status. Defaults to False.\\n        packed (bool, optional): Whether to create a single QKV packed or not. Defaults to False.\\n\\n    Returns:\\n        torch.Tensor: A new tensor\\n    \"\n    (batch, num_heads, seq_len, head_dim) = (shape.batch, shape.num_heads, shape.seq_len, shape.head_dim)\n    if type == 'nested':\n        if isinstance(seq_len, list):\n\n            def _size(i):\n                return (seq_len[i], num_heads, head_dim) if not packed else (seq_len[i], 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(_size(i), device=device, dtype=dtype, requires_grad=requires_grad) for i in range(batch)])\n        else:\n            size = (seq_len, num_heads, head_dim) if not packed else (seq_len, 3 * num_heads * head_dim)\n            return torch.nested.nested_tensor([torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad) for _ in range(batch)])\n    else:\n        assert isinstance(seq_len, int)\n        size = (batch, seq_len, num_heads, head_dim) if not packed else (batch, seq_len, 3 * num_heads * head_dim)\n        return torch.randn(size, device=device, dtype=dtype, requires_grad=requires_grad)"
        ]
    },
    {
        "func_name": "calculate_nt_tolerances",
        "original": "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)",
        "mutated": [
            "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    if False:\n        i = 10\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)",
            "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)",
            "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)",
            "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)",
            "def calculate_nt_tolerances(nt_ref_hp, nt_ref_lp, default_dtype, fudge_factor=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ref_atol = default_atol[default_dtype]\n    ref_rtol = default_rtol[default_dtype]\n    for (tensor_component_ref, tensor_component_ref_lp) in zip(nt_ref_hp.unbind(), nt_ref_lp.unbind()):\n        ref_atol = max((fudge_factor * torch.abs(tensor_component_ref - tensor_component_ref_lp)).max().item(), ref_atol)\n        ref_rtol = max(get_rtol(tensor_component_ref, tensor_component_ref_lp), ref_rtol)\n    return (ref_atol, ref_rtol)"
        ]
    },
    {
        "func_name": "test_self_attn_TxT_attn_mask",
        "original": "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)",
        "mutated": [
            "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    if False:\n        i = 10\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)",
            "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)",
            "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)",
            "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)",
            "@onlyCUDA\n@unittest.skip('4D mask not supported yet - activate when 4D mask supported')\ndef test_self_attn_TxT_attn_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    embed_dim = 16\n    num_heads = 4\n    batch_size = 10\n    tgt_len = 16\n    query = torch.rand(batch_size, tgt_len, embed_dim, device=device)\n    attn_mask = torch.randint(0, 2, (tgt_len, tgt_len)).cuda().float()\n    attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, 0.0)\n    attn_mask_4d = attn_mask.expand(batch_size, num_heads, tgt_len, tgt_len)\n    mta_model = torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).cuda()\n    mta_model.eval()\n    with torch.inference_mode():\n        output_mask_4d = mta_model(query, query, query, attn_mask=attn_mask_4d)[0]\n        output_mask_4d = output_mask_4d.transpose(0, 1)\n        output_mask_TxT = mta_model(query, query, query, attn_mask=attn_mask)[0]\n        output_mask_TxT = output_mask_TxT.transpose(0, 1)\n        self.assertEqual(output_mask_4d, output_mask_TxT)"
        ]
    },
    {
        "func_name": "test_train_with_pad_and_catch_error",
        "original": "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')",
        "mutated": [
            "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    if False:\n        i = 10\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')",
            "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')",
            "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')",
            "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')",
            "@slowTest\ndef test_train_with_pad_and_catch_error(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    iters = 100\n    pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool).to(device)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=2, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    for i in range(iters):\n        encoder.train()\n        optimizer.zero_grad()\n        inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n        outputs = encoder(inputs, src_key_padding_mask=pad_mask)\n        loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            test = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1).to(device)\n            ex = None\n            try:\n                test_train_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.uint8))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported uint8 type exception')\n            test_train_bool = encoder(test, src_key_padding_mask=pad_mask)\n            encoder.eval()\n            ex = None\n            try:\n                test_eval_uint8 = encoder(test, src_key_padding_mask=pad_mask.to(torch.int64))\n            except AssertionError as e:\n                continue\n            self.assertFalse(e, 'Failed to catch unsupported Long type exception')\n            test_eval_bool = encoder(test, src_key_padding_mask=pad_mask)\n            l1_bool = nn.L1Loss()(test_train_bool[:, 0:2, :], test_eval_bool[:, 0:2, :]).item()\n            self.assertTrue(l1_bool < 0.0001, 'Eval/Train difference in pad_mask BOOL')"
        ]
    },
    {
        "func_name": "test_multiheadattention_fastpath_attn_mask",
        "original": "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)",
        "mutated": [
            "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    if False:\n        i = 10\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)",
            "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)",
            "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)",
            "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)",
            "@parametrize('attn_mask_dim', [2, 3, None])\n@parametrize('key_padding_mask_dim', [2, None])\n@parametrize('mask_dtype', [torch.bool, torch.float32])\ndef test_multiheadattention_fastpath_attn_mask(self, device, attn_mask_dim, key_padding_mask_dim, mask_dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with torch.no_grad():\n        B = 2\n        L = 4\n        D = 8\n        H = 4\n        if attn_mask_dim == 2:\n            attn_mask = make_tensor((L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim == 3:\n            attn_mask = make_tensor((B * H, L, L), dtype=mask_dtype, device=device)\n        elif attn_mask_dim is None:\n            attn_mask = None\n        if key_padding_mask_dim == 2:\n            key_padding_mask = make_tensor((B, L), dtype=mask_dtype, device=device)\n        elif key_padding_mask_dim is None:\n            key_padding_mask = None\n        mha = nn.MultiheadAttention(D, H, batch_first=True, device=device)\n        X = torch.randn(B, L, D, device=device)\n        mha.train()\n        (out, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        mha.eval()\n        (out_fp, _) = mha(X, X, X, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)\n        self.assertEqual(out, out_fp)"
        ]
    },
    {
        "func_name": "test_transformerencoderlayer_src_mask",
        "original": "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)",
        "mutated": [
            "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    if False:\n        i = 10\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)",
            "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)",
            "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)",
            "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)",
            "@parametrize('nhead', [1, 4, 8])\ndef test_transformerencoderlayer_src_mask(self, device, nhead):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    model(src, src_mask=src_mask)\n    model.eval()\n    with torch.no_grad():\n        model(src, src_mask=src_mask)"
        ]
    },
    {
        "func_name": "test_transformerencoder_fastpath",
        "original": "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    \"\"\"\n        Test TransformerEncoder fastpath output matches slowpath output\n        \"\"\"\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)",
        "mutated": [
            "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    if False:\n        i = 10\n    '\\n        Test TransformerEncoder fastpath output matches slowpath output\\n        '\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Test TransformerEncoder fastpath output matches slowpath output\\n        '\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Test TransformerEncoder fastpath output matches slowpath output\\n        '\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Test TransformerEncoder fastpath output matches slowpath output\\n        '\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('use_torchscript', [False])\n@parametrize('enable_nested_tensor', [True, False])\n@parametrize('use_autocast', [True, False])\n@parametrize('d_model', [12, 256])\ndef test_transformerencoder_fastpath(self, device, use_torchscript, enable_nested_tensor, use_autocast, d_model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Test TransformerEncoder fastpath output matches slowpath output\\n        '\n    torch.manual_seed(1234)\n    nhead = 4\n    dim_feedforward = d_model\n    batch_first = True\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=batch_first), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device).eval()\n    if use_torchscript:\n        model = torch.jit.script(model)\n    input_mask_pairs = [(torch.rand(3, 2, d_model), [[0, 1], [0, 1], [1, 1]]), (torch.rand(2, 100, d_model), [[0] * 98 + [1] * 2, [0] * 90 + [1] * 10]), (torch.rand(2, 1024, d_model), [[0] * 1020 + [1] * 4, [0] * 1024]), (torch.rand(1, 1026, d_model), [[0] * 1024 + [1] * 2]), (torch.rand(4, 1040, d_model), [[0] * 1024 + [1] * 16, [0] * 1025 + [1] * 15, [0] * 1031 + [1] * 9, [0] * 1040])]\n    input_mask_pairs = [(torch.tensor(pair[0], device=device, dtype=torch.get_default_dtype()), torch.tensor(pair[1], device=device, dtype=torch.bool)) for pair in input_mask_pairs]\n    maybe_autocast = torch.autocast('cuda', dtype=torch.float16) if use_autocast else contextlib.nullcontext()\n    with maybe_autocast:\n        for (input, src_key_padding_mask) in input_mask_pairs:\n            with torch.no_grad():\n                fastpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            slowpath_output = model(input, src_key_padding_mask=src_key_padding_mask)\n            (bs, true_seqlen, embed_dim) = fastpath_output.shape\n            expanded_seqlen = src_key_padding_mask.shape[1]\n            fastpath_output_expanded = torch.zeros(bs, expanded_seqlen, embed_dim, device=device)\n            fastpath_output_expanded[:, :true_seqlen, :] = fastpath_output\n            fastpath_output_expanded = fastpath_output_expanded.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            slowpath_output = slowpath_output.masked_fill(src_key_padding_mask.unsqueeze(-1), 0)\n            torch.testing.assert_close(fastpath_output_expanded, slowpath_output, rtol=1e-07, atol=1e-05)"
        ]
    },
    {
        "func_name": "test_transformerencoder_square_input",
        "original": "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    \"\"\"\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\n        batch size == sequence length\n        \"\"\"\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
        "mutated": [
            "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n    '\\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\\n        batch size == sequence length\\n        '\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\\n        batch size == sequence length\\n        '\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\\n        batch size == sequence length\\n        '\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\\n        batch size == sequence length\\n        '\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "@parametrize('with_no_grad', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [False])\ndef test_transformerencoder_square_input(self, with_no_grad, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Test for edge cases when input of shape (batch size, sequence length, embedding dimension) has\\n        batch size == sequence length\\n        '\n    model = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=16, dropout=0.0, batch_first=True), num_layers=2, enable_nested_tensor=enable_nested_tensor).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(model.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    if training:\n        model = model.train()\n    else:\n        model = model.eval()\n    x = torch.arange(0, 16).reshape(2, 2, 4).to(torch.get_default_dtype()).to(device)\n    src_mask = torch.Tensor([[0, 1], [0, 0]]).to(torch.bool).to(device)\n    if with_no_grad:\n        cm = torch.no_grad()\n    else:\n        cm = contextlib.nullcontext()\n    with cm:\n        result = model(x, mask=src_mask)\n    ref_output = torch.Tensor([[[2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351], [2.420306205749512, 0.01762924157083, -0.607857942581177, -0.085519507527351]], [[2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689], [2.419836044311523, 0.017548924311996, -0.608187675476074, -0.085347734391689]]]).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)"
        ]
    },
    {
        "func_name": "get_a_test_layer",
        "original": "def get_a_test_layer(activation, batch_first=False):\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer",
        "mutated": [
            "def get_a_test_layer(activation, batch_first=False):\n    if False:\n        i = 10\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer",
            "def get_a_test_layer(activation, batch_first=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer",
            "def get_a_test_layer(activation, batch_first=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer",
            "def get_a_test_layer(activation, batch_first=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer",
            "def get_a_test_layer(activation, batch_first=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    d_model = 4\n    nhead = 2\n    dim_feedforward = 16\n    dropout = 0.0\n    layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n    with torch.no_grad():\n        for (idx, p) in enumerate(layer.parameters()):\n            x = p.data\n            sz = x.view(-1).size(0)\n            shape = x.shape\n            x = torch.cos(torch.arange(0, sz).float().view(shape))\n            p.data.copy_(x)\n    return layer"
        ]
    },
    {
        "func_name": "perm_fn",
        "original": "def perm_fn(x):\n    return x.transpose(1, 0) if batch_first else x",
        "mutated": [
            "def perm_fn(x):\n    if False:\n        i = 10\n    return x.transpose(1, 0) if batch_first else x",
            "def perm_fn(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return x.transpose(1, 0) if batch_first else x",
            "def perm_fn(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return x.transpose(1, 0) if batch_first else x",
            "def perm_fn(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return x.transpose(1, 0) if batch_first else x",
            "def perm_fn(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return x.transpose(1, 0) if batch_first else x"
        ]
    },
    {
        "func_name": "_test",
        "original": "def _test(batch_first, training, enable_nested_tensor):\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
        "mutated": [
            "def _test(batch_first, training, enable_nested_tensor):\n    if False:\n        i = 10\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "def _test(batch_first, training, enable_nested_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "def _test(batch_first, training, enable_nested_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "def _test(batch_first, training, enable_nested_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)",
            "def _test(batch_first, training, enable_nested_tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def perm_fn(x):\n        return x.transpose(1, 0) if batch_first else x\n    encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n    model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n    result = model(encoder_input)\n    ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    src_mask = torch.zeros([5, 5]).to(device) == 1\n    result = model(encoder_input, mask=src_mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask = torch.zeros([2, 5]).to(device) == 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    mask[0, 1] = 1\n    mask[1, 3] = 1\n    mask[1, 4] = 1\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    norm = nn.LayerNorm(4)\n    model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n    if not training:\n        model = model.eval()\n    result = model(encoder_input, src_key_padding_mask=mask)\n    ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n    self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n    torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)"
        ]
    },
    {
        "func_name": "test_transformerencoder",
        "original": "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)",
        "mutated": [
            "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)",
            "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)",
            "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)",
            "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)",
            "@parametrize('batch_first', [True, False])\n@parametrize('training', [True, False])\n@parametrize('enable_nested_tensor', [True, False])\ndef test_transformerencoder(self, batch_first, training, enable_nested_tensor, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def get_a_test_layer(activation, batch_first=False):\n        d_model = 4\n        nhead = 2\n        dim_feedforward = 16\n        dropout = 0.0\n        layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, batch_first=batch_first).to(device)\n        with torch.no_grad():\n            for (idx, p) in enumerate(layer.parameters()):\n                x = p.data\n                sz = x.view(-1).size(0)\n                shape = x.shape\n                x = torch.cos(torch.arange(0, sz).float().view(shape))\n                p.data.copy_(x)\n        return layer\n    activation = F.relu\n\n    def _test(batch_first, training, enable_nested_tensor):\n\n        def perm_fn(x):\n            return x.transpose(1, 0) if batch_first else x\n        encoder_layer = get_a_test_layer(activation=activation, batch_first=batch_first)\n        model = nn.TransformerEncoder(encoder_layer, 1, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        encoder_input = perm_fn(torch.tensor([[[0.7462, 0.6653, 0.5679, 0.4891], [0.5387, 0.1655, 0.3565, 0.0471]], [[0.8335, 0.2799, 0.5031, 0.2947], [0.1402, 0.0318, 0.7636, 0.1346]], [[0.6333, 0.9344, 0.1376, 0.9938], [0.8924, 0.2872, 0.6692, 0.2944]], [[0.9897, 0.6915, 0.3154, 0.1733], [0.8645, 0.3513, 0.3064, 0.0767]], [[0.8117, 0.2366, 0.4838, 0.7881], [0.3718, 0.4945, 0.9511, 0.0864]]])).to(device)\n        result = model(encoder_input)\n        ref_output = perm_fn(torch.tensor([[[2.428589, 0.020835, -0.602055, -0.085249], [2.427987, 0.021213, -0.602496, -0.084103]], [[2.424689, 0.019155, -0.604793, -0.085672], [2.413863, 0.022211, -0.612486, -0.07249]], [[2.433774, 0.021598, -0.598343, -0.087548], [2.425104, 0.019748, -0.604515, -0.084839]], [[2.436185, 0.022682, -0.596625, -0.087261], [2.433556, 0.021891, -0.598509, -0.086832]], [[2.416246, 0.017512, -0.610712, -0.082961], [2.422901, 0.024187, -0.606178, -0.074929]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        src_mask = torch.zeros([5, 5]).to(device) == 1\n        result = model(encoder_input, mask=src_mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask = torch.zeros([2, 5]).to(device) == 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        mask[0, 1] = 1\n        mask[1, 3] = 1\n        mask[1, 4] = 1\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.429026, 0.020793, -0.601741, -0.085642], [2.428811, 0.021445, -0.601912, -0.084252]], [[2.425009, 0.019155, -0.604566, -0.085899], [2.415408, 0.02249, -0.611415, -0.073]], [[2.434199, 0.021682, -0.598039, -0.087699], [2.42598, 0.019941, -0.603896, -0.085091]], [[2.436457, 0.022736, -0.59643, -0.08736], [2.434021, 0.022093, -0.598179, -0.08679]], [[2.416531, 0.017498, -0.610513, -0.083181], [2.4242, 0.024653, -0.605266, -0.074959]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 2, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419051, 0.017446, -0.608738, -0.085003], [2.419102, 0.017452, -0.608703, -0.085026]], [[2.419043, 0.017445, -0.608744, -0.084999], [2.419052, 0.017446, -0.608738, -0.085004]], [[2.419067, 0.017448, -0.608727, -0.08501], [2.419098, 0.017452, -0.608706, -0.085024]], [[2.419072, 0.017449, -0.608724, -0.085012], [2.419119, 0.017455, -0.608691, -0.085034]], [[2.419019, 0.017442, -0.608761, -0.084989], [2.419075, 0.017449, -0.608722, -0.085014]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]], [[2.419101, 0.017453, -0.608703, -0.085025], [2.419101, 0.017453, -0.608704, -0.085025]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        norm = nn.LayerNorm(4)\n        model = nn.TransformerEncoder(encoder_layer, 2, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695949, -0.357635, -0.893077, -0.445238], [1.695955, -0.357639, -0.89305, -0.445266]], [[1.695948, -0.357634, -0.893082, -0.445233], [1.69595, -0.357635, -0.893077, -0.445238]], [[1.695951, -0.357636, -0.893069, -0.445246], [1.695955, -0.357639, -0.893052, -0.445264]], [[1.695952, -0.357636, -0.893066, -0.445249], [1.695957, -0.357641, -0.893041, -0.445276]], [[1.695946, -0.357632, -0.893095, -0.44522], [1.695952, -0.357637, -0.893065, -0.445251]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n        model = nn.TransformerEncoder(encoder_layer, 6, norm=norm, enable_nested_tensor=enable_nested_tensor).to(device)\n        if not training:\n            model = model.eval()\n        result = model(encoder_input, src_key_padding_mask=mask)\n        ref_output = perm_fn(torch.tensor([[[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]], [[1.695955, -0.357639, -0.893051, -0.445265], [1.695955, -0.357639, -0.893051, -0.445265]]])).to(device)\n        self.assertEqual(tuple(result.shape), tuple(ref_output.shape))\n        torch.testing.assert_close(result, ref_output, rtol=1e-07, atol=1e-05)\n    with set_default_dtype(torch.double):\n        if training:\n            cm = contextlib.nullcontext()\n        else:\n            cm = torch.no_grad()\n        with cm:\n            _test(batch_first, training, enable_nested_tensor)"
        ]
    },
    {
        "func_name": "test_encoder_padding_and_src_mask_bool",
        "original": "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)",
        "mutated": [
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_encoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    encoder_layer = nn.TransformerEncoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    encoder_norm = nn.LayerNorm(16)\n    encoder = nn.TransformerEncoder(encoder_layer, 2, encoder_norm)\n    inputs = torch.randn(2, 3, 16)\n    src_mask = torch.ones(3, 3, dtype=torch.bool).triu_(diagonal=1)\n    input_seq_len = torch.tensor([3, 2])\n    padding_mask = torch.arange(3)[None, :].cpu() >= input_seq_len[:, None]\n    with self.assertNoLogs(None):\n        encoder(inputs, mask=src_mask, src_key_padding_mask=padding_mask)"
        ]
    },
    {
        "func_name": "transformer_decoder",
        "original": "def transformer_decoder(inputs, input_seq_len, memory):\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)",
        "mutated": [
            "def transformer_decoder(inputs, input_seq_len, memory):\n    if False:\n        i = 10\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)",
            "def transformer_decoder(inputs, input_seq_len, memory):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)",
            "def transformer_decoder(inputs, input_seq_len, memory):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)",
            "def transformer_decoder(inputs, input_seq_len, memory):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)",
            "def transformer_decoder(inputs, input_seq_len, memory):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n    decoder_norm = nn.LayerNorm(16)\n    decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n    src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n    padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n    return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)"
        ]
    },
    {
        "func_name": "test_decoder_padding_and_src_mask_bool",
        "original": "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)",
        "mutated": [
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)",
            "@unittest.skipIf(sys.version_info < (3, 11), 'not supported on pre-3.11 Python')\ndef test_decoder_padding_and_src_mask_bool(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def transformer_decoder(inputs, input_seq_len, memory):\n        decoder_layer = nn.TransformerDecoderLayer(d_model=16, nhead=2, dim_feedforward=32, dropout=0.1, activation='relu', batch_first=True)\n        decoder_norm = nn.LayerNorm(16)\n        decoder = nn.TransformerDecoder(decoder_layer, 2, decoder_norm)\n        src_mask = torch.ones(inputs.shape[1], inputs.shape[1], dtype=torch.bool).triu_(diagonal=1)\n        padding_mask = torch.arange(inputs.shape[1])[None, :].cpu() >= input_seq_len[:, None]\n        return decoder(inputs, memory, tgt_mask=src_mask, tgt_key_padding_mask=padding_mask, memory_key_padding_mask=padding_mask)\n    inputs = torch.randn(2, 3, 16)\n    memory = torch.randn(2, 3, 16)\n    input_seq_len = torch.tensor([3, 2])\n    with self.assertNoLogs(None):\n        transformer_decoder(inputs, input_seq_len, memory)"
        ]
    },
    {
        "func_name": "test_encoder_is_causal",
        "original": "def test_encoder_is_causal(self):\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)",
        "mutated": [
            "def test_encoder_is_causal(self):\n    if False:\n        i = 10\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)",
            "def test_encoder_is_causal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)",
            "def test_encoder_is_causal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)",
            "def test_encoder_is_causal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)",
            "def test_encoder_is_causal(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    d_model = 3\n    layer = torch.nn.TransformerEncoderLayer(d_model, 1, 6, batch_first=True)\n    layer.eval()\n    x = torch.randn(1, 5, d_model)\n    unmasked_output = layer(x)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(x.size(1))\n    is_causal_output = layer(x, src_mask=mask, is_causal=True)\n    masked_output = layer(x, src_mask=mask)\n    self.assertEqual(masked_output, is_causal_output)"
        ]
    },
    {
        "func_name": "test_mha_native_args",
        "original": "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)",
        "mutated": [
            "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    if False:\n        i = 10\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)",
            "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)",
            "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)",
            "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)",
            "@onlyCUDA\n@parametrize('nb_heads', [1, 8])\n@parametrize('bias', [True, False])\ndef test_mha_native_args(self, nb_heads, bias):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (B, L, F) = (8, 100, 128)\n    batch_first = True\n    fast_path = True\n    use_pad_mask = bias % 2 == 1\n    mha = nn.MultiheadAttention(embed_dim=F, num_heads=nb_heads, batch_first=batch_first, bias=bias).cuda()\n    mha.eval()\n    ctx = torch.no_grad if fast_path else contextlib.nullcontext\n    with ctx():\n        x = torch.randn(B, L, F).cuda()\n        if not batch_first:\n            x = x.transpose(0, 1)\n        pad_mask = None\n        if use_pad_mask:\n            pad_mask = torch.zeros((B, L), dtype=torch.bool).cuda()\n        mha(query=x, key=x, value=x, key_padding_mask=pad_mask)"
        ]
    },
    {
        "func_name": "test_kpm_mask_trailing_column_with_nested_tensor",
        "original": "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)",
        "mutated": [
            "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    if False:\n        i = 10\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)",
            "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)",
            "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)",
            "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)",
            "def test_kpm_mask_trailing_column_with_nested_tensor(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=False)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    x = torch.randn(10, 6, 256).to(device)\n    mask = torch.ones(6, 10)\n    mask[0, :] = 0\n    mask = mask.bool().to(device)\n    out = transformer_encoder(src=x, src_key_padding_mask=mask)\n    self.assertEqual(out.shape[1], 6)"
        ]
    },
    {
        "func_name": "test_with_nested_tensor_input",
        "original": "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)",
        "mutated": [
            "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    if False:\n        i = 10\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)",
            "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)",
            "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)",
            "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)",
            "@onlyCUDA\ndef test_with_nested_tensor_input(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=512, activation='gelu', norm_first=False, batch_first=True)\n    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3, enable_nested_tensor=True).to(device)\n    transformer_encoder.eval()\n    with torch.no_grad():\n        x = torch.randn(6, 10, 256).to(device)\n        mask = torch.ones(6, 10)\n        mask[0, 0:] = 0\n        mask[2, 2:] = 0\n        mask[4, 4:] = 0\n        mask[5, 8:] = 0\n        mask = mask.bool().to(device)\n        x = torch._nested_tensor_from_mask(x, mask.logical_not(), mask_check=False)\n        out = transformer_encoder(src=x, src_key_padding_mask=None)\n    self.assertEqual(out.is_nested, True)"
        ]
    },
    {
        "func_name": "test_script_encoder_subclass",
        "original": "def test_script_encoder_subclass(self, device):\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)",
        "mutated": [
            "def test_script_encoder_subclass(self, device):\n    if False:\n        i = 10\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)",
            "def test_script_encoder_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)",
            "def test_script_encoder_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)",
            "def test_script_encoder_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)",
            "def test_script_encoder_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    encoder = nn.TransformerEncoder(MyCustomLayer(d_model=256, nhead=8), num_layers=6).to(device=device)\n    torch.jit.script(encoder)"
        ]
    },
    {
        "func_name": "test_transformerencoderlayer_subclass",
        "original": "def test_transformerencoderlayer_subclass(self, device):\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)",
        "mutated": [
            "def test_transformerencoderlayer_subclass(self, device):\n    if False:\n        i = 10\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    model = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True).to(device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, src_mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, src_mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, src_mask=src_mask)\n        scripted_result = script_model(src, src_mask=src_mask)\n        self.assertEqual(result, scripted_result)"
        ]
    },
    {
        "func_name": "test_transformerencoderlayer_subclass_model",
        "original": "def test_transformerencoderlayer_subclass_model(self, device):\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)",
        "mutated": [
            "def test_transformerencoderlayer_subclass_model(self, device):\n    if False:\n        i = 10\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass_model(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass_model(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass_model(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)",
            "def test_transformerencoderlayer_subclass_model(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    class MyCustomLayer(nn.TransformerEncoderLayer):\n        pass\n    nhead = 4\n    batch_size = 2\n    seqlen = 4\n    d_model = 8\n    dim_feedforward = 32\n    layer = MyCustomLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n    model = nn.TransformerEncoder(layer, num_layers=6).to(device=device)\n    script_model = torch.jit.script(model)\n    src = torch.rand(batch_size, seqlen, d_model).to(device)\n    src_mask = torch.zeros(seqlen, seqlen).to(torch.bool).to(device)\n    torch.manual_seed(42)\n    result = model(src, mask=src_mask)\n    torch.manual_seed(42)\n    scripted_result = script_model(src, mask=src_mask)\n    self.assertEqual(result, scripted_result)\n    model.eval()\n    script_model = torch.jit.script(model)\n    with torch.no_grad():\n        result = model(src, mask=src_mask)\n        scripted_result = script_model(src, mask=src_mask)\n        self.assertEqual(result, scripted_result)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()",
        "mutated": [
            "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    if False:\n        i = 10\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()",
            "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()",
            "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()",
            "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()",
            "def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    cfg = fairseq_transformer.TransformerConfig()\n    cfg.decoder.embed_dim = embed_dim\n    cfg.decoder.output_dim = embed_dim\n    cfg.decoder.attention_heads = attention_heads\n    cfg.decoder.ffn_embed_dim = ffn_embed_dim\n    cfg.dropout = dropout\n    cfg.decoder.normalize_before = normalize_before\n    cfg.decoder.layers = num_layers\n    cfg.no_token_positional_embeddings = True\n    cfg.no_scale_embedding = True\n    cfg.activation_fn = activation\n    dictionary = {}\n    self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n    if torch_encoder is not None:\n        self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n    self.decoder = self.decoder.eval().cuda().half()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
        "mutated": [
            "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    if False:\n        i = 10\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]"
        ]
    },
    {
        "func_name": "test_decoder_only_layer",
        "original": "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    if False:\n        i = 10\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]",
            "@onlyCUDA\n@unittest.skipIf(not TEST_FAIRSEQ, 'Fairseq not found')\ndef test_decoder_only_layer(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    DEFAULT_PADDING_IDX = 0\n\n    class FairseqDecoder(torch.nn.Module):\n\n        def __init__(self, embed_dim, attention_heads, ffn_embed_dim, num_layers, embedding_layer, dropout=0, normalize_before=False, torch_encoder=None, activation='relu'):\n            super().__init__()\n            cfg = fairseq_transformer.TransformerConfig()\n            cfg.decoder.embed_dim = embed_dim\n            cfg.decoder.output_dim = embed_dim\n            cfg.decoder.attention_heads = attention_heads\n            cfg.decoder.ffn_embed_dim = ffn_embed_dim\n            cfg.dropout = dropout\n            cfg.decoder.normalize_before = normalize_before\n            cfg.decoder.layers = num_layers\n            cfg.no_token_positional_embeddings = True\n            cfg.no_scale_embedding = True\n            cfg.activation_fn = activation\n            dictionary = {}\n            self.decoder = fairseq_transformer.TransformerDecoder(cfg, dictionary, embedding_layer, no_encoder_attn=True, output_projection=None)\n            if torch_encoder is not None:\n                self.decoder = torch_to_fairseq(torch_encoder, self.decoder)\n            self.decoder = self.decoder.eval().cuda().half()\n\n        def forward(self, tokens, src_lengths=None, with_triangle_mask=False, incremental_state=None):\n            return self.decoder(prev_output_tokens=tokens, encoder_out=None, incremental_state=incremental_state, features_only=True, full_context_alignment=not with_triangle_mask, alignment_layer=None, alignment_heads=None, src_lengths=src_lengths, return_all_hiddens=False)[0]"
        ]
    },
    {
        "func_name": "sdp_ref",
        "original": "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output",
        "mutated": [
            "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    if False:\n        i = 10\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output",
            "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output",
            "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output",
            "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output",
            "def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    E = q.size(-1)\n    q = q / math.sqrt(E)\n    if attn_mask is not None:\n        attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n    else:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    if dropout_p > 0.0:\n        attn = torch.nn.functional.dropout(attn, p=dropout_p)\n    output = torch.bmm(attn, v)\n    return output"
        ]
    },
    {
        "func_name": "rand_tensor",
        "original": "def rand_tensor(*shape):\n    return torch.randn(shape, device=device, dtype=dtype)",
        "mutated": [
            "def rand_tensor(*shape):\n    if False:\n        i = 10\n    return torch.randn(shape, device=device, dtype=dtype)",
            "def rand_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return torch.randn(shape, device=device, dtype=dtype)",
            "def rand_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return torch.randn(shape, device=device, dtype=dtype)",
            "def rand_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return torch.randn(shape, device=device, dtype=dtype)",
            "def rand_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return torch.randn(shape, device=device, dtype=dtype)"
        ]
    },
    {
        "func_name": "ones_tensor",
        "original": "def ones_tensor(*shape):\n    return torch.ones(shape, dtype=torch.float32)",
        "mutated": [
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n    return torch.ones(shape, dtype=torch.float32)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return torch.ones(shape, dtype=torch.float32)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return torch.ones(shape, dtype=torch.float32)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return torch.ones(shape, dtype=torch.float32)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return torch.ones(shape, dtype=torch.float32)"
        ]
    },
    {
        "func_name": "func",
        "original": "def func():\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)",
        "mutated": [
            "def func():\n    if False:\n        i = 10\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)",
            "def func():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)",
            "def func():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)",
            "def func():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)",
            "def func():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)"
        ]
    },
    {
        "func_name": "test_incompatible_mask",
        "original": "def test_incompatible_mask(self, device):\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)",
        "mutated": [
            "def test_incompatible_mask(self, device):\n    if False:\n        i = 10\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)",
            "def test_incompatible_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)",
            "def test_incompatible_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)",
            "def test_incompatible_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)",
            "def test_incompatible_mask(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, dtype=torch.float32)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n    mha.out_proj.weight = Parameter(torch.ones((E, E)))\n    qkv = qkv.to(float)\n    kpm = ones_tensor(S, L) * float('-inf')\n    am = ones_tensor(L, L).to(bool)\n\n    def func():\n        return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n    self.assertRaises(RuntimeError, func)"
        ]
    },
    {
        "func_name": "test_scaled_dot_product_attention",
        "original": "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)",
        "mutated": [
            "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n    if False:\n        i = 10\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)",
            "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)",
            "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)",
            "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)",
            "@parametrize('input_dim,attn_mask_dim,is_causal', [(3, None, False), (3, 2, False), (3, 2, True), (3, 3, False), (3, 3, True), (4, None, False), (4, 2, False), (4, 2, True), (4, 4, False), (4, 4, True)], name_fn=lambda input_dim, attn_dim, is_causal: f'{input_dim}D_input_dim_' + (f\"{attn_dim}D_{('causal_' if is_causal else '')}attn_mask\" if attn_dim is not None else 'no_attn_mask'))\n@parametrize('dropout_p', [0.0, 0.2, 0.5])\n@sdp_kernel(enable_flash=False, enable_mem_efficient=False)\ndef test_scaled_dot_product_attention(self, device, input_dim, attn_mask_dim, is_causal, dropout_p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def sdp_ref(q, k, v, attn_mask=None, dropout_p=0.0):\n        E = q.size(-1)\n        q = q / math.sqrt(E)\n        if attn_mask is not None:\n            attn = torch.baddbmm(attn_mask, q, k.transpose(-2, -1))\n        else:\n            attn = torch.bmm(q, k.transpose(-2, -1))\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        if dropout_p > 0.0:\n            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n        output = torch.bmm(attn, v)\n        return output\n    dtypes = [torch.double, torch.float]\n    for dtype in dtypes:\n\n        def rand_tensor(*shape):\n            return torch.randn(shape, device=device, dtype=dtype)\n        (N, N_prime, L, S, E) = (5, 2, 4, 3, 6)\n        if input_dim == 3:\n            query = rand_tensor(N, L, E)\n            key = rand_tensor(N, S, E)\n            value = rand_tensor(N, S, E)\n        elif input_dim == 4:\n            query = rand_tensor(N, N_prime, L, E)\n            key = rand_tensor(N, N_prime, S, E)\n            value = rand_tensor(N, N_prime, S, E)\n        else:\n            self.fail(f'Invalid input_dim {input_dim} encountered in SDP test')\n        attn_mask = None\n        if attn_mask_dim is not None:\n            assert attn_mask_dim in [2, input_dim]\n            mask_size = (L, S) if attn_mask_dim == 2 else (N, L, S) if input_dim == 3 else (N, N_prime, L, S)\n            attn_mask = torch.ones(mask_size, device=device, dtype=torch.bool).tril() if is_causal else torch.randint(0, 2, size=mask_size, device=device, dtype=torch.bool)\n        with freeze_rng_state():\n            attn_mask_float = attn_mask\n            if attn_mask_float is not None:\n                attn_mask_float = torch.zeros_like(attn_mask, dtype=query.dtype)\n                attn_mask_float.masked_fill_(attn_mask.logical_not(), float('-inf'))\n            (q, k, v) = (query.view(-1, L, E), key.view(-1, S, E), value.view(-1, S, E))\n            a = attn_mask_float\n            if a is not None and attn_mask_dim > 3:\n                a = a.view(-1, L, S)\n            expected = sdp_ref(q, k, v, attn_mask=a, dropout_p=dropout_p)\n            if input_dim > 3:\n                expected = expected.view(-1, N_prime, L, E)\n        with freeze_rng_state():\n            if is_causal:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, dropout_p, is_causal)\n                with self.assertRaisesRegex(RuntimeError, 'Explicit attn_mask should not be set when is_causal=True'):\n                    torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            else:\n                actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal)\n            self.assertEqual(actual, expected)\n    if attn_mask_dim is None:\n        q = q.double().clone()\n        k = k.double().clone()\n        v = v.double().clone()\n        q.requires_grad_()\n        k.requires_grad_()\n        v.requires_grad_()\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(sdp_ref, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (q, k, v, attn_mask, dropout_p))\n\n    def test_incompatible_mask(self, device):\n\n        def ones_tensor(*shape):\n            return torch.ones(shape, dtype=torch.float32)\n        (S, L, E, H) = (1, 2, 4, 1)\n        qkv = ones_tensor(S, L, E)\n        mha = nn.MultiheadAttention(E, H)\n        mha.in_proj_weight = Parameter(torch.ones((E * 3, E)))\n        mha.out_proj.weight = Parameter(torch.ones((E, E)))\n        qkv = qkv.to(float)\n        kpm = ones_tensor(S, L) * float('-inf')\n        am = ones_tensor(L, L).to(bool)\n\n        def func():\n            return mha(qkv, qkv, qkv, need_weights=False, key_padding_mask=kpm, attn_mask=am)\n        self.assertRaises(RuntimeError, func)"
        ]
    },
    {
        "func_name": "_test_fastpath",
        "original": "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)",
        "mutated": [
            "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    if False:\n        i = 10\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)",
            "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)",
            "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)",
            "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)",
            "def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n        fastpath_mock.return_value = mock_return_value\n        model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n        self.assertTrue(fastpath_mock.called)\n        for (call_args, _) in fastpath_mock.call_args_list:\n            self.assertEqual(call_args[0].is_nested, nested_tensors)"
        ]
    },
    {
        "func_name": "test_mask_check_fastpath",
        "original": "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    \"\"\"\n        Test that fastpath is executed independently of the masks that are passed.\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\n        \"\"\"\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)",
        "mutated": [
            "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    if False:\n        i = 10\n    '\\n        Test that fastpath is executed independently of the masks that are passed.\\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\\n        '\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)",
            "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Test that fastpath is executed independently of the masks that are passed.\\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\\n        '\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)",
            "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Test that fastpath is executed independently of the masks that are passed.\\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\\n        '\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)",
            "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Test that fastpath is executed independently of the masks that are passed.\\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\\n        '\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)",
            "@unittest.skipIf(TEST_WITH_CROSSREF, 'Fastpath not available with crossref')\n@torch.no_grad()\ndef test_mask_check_fastpath(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Test that fastpath is executed independently of the masks that are passed.\\n        If the passed key padding mask is left aligned or mask_check=False, test that nested tensors are used\\n        (sparsity fastpath), otherwise use fastpath with traditional tensors.\\n        Also test that fast path is executed with both key padding mask and attention mask passed at the same time.\\n        '\n    x = torch.Tensor([[[1, 2], [3, 4], [5, 6]]]).to(torch.float)\n\n    def _test_fastpath(model, key_padding_mask, mock_return_value, attn_mask=None, nested_tensors=True):\n        with patch('torch._transformer_encoder_layer_fwd') as fastpath_mock:\n            fastpath_mock.return_value = mock_return_value\n            model(x, src_key_padding_mask=key_padding_mask, mask=attn_mask)\n            self.assertTrue(fastpath_mock.called)\n            for (call_args, _) in fastpath_mock.call_args_list:\n                self.assertEqual(call_args[0].is_nested, nested_tensors)\n    encoder_layer = torch.nn.TransformerEncoderLayer(d_model=2, nhead=2, dim_feedforward=8, batch_first=True)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=True)\n    model.eval()\n    aligned_key_padding_mask = torch.Tensor([[0, 0, 1]]).to(torch.bool)\n    not_aligned_key_padding_mask = torch.Tensor([[1, 0, 1]]).to(torch.bool)\n    attn_mask = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]]).to(torch.bool)\n    nested_tensor_return_value = torch.nested.nested_tensor([torch.ones((2, 2), dtype=torch.float)])\n    tensor_return_value = torch.ones((1, 3, 2), dtype=torch.float)\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=False, mask_check=True)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, not_aligned_key_padding_mask, tensor_return_value, nested_tensors=False)\n    _test_fastpath(model, aligned_key_padding_mask, tensor_return_value, attn_mask=attn_mask, nested_tensors=False)\n    model = torch.nn.TransformerEncoder(encoder_layer, num_layers=2, enable_nested_tensor=True, mask_check=False)\n    model.eval()\n    _test_fastpath(model, aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)\n    _test_fastpath(model, not_aligned_key_padding_mask, nested_tensor_return_value, nested_tensors=True)"
        ]
    },
    {
        "func_name": "test_bias_is_none",
        "original": "def test_bias_is_none(self):\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)",
        "mutated": [
            "def test_bias_is_none(self):\n    if False:\n        i = 10\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)",
            "def test_bias_is_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)",
            "def test_bias_is_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)",
            "def test_bias_is_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)",
            "def test_bias_is_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = torch.rand((1, 5, 10))\n    model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)\n    model.eval()\n    model(x, x, x)"
        ]
    },
    {
        "func_name": "test_train_with_is_causal",
        "original": "def test_train_with_is_causal(self, device):\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)",
        "mutated": [
            "def test_train_with_is_causal(self, device):\n    if False:\n        i = 10\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)",
            "def test_train_with_is_causal(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)",
            "def test_train_with_is_causal(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)",
            "def test_train_with_is_causal(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)",
            "def test_train_with_is_causal(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (S, L, E, H) = (1, 2, 2, 1)\n    layer = nn.TransformerEncoderLayer(d_model=2, dim_feedforward=4, nhead=H, batch_first=True, activation='gelu', dropout=0)\n    criterion = nn.MSELoss()\n    encoder = nn.TransformerEncoder(layer, 2).to(device)\n    optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n    encoder.train()\n    encoder.train()\n    optimizer.zero_grad()\n    inputs = torch.randn(S, L, E).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(inputs.size(1), device=device)\n    outputs = encoder(inputs, mask=mask, is_causal=True)\n    loss = criterion(outputs[:, 0:2, :], inputs[:, 0:2, :])\n    loss.backward()\n    optimizer.step()\n    t_qvk = torch.randn((S, L, E), device=device, dtype=torch.float32)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(S, device=device)\n    (attn_out, _) = mha(t_qvk, t_qvk, t_qvk, attn_mask=mask, is_causal=True)\n    attn_mask = torch.randint(0, 2, size=(L, L), device=device, dtype=torch.bool)\n    with self.assertRaises(RuntimeError):\n        _ = mha(t_qvk, t_qvk, t_qvk, is_causal=True)\n    causal_mask = torch.triu(torch.ones(L, L, device=inputs.device) * float('-inf'), diagonal=1).to(torch.bool)\n    mock_layer = MagicMock(torch.nn.MultiheadAttention(E, H), return_value=inputs)\n    encoder.layers[1] = mock_layer\n    outputs = encoder(inputs, mask=causal_mask)\n    mock_layer.assert_called_with(ANY, src_mask=ANY, is_causal=True, src_key_padding_mask=ANY)\n    self.is_causal_kernels(['math'], device)"
        ]
    },
    {
        "func_name": "ones_tensor",
        "original": "def ones_tensor(*shape):\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)",
        "mutated": [
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)",
            "def ones_tensor(*shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return torch.ones(shape, device=device, dtype=torch.float32).to(device)"
        ]
    },
    {
        "func_name": "is_causal_kernels",
        "original": "def is_causal_kernels(self, kernels, device):\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()",
        "mutated": [
            "def is_causal_kernels(self, kernels, device):\n    if False:\n        i = 10\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()",
            "def is_causal_kernels(self, kernels, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()",
            "def is_causal_kernels(self, kernels, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()",
            "def is_causal_kernels(self, kernels, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()",
            "def is_causal_kernels(self, kernels, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def ones_tensor(*shape):\n        return torch.ones(shape, device=device, dtype=torch.float32).to(device)\n    (S, L, E, H) = (1, 2, 4, 1)\n    qkv = ones_tensor(S, L, E)\n    mha = nn.MultiheadAttention(E, H).to(device)\n    mha.in_proj_weight = Parameter(torch.ones((E * 3, E), device=device))\n    mha.out_proj.weight = Parameter(torch.ones((E, E), device=device))\n    expected = torch.ones(size=(S, L, E)).to(device) * 16\n    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv.size(1), device=device)\n    for kernel in kernels:\n        with torch.backends.cuda.sdp_kernel(enable_math=kernel == 'math', enable_flash=kernel == 'flash', enable_mem_efficient=kernel == 'meff'):\n            (actual, _) = mha(qkv, qkv, qkv, attn_mask=mask, need_weights=False, is_causal=True)\n            self.assertTrue(torch.equal(actual, expected))\n            if kernel != 'math':\n                with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                    (qkv_f, mha_f) = (ones_tensor(S, L, 2), nn.MultiheadAttention(2, H).to(device))\n                    mask = torch.nn.Transformer.generate_square_subsequent_mask(qkv_f.size(1), device=device)\n                    _ = mha_f(qkv_f, qkv_f, qkv_f, attn_mask=mask, need_weights=False, is_causal=True)\n                    torch.cuda.synchronize()"
        ]
    },
    {
        "func_name": "test_is_causal_gpu",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    if False:\n        i = 10\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not supposrt fused SDPA or pre-SM80 hardware')\ndef test_is_causal_gpu(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device = 'cuda'\n    self.is_causal_kernels(['math', 'meff'], device)"
        ]
    },
    {
        "func_name": "test_script_mha_in_proj_weight_none",
        "original": "def test_script_mha_in_proj_weight_none(self):\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)",
        "mutated": [
            "def test_script_mha_in_proj_weight_none(self):\n    if False:\n        i = 10\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)",
            "def test_script_mha_in_proj_weight_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)",
            "def test_script_mha_in_proj_weight_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)",
            "def test_script_mha_in_proj_weight_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)",
            "def test_script_mha_in_proj_weight_none(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=8, kdim=256, vdim=256).eval()\n    torch.jit.script(mha)"
        ]
    },
    {
        "func_name": "test_flash_backward_failure_sm86plus",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    if False:\n        i = 10\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION or not isSM86or89Device, 'Does not support fused SDPA or not SM86+ hardware')\n@parametrize('head_dim', [193, 204, 256])\ndef test_flash_backward_failure_sm86plus(self, device, head_dim: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype)\n    size = (2, 2, 4, head_dim)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=False, enable_math=True):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n    with sdp_kernel(enable_mem_efficient=False, enable_flash=True, enable_math=False):\n        flash_ref = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)\n        self.assertEqual(math_ref, flash_ref, atol=0.001, rtol=0.001)\n        q = make_tensor(size, requires_grad=True)\n        k = make_tensor(size, requires_grad=True)\n        v = make_tensor(size, requires_grad=True)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_dispatch_fails_no_backend",
        "original": "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))",
        "mutated": [
            "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    if False:\n        i = 10\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))",
            "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))",
            "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))",
            "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))",
            "@onlyCUDA\ndef test_dispatch_fails_no_backend(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float16\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=False):\n        size = (2, 3, 4)\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch._fused_sdp_choice(q, k, v))\n        self.assertRaisesRegex(RuntimeError, 'No viable backend for scaled_dot_product_attention was found.', lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v))"
        ]
    },
    {
        "func_name": "test_invalid_fused_inputs_dim_3",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_dim_3(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        size = (2, 3, 8)\n        dtype = torch.float16\n        q = torch.randn(size, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels requires query, key and value to be 4 dimensional'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_fused_inputs_broadcast",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_broadcast(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        size = (2, 4, 3, 8)\n        size_broadcast = (1, 4, 3, 8)\n        q = torch.randn(size_broadcast, device=device, dtype=dtype)\n        k = torch.randn(size, device=device, dtype=dtype)\n        v = torch.randn(size, device=device, dtype=dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_sequence_lengths",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_sequence_lengths(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 0, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support zero seq_len_q or seq_len_kv.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_last_dim_stride",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_last_dim_stride(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 8, 8)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        q.as_strided_(size, [2, 2, 2, 2])\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels require the last dimension of the input to have stride 1.'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_fused_inputs_head_dim",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not flash_attention fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_head_dim(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        dtype = torch.float16\n        make_tensor = partial(torch.rand, device=device, dtype=dtype)\n        size = SdpaShape(2, 2, 3, 9) if kernel == SDPBackend.EFFICIENT_ATTENTION else SdpaShape(2, 2, 3, 257)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_fused_inputs_invalid_dtype",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Does not support fused scaled dot product attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_fused_inputs_invalid_dtype(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, device=device, dtype=torch.float64)\n        (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_invalid_fused_inputs_attn_mask_present",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION])\ndef test_invalid_fused_inputs_attn_mask_present(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        size = SdpaShape(2, 2, 3, 16)\n        make_tensor = partial(torch.rand, size, device=device, dtype=torch.float16)\n        (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n        mask = torch.ones((2, 2, 3, 3), device=device, dtype=q.dtype)\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, mask, 0.0, False))"
        ]
    },
    {
        "func_name": "test_unaligned_tensors",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    if False:\n        i = 10\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_unaligned_tensors(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float16\n    size = SdpaShape(2, 2, 8, 5)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False):\n        self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_flash_fail_fp32",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    if False:\n        i = 10\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support fused SDPA or pre-SM80 hardware')\ndef test_flash_fail_fp32(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, BFloat16}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_flash_autocast_fp32_float16",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    if False:\n        i = 10\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_float16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)"
        ]
    },
    {
        "func_name": "test_flash_autocast_fp32_bfloat16",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    if False:\n        i = 10\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\ndef test_flash_autocast_fp32_bfloat16(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n            _ = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False)"
        ]
    },
    {
        "func_name": "test_invalid_inputs_different_datatypes",
        "original": "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
        "mutated": [
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_datatypes(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4, 8, 16)\n        query = torch.randn(shape, dtype=torch.float32, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))"
        ]
    },
    {
        "func_name": "test_invalid_inputs_different_devices",
        "original": "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
        "mutated": [
            "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@onlyCUDA\n@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_different_devices(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shape = (1, 4, 8, 16)\n    query = torch.randn(shape, dtype=torch.float32, device=device)\n    key = torch.randn(shape, dtype=torch.float16, device='cpu')\n    value = torch.randn(shape, dtype=torch.float16, device='cpu')\n    self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))"
        ]
    },
    {
        "func_name": "test_invalid_inputs_1_dimensional_inputs",
        "original": "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
        "mutated": [
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))",
            "@parametrize('kernel', [SDPBackend.MATH, SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_invalid_inputs_1_dimensional_inputs(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with sdp_kernel(**backend_map[kernel]):\n        shape = (1, 4)\n        query = torch.randn(4, dtype=torch.float16, device=device)\n        key = torch.randn(shape, dtype=torch.float16, device=device)\n        value = torch.randn(shape, dtype=torch.float16, device=device)\n        self.assertRaises(RuntimeError, lambda : F.scaled_dot_product_attention(query, key, value))"
        ]
    },
    {
        "func_name": "test_fused_kernels_nested_broadcasting_error_cases",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    if False:\n        i = 10\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_error_cases(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    seq_lens_q = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_v = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(batch, num_heads, seq_lens_q, head_dim)\n    k_shape = SdpaShape(1, num_heads, 1, head_dim)\n    v_shape = SdpaShape(batch, num_heads, seq_lens_v, head_dim)\n    query = rand_nested_tensor(q_shape).transpose(1, 2)\n    key = rand_nested_tensor(k_shape).transpose(1, 2)\n    value = rand_nested_tensor(v_shape).transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)"
        ]
    },
    {
        "func_name": "test_nested_fails_on_padding_head_dim",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    if False:\n        i = 10\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_nested_fails_on_padding_head_dim(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.bfloat16\n    seq_len_list = [2, 4, 5, 6, 7]\n    shape = SdpaShape(5, 8, seq_len_list, 57)\n    make_tensor = partial(rand_sdpa_tensor, shape=shape, type='nested', device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with torch.backends.cuda.sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n        with self.assertWarnsRegex(UserWarning, 'For NestedTensor inputs, Flash attention requires'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_mem_efficient_fail_bfloat16_sm50",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    if False:\n        i = 10\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION or not isSM5xDevice, 'Does not support fused SDPA or not SM50 hardware')\ndef test_mem_efficient_fail_bfloat16_sm50(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.bfloat16\n    size = SdpaShape(16, 16, 32, 32)\n    make_tensor = partial(torch.rand, size, device=device, dtype=dtype)\n    (q, k, v) = (make_tensor(), make_tensor(), make_tensor())\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Expected query, key and value to all be of dtype: {Half, Float}'):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_fused_kernels_seq_len_0_inputs",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_0_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_zeros = 10\n    indices = torch.randint(low=0, high=batch, size=(num_zeros,))\n    seq_lens.scatter_(0, indices, 0)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n            torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)"
        ]
    },
    {
        "func_name": "test_fused_kernels_nested_broadcasting_requires_grad_failure",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    if False:\n        i = 10\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_requires_grad_failure(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16, requires_grad=True)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = SdpaShape(1, num_heads, 1, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float16, requires_grad=True)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, 'Both fused kernels do not support training with broadcasted NT inputs'):\n            with self.assertRaisesRegex(RuntimeError, 'No available kernel'):\n                out = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)"
        ]
    },
    {
        "func_name": "test_flash_attention_fail_with_non_square_causal_attention",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    if False:\n        i = 10\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support flash attention')\ndef test_flash_attention_fail_with_non_square_causal_attention(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.bfloat16\n    q_shape = SdpaShape(1, 1, 8, 16)\n    kv_shape = SdpaShape(1, 1, 12, 16)\n    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)\n    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)\n    (q, k, v) = (make_q(), make_kv(), make_kv())\n    warning_str = 'Flash attention does not support the is_causal flag when seqlen_q != seqlen_k.'\n    with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n        with self.assertWarnsRegex(UserWarning, warning_str):\n            self.assertRaises(RuntimeError, lambda : torch.nn.functional.scaled_dot_product_attention(q, k, v, None, 0.0, is_causal=True))"
        ]
    },
    {
        "func_name": "_get_block_size",
        "original": "def _get_block_size(device, head_dim, is_causal):\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)",
        "mutated": [
            "def _get_block_size(device, head_dim, is_causal):\n    if False:\n        i = 10\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)",
            "def _get_block_size(device, head_dim, is_causal):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)",
            "def _get_block_size(device, head_dim, is_causal):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)",
            "def _get_block_size(device, head_dim, is_causal):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)",
            "def _get_block_size(device, head_dim, is_causal):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    is_dropout = True\n    assert head_dim <= 256\n    (major, minor) = torch.cuda.get_device_capability(device)\n    is_sm8x = major == 8 and minor > 0\n    is_sm80 = major == 8 and minor == 0\n    is_sm90 = major == 9 and minor == 0\n    if head_dim <= 32:\n        return (128, 128)\n    if head_dim <= 64:\n        return (128, 128) if not is_dropout else (128, 64)\n    elif head_dim <= 96:\n        return (64, 64) if is_sm8x and is_causal else (128, 64)\n    elif head_dim <= 128:\n        if is_sm8x:\n            return (64, 64) if not is_dropout and is_causal else (128, 32)\n        else:\n            return (128, 64 if not is_dropout else 32)\n    elif head_dim <= 160:\n        if is_sm8x:\n            return (128, 64) if not is_causal else (64, 64)\n        else:\n            return (128, 32)\n    elif head_dim <= 192:\n        return (128, 64) if not is_dropout else (64, 64)\n    elif head_dim <= 224:\n        return (128, 64) if is_sm80 or is_sm90 else (64, 64)\n    elif head_dim <= 256:\n        return (128, 64) if is_sm80 else (64, 64)"
        ]
    },
    {
        "func_name": "pad_last_dim",
        "original": "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)",
        "mutated": [
            "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    if False:\n        i = 10\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)",
            "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)",
            "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)",
            "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)",
            "def pad_last_dim(input_tensor, alignment_size, slice: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    last_dim_size = input_tensor.size(-1)\n    if last_dim_size % alignment_size == 0:\n        return (input_tensor, last_dim_size)\n    pad_count = alignment_size - last_dim_size % alignment_size\n    padded_tensor = F.pad(input_tensor, (0, pad_count))\n    if slice:\n        return (padded_tensor[..., :last_dim_size], last_dim_size)\n    return (padded_tensor, last_dim_size)"
        ]
    },
    {
        "func_name": "test_sdp_math_gradcheck",
        "original": "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))",
        "mutated": [
            "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))",
            "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))",
            "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))",
            "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))",
            "@parametrize('contiguous_inputs', [True, False])\ndef test_sdp_math_gradcheck(self, device, contiguous_inputs: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        assert gradcheck(lambda *args, **kwargs: wrapper_set_seed(torch.nn.functional.scaled_dot_product_attention, *args, **kwargs), (query, key, value, None, 0.0, False))"
        ]
    },
    {
        "func_name": "test_fused_sdp_choice_cpu",
        "original": "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value",
        "mutated": [
            "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    if False:\n        i = 10\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value",
            "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value",
            "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value",
            "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value",
            "@onlyCPU\n@parametrize('type', ['dense', 'nested'])\n@parametrize('dropout', [0.0, 0.7])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16, torch.half])\ndef test_fused_sdp_choice_cpu(self, device, type: str, dropout: float, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=dtype)\n    size = SdpaShape(2, 8, 128, 64)\n    (q, k, v) = (make_tensor(size), make_tensor(size), make_tensor(size))\n    if type == 'nested' or dropout > 0.0 or dtype not in [torch.float32, torch.float64, torch.bfloat16]:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.MATH.value\n    else:\n        assert torch._fused_sdp_choice(q, k, v, dropout_p=dropout) == SDPBackend.FLASH_ATTENTION.value"
        ]
    },
    {
        "func_name": "test_scaled_dot_product_fused_attention_vs_math_cpu",
        "original": "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)",
        "mutated": [
            "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    if False:\n        i = 10\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)",
            "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)",
            "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)",
            "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)",
            "@onlyCPU\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION])\n@parametrize('dtype', [torch.float64, torch.float32, torch.bfloat16])\n@parametrize('batch_size', [2, 12])\n@parametrize('seq_len', [267, 1030])\n@parametrize('n_head', [1, 3])\n@parametrize('head_dim', [8, 16])\n@parametrize('causal', [True, False])\n@parametrize('train', [True, False])\ndef test_scaled_dot_product_fused_attention_vs_math_cpu(self, device, fused_kernel, dtype, batch_size, seq_len, n_head, head_dim, causal, train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    atol = 1e-05\n    rtol = 5e-06\n    if dtype is torch.bfloat16:\n        atol = 0.02\n        rtol = 0.02\n    n_embd = n_head * head_dim\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=dtype, packed=True, requires_grad=False)\n    shape = SdpaShape(batch_size, n_head, seq_len, head_dim)\n    x = make_tensor(shape)\n    x2 = x.clone()\n    if train:\n        x.requires_grad_(True)\n        x2.requires_grad_(True)\n    (q, k, v) = x.split(n_embd, dim=2)\n    (q2, k2, v2) = x2.split(n_embd, dim=2)\n    if dtype is torch.bfloat16:\n        q2 = q2.float()\n        k2 = k2.float()\n        v2 = v2.float()\n    k = k.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q = q.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v = v.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    k2 = k2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    q2 = q2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    v2 = v2.view(batch_size, seq_len, n_head, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(q2, k2, v2, attn_mask=None, dropout_p=0.0, is_causal=causal)\n    if dtype is torch.bfloat16:\n        math_ref = math_ref.bfloat16()\n    self.assertEqual(actual, math_ref, atol=atol, rtol=rtol)\n    if train:\n        actual.sum().backward()\n        math_ref.sum().backward()\n        (grad_x, grad_x2) = (x.grad, x2.grad)\n        (grad_q_actual, grad_k_actual, grad_v_actual) = grad_x.split(n_embd, dim=2)\n        (grad_q_ref, grad_k_ref, grad_v_ref) = grad_x2.split(n_embd, dim=2)\n        self.assertEqual(grad_q_actual, grad_q_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_k_actual, grad_k_ref, atol=atol, rtol=rtol)\n        self.assertEqual(grad_v_actual, grad_v_ref, atol=atol, rtol=rtol)"
        ]
    },
    {
        "func_name": "ref",
        "original": "def ref(x):\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4",
        "mutated": [
            "def ref(x):\n    if False:\n        i = 10\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4",
            "def ref(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4",
            "def ref(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4",
            "def ref(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4",
            "def ref(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    v1 = torch.matmul(x, x.transpose(-1, -2))\n    v2 = v1 / -0.0001\n    v3 = v2.softmax(dim=-1)\n    v4 = torch.matmul(v3, x)\n    return v4"
        ]
    },
    {
        "func_name": "test_scaled_dot_product_attention_math_with_negative_scale",
        "original": "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)",
        "mutated": [
            "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)",
            "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)",
            "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)",
            "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)",
            "@parametrize('kernel', [SDPBackend.MATH])\ndef test_scaled_dot_product_attention_math_with_negative_scale(self, device, kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def ref(x):\n        v1 = torch.matmul(x, x.transpose(-1, -2))\n        v2 = v1 / -0.0001\n        v3 = v2.softmax(dim=-1)\n        v4 = torch.matmul(v3, x)\n        return v4\n    x = torch.randn(1, 3, 64, 64, device=device)\n    ref_result = ref(x)\n    with sdp_kernel(**backend_map[kernel]):\n        sdp_math = torch.nn.functional.scaled_dot_product_attention(x, x, x, scale=-1.0 / 0.0001)\n    self.assertEqual(ref_result, sdp_math)"
        ]
    },
    {
        "func_name": "convert_flash_attn_S_to_softmax",
        "original": "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    \"\"\"FlashAttention stores the S matrix in a different way.\n        Arguments:\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\n            query_padding_mask: (batch_size, seqlen_q)\n            key_padding_mask: (batch_size, seqlen_k)\n        \"\"\"\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted",
        "mutated": [
            "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    if False:\n        i = 10\n    'FlashAttention stores the S matrix in a different way.\\n        Arguments:\\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\\n            query_padding_mask: (batch_size, seqlen_q)\\n            key_padding_mask: (batch_size, seqlen_k)\\n        '\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted",
            "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'FlashAttention stores the S matrix in a different way.\\n        Arguments:\\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\\n            query_padding_mask: (batch_size, seqlen_q)\\n            key_padding_mask: (batch_size, seqlen_k)\\n        '\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted",
            "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'FlashAttention stores the S matrix in a different way.\\n        Arguments:\\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\\n            query_padding_mask: (batch_size, seqlen_q)\\n            key_padding_mask: (batch_size, seqlen_k)\\n        '\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted",
            "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'FlashAttention stores the S matrix in a different way.\\n        Arguments:\\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\\n            query_padding_mask: (batch_size, seqlen_q)\\n            key_padding_mask: (batch_size, seqlen_k)\\n        '\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted",
            "def convert_flash_attn_S_to_softmax(self, S, query_padding_mask, key_padding_mask, head_dim, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'FlashAttention stores the S matrix in a different way.\\n        Arguments:\\n            S: (batch_size, nheads, seqlen_q, seqlen_k)\\n            query_padding_mask: (batch_size, seqlen_q)\\n            key_padding_mask: (batch_size, seqlen_k)\\n        '\n    (b, h, seqlen_q, seqlen_k) = S.shape\n    warps_n = 4\n    (blocksize_m, blocksize_n) = _get_block_size(S.device, head_dim, causal)\n    nblocks_m = (seqlen_q + blocksize_m - 1) // blocksize_m\n    nblocks_n = (seqlen_k + blocksize_n - 1) // blocksize_n\n    mmas_n = (blocksize_n + 16 - 1) // 16\n    S_flat = S.view(b, h, nblocks_m, blocksize_m, nblocks_n, blocksize_n)\n    S_flat = S_flat.permute(0, 1, 2, 4, 3, 5)\n    S_flat = S_flat.reshape(b, h, nblocks_m, nblocks_n, blocksize_m * blocksize_n)\n    S_converted = S_flat.view(b, h, nblocks_m, nblocks_n, mmas_n, -1, warps_n, 8, 4, 2, 2, 2)\n    S_converted = S_converted.permute(0, 1, 2, 5, 6, 10, 7, 3, 4, 9, 8, 11)\n    S_converted = S_converted.reshape(b, h, nblocks_m * S_converted.size(3) * warps_n * 2 * 8, nblocks_n * mmas_n * 2 * 4 * 2)\n    if causal:\n        causal_mask = torch.triu(torch.ones(seqlen_q, seqlen_k, dtype=torch.bool, device=S.device), 1)\n        S_converted.masked_fill_(causal_mask, 0.0)\n    seqlen_q_og = query_padding_mask.shape[-1] if query_padding_mask is not None else seqlen_q\n    if query_padding_mask is not None:\n        if seqlen_q_og < seqlen_q:\n            query_padding_mask = F.pad(query_padding_mask, (0, seqlen_q - seqlen_q_og))\n        else:\n            query_padding_mask = query_padding_mask[:, :seqlen_q]\n        q_mask_fill = ~query_padding_mask.view(query_padding_mask.shape[0], 1, query_padding_mask.shape[1], 1)\n        S_converted = S_converted.masked_fill(q_mask_fill, 0.0)\n    seqlen_k_og = key_padding_mask.shape[-1] if key_padding_mask is not None else seqlen_k\n    if key_padding_mask is not None:\n        if seqlen_k_og < seqlen_k:\n            key_padding_mask = F.pad(key_padding_mask, (0, seqlen_k - seqlen_k_og))\n        else:\n            key_padding_mask = key_padding_mask[:, :seqlen_k]\n        k_mask_fill = ~key_padding_mask.view(key_padding_mask.shape[0], 1, 1, key_padding_mask.shape[1])\n        S_converted = S_converted.masked_fill(k_mask_fill, 0.0)\n    if seqlen_q_og < seqlen_q:\n        S_converted = S_converted[:, :, :seqlen_q_og, :]\n    else:\n        S_converted = F.pad(S_converted, (0, 0, 0, seqlen_q_og - seqlen_q))\n    if seqlen_k_og < seqlen_k:\n        S_converted = S_converted[:, :, :, :seqlen_k_og]\n    else:\n        S_converted = F.pad(S_converted, (0, seqlen_k_og - seqlen_k))\n    return S_converted"
        ]
    },
    {
        "func_name": "query_key_value_clones",
        "original": "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    \"\"\" Clones the query, key, and value tensors and moves them to the specified dtype. \"\"\"\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)",
        "mutated": [
            "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    if False:\n        i = 10\n    ' Clones the query, key, and value tensors and moves them to the specified dtype. '\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)",
            "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Clones the query, key, and value tensors and moves them to the specified dtype. '\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)",
            "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Clones the query, key, and value tensors and moves them to the specified dtype. '\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)",
            "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Clones the query, key, and value tensors and moves them to the specified dtype. '\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)",
            "def query_key_value_clones(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Clones the query, key, and value tensors and moves them to the specified dtype. '\n    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)\n    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)\n    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)\n    return (query_ref, key_ref, value_ref)"
        ]
    },
    {
        "func_name": "test_mem_efficient_attetntion_mask_variants",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    if False:\n        i = 10\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('mask_dim', [1, 2, 3, 4])\ndef test_mem_efficient_attetntion_mask_variants(self, device, mask_dim: List[int]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float16\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 32)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    if mask_dim == 1:\n        mask = torch.randn((seq_len_kv,), device=device, dtype=dtype)\n    elif mask_dim == 2:\n        mask = torch.randn((seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 3:\n        mask = torch.randn((num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    elif mask_dim == 4:\n        mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()"
        ]
    },
    {
        "func_name": "test_mem_eff_attention_pad_mask",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    if False:\n        i = 10\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_pad_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 15)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()"
        ]
    },
    {
        "func_name": "test_mem_eff_attention_non_contiguous_mask",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    if False:\n        i = 10\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_non_contiguous_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (8, 8, 64)\n    (seq_len_q, seq_len_kv) = (64, 16)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    mask = torch.as_strided(mask, (batch, num_heads, seq_len_q, seq_len_kv), (0, 0, 0, 1))\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()"
        ]
    },
    {
        "func_name": "test_mem_eff_attention_long_sequence_mask",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if False:\n        i = 10\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('dtype', [torch.float, torch.float16])\ndef test_mem_eff_attention_long_sequence_mask(self, device, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if torch.cuda.get_device_properties('cuda').total_memory < 80 * 2 ** 30:\n        unittest.skip('This test requires substatnial GPU memory.')\n        return\n    make_tensor = partial(torch.rand, device=device, dtype=dtype, requires_grad=True)\n    (batch, num_heads, head_dim) = (1, 32, 64)\n    (seq_len_q, seq_len_kv) = (8192, 8192)\n    query = make_tensor(SdpaShape(batch, num_heads, seq_len_q, head_dim))\n    kv_shape = SdpaShape(batch, num_heads, seq_len_kv, head_dim)\n    (key, value) = (make_tensor(kv_shape), make_tensor(kv_shape))\n    mask = torch.randn((batch, num_heads, seq_len_q, seq_len_kv), device=device, dtype=dtype)\n    with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n        out = F.scaled_dot_product_attention(query, key, value, mask)\n    out.sum().backward()"
        ]
    },
    {
        "func_name": "test_scaled_dot_product_attention_fused_kernels_packed",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    if False:\n        i = 10\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('is_contiguous', [True, False])\ndef test_scaled_dot_product_attention_fused_kernels_packed(self, device, type: str, is_contiguous: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    make_tensor = partial(rand_sdpa_tensor, type=type, device=device, dtype=torch.float16, packed=True)\n    (batch_size, seq_len, num_heads, head_dim) = (32, 64, 16, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    qkv = make_tensor(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if is_contiguous:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous(), key.contiguous(), value.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.002, rtol=0.01)"
        ]
    },
    {
        "func_name": "rand_nt",
        "original": "def rand_nt(shape):\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))",
        "mutated": [
            "def rand_nt(shape):\n    if False:\n        i = 10\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))",
            "def rand_nt(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))",
            "def rand_nt(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))",
            "def rand_nt(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))",
            "def rand_nt(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n    return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))"
        ]
    },
    {
        "func_name": "rand_tensor",
        "original": "def rand_tensor(shape):\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))",
        "mutated": [
            "def rand_tensor(shape):\n    if False:\n        i = 10\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))",
            "def rand_tensor(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))",
            "def rand_tensor(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))",
            "def rand_tensor(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))",
            "def rand_tensor(shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch, seq_len, num_heads, head_dim) = shape\n    tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n    return (tensor, tensor.to(dtype=torch.float16))"
        ]
    },
    {
        "func_name": "test_scaled_dot_product_attention_fused_kernels_packed_accuracy",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n    if False:\n        i = 10\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('type', ['dense', 'nested'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_scaled_dot_product_attention_fused_kernels_packed_accuracy(self, device, type: str, fused_kernel: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def rand_nt(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensors = [6 * torch.rand((seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3 for _ in range(batch)]\n        return (torch.nested.nested_tensor(tensors, device=device, dtype=torch.float32), torch.nested.nested_tensor(tensors, device=device, dtype=torch.float16))\n\n    def rand_tensor(shape):\n        (batch, seq_len, num_heads, head_dim) = shape\n        tensor = 6 * torch.rand((batch, seq_len, 3 * num_heads * head_dim), device=device, dtype=torch.float32) - 3\n        return (tensor, tensor.to(dtype=torch.float16))\n    (batch_size, seq_len, num_heads, head_dim) = (16, 8, 4, 64)\n    shape = (batch_size, seq_len, num_heads, head_dim)\n    (qkv, qkv_low_precision) = rand_tensor(shape) if type == 'dense' else rand_nt(shape)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_low_precision.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(**backend_map[SDPBackend.MATH]):\n        math_ref_lp = torch.nn.functional.scaled_dot_product_attention(query_lp.contiguous(), key_lp.contiguous(), value_lp.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n        math_query = query.contiguous()\n        math_key = key.contiguous()\n        math_value = value.contiguous()\n        math_ref = torch.nn.functional.scaled_dot_product_attention(math_query, math_key, math_value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    actual_test = actual\n    math_ref_test = math_ref\n    math_ref_lp_test = math_ref_lp\n    if actual_test.is_nested:\n        actual_test = torch.nested.to_padded_tensor(actual_test.contiguous(), padding=0.0)\n        math_ref_test = torch.nested.to_padded_tensor(math_ref_test, padding=0.0)\n        math_ref_lp_test = torch.nested.to_padded_tensor(math_ref_lp_test, padding=0.0)\n    actual_test = actual_test.to(dtype=torch.float32).contiguous()\n    math_ref_test = math_ref_test.to(dtype=torch.float32).contiguous()\n    math_ref_lp_test = math_ref_lp_test.to(dtype=torch.float32).contiguous()\n    self.assertEqual(math_ref_test, math_ref_lp_test, atol=0.007, rtol=0.007)\n    self.assertEqual(actual_test, math_ref_test, atol=0.005, rtol=0.005)"
        ]
    },
    {
        "func_name": "test_sdp_mem_efficient_grad_against_math",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\ndef test_sdp_mem_efficient_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(torch.float32).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=True, enable_flash=False):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(torch.float32)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=1e-05, rtol=1e-05)"
        ]
    },
    {
        "func_name": "test_sdp_flash_attention_grad_against_math",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Flash Attention was not built for this system')\n@parametrize('contiguous_inputs', [True, False])\n@parametrize('is_causal', [True, False])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\ndef test_sdp_flash_attention_grad_against_math(self, device, contiguous_inputs: bool, is_causal: bool, dtype: torch.dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (4, 4, 2, 16)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float64, requires_grad=True, packed=True)\n    qkv = make_tensor(SdpaShape(batch_size, num_heads, seq_len, head_dim))\n    qkv_lp = qkv.detach().clone().to(dtype).requires_grad_()\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    (query_lp, key_lp, value_lp) = qkv_lp.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    query_lp = query_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key_lp = key_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value_lp = value_lp.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if contiguous_inputs:\n        query = query.contiguous()\n        key = key.contiguous()\n        value = value.contiguous()\n        query_lp = query_lp.contiguous()\n        key_lp = key_lp.contiguous()\n        value_lp = value_lp.contiguous()\n    with sdp_kernel(enable_math=True, enable_mem_efficient=False, enable_flash=False):\n        out = torch.nn.functional.scaled_dot_product_attention(query, key, value, None, 0.0, is_causal)\n    with sdp_kernel(enable_math=False, enable_mem_efficient=False, enable_flash=True):\n        out_lp = torch.nn.functional.scaled_dot_product_attention(query_lp, key_lp, value_lp, None, 0.0, is_causal)\n    rand_upward = torch.rand_like(out)\n    rand_upward_lp = rand_upward.to(dtype)\n    out.backward(rand_upward)\n    out_lp.backward(rand_upward_lp)\n    atol = 0.0007 if dtype == torch.float16 else 0.007\n    rtol = 0.0007 if dtype == torch.float16 else 0.007\n    self.assertEqual(qkv.grad, qkv_lp.grad.to(torch.float64), atol=atol, rtol=rtol)"
        ]
    },
    {
        "func_name": "test_fused_sdp_choice",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('type', ['dense', 'nested'])\ndef test_fused_sdp_choice(self, device, type: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (2, 128, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float16, packed=True, requires_grad=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    if PLATFORM_SUPPORTS_FLASH_ATTENTION:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.FLASH_ATTENTION.value\n    else:\n        assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value\n    make_tensor = partial(rand_sdpa_tensor, device=device, dtype=torch.float32, packed=True)\n    qkv = make_tensor(shape, type=type)\n    (query, key, value) = qkv.chunk(3, dim=-1)\n    query = query.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, num_heads, head_dim).transpose(1, 2)\n    assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value"
        ]
    },
    {
        "func_name": "test_sdp_choice_with_determinism",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_sdp_choice_with_determinism(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\n            assert torch._fused_sdp_choice(query, key, value) == SDPBackend.EFFICIENT_ATTENTION.value"
        ]
    },
    {
        "func_name": "test_mem_eff_backwards_throws_determinism_warning",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    if False:\n        i = 10\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Platform does not support fused SDPA')\n@parametrize('warn_only', [True, False])\ndef test_mem_eff_backwards_throws_determinism_warning(self, device, warn_only):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (batch_size, seq_len, num_heads, head_dim) = (1, 64, 8, 64)\n    shape = SdpaShape(batch_size, num_heads, seq_len, head_dim)\n    make_tensor = partial(rand_sdpa_tensor, type='dense', device=device, dtype=torch.float32, packed=False, requires_grad=True)\n    (query, key, value) = (make_tensor(shape), make_tensor(shape), make_tensor(shape))\n    warning_context = self.assertWarnsRegex(UserWarning, 'Memory Efficient attention defaults to a non-deterministic algorithm.') if warn_only else contextlib.nullcontext()\n    with use_deterministic_algorithims(True, warn_only=warn_only):\n        with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n            with warning_context:\n                torch.nn.functional.scaled_dot_product_attention(query, key, value).sum().backward()"
        ]
    },
    {
        "func_name": "test_mem_eff_backwards_determinism",
        "original": "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)",
        "mutated": [
            "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    if False:\n        i = 10\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)",
            "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)",
            "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)",
            "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)",
            "@unittest.skip('This test is not behaving deterministaclly non-deterministaclly on CI/CD')\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Platform does not support fused SDPA')\ndef test_mem_eff_backwards_determinism(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = torch.float32\n    (batch_size, seq_len, n_heads, head_dim) = (1, 1024, 8, 64)\n    query = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len, head_dim, device=device, dtype=dtype, requires_grad=True)\n    with sdp_kernel(enable_mem_efficient=True, enable_math=False, enable_flash=False):\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertTrue(diff_anwser_once)\n    with use_deterministic_algorithims(True, warn_only=False):\n        query.grad = None\n        out = F.scaled_dot_product_attention(query, key, value)\n        upward_grad = torch.rand_like(out)\n        out.backward(upward_grad)\n        intial_query_grad = query.grad\n        diff_anwser_once = False\n        for _ in range(100):\n            query.grad = None\n            out = F.scaled_dot_product_attention(query, key, value)\n            out.backward(upward_grad)\n            if not torch.equal(intial_query_grad, query.grad):\n                diff_anwser_once = True\n                break\n        self.assertFalse(diff_anwser_once)"
        ]
    },
    {
        "func_name": "_get_mem_eff_drop_mask",
        "original": "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
        "mutated": [
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask"
        ]
    },
    {
        "func_name": "test_mem_efficient_attention_vs_math_ref_grads",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False, True])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 2.0\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)"
        ]
    },
    {
        "func_name": "_get_mem_eff_drop_mask",
        "original": "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
        "mutated": [
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n    mask = (rand_uniform > p).to(torch.float32)\n    return mask"
        ]
    },
    {
        "func_name": "test_mem_efficient_attention_attn_mask_vs_math_ref_grads",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Does not support SDPA')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 128, 256, 312, 512, 1024, 2048] if SM80OrLater else [4, 8, 64, 128, 152, 256, 512])\n@parametrize('seq_len_k', [4, 8, 64, 65, 128, 256, 408, 512, 1024, 2048] if SM80OrLater else [4, 8, 37, 64, 128, 256, 512])\n@parametrize('head_dim', [8, 16, 32, 64, 72, 96, 128] if SM80OrLater else [8, 16, 32, 64])\n@parametrize('is_causal', [False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16, torch.bfloat16, torch.float32] if SM80OrLater else [torch.float16, torch.float32])\n@parametrize('scale', [None, 'l1'])\ndef test_mem_efficient_attention_attn_mask_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, p, seed, offset)\n        mask = (rand_uniform > p).to(torch.float32)\n        return mask\n    if max(seq_len_q, seq_len_k) >= 2048 and torch.cuda.get_device_properties('cuda').total_memory < 40 * 2 ** 30:\n        unittest.skip('Reference implementation OOM')\n        return\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    attn_mask = torch.rand(seq_len_q, seq_len_k, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    attn_mask_ref_lp = attn_mask.detach().to(dtype).requires_grad_(True)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    attn_mask_ref = attn_mask.detach().to(higher_precision_dtype).requires_grad_(True)\n    with sdp_kernel(enable_mem_efficient=True, enable_flash=False, enable_math=False):\n        torch.manual_seed(seed)\n        out = F.scaled_dot_product_attention(query, key, value, attn_mask, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    if dropout_p == 0.0:\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n    else:\n        if seq_len_q > 1024:\n            self.skipTest('Will call _fill_mem_eff_dropout_mask with too many threads!')\n        torch.manual_seed(seed)\n        dropout_mask = _get_mem_eff_drop_mask(batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, seed, 0, device=device)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, attn_mask_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, attn_mask_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    mask_fudge_factor = 1.0 if attn_mask is None else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor * mask_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    mask_fudge_factor = 12 if attn_mask.numel() > 512 else 22\n    (grad_attn_mask_atol, grad_attn_mask_rtol) = get_tolerances(attn_mask_ref.grad, attn_mask_ref_lp.grad, mask_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)\n    self.assertEqual(attn_mask.grad, attn_mask_ref.grad.to(attn_mask.grad.dtype), atol=grad_attn_mask_atol, rtol=grad_attn_mask_rtol)"
        ]
    },
    {
        "func_name": "test_flash_attention_vs_math_ref_grads",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [4, 8, 64, 143, 256, 512, 1024, 2048])\n@parametrize('seq_len_k', [4, 8, 64, 128, 256, 587, 1024, 2048])\n@parametrize('head_dim', [8, 16, 21, 32, 64, 72, 96, 128, 160, 192, 203, 256])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22, 0.48])\n@parametrize('dtype', [torch.float16, torch.bfloat16])\n@parametrize('scale', [None, 'l1'])\ndef test_flash_attention_vs_math_ref_grads(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isSM86or89Device and head_dim in range(193, 256 + 1):\n        self.skipTest('Flash attention on sm86 and sm89 for headdim > 192 currently disabled')\n    if is_causal and seq_len_q != seq_len_k:\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        (q_padded, q_og_size) = pad_last_dim(query, 8)\n        (k_padded, k_og_size) = pad_last_dim(key, 8)\n        (v_padded, v_og_size) = pad_last_dim(value, 8)\n        if scale is None:\n            scale = 1 / math.sqrt(q_og_size)\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(q_padded, k_padded, v_padded, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        out = out[..., :v_og_size]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)[:, :, :seq_len_q, :seq_len_k]\n        dropout_mask = softmax_mask >= 0\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    if isSM86or89Device and head_dim in range(193, 256):\n        self.assertRaises(RuntimeError, lambda : out.backward(upstream_grad))\n        return\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    output_fudge_factor = 3 if head_dim % 8 != 0 else 1\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref, output_fudge_factor)\n    query_fudge_factor = 4\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad)\n    value_fudge_factor = 2\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)"
        ]
    },
    {
        "func_name": "_get_mem_eff_drop_mask",
        "original": "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask",
        "mutated": [
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    if False:\n        i = 10\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask",
            "def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n    rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n    mask = (rand_uniform > dropout_p).to(torch.float32)\n    return mask"
        ]
    },
    {
        "func_name": "get_dropout_mask",
        "original": "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask",
        "mutated": [
            "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if False:\n        i = 10\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask",
            "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask",
            "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask",
            "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask",
            "def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n        output_seed = output_seed.item()\n        output_offset = output_offset.item()\n        return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n    else:\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n        key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        return dropout_mask"
        ]
    },
    {
        "func_name": "test_fused_attention_vs_math_ref_grads_cudagraph",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n    if False:\n        i = 10\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [1, 8])\n@parametrize('seq_len_q', [256, 512, 1024])\n@parametrize('seq_len_k', [256, 512, 1024])\n@parametrize('head_dim', [32, 64])\n@parametrize('is_causal', [True, False])\n@parametrize('dropout_p', [0.0, 0.22])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_attention_vs_math_ref_grads_cudagraph(self, device, batch_size: int, seq_len_q: int, seq_len_k: int, head_dim: int, is_causal: bool, dropout_p: float, dtype: torch.dtype, scale: str, fused_kernel: SDPBackend):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, seed, offset, device=device):\n        mask = torch.empty((batch_size, n_heads, q_len, kv_len), device=device, dtype=torch.float32)\n        rand_uniform = torch._fill_mem_eff_dropout_mask_(mask, dropout_p, seed, offset)\n        mask = (rand_uniform > dropout_p).to(torch.float32)\n        return mask\n\n    def get_dropout_mask(output, fused_kernel, batch_size, n_heads, q_len, kv_len, dropout_p, device=device):\n        if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n            (output_seed, output_offset) = (output_tuple[2], output_tuple[3])\n            output_seed = output_seed.item()\n            output_offset = output_offset.item()\n            return _get_mem_eff_drop_mask(batch_size, n_heads, q_len, kv_len, dropout_p, output_seed, output_offset, device=device)\n        else:\n            dbug_mask = output_tuple[-1]\n            query_padding_mask = torch.ones(batch_size, seq_len_q, device=device, dtype=torch.bool)\n            key_padding_mask = torch.ones(batch_size, seq_len_k, device=device, dtype=torch.bool)\n            softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n            dropout_mask = softmax_mask >= 0\n            return dropout_mask\n    if fused_kernel == SDPBackend.FLASH_ATTENTION and is_causal and (seq_len_q != seq_len_k):\n        self.skipTest('Flash V2 does not accept is_casual when seq_len_q != seq_len_k')\n    seed = 42\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    query = torch.rand(batch_size, n_heads, seq_len_q, head_dim, device=device, dtype=dtype, requires_grad=True)\n    key = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    value = torch.rand(batch_size, n_heads, seq_len_k, head_dim, device=device, dtype=dtype, requires_grad=True)\n    fused_op = torch.ops.aten._scaled_dot_product_efficient_attention if fused_kernel == SDPBackend.EFFICIENT_ATTENTION else torch.ops.aten._scaled_dot_product_flash_attention\n    (query_ref_lp, key_ref_lp, value_ref_lp) = self.query_key_value_clones(query, key, value, dtype=dtype)\n    higher_precision_dtype = torch.float64 if dtype == torch.float32 else torch.float32\n    (query_ref, key_ref, value_ref) = self.query_key_value_clones(query, key, value, dtype=higher_precision_dtype)\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    torch.manual_seed(seed)\n    kwargs = {'dropout_p': dropout_p, 'is_causal': is_causal, 'scale': scale}\n    if fused_kernel == SDPBackend.EFFICIENT_ATTENTION:\n        kwargs['compute_log_sumexp'] = True\n        kwargs['attn_bias'] = None\n    if fused_kernel == SDPBackend.FLASH_ATTENTION:\n        kwargs['return_debug_mask'] = dropout_p > 0.0\n    with torch.cuda.stream(s):\n        output_tuple = fused_op(query, key, value, **kwargs)\n    torch.cuda.current_stream().wait_stream(s)\n    out = output_tuple[0]\n    upstream_grad = torch.rand_like(out, requires_grad=False)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        out.backward(upstream_grad)\n    for x in (query, key, value):\n        x.grad = None\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        tmp = torch.rand_like(query, device=query.device)\n        output_tuple = fused_op(query, key, value, **kwargs)\n        assert all((not isinstance(o, torch.Tensor) or o.is_cuda for o in output_tuple))\n    g.replay()\n    out_first = output_tuple[0].clone()\n    g.replay()\n    out = output_tuple[0]\n    if dropout_p == 0.0:\n        self.assertEqual(out_first, out, atol=0, rtol=0)\n    else:\n        self.assertNotEqual(out_first, out)\n    with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n        if dropout_p == 0.0:\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        else:\n            dropout_mask = get_dropout_mask(output_tuple, fused_kernel, batch_size, n_heads, seq_len_q, seq_len_k, dropout_p, device)\n            out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n            out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=dropout_mask)[0]\n    g1 = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g1):\n        out.backward(upstream_grad)\n    g1.replay()\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = get_tolerances(out_ref, out_lp_ref)\n    dropout_fudge_factor = 1.0 if dropout_p == 0.0 else 1.5\n    query_fudge_factor = dropout_fudge_factor\n    (grad_q_ref_atol, grad_q_ref_rtol) = get_tolerances(query_ref.grad, query_ref_lp.grad, query_fudge_factor)\n    key_fudge_factor = 8 * dropout_fudge_factor\n    (grad_k_ref_atol, grad_k_ref_rtol) = get_tolerances(key_ref.grad, key_ref_lp.grad, key_fudge_factor)\n    value_fudge_factor = 7 if not SM80OrLater and dtype == torch.float16 else 1.0\n    (grad_v_ref_atol, grad_v_ref_rtol) = get_tolerances(value_ref.grad, value_ref_lp.grad, value_fudge_factor)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad, key_ref.grad.to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)"
        ]
    },
    {
        "func_name": "test_fused_kernels_seq_len_1_inputs",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('fused_kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\ndef test_fused_kernels_seq_len_1_inputs(self, device, fused_kernel):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float16)\n    (batch, num_heads, head_dim) = (32, 16, 64)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,))\n    num_ones = 10\n    indices = torch.randint(low=0, high=batch, size=(num_ones,))\n    seq_lens.scatter_(0, indices, 1)\n    shape = SdpaShape(batch, num_heads, seq_lens.tolist(), head_dim)\n    query = rand_nested_tensor(shape)\n    key = rand_nested_tensor(shape)\n    value = rand_nested_tensor(shape)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[fused_kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query.contiguous().to(torch.float32), key.contiguous().to(torch.float32), value.contiguous().to(torch.float32), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(torch.float16), atol=0.001, rtol=0.01)"
        ]
    },
    {
        "func_name": "_broadcast",
        "original": "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result",
        "mutated": [
            "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if False:\n        i = 10\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result",
            "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result",
            "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result",
            "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result",
            "def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if batch_broadcasted and num_heads_broadcasted:\n        result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n    elif batch_broadcasted:\n        result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n    elif num_heads_broadcasted:\n        result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n    else:\n        result = t.to(torch.float32)\n    return result"
        ]
    },
    {
        "func_name": "test_fused_kernels_nested_broadcasting",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    if False:\n        i = 10\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_FUSED_ATTENTION, 'Fused SDPA was not built for this system')\n@parametrize('kernel', [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION] if PLATFORM_SUPPORTS_FLASH_ATTENTION else [SDPBackend.EFFICIENT_ATTENTION])\n@parametrize('expand_q_batch', [True, False])\n@parametrize('expand_k_batch', [True, False])\n@parametrize('expand_v_batch', [True, False])\n@parametrize('expand_q_num_heads', [True, False])\n@parametrize('expand_k_num_heads', [True, False])\n@parametrize('expand_v_num_heads', [True, False])\ndef test_fused_kernels_nested_broadcasting(self, device, kernel, expand_q_batch, expand_k_batch, expand_v_batch, expand_q_num_heads, expand_k_num_heads, expand_v_num_heads):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    is_efficient = kernel == SDPBackend.EFFICIENT_ATTENTION\n    dtype = torch.float32 if is_efficient else torch.float16\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=dtype)\n    (batch, num_heads, head_dim) = (32, 8, 64)\n    head_dim_v = 32 if is_efficient else head_dim\n    seq_lens_q = torch.randint(low=1, high=5, size=(1,)).item() if expand_q_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    seq_lens_kv = torch.randint(low=1, high=5, size=(1,)).item() if expand_k_batch or expand_v_batch else torch.randint(low=1, high=32, size=(batch,)).tolist()\n    batch_q = 1 if expand_q_batch else batch\n    batch_k = 1 if expand_k_batch else batch\n    batch_v = 1 if expand_v_batch else batch\n    batch = max(batch_q, batch_k, batch_v)\n    num_heads_q = 1 if expand_q_num_heads else num_heads\n    num_heads_k = 1 if expand_k_num_heads else num_heads\n    num_heads_v = 1 if expand_v_num_heads else num_heads\n    num_heads = max(num_heads_q, num_heads_k, num_heads_v)\n    q_shape = SdpaShape(batch_q, num_heads_q, seq_lens_q, head_dim)\n    k_shape = SdpaShape(batch_k, num_heads_k, seq_lens_kv, head_dim)\n    v_shape = SdpaShape(batch_v, num_heads_v, seq_lens_kv, head_dim_v)\n    query = rand_nested_tensor(q_shape)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n\n    def _broadcast(t, batch_broadcasted, num_heads_broadcasted):\n        if batch_broadcasted and num_heads_broadcasted:\n            result = torch.nested.nested_tensor([t[0].expand(-1, num_heads, t.size(-1)) for _ in range(batch)], dtype=torch.float32)\n        elif batch_broadcasted:\n            result = torch.nested.nested_tensor([t[0] for _ in range(batch)], dtype=torch.float32)\n        elif num_heads_broadcasted:\n            result = torch.nested.nested_tensor([x.expand(-1, num_heads, t.size(-1)) for x in t.unbind()], dtype=torch.float32)\n        else:\n            result = t.to(torch.float32)\n        return result\n    query_expanded = _broadcast(query, expand_q_batch, expand_q_num_heads).transpose(1, 2)\n    key_expanded = _broadcast(key, expand_k_batch, expand_k_num_heads).transpose(1, 2)\n    value_expanded = _broadcast(value, expand_v_batch, expand_v_num_heads).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(**backend_map[kernel]):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key_expanded.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous().to(dtype), atol=0.001, rtol=0.01)"
        ]
    },
    {
        "func_name": "test_fused_kernels_nested_broadcasting_query_dense",
        "original": "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)",
        "mutated": [
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    if False:\n        i = 10\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)",
            "@unittest.skipIf(not PLATFORM_SUPPORTS_MEM_EFF_ATTENTION, 'Fused SDPA was not built for this system')\ndef test_fused_kernels_nested_broadcasting_query_dense(self, device):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rand_nested_tensor = partial(rand_sdpa_tensor, type='nested', device=device, dtype=torch.float32)\n    (batch, num_heads, head_dim, head_dim_v) = (32, 16, 64, 96)\n    seq_lens = torch.randint(low=1, high=32, size=(batch,)).tolist()\n    q_shape = (1, 1, num_heads, head_dim)\n    k_shape = SdpaShape(batch, num_heads, seq_lens, head_dim)\n    v_shape = SdpaShape(batch, 1, seq_lens, head_dim_v)\n    query = torch.randn(q_shape, device=device, dtype=torch.float32)\n    key = rand_nested_tensor(k_shape)\n    value = rand_nested_tensor(v_shape)\n    query_expanded = torch.nested.nested_tensor([query.squeeze(0) for _ in range(batch)]).transpose(1, 2)\n    value_expanded = torch.nested.nested_tensor([t.expand(-1, num_heads, head_dim_v) for t in value.unbind()]).transpose(1, 2)\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n    with sdp_kernel(enable_flash=False, enable_math=False, enable_mem_efficient=True):\n        actual = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False)\n    with sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n        math_ref = torch.nn.functional.scaled_dot_product_attention(query_expanded.contiguous(), key.contiguous(), value_expanded.contiguous(), attn_mask=None, dropout_p=0.0, is_causal=False)\n    self.assertEqual(actual.contiguous(), math_ref.contiguous(), atol=0.001, rtol=0.01)"
        ]
    },
    {
        "func_name": "rand_nt",
        "original": "def rand_nt(sequence_list, num_heads, head_dim):\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)",
        "mutated": [
            "def rand_nt(sequence_list, num_heads, head_dim):\n    if False:\n        i = 10\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)",
            "def rand_nt(sequence_list, num_heads, head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)",
            "def rand_nt(sequence_list, num_heads, head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)",
            "def rand_nt(sequence_list, num_heads, head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)",
            "def rand_nt(sequence_list, num_heads, head_dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n    return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)"
        ]
    },
    {
        "func_name": "test_flash_attention_vs_math_ref_grads_nestedtensor",
        "original": "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
        "mutated": [
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if False:\n        i = 10\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)",
            "@onlyCUDA\n@unittest.skipIf(not PLATFORM_SUPPORTS_FLASH_ATTENTION, 'Does not support SDPA or pre-SM80 hardware')\n@parametrize('batch_size', [8, 32])\n@parametrize('max_seq_len_q', [32, 256])\n@parametrize('max_seq_len_kv', [32, 256])\n@parametrize('head_dim', [8, 64])\n@parametrize('dropout_p', [0.0, 0.1])\n@parametrize('dtype', [torch.float16])\n@parametrize('scale', [None, 'l1'])\n@parametrize('is_causal', [True, False])\ndef test_flash_attention_vs_math_ref_grads_nestedtensor(self, device, batch_size: int, max_seq_len_q: int, max_seq_len_kv: int, head_dim: int, dropout_p: float, dtype: torch.dtype, scale: str, is_causal: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if is_causal:\n        self.assertRaisesRegex(RuntimeError, 'Nested tensors for query / key are not supported when is_causal=True')\n        return\n    scale = scale if scale is None else 1 / head_dim\n    n_heads = 4\n    seq_lens_q = torch.randint(low=1, high=max_seq_len_q, size=(batch_size,))\n    seq_lens_q[torch.randint(0, batch_size, size=(1,))] = max_seq_len_q\n    seq_lens_kv = torch.randint(low=1, high=max_seq_len_kv, size=(batch_size,))\n    seq_lens_kv[torch.randint(0, batch_size, size=(1,))] = max_seq_len_kv\n\n    def rand_nt(sequence_list, num_heads, head_dim):\n        tensors = [torch.rand((num_heads, seq_len, head_dim)) for seq_len in sequence_list]\n        return torch.nested.nested_tensor(tensors, requires_grad=True, device=device, dtype=dtype)\n    query = rand_nt(seq_lens_q, n_heads, head_dim)\n    key = rand_nt(seq_lens_kv, n_heads, head_dim)\n    value = rand_nt(seq_lens_kv, n_heads, head_dim)\n    query_ref_lp = query.clone().detach().requires_grad_(True)\n    key_ref_lp = key.clone().detach().requires_grad_(True)\n    value_ref_lp = value.clone().detach().requires_grad_(True)\n    query_ref = query.clone().detach().to(torch.float32).requires_grad_(True)\n    key_ref = key.clone().detach().to(torch.float32).requires_grad_(True)\n    value_ref = value.clone().detach().to(torch.float32).requires_grad_(True)\n    is_dropout = dropout_p > 0.0\n    if not is_dropout:\n        with sdp_kernel(enable_math=False, enable_flash=True, enable_mem_efficient=False):\n            out = F.scaled_dot_product_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale)\n        with sdp_kernel(enable_math=True, enable_flash=False, enable_mem_efficient=False):\n            out_ref = F.scaled_dot_product_attention(query_ref, key_ref, value_ref, is_causal=is_causal, scale=scale)\n            out_lp_ref = F.scaled_dot_product_attention(query_ref_lp, key_ref_lp, value_ref_lp, is_causal=is_causal, scale=scale)\n    else:\n        output_tuple = torch.ops.aten._scaled_dot_product_flash_attention(query, key, value, dropout_p=dropout_p, is_causal=is_causal, scale=scale, return_debug_mask=is_dropout)\n        out = output_tuple[0]\n        dbug_mask = output_tuple[-1]\n        query_padding_mask = torch.arange(max_seq_len_q).unsqueeze(0).expand(batch_size, max_seq_len_q) < seq_lens_q.unsqueeze(-1)\n        query_padding_mask = query_padding_mask.to('cuda')\n        key_padding_mask = torch.arange(max_seq_len_kv).unsqueeze(0).expand(batch_size, max_seq_len_kv) < seq_lens_kv.unsqueeze(-1)\n        key_padding_mask = key_padding_mask.to('cuda')\n        softmax_mask = self.convert_flash_attn_S_to_softmax(dbug_mask, query_padding_mask, key_padding_mask, head_dim=head_dim, causal=is_causal)\n        dropout_mask = softmax_mask >= 0\n        nt_stack = []\n        for tensor_component in range(batch_size):\n            batch_stack = []\n            for head in range(n_heads):\n                batch_stack.append(dropout_mask[tensor_component, head, 0:seq_lens_q[tensor_component], 0:seq_lens_kv[tensor_component]].unsqueeze(0))\n            nt_stack.append(torch.cat(batch_stack))\n        nested_dropout_mask = torch.nested.nested_tensor(nt_stack)\n        out_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref, key_ref, value_ref, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n        out_lp_ref = torch.ops.aten._scaled_dot_product_attention_math(query_ref_lp, key_ref_lp, value_ref_lp, dropout_p=dropout_p, is_causal=is_causal, scale=scale, dropout_mask=nested_dropout_mask)[0]\n    upstream_grad = out.detach().clone().contiguous()\n    out.backward(upstream_grad)\n    out_ref.backward(upstream_grad.to(out_ref.dtype))\n    out_lp_ref.backward(upstream_grad.to(out_lp_ref.dtype))\n    (output_ref_atol, output_ref_rtol) = calculate_nt_tolerances(out_ref, out_lp_ref, out.dtype)\n    (grad_q_ref_atol, grad_q_ref_rtol) = calculate_nt_tolerances(query_ref.grad, query_ref_lp.grad, query.grad.dtype, fudge_factor=4)\n    (grad_k_ref_atol, grad_k_ref_rtol) = calculate_nt_tolerances(key_ref.grad, key_ref_lp.grad, key.grad.dtype)\n    (grad_v_ref_atol, grad_v_ref_rtol) = calculate_nt_tolerances(value_ref.grad, value_ref_lp.grad, value.grad.dtype)\n    self.assertEqual(out, out_ref.to(out.dtype), atol=output_ref_atol, rtol=output_ref_rtol)\n    self.assertEqual(query.grad, query_ref.grad.to(query.grad.dtype), atol=grad_q_ref_atol, rtol=grad_q_ref_rtol)\n    self.assertEqual(key.grad.contiguous(), key_ref.grad.contiguous().to(key.grad.dtype), atol=grad_k_ref_atol, rtol=grad_k_ref_rtol)\n    self.assertEqual(value.grad, value_ref.grad.to(value.grad.dtype), atol=grad_v_ref_atol, rtol=grad_v_ref_rtol)"
        ]
    }
]