[
    {
        "func_name": "saveid",
        "original": "@property\ndef saveid(self):\n    return 'valuerl'",
        "mutated": [
            "@property\ndef saveid(self):\n    if False:\n        i = 10\n    return 'valuerl'",
            "@property\ndef saveid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 'valuerl'",
            "@property\ndef saveid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 'valuerl'",
            "@property\ndef saveid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 'valuerl'",
            "@property\ndef saveid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 'valuerl'"
        ]
    },
    {
        "func_name": "create_params",
        "original": "def create_params(self, env_config, learner_config):\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]",
        "mutated": [
            "def create_params(self, env_config, learner_config):\n    if False:\n        i = 10\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]",
            "def create_params(self, env_config, learner_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]",
            "def create_params(self, env_config, learner_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]",
            "def create_params(self, env_config, learner_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]",
            "def create_params(self, env_config, learner_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.obs_dim = np.prod(env_config['obs_dims'])\n    self.action_dim = env_config['action_dim']\n    self.reward_scale = env_config['reward_scale']\n    self.discount = env_config['discount']\n    self.hidden_dim = learner_config['hidden_dim']\n    self.bayesian_config = learner_config['bayesian']\n    self.value_expansion = learner_config['value_expansion']\n    self.explore_chance = learner_config['ddpg_explore_chance']\n    with tf.variable_scope(self.name):\n        self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)\n        if self.bayesian_config:\n            self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n            self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config['ensemble_size'], train_sample_count=self.bayesian_config['train_sample_count'], eval_sample_count=self.bayesian_config['eval_sample_count'])\n        else:\n            self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n            self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)\n    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'policy' in v.name]\n    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if 'Q' in v.name]\n    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n    self.copy_to_old_ops = [tf.assign(p_old, p) for (p_old, p) in zip(self.old_Q.params_list, self.Q.params_list)]\n    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]"
        ]
    },
    {
        "func_name": "update_epoch",
        "original": "def update_epoch(self, sess, epoch, updates, frames, hours):\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})",
        "mutated": [
            "def update_epoch(self, sess, epoch, updates, frames, hours):\n    if False:\n        i = 10\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})",
            "def update_epoch(self, sess, epoch, updates, frames, hours):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})",
            "def update_epoch(self, sess, epoch, updates, frames, hours):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})",
            "def update_epoch(self, sess, epoch, updates, frames, hours):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})",
            "def update_epoch(self, sess, epoch, updates, frames, hours):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})"
        ]
    },
    {
        "func_name": "copy_to_old",
        "original": "def copy_to_old(self, sess):\n    sess.run(self.copy_to_old_ops)",
        "mutated": [
            "def copy_to_old(self, sess):\n    if False:\n        i = 10\n    sess.run(self.copy_to_old_ops)",
            "def copy_to_old(self, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sess.run(self.copy_to_old_ops)",
            "def copy_to_old(self, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sess.run(self.copy_to_old_ops)",
            "def copy_to_old(self, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sess.run(self.copy_to_old_ops)",
            "def copy_to_old(self, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sess.run(self.copy_to_old_ops)"
        ]
    },
    {
        "func_name": "build_evalution_graph",
        "original": "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions",
        "mutated": [
            "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    if False:\n        i = 10\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions",
            "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions",
            "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions",
            "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions",
            "def build_evalution_graph(self, obs, get_full_info=False, mode='regular', n_samples=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert mode in {'regular', 'explore', 'exploit'}\n    policy_actions_pretanh = self.policy(obs)\n    if mode == 'regular' or mode == 'exploit':\n        policy_actions = tf.tanh(policy_actions_pretanh)\n    elif mode == 'explore':\n        (_, _, exploring_policy_actions, _) = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)\n        policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))\n    else:\n        raise Exception('this should never happen')\n    if get_full_info:\n        return (policy_actions_pretanh, policy_actions)\n    else:\n        return policy_actions"
        ]
    },
    {
        "func_name": "build_training_graph",
        "original": "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)",
        "mutated": [
            "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    if False:\n        i = 10\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)",
            "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)",
            "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)",
            "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)",
            "def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    average_model_use = tf.constant(0.0)\n    empirical_Q_info = tf.concat([obs, empirical_actions], 1)\n    if worldmodel is None:\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, reduce_mode='mean')\n        next_policy_actions = self.build_evalution_graph(next_obs)\n        policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)\n        next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode='mean')\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1.0 - dones)\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n    else:\n        (targets, confidence, Q_guesses, reach_probs) = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion['rollout_len'], model_ensembling=worldmodel.bayesian_config is not False)\n        if self.value_expansion['mean_k_return']:\n            target_counts = self.value_expansion['rollout_len'] + 1 - tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, self.value_expansion['rollout_len'] + 1])\n            k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)\n        elif self.value_expansion['lambda_return']:\n            cont_coeffs = self.value_expansion['lambda_return'] ** tf.cast(tf.reshape(tf.range(self.value_expansion['rollout_len'] + 1), [1, 1, self.value_expansion['rollout_len'] + 1]), tf.float32)\n            stop_coeffs = tf.concat([(1 - self.value_expansion['lambda_return']) * tf.ones_like(targets)[:, :, :-1], tf.ones_like(targets)[:, :, -1:]], 2)\n            k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)\n        elif self.value_expansion['steve_reweight']:\n            k_returns = tf.reduce_sum(targets * confidence, 2)\n            average_model_use = 1.0 - tf.reduce_mean(confidence[:, 0, 0])\n        else:\n            k_returns = targets[:, :, -1]\n        Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode='random')\n        if self.value_expansion['tdk_trick']:\n            Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)\n            reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:, 0]), 1), reach_probs[:, :-1]], 1)\n            Q_target = k_returns\n        else:\n            Q_target = k_returns[:, 0]\n        (policy_action_pretanh, policy_actions) = self.build_evalution_graph(obs, get_full_info=True)\n        policy_Q_info = tf.concat([obs, policy_actions], 1)\n        state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode='mean')\n        policy_losses = -state_value_estimate\n        Q_losses = 0.5 * tf.square(Q_guess - tf.stop_gradient(Q_target))\n        if self.value_expansion['tdk_trick']:\n            Q_losses *= reach_probs\n    policy_loss = tf.reduce_mean(policy_losses)\n    Q_loss = tf.reduce_mean(Q_losses)\n    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * 0.001\n    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)\n    return ((policy_loss + policy_reg_loss, Q_loss), inspect)"
        ]
    },
    {
        "func_name": "rollout_loop_body",
        "original": "def rollout_loop_body(r_i, xxx_todo_changeme):\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))",
        "mutated": [
            "def rollout_loop_body(r_i, xxx_todo_changeme):\n    if False:\n        i = 10\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))",
            "def rollout_loop_body(r_i, xxx_todo_changeme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))",
            "def rollout_loop_body(r_i, xxx_todo_changeme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))",
            "def rollout_loop_body(r_i, xxx_todo_changeme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))",
            "def rollout_loop_body(r_i, xxx_todo_changeme):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n    (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n    if model_ensembling:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n    else:\n        (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n        next_obs = tf.reduce_mean(next_obs, -2)\n        next_dones = tf.reduce_mean(next_dones, -1)\n    action_ta = action_ta.write(r_i, action)\n    obs_ta = obs_ta.write(r_i, obs)\n    dones_ta = dones_ta.write(r_i, done)\n    extra_info_ta = extra_info_ta.write(r_i, extra_info)\n    return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))"
        ]
    },
    {
        "func_name": "build_Q_expansion_graph",
        "original": "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)",
        "mutated": [
            "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    if False:\n        i = 10\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)",
            "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)",
            "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)",
            "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)",
            "def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (ensemble_idxs, transition_sample_n, reward_sample_n) = worldmodel.get_ensemble_idx_info()\n    q_sample_n = self.bayesian_config['eval_sample_count'] if self.bayesian_config is not False else 1\n    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards, 1), 1), [1, transition_sample_n, reward_sample_n])\n    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])\n    if model_ensembling:\n        obs = tf.tile(tf.expand_dims(obs, 1), [1, transition_sample_n, 1])\n        obs.set_shape([None, transition_sample_n, self.obs_dim])\n        first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])\n        first_done.set_shape([None, transition_sample_n])\n    extra_info = worldmodel.init_extra_info(obs)\n    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    obs_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    done_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n    extra_info_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)\n\n    def rollout_loop_body(r_i, xxx_todo_changeme):\n        (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme\n        (action_pretanh, action) = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)\n        if model_ensembling:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)\n        else:\n            (next_obs, next_dones, next_extra_info) = worldmodel.transition(obs, action, extra_info)\n            next_obs = tf.reduce_mean(next_obs, -2)\n            next_dones = tf.reduce_mean(next_dones, -1)\n        action_ta = action_ta.write(r_i, action)\n        obs_ta = obs_ta.write(r_i, obs)\n        dones_ta = dones_ta.write(r_i, done)\n        extra_info_ta = extra_info_ta.write(r_i, extra_info)\n        return (r_i + 1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta))\n    (_, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta)) = tf.while_loop(lambda r_i, _: r_i < rollout_len, rollout_loop_body, [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)])\n    (final_action_pretanh, final_action) = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)\n    obss = obs_ta.stack()\n    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))\n    obss = tf.transpose(obss, [1, 0, 2, 3])\n    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))\n    all_obss = tf.concat([obss, final_obs], 1)\n    next_obss = all_obss[:, 1:]\n    dones = done_ta.stack()\n    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))\n    dones = tf.transpose(dones, [1, 0, 2])\n    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))\n    all_dones = tf.concat([dones, final_done], 1)\n    actions = action_ta.stack()\n    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))\n    actions = tf.transpose(actions, [1, 0, 2, 3])\n    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))\n    all_actions = tf.concat([actions, final_action], 1)\n    continue_probs = tf.cumprod(1.0 - all_dones, axis=1)\n    rewards = worldmodel.get_rewards(obss, actions, next_obss)\n    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards], 1)\n    if self.value_expansion['tdk_trick']:\n        guess_info = tf.concat([obss, actions], -1)\n        Q_guesses = self.Q(guess_info, reduce_mode='random')\n        Q_guesses = tf.reduce_mean(Q_guesses, -1)\n        reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)\n    else:\n        Q_guesses = None\n        reached_this_point_to_guess_prob = None\n    target_info = tf.concat([all_obss, all_actions], -1)\n    Q_targets = self.old_Q(target_info, reduce_mode='none')\n    rollout_frames = rollout_len + 1\n    ts_count_mat = tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32)\n    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat\n    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1.0 + ts_count_mat)\n    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])\n    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:, 0]), 1), continue_probs[:, :-1]], 1)\n    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs + 1e-08, 2)\n    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)\n    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)\n    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix\n    rewards = tf.cumsum(rewards, axis=2)\n    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix\n    sampled_targets = tf.expand_dims(rewards, -2) * self.reward_scale + tf.expand_dims(values, -1)\n    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))\n    (target_means, target_variances) = tf.nn.moments(sampled_targets, 3)\n    if self.value_expansion['covariances']:\n        targetdiffs = sampled_targets - tf.expand_dims(target_means, 3)\n        target_covariances = tf.einsum('abij,abjk->abik', targetdiffs, tf.transpose(targetdiffs, [0, 1, 3, 2]))\n        target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]), 0, 0) * 0.001, 0), 0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])], 0))), -1)\n    else:\n        target_confidence = 1.0 / (target_variances + 1e-08)\n    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)\n    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)\n    return (target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob)"
        ]
    }
]