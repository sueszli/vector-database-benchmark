[
    {
        "func_name": "__init__",
        "original": "def __init__(self) -> None:\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})",
        "mutated": [
            "def __init__(self) -> None:\n    if False:\n        i = 10\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.transform_target_param = False\n    self._variable_keys = self._variable_keys.union({'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'target_param', 'fold_shuffle_param', 'fold_generator', 'fold_groups_param'})"
        ]
    },
    {
        "func_name": "_calculate_metrics",
        "original": "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    \"\"\"\n        Calculate all metrics in _all_metrics.\n        \"\"\"\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)",
        "mutated": [
            "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    if False:\n        i = 10\n    '\\n        Calculate all metrics in _all_metrics.\\n        '\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)",
            "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Calculate all metrics in _all_metrics.\\n        '\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)",
            "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Calculate all metrics in _all_metrics.\\n        '\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)",
            "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Calculate all metrics in _all_metrics.\\n        '\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)",
            "def _calculate_metrics(self, y_test, pred, pred_prob, weights: Optional[list]=None, **additional_kwargs) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Calculate all metrics in _all_metrics.\\n        '\n    from pycaret.utils.generic import calculate_metrics\n    with redirect_output(self.logger):\n        try:\n            return calculate_metrics(metrics=self._all_metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)\n        except Exception:\n            ml_usecase = get_ml_task(y_test)\n            if ml_usecase == MLUsecase.CLASSIFICATION:\n                metrics = get_all_class_metric_containers(self.variables, True)\n            elif ml_usecase == MLUsecase.REGRESSION:\n                metrics = get_all_reg_metric_containers(self.variables, True)\n            return calculate_metrics(metrics=metrics, y_test=y_test, pred=pred, pred_proba=pred_prob, weights=weights, **additional_kwargs)"
        ]
    },
    {
        "func_name": "_is_unsupervised",
        "original": "def _is_unsupervised(self) -> bool:\n    return False",
        "mutated": [
            "def _is_unsupervised(self) -> bool:\n    if False:\n        i = 10\n    return False",
            "def _is_unsupervised(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return False",
            "def _is_unsupervised(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return False",
            "def _is_unsupervised(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return False",
            "def _is_unsupervised(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return False"
        ]
    },
    {
        "func_name": "_get_final_model_from_pipeline",
        "original": "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    \"\"\"Extracts and returns the final model from the pipeline.\n\n        Parameters\n        ----------\n        pipeline : Pipeline\n            The pipeline with a final model\n\n        check_is_fitted : bool, default=False\n            If True, will check if final model is fitted and raise an exception\n            if it is not, by default False.\n\n        Returns\n        -------\n        Model\n            The final model in the pipeline.\n\n        \"\"\"\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model",
        "mutated": [
            "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    if False:\n        i = 10\n    'Extracts and returns the final model from the pipeline.\\n\\n        Parameters\\n        ----------\\n        pipeline : Pipeline\\n            The pipeline with a final model\\n\\n        check_is_fitted : bool, default=False\\n            If True, will check if final model is fitted and raise an exception\\n            if it is not, by default False.\\n\\n        Returns\\n        -------\\n        Model\\n            The final model in the pipeline.\\n\\n        '\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model",
            "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Extracts and returns the final model from the pipeline.\\n\\n        Parameters\\n        ----------\\n        pipeline : Pipeline\\n            The pipeline with a final model\\n\\n        check_is_fitted : bool, default=False\\n            If True, will check if final model is fitted and raise an exception\\n            if it is not, by default False.\\n\\n        Returns\\n        -------\\n        Model\\n            The final model in the pipeline.\\n\\n        '\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model",
            "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Extracts and returns the final model from the pipeline.\\n\\n        Parameters\\n        ----------\\n        pipeline : Pipeline\\n            The pipeline with a final model\\n\\n        check_is_fitted : bool, default=False\\n            If True, will check if final model is fitted and raise an exception\\n            if it is not, by default False.\\n\\n        Returns\\n        -------\\n        Model\\n            The final model in the pipeline.\\n\\n        '\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model",
            "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Extracts and returns the final model from the pipeline.\\n\\n        Parameters\\n        ----------\\n        pipeline : Pipeline\\n            The pipeline with a final model\\n\\n        check_is_fitted : bool, default=False\\n            If True, will check if final model is fitted and raise an exception\\n            if it is not, by default False.\\n\\n        Returns\\n        -------\\n        Model\\n            The final model in the pipeline.\\n\\n        '\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model",
            "def _get_final_model_from_pipeline(self, pipeline: Pipeline, check_is_fitted: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Extracts and returns the final model from the pipeline.\\n\\n        Parameters\\n        ----------\\n        pipeline : Pipeline\\n            The pipeline with a final model\\n\\n        check_is_fitted : bool, default=False\\n            If True, will check if final model is fitted and raise an exception\\n            if it is not, by default False.\\n\\n        Returns\\n        -------\\n        Model\\n            The final model in the pipeline.\\n\\n        '\n    model = pipeline._final_estimator\n    if check_is_fitted:\n        check_fitted(model)\n    return model"
        ]
    },
    {
        "func_name": "_choose_better",
        "original": "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    \"\"\"\n        When choose_better is set to True, optimize metric in scoregrid is\n        compared with base model created using create_model so that the\n        functions return the model with better score only. This will ensure\n        model performance is at least equivalent to what is seen in compare_models\n        \"\"\"\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model",
        "mutated": [
            "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    if False:\n        i = 10\n    '\\n        When choose_better is set to True, optimize metric in scoregrid is\\n        compared with base model created using create_model so that the\\n        functions return the model with better score only. This will ensure\\n        model performance is at least equivalent to what is seen in compare_models\\n        '\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model",
            "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        When choose_better is set to True, optimize metric in scoregrid is\\n        compared with base model created using create_model so that the\\n        functions return the model with better score only. This will ensure\\n        model performance is at least equivalent to what is seen in compare_models\\n        '\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model",
            "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        When choose_better is set to True, optimize metric in scoregrid is\\n        compared with base model created using create_model so that the\\n        functions return the model with better score only. This will ensure\\n        model performance is at least equivalent to what is seen in compare_models\\n        '\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model",
            "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        When choose_better is set to True, optimize metric in scoregrid is\\n        compared with base model created using create_model so that the\\n        functions return the model with better score only. This will ensure\\n        model performance is at least equivalent to what is seen in compare_models\\n        '\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model",
            "def _choose_better(self, models_and_results: list, compare_dimension: str, fold: int, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, display: Optional[CommonDisplay]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        When choose_better is set to True, optimize metric in scoregrid is\\n        compared with base model created using create_model so that the\\n        functions return the model with better score only. This will ensure\\n        model performance is at least equivalent to what is seen in compare_models\\n        '\n    self.logger.info('choose_better activated')\n    if display is not None:\n        display.update_monitor(1, 'Compiling Final Results')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for (i, x) in enumerate(models_and_results):\n        if not isinstance(x, tuple):\n            models_and_results[i] = (x, None)\n        elif isinstance(x[0], str):\n            models_and_results[i] = (x[1], None)\n        elif len(x) != 2:\n            raise ValueError(f'{x} must have length 2 but has {len(x)}')\n    metric = self._get_metric_by_name_or_id(compare_dimension)\n    best_result = None\n    best_model = None\n    for (model, result) in models_and_results:\n        if result is not None and is_fitted(model):\n            try:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=True)\n                result = result.loc[indices][compare_dimension]\n            except KeyError:\n                indices = self._get_return_train_score_indices_for_logging(return_train_score=False)\n                result = result.loc[indices][compare_dimension]\n        else:\n            self.logger.info('SubProcess create_model() called ==================================')\n            (model, _) = self._create_model(model, verbose=False, system=False, fold=fold, fit_kwargs=fit_kwargs, groups=groups)\n            self.logger.info('SubProcess create_model() end ==================================')\n            result = self.pull(pop=True).loc[self._get_return_train_score_indices_for_logging(return_train_score=False)][compare_dimension]\n        self.logger.info(f'{model} result for {compare_dimension} is {result}')\n        if not metric.greater_is_better:\n            result *= -1\n        if best_result is None or best_result < result:\n            best_result = result\n            best_model = model\n    self.logger.info(f'{best_model} is best model')\n    self.logger.info('choose_better completed')\n    return best_model"
        ]
    },
    {
        "func_name": "_get_cv_n_folds",
        "original": "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)",
        "mutated": [
            "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    if False:\n        i = 10\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)",
            "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)",
            "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)",
            "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)",
            "def _get_cv_n_folds(self, fold, X, y=None, groups=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    import pycaret.utils.generic\n    return pycaret.utils.generic.get_cv_n_folds(fold, default=self.fold_generator, X=X, y=y, groups=groups)"
        ]
    },
    {
        "func_name": "_set_up_logging",
        "original": "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)",
        "mutated": [
            "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if False:\n        i = 10\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)",
            "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)",
            "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)",
            "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)",
            "def _set_up_logging(self, runtime, log_data, log_profile, experiment_custom_tags=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if experiment_custom_tags is not None:\n        if not isinstance(experiment_custom_tags, dict):\n            raise TypeError('experiment_custom_tags parameter must be dict if not None')\n    if self.logging_param:\n        self.logging_param.log_experiment(self, log_profile, log_data, experiment_custom_tags, runtime)"
        ]
    },
    {
        "func_name": "_parallel_compare_models",
        "original": "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)",
        "mutated": [
            "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    if False:\n        i = 10\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)",
            "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)",
            "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)",
            "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)",
            "def _parallel_compare_models(self, parallel: Optional[ParallelBackend], caller_params: Optional[dict], turbo: bool) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    params = dict(caller_params)\n    parallel.attach(self)\n    if params.get('include', None) is None:\n        _models = self.models()\n        if turbo:\n            _models = _models[_models.Turbo]\n        params['include'] = _models.index.tolist()\n    del params['self']\n    del params['__class__']\n    del params['parallel']\n    return parallel.compare_models(self, params)"
        ]
    },
    {
        "func_name": "_get_greater_is_worse_columns",
        "original": "def _get_greater_is_worse_columns(self) -> Set[str]:\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns",
        "mutated": [
            "def _get_greater_is_worse_columns(self) -> Set[str]:\n    if False:\n        i = 10\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns",
            "def _get_greater_is_worse_columns(self) -> Set[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns",
            "def _get_greater_is_worse_columns(self) -> Set[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns",
            "def _get_greater_is_worse_columns(self) -> Set[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns",
            "def _get_greater_is_worse_columns(self) -> Set[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    greater_is_worse_columns = {id_or_display_name(v, input_ml_usecase, target_ml_usecase).upper() for (k, v) in self._all_metrics.items() if not v.greater_is_better}\n    greater_is_worse_columns.add('TT (Sec)')\n    return greater_is_worse_columns"
        ]
    },
    {
        "func_name": "highlight_max",
        "original": "def highlight_max(s):\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
        "mutated": [
            "def highlight_max(s):\n    if False:\n        i = 10\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_max(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_max(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_max(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_max(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    to_highlight = s == s.max()\n    return ['background-color: yellow' if v else '' for v in to_highlight]"
        ]
    },
    {
        "func_name": "highlight_min",
        "original": "def highlight_min(s):\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
        "mutated": [
            "def highlight_min(s):\n    if False:\n        i = 10\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_min(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_min(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_min(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]",
            "def highlight_min(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    to_highlight = s == s.min()\n    return ['background-color: yellow' if v else '' for v in to_highlight]"
        ]
    },
    {
        "func_name": "highlight_cols",
        "original": "def highlight_cols(s):\n    color = 'lightgrey'\n    return f'background-color: {color}'",
        "mutated": [
            "def highlight_cols(s):\n    if False:\n        i = 10\n    color = 'lightgrey'\n    return f'background-color: {color}'",
            "def highlight_cols(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    color = 'lightgrey'\n    return f'background-color: {color}'",
            "def highlight_cols(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    color = 'lightgrey'\n    return f'background-color: {color}'",
            "def highlight_cols(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    color = 'lightgrey'\n    return f'background-color: {color}'",
            "def highlight_cols(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    color = 'lightgrey'\n    return f'background-color: {color}'"
        ]
    },
    {
        "func_name": "_highlight_models",
        "original": "def _highlight_models(self, master_display_: Any) -> Any:\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style",
        "mutated": [
            "def _highlight_models(self, master_display_: Any) -> Any:\n    if False:\n        i = 10\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style",
            "def _highlight_models(self, master_display_: Any) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style",
            "def _highlight_models(self, master_display_: Any) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style",
            "def _highlight_models(self, master_display_: Any) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style",
            "def _highlight_models(self, master_display_: Any) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def highlight_max(s):\n        to_highlight = s == s.max()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_min(s):\n        to_highlight = s == s.min()\n        return ['background-color: yellow' if v else '' for v in to_highlight]\n\n    def highlight_cols(s):\n        color = 'lightgrey'\n        return f'background-color: {color}'\n    greater_is_worse_columns = self._get_greater_is_worse_columns()\n    if master_display_ is not None:\n        return master_display_.apply(highlight_max, subset=[x for x in master_display_.columns[1:] if x not in greater_is_worse_columns]).apply(highlight_min, subset=[x for x in master_display_.columns[1:] if x in greater_is_worse_columns]).applymap(highlight_cols, subset=['TT (Sec)'])\n    else:\n        return pd.DataFrame().style"
        ]
    },
    {
        "func_name": "_process_sort",
        "original": "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    \"\"\"This function is extracted from different parts from the\n        compare_models function, and it is used for parallel compare_models\n        \"\"\"\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)",
        "mutated": [
            "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    if False:\n        i = 10\n    'This function is extracted from different parts from the\\n        compare_models function, and it is used for parallel compare_models\\n        '\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)",
            "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This function is extracted from different parts from the\\n        compare_models function, and it is used for parallel compare_models\\n        '\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)",
            "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This function is extracted from different parts from the\\n        compare_models function, and it is used for parallel compare_models\\n        '\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)",
            "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This function is extracted from different parts from the\\n        compare_models function, and it is used for parallel compare_models\\n        '\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)",
            "def _process_sort(self, sort: Any) -> Tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This function is extracted from different parts from the\\n        compare_models function, and it is used for parallel compare_models\\n        '\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        sort = sort.upper()\n    return (sort, sort_ascending)"
        ]
    },
    {
        "func_name": "compare_models",
        "original": "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    \"\"\"\n        This function train all the models available in the model library and scores them\n        using Cross Validation. The output prints a score grid with Accuracy,\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\n\n        This function returns all of the models compared, sorted by the value of the selected metric.\n\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\n        training time. By default turbo parameter is set to True.\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> best_model = compare_models()\n\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\n        and 'mlp' are used but this may result in longer training time.\n\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\n\n        This will return a comparison of all models except K Nearest Neighbour and\n        Gradient Boosting Classifier.\n\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\n\n        This will return comparison of all models except K Nearest Neighbour,\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\n        Multi Level Perceptron.\n\n\n        >>> tuned_model = tune_model(create_model('lr'))\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\n\n        This will compare a tuned Linear Regression model with an untuned one.\n\n        Parameters\n        ----------\n        exclude: list of strings, default = None\n            In order to omit certain models from the comparison model ID's can be passed as\n            a list of strings in exclude param.\n\n        include: list of strings or objects, default = None\n            In order to run only certain models for the comparison, the model ID's can be\n            passed as a list of strings in include param. The list can also include estimator\n            objects to be compared.\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        cross_validation: bool, default = True\n            When cross_validation set to False fold parameter is ignored and models are trained\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\n\n        sort: str, default = 'Accuracy'\n            The scoring measure specified is used for sorting the average score grid\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\n\n        n_select: int, default = 1\n            Number of top_n models to return. use negative argument for bottom selection.\n            for example, n_select = -3 means bottom 3 models.\n\n        budget_time: int or float, default = None\n            If not 0 or None, will terminate execution of the function after budget_time\n            minutes have passed and return results up to that point.\n\n        turbo: bool, default = True\n            When turbo is set to True, it excludes estimators that have longer\n            training time.\n\n        errors: str, default = 'ignore'\n            If 'ignore', will suppress model exceptions and continue.\n            If 'raise', will allow exceptions to be raised.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\n            therefore it is recommended to set errors parameter to 'ignore'.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\n            you can use ``FugueBackend(session)`` to make this function running using\n            Spark. For more details, see\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\n\n\n        caller_params: dict, default = None\n            The parameters used to call this function in the subclass. There are inconsistencies\n            in this function's signature between this base class and the subclasses, so this is\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\n\n\n        extra_params: Any\n            Extra parameters used to call the same method in the derived class. These parameters\n            are mainly used when ``parallel`` is not None.\n\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are also returned.\n\n        list\n            List of fitted model objects that were compared.\n\n        Warnings\n        --------\n        - compare_models() though attractive, might be time consuming with large\n        datasets. By default turbo is set to True, which excludes models that\n        have longer training times. Changing turbo parameter to False may result\n        in very high training times with datasets where number of samples exceed\n        10,000.\n\n        - If target variable is multiclass (more than 2 classes), AUC will be\n        returned as zero (0.0)\n\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\n\n        \"\"\"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models",
        "mutated": [
            "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    if False:\n        i = 10\n    \"\\n        This function train all the models available in the model library and scores them\\n        using Cross Validation. The output prints a score grid with Accuracy,\\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\\n\\n        This function returns all of the models compared, sorted by the value of the selected metric.\\n\\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\\n        training time. By default turbo parameter is set to True.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> best_model = compare_models()\\n\\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\\n        and 'mlp' are used but this may result in longer training time.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\\n\\n        This will return a comparison of all models except K Nearest Neighbour and\\n        Gradient Boosting Classifier.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\\n\\n        This will return comparison of all models except K Nearest Neighbour,\\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\\n        Multi Level Perceptron.\\n\\n\\n        >>> tuned_model = tune_model(create_model('lr'))\\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\\n\\n        This will compare a tuned Linear Regression model with an untuned one.\\n\\n        Parameters\\n        ----------\\n        exclude: list of strings, default = None\\n            In order to omit certain models from the comparison model ID's can be passed as\\n            a list of strings in exclude param.\\n\\n        include: list of strings or objects, default = None\\n            In order to run only certain models for the comparison, the model ID's can be\\n            passed as a list of strings in include param. The list can also include estimator\\n            objects to be compared.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and models are trained\\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\\n\\n        sort: str, default = 'Accuracy'\\n            The scoring measure specified is used for sorting the average score grid\\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\\n\\n        n_select: int, default = 1\\n            Number of top_n models to return. use negative argument for bottom selection.\\n            for example, n_select = -3 means bottom 3 models.\\n\\n        budget_time: int or float, default = None\\n            If not 0 or None, will terminate execution of the function after budget_time\\n            minutes have passed and return results up to that point.\\n\\n        turbo: bool, default = True\\n            When turbo is set to True, it excludes estimators that have longer\\n            training time.\\n\\n        errors: str, default = 'ignore'\\n            If 'ignore', will suppress model exceptions and continue.\\n            If 'raise', will allow exceptions to be raised.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\\n            therefore it is recommended to set errors parameter to 'ignore'.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n\\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\\n            you can use ``FugueBackend(session)`` to make this function running using\\n            Spark. For more details, see\\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\\n\\n\\n        caller_params: dict, default = None\\n            The parameters used to call this function in the subclass. There are inconsistencies\\n            in this function's signature between this base class and the subclasses, so this is\\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\\n\\n\\n        extra_params: Any\\n            Extra parameters used to call the same method in the derived class. These parameters\\n            are mainly used when ``parallel`` is not None.\\n\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        list\\n            List of fitted model objects that were compared.\\n\\n        Warnings\\n        --------\\n        - compare_models() though attractive, might be time consuming with large\\n        datasets. By default turbo is set to True, which excludes models that\\n        have longer training times. Changing turbo parameter to False may result\\n        in very high training times with datasets where number of samples exceed\\n        10,000.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be\\n        returned as zero (0.0)\\n\\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\\n\\n        \"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models",
            "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function train all the models available in the model library and scores them\\n        using Cross Validation. The output prints a score grid with Accuracy,\\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\\n\\n        This function returns all of the models compared, sorted by the value of the selected metric.\\n\\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\\n        training time. By default turbo parameter is set to True.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> best_model = compare_models()\\n\\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\\n        and 'mlp' are used but this may result in longer training time.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\\n\\n        This will return a comparison of all models except K Nearest Neighbour and\\n        Gradient Boosting Classifier.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\\n\\n        This will return comparison of all models except K Nearest Neighbour,\\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\\n        Multi Level Perceptron.\\n\\n\\n        >>> tuned_model = tune_model(create_model('lr'))\\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\\n\\n        This will compare a tuned Linear Regression model with an untuned one.\\n\\n        Parameters\\n        ----------\\n        exclude: list of strings, default = None\\n            In order to omit certain models from the comparison model ID's can be passed as\\n            a list of strings in exclude param.\\n\\n        include: list of strings or objects, default = None\\n            In order to run only certain models for the comparison, the model ID's can be\\n            passed as a list of strings in include param. The list can also include estimator\\n            objects to be compared.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and models are trained\\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\\n\\n        sort: str, default = 'Accuracy'\\n            The scoring measure specified is used for sorting the average score grid\\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\\n\\n        n_select: int, default = 1\\n            Number of top_n models to return. use negative argument for bottom selection.\\n            for example, n_select = -3 means bottom 3 models.\\n\\n        budget_time: int or float, default = None\\n            If not 0 or None, will terminate execution of the function after budget_time\\n            minutes have passed and return results up to that point.\\n\\n        turbo: bool, default = True\\n            When turbo is set to True, it excludes estimators that have longer\\n            training time.\\n\\n        errors: str, default = 'ignore'\\n            If 'ignore', will suppress model exceptions and continue.\\n            If 'raise', will allow exceptions to be raised.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\\n            therefore it is recommended to set errors parameter to 'ignore'.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n\\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\\n            you can use ``FugueBackend(session)`` to make this function running using\\n            Spark. For more details, see\\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\\n\\n\\n        caller_params: dict, default = None\\n            The parameters used to call this function in the subclass. There are inconsistencies\\n            in this function's signature between this base class and the subclasses, so this is\\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\\n\\n\\n        extra_params: Any\\n            Extra parameters used to call the same method in the derived class. These parameters\\n            are mainly used when ``parallel`` is not None.\\n\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        list\\n            List of fitted model objects that were compared.\\n\\n        Warnings\\n        --------\\n        - compare_models() though attractive, might be time consuming with large\\n        datasets. By default turbo is set to True, which excludes models that\\n        have longer training times. Changing turbo parameter to False may result\\n        in very high training times with datasets where number of samples exceed\\n        10,000.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be\\n        returned as zero (0.0)\\n\\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\\n\\n        \"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models",
            "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function train all the models available in the model library and scores them\\n        using Cross Validation. The output prints a score grid with Accuracy,\\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\\n\\n        This function returns all of the models compared, sorted by the value of the selected metric.\\n\\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\\n        training time. By default turbo parameter is set to True.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> best_model = compare_models()\\n\\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\\n        and 'mlp' are used but this may result in longer training time.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\\n\\n        This will return a comparison of all models except K Nearest Neighbour and\\n        Gradient Boosting Classifier.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\\n\\n        This will return comparison of all models except K Nearest Neighbour,\\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\\n        Multi Level Perceptron.\\n\\n\\n        >>> tuned_model = tune_model(create_model('lr'))\\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\\n\\n        This will compare a tuned Linear Regression model with an untuned one.\\n\\n        Parameters\\n        ----------\\n        exclude: list of strings, default = None\\n            In order to omit certain models from the comparison model ID's can be passed as\\n            a list of strings in exclude param.\\n\\n        include: list of strings or objects, default = None\\n            In order to run only certain models for the comparison, the model ID's can be\\n            passed as a list of strings in include param. The list can also include estimator\\n            objects to be compared.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and models are trained\\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\\n\\n        sort: str, default = 'Accuracy'\\n            The scoring measure specified is used for sorting the average score grid\\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\\n\\n        n_select: int, default = 1\\n            Number of top_n models to return. use negative argument for bottom selection.\\n            for example, n_select = -3 means bottom 3 models.\\n\\n        budget_time: int or float, default = None\\n            If not 0 or None, will terminate execution of the function after budget_time\\n            minutes have passed and return results up to that point.\\n\\n        turbo: bool, default = True\\n            When turbo is set to True, it excludes estimators that have longer\\n            training time.\\n\\n        errors: str, default = 'ignore'\\n            If 'ignore', will suppress model exceptions and continue.\\n            If 'raise', will allow exceptions to be raised.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\\n            therefore it is recommended to set errors parameter to 'ignore'.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n\\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\\n            you can use ``FugueBackend(session)`` to make this function running using\\n            Spark. For more details, see\\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\\n\\n\\n        caller_params: dict, default = None\\n            The parameters used to call this function in the subclass. There are inconsistencies\\n            in this function's signature between this base class and the subclasses, so this is\\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\\n\\n\\n        extra_params: Any\\n            Extra parameters used to call the same method in the derived class. These parameters\\n            are mainly used when ``parallel`` is not None.\\n\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        list\\n            List of fitted model objects that were compared.\\n\\n        Warnings\\n        --------\\n        - compare_models() though attractive, might be time consuming with large\\n        datasets. By default turbo is set to True, which excludes models that\\n        have longer training times. Changing turbo parameter to False may result\\n        in very high training times with datasets where number of samples exceed\\n        10,000.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be\\n        returned as zero (0.0)\\n\\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\\n\\n        \"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models",
            "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function train all the models available in the model library and scores them\\n        using Cross Validation. The output prints a score grid with Accuracy,\\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\\n\\n        This function returns all of the models compared, sorted by the value of the selected metric.\\n\\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\\n        training time. By default turbo parameter is set to True.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> best_model = compare_models()\\n\\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\\n        and 'mlp' are used but this may result in longer training time.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\\n\\n        This will return a comparison of all models except K Nearest Neighbour and\\n        Gradient Boosting Classifier.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\\n\\n        This will return comparison of all models except K Nearest Neighbour,\\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\\n        Multi Level Perceptron.\\n\\n\\n        >>> tuned_model = tune_model(create_model('lr'))\\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\\n\\n        This will compare a tuned Linear Regression model with an untuned one.\\n\\n        Parameters\\n        ----------\\n        exclude: list of strings, default = None\\n            In order to omit certain models from the comparison model ID's can be passed as\\n            a list of strings in exclude param.\\n\\n        include: list of strings or objects, default = None\\n            In order to run only certain models for the comparison, the model ID's can be\\n            passed as a list of strings in include param. The list can also include estimator\\n            objects to be compared.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and models are trained\\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\\n\\n        sort: str, default = 'Accuracy'\\n            The scoring measure specified is used for sorting the average score grid\\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\\n\\n        n_select: int, default = 1\\n            Number of top_n models to return. use negative argument for bottom selection.\\n            for example, n_select = -3 means bottom 3 models.\\n\\n        budget_time: int or float, default = None\\n            If not 0 or None, will terminate execution of the function after budget_time\\n            minutes have passed and return results up to that point.\\n\\n        turbo: bool, default = True\\n            When turbo is set to True, it excludes estimators that have longer\\n            training time.\\n\\n        errors: str, default = 'ignore'\\n            If 'ignore', will suppress model exceptions and continue.\\n            If 'raise', will allow exceptions to be raised.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\\n            therefore it is recommended to set errors parameter to 'ignore'.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n\\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\\n            you can use ``FugueBackend(session)`` to make this function running using\\n            Spark. For more details, see\\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\\n\\n\\n        caller_params: dict, default = None\\n            The parameters used to call this function in the subclass. There are inconsistencies\\n            in this function's signature between this base class and the subclasses, so this is\\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\\n\\n\\n        extra_params: Any\\n            Extra parameters used to call the same method in the derived class. These parameters\\n            are mainly used when ``parallel`` is not None.\\n\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        list\\n            List of fitted model objects that were compared.\\n\\n        Warnings\\n        --------\\n        - compare_models() though attractive, might be time consuming with large\\n        datasets. By default turbo is set to True, which excludes models that\\n        have longer training times. Changing turbo parameter to False may result\\n        in very high training times with datasets where number of samples exceed\\n        10,000.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be\\n        returned as zero (0.0)\\n\\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\\n\\n        \"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models",
            "def compare_models(self, include: Optional[List[Union[str, Any]]]=None, exclude: Optional[List[str]]=None, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, sort: str='Accuracy', n_select: int=1, budget_time: Optional[float]=None, turbo: bool=True, errors: str='ignore', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, parallel: Optional[ParallelBackend]=None, caller_params: Optional[dict]=None) -> List[Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function train all the models available in the model library and scores them\\n        using Cross Validation. The output prints a score grid with Accuracy,\\n        AUC, Recall, Precision, F1, Kappa and MCC (averaged across folds).\\n\\n        This function returns all of the models compared, sorted by the value of the selected metric.\\n\\n        When turbo is set to True ('rbfsvm', 'gpc' and 'mlp') are excluded due to longer\\n        training time. By default turbo parameter is set to True.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> best_model = compare_models()\\n\\n        This will return the averaged score grid of all the models except 'rbfsvm', 'gpc'\\n        and 'mlp'. When turbo parameter is set to False, all models including 'rbfsvm', 'gpc'\\n        and 'mlp' are used but this may result in longer training time.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = False)\\n\\n        This will return a comparison of all models except K Nearest Neighbour and\\n        Gradient Boosting Classifier.\\n\\n        >>> best_model = compare_models( exclude = [ 'knn', 'gbc' ] , turbo = True)\\n\\n        This will return comparison of all models except K Nearest Neighbour,\\n        Gradient Boosting Classifier, SVM (RBF), Gaussian Process Classifier and\\n        Multi Level Perceptron.\\n\\n\\n        >>> tuned_model = tune_model(create_model('lr'))\\n        >>> best_model = compare_models( include = [ 'lr', tuned_model ])\\n\\n        This will compare a tuned Linear Regression model with an untuned one.\\n\\n        Parameters\\n        ----------\\n        exclude: list of strings, default = None\\n            In order to omit certain models from the comparison model ID's can be passed as\\n            a list of strings in exclude param.\\n\\n        include: list of strings or objects, default = None\\n            In order to run only certain models for the comparison, the model ID's can be\\n            passed as a list of strings in include param. The list can also include estimator\\n            objects to be compared.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and models are trained\\n            on entire training dataset, returning metrics calculated using the train (holdout) set.\\n\\n        sort: str, default = 'Accuracy'\\n            The scoring measure specified is used for sorting the average score grid\\n            Other options are 'AUC', 'Recall', 'Precision', 'F1', 'Kappa' and 'MCC'.\\n\\n        n_select: int, default = 1\\n            Number of top_n models to return. use negative argument for bottom selection.\\n            for example, n_select = -3 means bottom 3 models.\\n\\n        budget_time: int or float, default = None\\n            If not 0 or None, will terminate execution of the function after budget_time\\n            minutes have passed and return results up to that point.\\n\\n        turbo: bool, default = True\\n            When turbo is set to True, it excludes estimators that have longer\\n            training time.\\n\\n        errors: str, default = 'ignore'\\n            If 'ignore', will suppress model exceptions and continue.\\n            If 'raise', will allow exceptions to be raised.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model. The parameters will be applied to all models,\\n            therefore it is recommended to set errors parameter to 'ignore'.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n\\n        parallel: pycaret.internal.parallel.parallel_backend.ParallelBackend, default = None\\n            A ParallelBackend instance. For example if you have a SparkSession ``session``,\\n            you can use ``FugueBackend(session)`` to make this function running using\\n            Spark. For more details, see\\n            :class:`~pycaret.parallel.fugue_backend.FugueBackend`\\n\\n\\n        caller_params: dict, default = None\\n            The parameters used to call this function in the subclass. There are inconsistencies\\n            in this function's signature between this base class and the subclasses, so this is\\n            used to prevent inconsistencies. It must be set when ``parallel`` is not None\\n\\n\\n        extra_params: Any\\n            Extra parameters used to call the same method in the derived class. These parameters\\n            are mainly used when ``parallel`` is not None.\\n\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        list\\n            List of fitted model objects that were compared.\\n\\n        Warnings\\n        --------\\n        - compare_models() though attractive, might be time consuming with large\\n        datasets. By default turbo is set to True, which excludes models that\\n        have longer training times. Changing turbo parameter to False may result\\n        in very high training times with datasets where number of samples exceed\\n        10,000.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be\\n        returned as zero (0.0)\\n\\n        - If cross_validation parameter is set to False, no models will be logged with MLFlow.\\n\\n        \"\n    self._check_setup_ran()\n    if parallel is not None:\n        return self._parallel_compare_models(parallel, caller_params, turbo=turbo)\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing compare_models()')\n    self.logger.info(f'compare_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if not fit_kwargs:\n        fit_kwargs = {}\n    available_estimators = self._all_models\n    if include is not None:\n        for i in include:\n            if isinstance(i, str):\n                if i not in available_estimators:\n                    raise ValueError(f'Estimator {i} Not Available. Please see docstring for list of available estimators.')\n            elif not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if include is not None and exclude is not None:\n        raise TypeError('Cannot use exclude parameter when include is used to compare models.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if budget_time and type(budget_time) is not int and (type(budget_time) is not float):\n        raise TypeError('budget_time parameter only accepts integer or float values.')\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort = self._get_metric_by_name_or_id(sort)\n        if sort is None:\n            raise ValueError('Sort method not supported. See docstring for list of available parameters.')\n    possible_errors = ['ignore', 'raise']\n    if errors not in possible_errors:\n        raise ValueError(f\"errors parameter must be one of: {', '.join(possible_errors)}.\")\n    if self.is_multiclass:\n        if not sort.is_multiclass:\n            raise TypeError(f'{sort} metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase != MLUsecase.TIME_SERIES:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    pd.set_option('display.max_columns', 500)\n    self.logger.info('Preparing display monitor')\n    len_mod = len({k: v for (k, v) in self._all_models.items() if v.is_turbo}) if turbo else len(self._all_models)\n    if include:\n        len_mod = len(include)\n    elif exclude:\n        len_mod -= len(exclude)\n    progress_args = {'max': 4 * len_mod + 4 + min(len_mod, abs(n_select))}\n    master_display_columns = ['Model'] + [v.display_name for (k, v) in self._all_metrics.items()] + ['TT (Sec)']\n    master_display = pd.DataFrame(columns=master_display_columns)\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = DummyDisplay() if self._remote else CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    if display.can_update_text:\n        display.display(master_display, final_display=False)\n    input_ml_usecase = self._ml_usecase\n    target_ml_usecase = MLUsecase.TIME_SERIES\n    np.random.seed(self.seed)\n    display.move_progress()\n    if not (isinstance(sort, str) and (sort == 'TT' or sort == 'TT (Sec)')):\n        sort_ascending = not sort.greater_is_better\n        sort = id_or_display_name(sort, input_ml_usecase, target_ml_usecase)\n    else:\n        sort_ascending = True\n        sort = 'TT (Sec)'\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Loading Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if include:\n        model_library = include\n    else:\n        if turbo:\n            model_library = [k for (k, v) in self._all_models.items() if v.is_turbo]\n        else:\n            model_library = list(self._all_models.keys())\n        if exclude:\n            model_library = [x for x in model_library if x not in exclude]\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        if 'ensemble_forecaster' in model_library:\n            warnings.warn('Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library')\n            model_library.remove('ensemble_forecaster')\n    display.move_progress()\n    import secrets\n    URI = secrets.token_hex(nbytes=4)\n    master_display = None\n    master_display_ = None\n    total_runtime_start = time.time()\n    total_runtime = 0\n    over_time_budget = False\n    if budget_time and budget_time > 0:\n        self.logger.info(f'Time budget is {budget_time} minutes')\n    for (i, model) in enumerate(model_library):\n        model_id = model if isinstance(model, str) and all((isinstance(m, str) for m in model_library)) else str(i)\n        model_name = self._get_model_name(model)\n        if isinstance(model, str):\n            self.logger.info(f'Initializing {model_name}')\n        else:\n            self.logger.info(f'Initializing custom model {model_name}')\n        runtime_start = time.time()\n        total_runtime += (runtime_start - total_runtime_start) / 60\n        self.logger.info(f'Total runtime is {total_runtime} minutes')\n        over_time_budget = budget_time and budget_time > 0 and (total_runtime > budget_time)\n        if over_time_budget:\n            self.logger.info(f'Total runtime {total_runtime} is over time budget by {total_runtime - budget_time}, breaking loop')\n            break\n        total_runtime_start = runtime_start\n        '\\n            MONITOR UPDATE STARTS\\n            '\n        display.update_monitor(2, model_name)\n        '\\n            MONITOR UPDATE ENDS\\n            '\n        self.logger.info('SubProcess create_model() called ==================================')\n        create_model_args = dict(estimator=model, system=False, verbose=False, display=display, fold=fold, round=round, cross_validation=cross_validation, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, refit=False, error_score='raise' if errors == 'raise' else 0.0)\n        results_columns_to_ignore = ['Object', 'runtime', 'cutoff']\n        try:\n            (model, model_fit_time) = self._create_model(**create_model_args)\n            model_results = self.pull(pop=True)\n            assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n        except Exception as ex:\n            if errors == 'raise':\n                raise RuntimeError(f'create_model() failed for model {model}. {type(ex).__name__}: {ex}')\n            self.logger.warning(f'create_model() for {model} raised an exception or returned all 0.0, trying without fit_kwargs:')\n            self.logger.warning(traceback.format_exc())\n            try:\n                (model, model_fit_time) = self._create_model(**create_model_args)\n                model_results = self.pull(pop=True)\n                assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n            except Exception:\n                self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                self.logger.error(traceback.format_exc())\n                continue\n        self.logger.info('SubProcess create_model() end ==================================')\n        if model is None:\n            over_time_budget = True\n            self.logger.info('Time budged exceeded in create_model(), breaking loop')\n            break\n        runtime_end = time.time()\n        runtime = np.array(runtime_end - runtime_start).round(2)\n        self.logger.info('Creating metrics dataframe')\n        if cross_validation:\n            if 'cutoff' in model_results.columns:\n                model_results.drop('cutoff', axis=1, errors='ignore')\n            compare_models_ = pd.DataFrame(model_results.loc[self._get_return_train_score_indices_for_logging(return_train_score=False)]).T.reset_index(drop=True)\n        else:\n            compare_models_ = pd.DataFrame(model_results.iloc[0]).T\n        compare_models_.insert(len(compare_models_.columns), 'TT (Sec)', model_fit_time)\n        compare_models_.insert(0, 'Model', model_name)\n        compare_models_.insert(0, 'Object', [model])\n        compare_models_.insert(0, 'runtime', runtime)\n        compare_models_.index = [model_id]\n        if master_display is None:\n            master_display = compare_models_\n        else:\n            master_display = pd.concat([master_display, compare_models_], ignore_index=False)\n        master_display = master_display.round(round)\n        if self._ml_usecase != MLUsecase.TIME_SERIES:\n            master_display = master_display.sort_values(by=sort, ascending=sort_ascending)\n        else:\n            master_display = master_display.sort_values(by=sort.upper(), ascending=sort_ascending)\n        master_display_ = master_display.drop(results_columns_to_ignore, axis=1, errors='ignore').style.format(precision=round)\n        master_display_ = master_display_.set_properties(**{'text-align': 'left'})\n        master_display_ = master_display_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n        if display.can_update_text:\n            display.display(master_display_, final_display=False)\n    display.move_progress()\n    compare_models_ = self._highlight_models(master_display_)\n    display.update_monitor(1, 'Compiling Final Models')\n    display.move_progress()\n    sorted_models = []\n    if master_display is not None:\n        clamped_n_select = min(len(master_display), abs(n_select))\n        if n_select < 0:\n            n_select_range = range(len(master_display) - clamped_n_select, len(master_display))\n        else:\n            n_select_range = range(0, clamped_n_select)\n        if self.logging_param:\n            self.logging_param.log_model_comparison(master_display, 'compare_models')\n        for (index, row) in enumerate(master_display.iterrows()):\n            (_, row) = row\n            model = row['Object']\n            results = row.to_frame().T.drop(['Object', 'Model', 'runtime', 'TT (Sec)'], errors='ignore', axis=1)\n            avgs_dict_log = {k: v for (k, v) in results.iloc[0].items()}\n            full_logging = False\n            if index in n_select_range:\n                display.update_monitor(2, self._get_model_name(model))\n                create_model_args = dict(estimator=model, system=False, verbose=False, fold=fold, round=round, cross_validation=False, predict=False, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold)\n                if errors == 'raise':\n                    (model, model_fit_time) = self._create_model(**create_model_args)\n                    sorted_models.append(model)\n                else:\n                    try:\n                        (model, model_fit_time) = self._create_model(**create_model_args)\n                        sorted_models.append(model)\n                        assert np.sum(model_results.drop(results_columns_to_ignore, axis=1, errors='ignore').iloc[0]) != 0.0\n                    except Exception:\n                        self.logger.error(f'create_model() for {model} raised an exception or returned all 0.0:')\n                        self.logger.error(traceback.format_exc())\n                        model = None\n                        display.move_progress()\n                        continue\n                display.move_progress()\n                full_logging = True\n            if self.logging_param and cross_validation and (model is not None):\n                self._log_model(model=model, model_results=results, score_dict=avgs_dict_log, source='compare_models', runtime=row['runtime'], model_fit_time=row['TT (Sec)'], pipeline=self.pipeline, log_plots=self.log_plots_param if full_logging else [], log_holdout=full_logging, URI=URI, display=display, experiment_custom_tags=experiment_custom_tags)\n    if len(sorted_models) == 1:\n        sorted_models = sorted_models[0]\n    display.display(compare_models_, final_display=True)\n    pd.reset_option('display.max_columns')\n    self._display_container.append(compare_models_.data)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(sorted_models))\n    self.logger.info('compare_models() successfully completed......................................')\n    return sorted_models"
        ]
    },
    {
        "func_name": "_create_model_without_cv",
        "original": "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)",
        "mutated": [
            "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    if False:\n        i = 10\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)",
            "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)",
            "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)",
            "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)",
            "def _create_model_without_cv(self, model, data_X, data_y, fit_kwargs, round, predict, system, display: CommonDisplay, model_only: bool=True, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info('Cross validation set to False')\n        self.logger.info('Fitting Model')\n        model_fit_start = time.time()\n        with redirect_output(self.logger):\n            pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        display.move_progress()\n        if predict:\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                train_results = self.pull(pop=True).drop('Model', axis=1)\n                train_results.index = ['Train']\n            else:\n                train_results = None\n            self.predict_model(pipeline_with_model, verbose=False)\n            model_results = self.pull(pop=True).drop('Model', axis=1)\n            model_results.index = ['Test']\n            if train_results is not None:\n                model_results = pd.concat([model_results, train_results])\n            self._display_container.append(model_results)\n            model_results = model_results.style.format(precision=round)\n            if system:\n                display.display(model_results)\n            self.logger.info(f'_display_container: {len(self._display_container)}')\n        if not model_only:\n            return (pipeline_with_model, model_fit_time)\n    return (model, model_fit_time)"
        ]
    },
    {
        "func_name": "_create_model_with_cv",
        "original": "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    \"\"\"\n        MONITOR UPDATE STARTS\n        \"\"\"\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)",
        "mutated": [
            "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    if False:\n        i = 10\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)",
            "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)",
            "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)",
            "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)",
            "def _create_model_with_cv(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, f'Fitting {self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)} Folds')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    from sklearn.model_selection import cross_validate\n    metrics_dict = dict([(k, v.scorer) for (k, v) in metrics.items()])\n    self.logger.info('Starting cross validation')\n    n_jobs = self.gpu_n_jobs_param\n    from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n    if isinstance(model, (GaussianProcessClassifier, GaussianProcessRegressor)):\n        n_jobs = 1\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        self.logger.info(f'Cross validating with {cv}, n_jobs={n_jobs}')\n        with patch('sklearn.model_selection._validation._fit_and_score', fs):\n            model_fit_start = time.time()\n            with redirect_output(self.logger):\n                scores = cross_validate(pipeline_with_model, data_X, data_y, cv=cv, groups=groups, scoring=metrics_dict, fit_params=fit_kwargs, n_jobs=n_jobs, return_train_score=return_train_score, error_score=error_score)\n        model_fit_end = time.time()\n        model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        score_dict = {}\n        for (k, v) in metrics.items():\n            score_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                score_dict[v.display_name] = train_score\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            score_dict[v.display_name] += test_score\n        self.logger.info('Calculating mean and std')\n        avgs_dict = {}\n        for (k, v) in metrics.items():\n            avgs_dict[v.display_name] = []\n            if return_train_score:\n                train_score = scores[f'train_{k}'] * (1 if v.greater_is_better else -1)\n                train_score = train_score.tolist()\n                avgs_dict[v.display_name] = [np.mean(train_score), np.std(train_score)]\n            test_score = scores[f'test_{k}'] * (1 if v.greater_is_better else -1)\n            test_score = test_score.tolist()\n            avgs_dict[v.display_name] += [np.mean(test_score), np.std(test_score)]\n        display.move_progress()\n        self.logger.info('Creating metrics dataframe')\n        if hasattr(cv, 'n_splits'):\n            fold = cv.n_splits\n        elif hasattr(cv, 'get_n_splits'):\n            fold = cv.get_n_splits(groups=groups)\n        else:\n            raise ValueError(f'The cross validation class should implement a n_splits attribute or a get_n_splits method. {cv.__class__.__name__} has neither.')\n        if return_train_score:\n            model_results = pd.DataFrame({'Split': ['CV-Train'] * fold + ['CV-Val'] * fold + ['CV-Train'] * 2 + ['CV-Val'] * 2, 'Fold': np.arange(fold).tolist() + np.arange(fold).tolist() + ['Mean', 'Std'] * 2})\n        else:\n            model_results = pd.DataFrame({'Fold': np.arange(fold).tolist() + ['Mean', 'Std']})\n        model_scores = pd.concat([pd.DataFrame(score_dict), pd.DataFrame(avgs_dict)]).reset_index(drop=True)\n        model_results = pd.concat([model_results, model_scores], axis=1)\n        model_results.set_index(self._get_return_train_score_columns_for_display(return_train_score), inplace=True)\n        if refit:\n            display.update_monitor(1, 'Finalizing Model')\n            model_fit_start = time.time()\n            self.logger.info('Finalizing model')\n            with redirect_output(self.logger):\n                pipeline_with_model.fit(data_X, data_y, **fit_kwargs)\n                model_fit_end = time.time()\n            if return_train_score:\n                _SupervisedExperiment.predict_model(self, pipeline_with_model, data=pd.concat([data_X, data_y], axis=1), verbose=False)\n                metrics = self.pull(pop=True).drop('Model', axis=1)\n                df_score = pd.DataFrame({'Split': ['Train'], 'Fold': [None]})\n                df_score = pd.concat([df_score, metrics], axis=1)\n                df_score.set_index(['Split', 'Fold'], inplace=True)\n                model_results = pd.concat([model_results, df_score])\n            model_fit_time = np.array(model_fit_end - model_fit_start).round(2)\n        else:\n            model_fit_time /= self._get_cv_n_folds(cv, data_X, y=data_y, groups=groups)\n    model_results = model_results.round(round)\n    return (model, model_fit_time, model_results, avgs_dict)"
        ]
    },
    {
        "func_name": "_get_return_train_score_columns_for_display",
        "original": "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns",
        "mutated": [
            "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if False:\n        i = 10\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns",
            "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns",
            "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns",
            "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns",
            "def _get_return_train_score_columns_for_display(self, return_train_score: bool) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if return_train_score:\n        columns = ['Split', 'Fold']\n    else:\n        columns = ['Fold']\n    return columns"
        ]
    },
    {
        "func_name": "_get_return_train_score_indices_for_logging",
        "original": "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices",
        "mutated": [
            "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if False:\n        i = 10\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices",
            "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices",
            "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices",
            "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices",
            "def _get_return_train_score_indices_for_logging(self, return_train_score: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if return_train_score:\n        indices = ('CV-Val', 'Mean')\n    else:\n        indices = 'Mean'\n    return indices"
        ]
    },
    {
        "func_name": "_highlight_and_round_model_results",
        "original": "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results",
        "mutated": [
            "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if False:\n        i = 10\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results",
            "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results",
            "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results",
            "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results",
            "def _highlight_and_round_model_results(self, model_results: pd.DataFrame, return_train_score: bool, round: int) -> pandas.io.formats.style.Styler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if return_train_score:\n        indices = [('CV-Val', 'Mean'), ('CV-Train', 'Mean')]\n    else:\n        indices = ['Mean']\n    model_results = color_df(model_results, 'yellow', indices, axis=1)\n    model_results = model_results.format(precision=round)\n    return model_results"
        ]
    },
    {
        "func_name": "_create_model_get_train_X_y",
        "original": "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    \"\"\"Return appropriate training X and y values depending on whether\n        X_train and y_train are passed or not.\"\"\"\n    pass",
        "mutated": [
            "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    if False:\n        i = 10\n    'Return appropriate training X and y values depending on whether\\n        X_train and y_train are passed or not.'\n    pass",
            "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return appropriate training X and y values depending on whether\\n        X_train and y_train are passed or not.'\n    pass",
            "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return appropriate training X and y values depending on whether\\n        X_train and y_train are passed or not.'\n    pass",
            "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return appropriate training X and y values depending on whether\\n        X_train and y_train are passed or not.'\n    pass",
            "@abstractmethod\ndef _create_model_get_train_X_y(self, X_train, y_train):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return appropriate training X and y values depending on whether\\n        X_train and y_train are passed or not.'\n    pass"
        ]
    },
    {
        "func_name": "_create_model",
        "original": "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    \"\"\"\n        Internal version of ``create_model`` with private arguments.\n        \"\"\"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model",
        "mutated": [
            "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    if False:\n        i = 10\n    '\\n        Internal version of ``create_model`` with private arguments.\\n        '\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model",
            "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Internal version of ``create_model`` with private arguments.\\n        '\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model",
            "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Internal version of ``create_model`` with private arguments.\\n        '\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model",
            "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Internal version of ``create_model`` with private arguments.\\n        '\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model",
            "def _create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, system: bool=True, add_to_model_list: bool=True, X_train_data: Optional[pd.DataFrame]=None, y_train_data: Optional[pd.DataFrame]=None, metrics=None, display: Optional[CommonDisplay]=None, model_only: bool=True, return_train_score: bool=False, error_score: Union[str, float]=0.0, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Internal version of ``create_model`` with private arguments.\\n        '\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k not in ('X_train_data', 'y_train_data')])\n    self.logger.info('Initializing create_model()')\n    self.logger.info(f'create_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    available_estimators = set(self._all_models_internal.keys())\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if isinstance(estimator, str):\n        if estimator not in available_estimators:\n            raise ValueError(f'Estimator {estimator} not available. Please see docstring for list of available estimators.')\n    elif not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    if type(system) is not bool:\n        raise TypeError('System parameter can only take argument as True or False.')\n    if type(cross_validation) is not bool:\n        raise TypeError('cross_validation parameter can only take argument as True or False.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if not display:\n        progress_args = {'max': 4}\n        timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n        monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    (data_X, data_y) = self._create_model_get_train_X_y(X_train=X_train_data, y_train=y_train_data)\n    groups = self._get_groups(groups, data=data_X)\n    if metrics is None:\n        metrics = self._all_metrics\n    display.move_progress()\n    self.logger.info('Defining folds')\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        cv = self.get_fold_generator(fold=fold)\n        fit_kwargs = self.update_fit_kwargs_with_fh_from_cv(fit_kwargs=fit_kwargs, cv=cv)\n    else:\n        cv = self._get_cv_splitter(fold)\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Importing untrained model')\n    if isinstance(estimator, str) and estimator in available_estimators:\n        model_definition = self._all_models_internal[estimator]\n        model_args = model_definition.args\n        model_args = {**model_args, **kwargs}\n        model = model_definition.class_def(**model_args)\n        full_name = model_definition.name\n    else:\n        self.logger.info('Declaring custom model')\n        model = clone(estimator)\n        model.set_params(**kwargs)\n        full_name = self._get_model_name(model)\n    model = clone(model)\n    display.update_monitor(2, full_name)\n    if probability_threshold is not None:\n        if self._ml_usecase != MLUsecase.CLASSIFICATION or self.is_multiclass:\n            raise ValueError('Cannot use probability_threshold with non-binary classification usecases.')\n        if not isinstance(model, CustomProbabilityThresholdClassifier):\n            model = CustomProbabilityThresholdClassifier(classifier=model, probability_threshold=probability_threshold)\n        else:\n            model.set_params(probability_threshold=probability_threshold)\n    self.logger.info(f'{full_name} Imported successfully')\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    if not cross_validation:\n        display.update_monitor(1, f'Fitting {str(full_name)}')\n    else:\n        display.update_monitor(1, 'Initializing CV')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    if not cross_validation:\n        (model, model_fit_time) = self._create_model_without_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, predict=predict, system=system, display=display, model_only=model_only, return_train_score=return_train_score)\n        display.move_progress()\n        self.logger.info(str(model))\n        self.logger.info('create_model() successfully completed......................................')\n        gc.collect()\n        if not system:\n            return (model, model_fit_time)\n        return model\n    (model, model_fit_time, model_results, _) = self._create_model_with_cv(model=model, data_X=data_X, data_y=data_y, fit_kwargs=fit_kwargs, round=round, cv=cv, groups=groups, metrics=metrics, refit=refit, system=system, display=display, error_score=error_score, return_train_score=return_train_score)\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param and system and refit:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='create_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    display.move_progress()\n    self.logger.info('Uploading results into container')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        model_results.drop('cutoff', axis=1, inplace=True, errors='ignore')\n    self._display_container.append(model_results)\n    if add_to_model_list:\n        self.logger.info('Uploading model into container now')\n        self._master_model_container.append({'model': model, 'scores': model_results, 'cv': cv})\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    if system:\n        display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('create_model() successfully completed......................................')\n    gc.collect()\n    if not system:\n        return (model, model_fit_time)\n    return model"
        ]
    },
    {
        "func_name": "create_model",
        "original": "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    \"\"\"\n        This function creates a model and scores it using Cross Validation.\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\n        F1, Kappa and MCC by fold (default = 10 Fold).\n\n        This function returns a trained model object.\n\n        setup() function must be called before using create_model()\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> lr = create_model('lr')\n\n        This will create a trained Logistic Regression model.\n\n        Parameters\n        ----------\n        estimator : str / object, default = None\n            Enter ID of the estimators available in model library or pass an untrained model\n            object consistent with fit / predict API to train and evaluate model. All\n            estimators support binary or multiclass problem. List of estimators in model\n            library (ID - Name):\n\n            * 'lr' - Logistic Regression\n            * 'knn' - K Nearest Neighbour\n            * 'nb' - Naive Bayes\n            * 'dt' - Decision Tree Classifier\n            * 'svm' - SVM - Linear Kernel\n            * 'rbfsvm' - SVM - Radial Kernel\n            * 'gpc' - Gaussian Process Classifier\n            * 'mlp' - Multi Level Perceptron\n            * 'ridge' - Ridge Classifier\n            * 'rf' - Random Forest Classifier\n            * 'qda' - Quadratic Discriminant Analysis\n            * 'ada' - Ada Boost Classifier\n            * 'gbc' - Gradient Boosting Classifier\n            * 'lda' - Linear Discriminant Analysis\n            * 'et' - Extra Trees Classifier\n            * 'xgboost' - Extreme Gradient Boosting\n            * 'lightgbm' - Light Gradient Boosting\n            * 'catboost' - CatBoost Classifier\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        cross_validation: bool, default = True\n            When cross_validation set to False fold parameter is ignored and model is trained\n            on entire training dataset.\n\n        predict: bool, default = True\n            Whether to predict model on holdout if cross_validation == False.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        refit: bool, default = True\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n        system: bool, default = True\n            Must remain True all times. Only to be changed by internal functions.\n            If False, method will return a tuple of model and the model fit time.\n\n        add_to_model_list: bool, default = True\n            Whether to save model and results in _master_model_container.\n\n        X_train_data: pandas.DataFrame, default = None\n            If not None, will use this dataframe as training features.\n            Intended to be only changed by internal functions.\n\n        y_train_data: pandas.DataFrame, default = None\n            If not None, will use this dataframe as training target.\n            Intended to be only changed by internal functions.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        **kwargs:\n            Additional keyword arguments to pass to the estimator.\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are highlighted in yellow.\n\n        model\n            trained model object\n\n        Warnings\n        --------\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\n        returned as zero (0.0)\n\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\n        as zero (0.0)\n\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\n        more than quadratic. These estimators are hard to scale on datasets with more\n        than 10,000 samples.\n\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\n\n        \"\"\"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)",
        "mutated": [
            "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function creates a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        setup() function must be called before using create_model()\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n\\n        This will create a trained Logistic Regression model.\\n\\n        Parameters\\n        ----------\\n        estimator : str / object, default = None\\n            Enter ID of the estimators available in model library or pass an untrained model\\n            object consistent with fit / predict API to train and evaluate model. All\\n            estimators support binary or multiclass problem. List of estimators in model\\n            library (ID - Name):\\n\\n            * 'lr' - Logistic Regression\\n            * 'knn' - K Nearest Neighbour\\n            * 'nb' - Naive Bayes\\n            * 'dt' - Decision Tree Classifier\\n            * 'svm' - SVM - Linear Kernel\\n            * 'rbfsvm' - SVM - Radial Kernel\\n            * 'gpc' - Gaussian Process Classifier\\n            * 'mlp' - Multi Level Perceptron\\n            * 'ridge' - Ridge Classifier\\n            * 'rf' - Random Forest Classifier\\n            * 'qda' - Quadratic Discriminant Analysis\\n            * 'ada' - Ada Boost Classifier\\n            * 'gbc' - Gradient Boosting Classifier\\n            * 'lda' - Linear Discriminant Analysis\\n            * 'et' - Extra Trees Classifier\\n            * 'xgboost' - Extreme Gradient Boosting\\n            * 'lightgbm' - Light Gradient Boosting\\n            * 'catboost' - CatBoost Classifier\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and model is trained\\n            on entire training dataset.\\n\\n        predict: bool, default = True\\n            Whether to predict model on holdout if cross_validation == False.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        refit: bool, default = True\\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        system: bool, default = True\\n            Must remain True all times. Only to be changed by internal functions.\\n            If False, method will return a tuple of model and the model fit time.\\n\\n        add_to_model_list: bool, default = True\\n            Whether to save model and results in _master_model_container.\\n\\n        X_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training features.\\n            Intended to be only changed by internal functions.\\n\\n        y_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training target.\\n            Intended to be only changed by internal functions.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the estimator.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are highlighted in yellow.\\n\\n        model\\n            trained model object\\n\\n        Warnings\\n        --------\\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\\n        returned as zero (0.0)\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0)\\n\\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\\n        more than quadratic. These estimators are hard to scale on datasets with more\\n        than 10,000 samples.\\n\\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\\n\\n        \"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)",
            "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function creates a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        setup() function must be called before using create_model()\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n\\n        This will create a trained Logistic Regression model.\\n\\n        Parameters\\n        ----------\\n        estimator : str / object, default = None\\n            Enter ID of the estimators available in model library or pass an untrained model\\n            object consistent with fit / predict API to train and evaluate model. All\\n            estimators support binary or multiclass problem. List of estimators in model\\n            library (ID - Name):\\n\\n            * 'lr' - Logistic Regression\\n            * 'knn' - K Nearest Neighbour\\n            * 'nb' - Naive Bayes\\n            * 'dt' - Decision Tree Classifier\\n            * 'svm' - SVM - Linear Kernel\\n            * 'rbfsvm' - SVM - Radial Kernel\\n            * 'gpc' - Gaussian Process Classifier\\n            * 'mlp' - Multi Level Perceptron\\n            * 'ridge' - Ridge Classifier\\n            * 'rf' - Random Forest Classifier\\n            * 'qda' - Quadratic Discriminant Analysis\\n            * 'ada' - Ada Boost Classifier\\n            * 'gbc' - Gradient Boosting Classifier\\n            * 'lda' - Linear Discriminant Analysis\\n            * 'et' - Extra Trees Classifier\\n            * 'xgboost' - Extreme Gradient Boosting\\n            * 'lightgbm' - Light Gradient Boosting\\n            * 'catboost' - CatBoost Classifier\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and model is trained\\n            on entire training dataset.\\n\\n        predict: bool, default = True\\n            Whether to predict model on holdout if cross_validation == False.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        refit: bool, default = True\\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        system: bool, default = True\\n            Must remain True all times. Only to be changed by internal functions.\\n            If False, method will return a tuple of model and the model fit time.\\n\\n        add_to_model_list: bool, default = True\\n            Whether to save model and results in _master_model_container.\\n\\n        X_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training features.\\n            Intended to be only changed by internal functions.\\n\\n        y_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training target.\\n            Intended to be only changed by internal functions.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the estimator.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are highlighted in yellow.\\n\\n        model\\n            trained model object\\n\\n        Warnings\\n        --------\\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\\n        returned as zero (0.0)\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0)\\n\\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\\n        more than quadratic. These estimators are hard to scale on datasets with more\\n        than 10,000 samples.\\n\\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\\n\\n        \"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)",
            "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function creates a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        setup() function must be called before using create_model()\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n\\n        This will create a trained Logistic Regression model.\\n\\n        Parameters\\n        ----------\\n        estimator : str / object, default = None\\n            Enter ID of the estimators available in model library or pass an untrained model\\n            object consistent with fit / predict API to train and evaluate model. All\\n            estimators support binary or multiclass problem. List of estimators in model\\n            library (ID - Name):\\n\\n            * 'lr' - Logistic Regression\\n            * 'knn' - K Nearest Neighbour\\n            * 'nb' - Naive Bayes\\n            * 'dt' - Decision Tree Classifier\\n            * 'svm' - SVM - Linear Kernel\\n            * 'rbfsvm' - SVM - Radial Kernel\\n            * 'gpc' - Gaussian Process Classifier\\n            * 'mlp' - Multi Level Perceptron\\n            * 'ridge' - Ridge Classifier\\n            * 'rf' - Random Forest Classifier\\n            * 'qda' - Quadratic Discriminant Analysis\\n            * 'ada' - Ada Boost Classifier\\n            * 'gbc' - Gradient Boosting Classifier\\n            * 'lda' - Linear Discriminant Analysis\\n            * 'et' - Extra Trees Classifier\\n            * 'xgboost' - Extreme Gradient Boosting\\n            * 'lightgbm' - Light Gradient Boosting\\n            * 'catboost' - CatBoost Classifier\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and model is trained\\n            on entire training dataset.\\n\\n        predict: bool, default = True\\n            Whether to predict model on holdout if cross_validation == False.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        refit: bool, default = True\\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        system: bool, default = True\\n            Must remain True all times. Only to be changed by internal functions.\\n            If False, method will return a tuple of model and the model fit time.\\n\\n        add_to_model_list: bool, default = True\\n            Whether to save model and results in _master_model_container.\\n\\n        X_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training features.\\n            Intended to be only changed by internal functions.\\n\\n        y_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training target.\\n            Intended to be only changed by internal functions.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the estimator.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are highlighted in yellow.\\n\\n        model\\n            trained model object\\n\\n        Warnings\\n        --------\\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\\n        returned as zero (0.0)\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0)\\n\\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\\n        more than quadratic. These estimators are hard to scale on datasets with more\\n        than 10,000 samples.\\n\\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\\n\\n        \"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)",
            "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function creates a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        setup() function must be called before using create_model()\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n\\n        This will create a trained Logistic Regression model.\\n\\n        Parameters\\n        ----------\\n        estimator : str / object, default = None\\n            Enter ID of the estimators available in model library or pass an untrained model\\n            object consistent with fit / predict API to train and evaluate model. All\\n            estimators support binary or multiclass problem. List of estimators in model\\n            library (ID - Name):\\n\\n            * 'lr' - Logistic Regression\\n            * 'knn' - K Nearest Neighbour\\n            * 'nb' - Naive Bayes\\n            * 'dt' - Decision Tree Classifier\\n            * 'svm' - SVM - Linear Kernel\\n            * 'rbfsvm' - SVM - Radial Kernel\\n            * 'gpc' - Gaussian Process Classifier\\n            * 'mlp' - Multi Level Perceptron\\n            * 'ridge' - Ridge Classifier\\n            * 'rf' - Random Forest Classifier\\n            * 'qda' - Quadratic Discriminant Analysis\\n            * 'ada' - Ada Boost Classifier\\n            * 'gbc' - Gradient Boosting Classifier\\n            * 'lda' - Linear Discriminant Analysis\\n            * 'et' - Extra Trees Classifier\\n            * 'xgboost' - Extreme Gradient Boosting\\n            * 'lightgbm' - Light Gradient Boosting\\n            * 'catboost' - CatBoost Classifier\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and model is trained\\n            on entire training dataset.\\n\\n        predict: bool, default = True\\n            Whether to predict model on holdout if cross_validation == False.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        refit: bool, default = True\\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        system: bool, default = True\\n            Must remain True all times. Only to be changed by internal functions.\\n            If False, method will return a tuple of model and the model fit time.\\n\\n        add_to_model_list: bool, default = True\\n            Whether to save model and results in _master_model_container.\\n\\n        X_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training features.\\n            Intended to be only changed by internal functions.\\n\\n        y_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training target.\\n            Intended to be only changed by internal functions.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the estimator.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are highlighted in yellow.\\n\\n        model\\n            trained model object\\n\\n        Warnings\\n        --------\\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\\n        returned as zero (0.0)\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0)\\n\\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\\n        more than quadratic. These estimators are hard to scale on datasets with more\\n        than 10,000 samples.\\n\\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\\n\\n        \"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)",
            "def create_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, cross_validation: bool=True, predict: bool=True, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, refit: bool=True, probability_threshold: Optional[float]=None, experiment_custom_tags: Optional[Dict[str, Any]]=None, verbose: bool=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function creates a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        setup() function must be called before using create_model()\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n\\n        This will create a trained Logistic Regression model.\\n\\n        Parameters\\n        ----------\\n        estimator : str / object, default = None\\n            Enter ID of the estimators available in model library or pass an untrained model\\n            object consistent with fit / predict API to train and evaluate model. All\\n            estimators support binary or multiclass problem. List of estimators in model\\n            library (ID - Name):\\n\\n            * 'lr' - Logistic Regression\\n            * 'knn' - K Nearest Neighbour\\n            * 'nb' - Naive Bayes\\n            * 'dt' - Decision Tree Classifier\\n            * 'svm' - SVM - Linear Kernel\\n            * 'rbfsvm' - SVM - Radial Kernel\\n            * 'gpc' - Gaussian Process Classifier\\n            * 'mlp' - Multi Level Perceptron\\n            * 'ridge' - Ridge Classifier\\n            * 'rf' - Random Forest Classifier\\n            * 'qda' - Quadratic Discriminant Analysis\\n            * 'ada' - Ada Boost Classifier\\n            * 'gbc' - Gradient Boosting Classifier\\n            * 'lda' - Linear Discriminant Analysis\\n            * 'et' - Extra Trees Classifier\\n            * 'xgboost' - Extreme Gradient Boosting\\n            * 'lightgbm' - Light Gradient Boosting\\n            * 'catboost' - CatBoost Classifier\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        cross_validation: bool, default = True\\n            When cross_validation set to False fold parameter is ignored and model is trained\\n            on entire training dataset.\\n\\n        predict: bool, default = True\\n            Whether to predict model on holdout if cross_validation == False.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        refit: bool, default = True\\n            Whether to refit the model on the entire dataset after CV. Ignored if cross_validation == False.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        system: bool, default = True\\n            Must remain True all times. Only to be changed by internal functions.\\n            If False, method will return a tuple of model and the model fit time.\\n\\n        add_to_model_list: bool, default = True\\n            Whether to save model and results in _master_model_container.\\n\\n        X_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training features.\\n            Intended to be only changed by internal functions.\\n\\n        y_train_data: pandas.DataFrame, default = None\\n            If not None, will use this dataframe as training target.\\n            Intended to be only changed by internal functions.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the estimator.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are highlighted in yellow.\\n\\n        model\\n            trained model object\\n\\n        Warnings\\n        --------\\n        - 'svm' and 'ridge' doesn't support predict_proba method. As such, AUC will be\\n        returned as zero (0.0)\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0)\\n\\n        - 'rbfsvm' and 'gpc' uses non-linear kernel and hence the fit time complexity is\\n        more than quadratic. These estimators are hard to scale on datasets with more\\n        than 10,000 samples.\\n\\n        - If cross_validation parameter is set to False, model will not be logged with MLFlow.\\n\\n        \"\n    assert not any((x in ('system', 'add_to_model_list', 'X_train_data', 'y_train_data', 'metrics') for x in kwargs))\n    return self._create_model(estimator=estimator, fold=fold, round=round, cross_validation=cross_validation, predict=predict, fit_kwargs=fit_kwargs, groups=groups, refit=refit, probability_threshold=probability_threshold, experiment_custom_tags=experiment_custom_tags, verbose=verbose, return_train_score=return_train_score, **kwargs)"
        ]
    },
    {
        "func_name": "get_iter",
        "original": "def get_iter(x):\n    if isinstance(x, dict):\n        return x.values()\n    return x",
        "mutated": [
            "def get_iter(x):\n    if False:\n        i = 10\n    if isinstance(x, dict):\n        return x.values()\n    return x",
            "def get_iter(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(x, dict):\n        return x.values()\n    return x",
            "def get_iter(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(x, dict):\n        return x.values()\n    return x",
            "def get_iter(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(x, dict):\n        return x.values()\n    return x",
            "def get_iter(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(x, dict):\n        return x.values()\n    return x"
        ]
    },
    {
        "func_name": "total_combinations_in_grid",
        "original": "def total_combinations_in_grid(grid):\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc",
        "mutated": [
            "def total_combinations_in_grid(grid):\n    if False:\n        i = 10\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc",
            "def total_combinations_in_grid(grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc",
            "def total_combinations_in_grid(grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc",
            "def total_combinations_in_grid(grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc",
            "def total_combinations_in_grid(grid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nc = 1\n\n    def get_iter(x):\n        if isinstance(x, dict):\n            return x.values()\n        return x\n    for v in get_iter(grid):\n        if isinstance(v, dict):\n            for v2 in get_iter(v):\n                nc *= len(v2)\n        else:\n            nc *= len(v)\n    return nc"
        ]
    },
    {
        "func_name": "get_optuna_tpe_sampler",
        "original": "def get_optuna_tpe_sampler():\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler",
        "mutated": [
            "def get_optuna_tpe_sampler():\n    if False:\n        i = 10\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler",
            "def get_optuna_tpe_sampler():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler",
            "def get_optuna_tpe_sampler():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler",
            "def get_optuna_tpe_sampler():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler",
            "def get_optuna_tpe_sampler():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    try:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n    except TypeError:\n        tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n    return tpe_sampler"
        ]
    },
    {
        "func_name": "tune_model",
        "original": "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    \"\"\"\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\n        The output prints a score grid that shows Accuracy, AUC, Recall\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\n\n        This function returns a trained model object.\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> xgboost = create_model('xgboost')\n        >>> tuned_xgboost = tune_model(xgboost)\n\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\n\n\n        Parameters\n        ----------\n        estimator : object, default = None\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        n_iter: integer, default = 10\n            Number of iterations within the Random Grid Search. For every iteration,\n            the model randomly selects one value from the pre-defined grid of\n            hyperparameters.\n\n        custom_grid: dictionary, default = None\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\n            Custom grids must be in a format supported by the chosen search library.\n\n        optimize: str, default = 'Accuracy'\n            Measure used to select the best model through hyperparameter tuning.\n            Can be either a string representing a metric or a custom scorer object\n            created using sklearn.make_scorer.\n\n        custom_scorer: object, default = None\n            Will be eventually depreciated.\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\n            created using sklearn.make_scorer.\n\n        search_library: str, default = 'scikit-learn'\n            The search library used to tune hyperparameters.\n            Possible values:\n\n            - 'scikit-learn' - default, requires no further installation\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\n\n        search_algorithm: str, default = None\n            The search algorithm depends on the ``search_library`` parameter.\n            Some search algorithms require additional libraries to be installed.\n            If None, will use search library-specific default algorithm.\n\n            - 'scikit-learn' possible values:\n                - 'random' : random grid search (default)\n                - 'grid' : grid search\n\n            - 'scikit-optimize' possible values:\n                - 'bayesian' : Bayesian search (default)\n\n            - 'tune-sklearn' possible values:\n                - 'random' : random grid search (default)\n                - 'grid' : grid search\n                - 'bayesian' : ``pip install scikit-optimize``\n                - 'hyperopt' : ``pip install hyperopt``\n                - 'optuna' : ``pip install optuna``\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\n\n            - 'optuna' possible values:\n                - 'random' : randomized search\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\n\n        early_stopping: bool or str or object, default = False\n            Use early stopping to stop fitting to a hyperparameter configuration\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\n            if the estimator doesn't have partial_fit attribute.\n            If False or None, early stopping will not be used.\n            Can be either an object accepted by the search library or one of the\n            following:\n\n            - 'asha' for Asynchronous Successive Halving Algorithm\n            - 'hyperband' for Hyperband\n            - 'median' for median stopping rule\n            - If False or None, early stopping will not be used.\n\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\n\n        early_stopping_max_iters: int, default = 10\n            Maximum number of epochs to run for each sampled configuration.\n            Ignored if early_stopping is False or None.\n\n        choose_better: bool, default = False\n            When set to set to True, base estimator is returned when the performance doesn't\n            improve by tune_model. This guarantees the returned object would perform at least\n            equivalent to base estimator created using create_model or model returned by\n            compare_models.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the tuner.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        return_tuner: bool, default = False\n            If True, will return a tuple of (model, tuner_object). Otherwise,\n            will return just the best model.\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n        tuner_verbose: bool or in, default = True\n            If True or above 0, will print messages from the tuner. Higher values\n            print more messages. Ignored if verbose parameter is False.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        **kwargs:\n            Additional keyword arguments to pass to the optimizer.\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are also returned.\n\n        model\n            Trained and tuned model object.\n\n        tuner_object\n            Only if return_tuner parameter is True. The object used for tuning.\n\n        Notes\n        -----\n\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\n        will be tuned.\n\n        - If a VotingClassifier is passed, the weights will be tuned.\n\n        Warnings\n        --------\n\n        - Using 'Grid' search algorithm with default parameter grids may result in very\n        long computation.\n\n\n        \"\"\"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model",
        "mutated": [
            "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall\\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> xgboost = create_model('xgboost')\\n        >>> tuned_xgboost = tune_model(xgboost)\\n\\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\\n\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        n_iter: integer, default = 10\\n            Number of iterations within the Random Grid Search. For every iteration,\\n            the model randomly selects one value from the pre-defined grid of\\n            hyperparameters.\\n\\n        custom_grid: dictionary, default = None\\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\\n            Custom grids must be in a format supported by the chosen search library.\\n\\n        optimize: str, default = 'Accuracy'\\n            Measure used to select the best model through hyperparameter tuning.\\n            Can be either a string representing a metric or a custom scorer object\\n            created using sklearn.make_scorer.\\n\\n        custom_scorer: object, default = None\\n            Will be eventually depreciated.\\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\\n            created using sklearn.make_scorer.\\n\\n        search_library: str, default = 'scikit-learn'\\n            The search library used to tune hyperparameters.\\n            Possible values:\\n\\n            - 'scikit-learn' - default, requires no further installation\\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\\n\\n        search_algorithm: str, default = None\\n            The search algorithm depends on the ``search_library`` parameter.\\n            Some search algorithms require additional libraries to be installed.\\n            If None, will use search library-specific default algorithm.\\n\\n            - 'scikit-learn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n\\n            - 'scikit-optimize' possible values:\\n                - 'bayesian' : Bayesian search (default)\\n\\n            - 'tune-sklearn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n                - 'bayesian' : ``pip install scikit-optimize``\\n                - 'hyperopt' : ``pip install hyperopt``\\n                - 'optuna' : ``pip install optuna``\\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\\n\\n            - 'optuna' possible values:\\n                - 'random' : randomized search\\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\\n\\n        early_stopping: bool or str or object, default = False\\n            Use early stopping to stop fitting to a hyperparameter configuration\\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\\n            if the estimator doesn't have partial_fit attribute.\\n            If False or None, early stopping will not be used.\\n            Can be either an object accepted by the search library or one of the\\n            following:\\n\\n            - 'asha' for Asynchronous Successive Halving Algorithm\\n            - 'hyperband' for Hyperband\\n            - 'median' for median stopping rule\\n            - If False or None, early stopping will not be used.\\n\\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\\n\\n        early_stopping_max_iters: int, default = 10\\n            Maximum number of epochs to run for each sampled configuration.\\n            Ignored if early_stopping is False or None.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the performance doesn't\\n            improve by tune_model. This guarantees the returned object would perform at least\\n            equivalent to base estimator created using create_model or model returned by\\n            compare_models.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the tuner.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        return_tuner: bool, default = False\\n            If True, will return a tuple of (model, tuner_object). Otherwise,\\n            will return just the best model.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        tuner_verbose: bool or in, default = True\\n            If True or above 0, will print messages from the tuner. Higher values\\n            print more messages. Ignored if verbose parameter is False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained and tuned model object.\\n\\n        tuner_object\\n            Only if return_tuner parameter is True. The object used for tuning.\\n\\n        Notes\\n        -----\\n\\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\\n        will be tuned.\\n\\n        - If a VotingClassifier is passed, the weights will be tuned.\\n\\n        Warnings\\n        --------\\n\\n        - Using 'Grid' search algorithm with default parameter grids may result in very\\n        long computation.\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model",
            "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall\\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> xgboost = create_model('xgboost')\\n        >>> tuned_xgboost = tune_model(xgboost)\\n\\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\\n\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        n_iter: integer, default = 10\\n            Number of iterations within the Random Grid Search. For every iteration,\\n            the model randomly selects one value from the pre-defined grid of\\n            hyperparameters.\\n\\n        custom_grid: dictionary, default = None\\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\\n            Custom grids must be in a format supported by the chosen search library.\\n\\n        optimize: str, default = 'Accuracy'\\n            Measure used to select the best model through hyperparameter tuning.\\n            Can be either a string representing a metric or a custom scorer object\\n            created using sklearn.make_scorer.\\n\\n        custom_scorer: object, default = None\\n            Will be eventually depreciated.\\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\\n            created using sklearn.make_scorer.\\n\\n        search_library: str, default = 'scikit-learn'\\n            The search library used to tune hyperparameters.\\n            Possible values:\\n\\n            - 'scikit-learn' - default, requires no further installation\\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\\n\\n        search_algorithm: str, default = None\\n            The search algorithm depends on the ``search_library`` parameter.\\n            Some search algorithms require additional libraries to be installed.\\n            If None, will use search library-specific default algorithm.\\n\\n            - 'scikit-learn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n\\n            - 'scikit-optimize' possible values:\\n                - 'bayesian' : Bayesian search (default)\\n\\n            - 'tune-sklearn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n                - 'bayesian' : ``pip install scikit-optimize``\\n                - 'hyperopt' : ``pip install hyperopt``\\n                - 'optuna' : ``pip install optuna``\\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\\n\\n            - 'optuna' possible values:\\n                - 'random' : randomized search\\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\\n\\n        early_stopping: bool or str or object, default = False\\n            Use early stopping to stop fitting to a hyperparameter configuration\\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\\n            if the estimator doesn't have partial_fit attribute.\\n            If False or None, early stopping will not be used.\\n            Can be either an object accepted by the search library or one of the\\n            following:\\n\\n            - 'asha' for Asynchronous Successive Halving Algorithm\\n            - 'hyperband' for Hyperband\\n            - 'median' for median stopping rule\\n            - If False or None, early stopping will not be used.\\n\\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\\n\\n        early_stopping_max_iters: int, default = 10\\n            Maximum number of epochs to run for each sampled configuration.\\n            Ignored if early_stopping is False or None.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the performance doesn't\\n            improve by tune_model. This guarantees the returned object would perform at least\\n            equivalent to base estimator created using create_model or model returned by\\n            compare_models.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the tuner.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        return_tuner: bool, default = False\\n            If True, will return a tuple of (model, tuner_object). Otherwise,\\n            will return just the best model.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        tuner_verbose: bool or in, default = True\\n            If True or above 0, will print messages from the tuner. Higher values\\n            print more messages. Ignored if verbose parameter is False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained and tuned model object.\\n\\n        tuner_object\\n            Only if return_tuner parameter is True. The object used for tuning.\\n\\n        Notes\\n        -----\\n\\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\\n        will be tuned.\\n\\n        - If a VotingClassifier is passed, the weights will be tuned.\\n\\n        Warnings\\n        --------\\n\\n        - Using 'Grid' search algorithm with default parameter grids may result in very\\n        long computation.\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model",
            "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall\\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> xgboost = create_model('xgboost')\\n        >>> tuned_xgboost = tune_model(xgboost)\\n\\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\\n\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        n_iter: integer, default = 10\\n            Number of iterations within the Random Grid Search. For every iteration,\\n            the model randomly selects one value from the pre-defined grid of\\n            hyperparameters.\\n\\n        custom_grid: dictionary, default = None\\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\\n            Custom grids must be in a format supported by the chosen search library.\\n\\n        optimize: str, default = 'Accuracy'\\n            Measure used to select the best model through hyperparameter tuning.\\n            Can be either a string representing a metric or a custom scorer object\\n            created using sklearn.make_scorer.\\n\\n        custom_scorer: object, default = None\\n            Will be eventually depreciated.\\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\\n            created using sklearn.make_scorer.\\n\\n        search_library: str, default = 'scikit-learn'\\n            The search library used to tune hyperparameters.\\n            Possible values:\\n\\n            - 'scikit-learn' - default, requires no further installation\\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\\n\\n        search_algorithm: str, default = None\\n            The search algorithm depends on the ``search_library`` parameter.\\n            Some search algorithms require additional libraries to be installed.\\n            If None, will use search library-specific default algorithm.\\n\\n            - 'scikit-learn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n\\n            - 'scikit-optimize' possible values:\\n                - 'bayesian' : Bayesian search (default)\\n\\n            - 'tune-sklearn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n                - 'bayesian' : ``pip install scikit-optimize``\\n                - 'hyperopt' : ``pip install hyperopt``\\n                - 'optuna' : ``pip install optuna``\\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\\n\\n            - 'optuna' possible values:\\n                - 'random' : randomized search\\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\\n\\n        early_stopping: bool or str or object, default = False\\n            Use early stopping to stop fitting to a hyperparameter configuration\\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\\n            if the estimator doesn't have partial_fit attribute.\\n            If False or None, early stopping will not be used.\\n            Can be either an object accepted by the search library or one of the\\n            following:\\n\\n            - 'asha' for Asynchronous Successive Halving Algorithm\\n            - 'hyperband' for Hyperband\\n            - 'median' for median stopping rule\\n            - If False or None, early stopping will not be used.\\n\\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\\n\\n        early_stopping_max_iters: int, default = 10\\n            Maximum number of epochs to run for each sampled configuration.\\n            Ignored if early_stopping is False or None.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the performance doesn't\\n            improve by tune_model. This guarantees the returned object would perform at least\\n            equivalent to base estimator created using create_model or model returned by\\n            compare_models.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the tuner.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        return_tuner: bool, default = False\\n            If True, will return a tuple of (model, tuner_object). Otherwise,\\n            will return just the best model.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        tuner_verbose: bool or in, default = True\\n            If True or above 0, will print messages from the tuner. Higher values\\n            print more messages. Ignored if verbose parameter is False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained and tuned model object.\\n\\n        tuner_object\\n            Only if return_tuner parameter is True. The object used for tuning.\\n\\n        Notes\\n        -----\\n\\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\\n        will be tuned.\\n\\n        - If a VotingClassifier is passed, the weights will be tuned.\\n\\n        Warnings\\n        --------\\n\\n        - Using 'Grid' search algorithm with default parameter grids may result in very\\n        long computation.\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model",
            "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall\\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> xgboost = create_model('xgboost')\\n        >>> tuned_xgboost = tune_model(xgboost)\\n\\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\\n\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        n_iter: integer, default = 10\\n            Number of iterations within the Random Grid Search. For every iteration,\\n            the model randomly selects one value from the pre-defined grid of\\n            hyperparameters.\\n\\n        custom_grid: dictionary, default = None\\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\\n            Custom grids must be in a format supported by the chosen search library.\\n\\n        optimize: str, default = 'Accuracy'\\n            Measure used to select the best model through hyperparameter tuning.\\n            Can be either a string representing a metric or a custom scorer object\\n            created using sklearn.make_scorer.\\n\\n        custom_scorer: object, default = None\\n            Will be eventually depreciated.\\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\\n            created using sklearn.make_scorer.\\n\\n        search_library: str, default = 'scikit-learn'\\n            The search library used to tune hyperparameters.\\n            Possible values:\\n\\n            - 'scikit-learn' - default, requires no further installation\\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\\n\\n        search_algorithm: str, default = None\\n            The search algorithm depends on the ``search_library`` parameter.\\n            Some search algorithms require additional libraries to be installed.\\n            If None, will use search library-specific default algorithm.\\n\\n            - 'scikit-learn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n\\n            - 'scikit-optimize' possible values:\\n                - 'bayesian' : Bayesian search (default)\\n\\n            - 'tune-sklearn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n                - 'bayesian' : ``pip install scikit-optimize``\\n                - 'hyperopt' : ``pip install hyperopt``\\n                - 'optuna' : ``pip install optuna``\\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\\n\\n            - 'optuna' possible values:\\n                - 'random' : randomized search\\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\\n\\n        early_stopping: bool or str or object, default = False\\n            Use early stopping to stop fitting to a hyperparameter configuration\\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\\n            if the estimator doesn't have partial_fit attribute.\\n            If False or None, early stopping will not be used.\\n            Can be either an object accepted by the search library or one of the\\n            following:\\n\\n            - 'asha' for Asynchronous Successive Halving Algorithm\\n            - 'hyperband' for Hyperband\\n            - 'median' for median stopping rule\\n            - If False or None, early stopping will not be used.\\n\\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\\n\\n        early_stopping_max_iters: int, default = 10\\n            Maximum number of epochs to run for each sampled configuration.\\n            Ignored if early_stopping is False or None.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the performance doesn't\\n            improve by tune_model. This guarantees the returned object would perform at least\\n            equivalent to base estimator created using create_model or model returned by\\n            compare_models.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the tuner.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        return_tuner: bool, default = False\\n            If True, will return a tuple of (model, tuner_object). Otherwise,\\n            will return just the best model.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        tuner_verbose: bool or in, default = True\\n            If True or above 0, will print messages from the tuner. Higher values\\n            print more messages. Ignored if verbose parameter is False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained and tuned model object.\\n\\n        tuner_object\\n            Only if return_tuner parameter is True. The object used for tuning.\\n\\n        Notes\\n        -----\\n\\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\\n        will be tuned.\\n\\n        - If a VotingClassifier is passed, the weights will be tuned.\\n\\n        Warnings\\n        --------\\n\\n        - Using 'Grid' search algorithm with default parameter grids may result in very\\n        long computation.\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model",
            "def tune_model(self, estimator, fold: Optional[Union[int, Any]]=None, round: int=4, n_iter: int=10, custom_grid: Optional[Union[Dict[str, list], Any]]=None, optimize: str='Accuracy', custom_scorer=None, search_library: str='scikit-learn', search_algorithm: Optional[str]=None, early_stopping: Any=False, early_stopping_max_iters: int=10, choose_better: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, return_tuner: bool=False, verbose: bool=True, tuner_verbose: Union[int, bool]=True, return_train_score: bool=False, **kwargs) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function tunes the hyperparameters of a model and scores it using Cross Validation.\\n        The output prints a score grid that shows Accuracy, AUC, Recall\\n        Precision, F1, Kappa and MCC by fold (by default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> xgboost = create_model('xgboost')\\n        >>> tuned_xgboost = tune_model(xgboost)\\n\\n        This will tune the hyperparameters of Extreme Gradient Boosting Classifier.\\n\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        n_iter: integer, default = 10\\n            Number of iterations within the Random Grid Search. For every iteration,\\n            the model randomly selects one value from the pre-defined grid of\\n            hyperparameters.\\n\\n        custom_grid: dictionary, default = None\\n            To use custom hyperparameters for tuning pass a dictionary with parameter name\\n            and values to be iterated. When set to None it uses pre-defined tuning grid.\\n            Custom grids must be in a format supported by the chosen search library.\\n\\n        optimize: str, default = 'Accuracy'\\n            Measure used to select the best model through hyperparameter tuning.\\n            Can be either a string representing a metric or a custom scorer object\\n            created using sklearn.make_scorer.\\n\\n        custom_scorer: object, default = None\\n            Will be eventually depreciated.\\n            custom_scorer can be passed to tune hyperparameters of the model. It must be\\n            created using sklearn.make_scorer.\\n\\n        search_library: str, default = 'scikit-learn'\\n            The search library used to tune hyperparameters.\\n            Possible values:\\n\\n            - 'scikit-learn' - default, requires no further installation\\n            - 'scikit-optimize' - scikit-optimize. ``pip install scikit-optimize`` https://scikit-optimize.github.io/stable/\\n            - 'tune-sklearn' - Ray Tune scikit API. Does not support GPU models.\\n            ``pip install tune-sklearn ray[tune]`` https://github.com/ray-project/tune-sklearn\\n            - 'optuna' - Optuna. ``pip install optuna`` https://optuna.org/\\n\\n        search_algorithm: str, default = None\\n            The search algorithm depends on the ``search_library`` parameter.\\n            Some search algorithms require additional libraries to be installed.\\n            If None, will use search library-specific default algorithm.\\n\\n            - 'scikit-learn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n\\n            - 'scikit-optimize' possible values:\\n                - 'bayesian' : Bayesian search (default)\\n\\n            - 'tune-sklearn' possible values:\\n                - 'random' : random grid search (default)\\n                - 'grid' : grid search\\n                - 'bayesian' : ``pip install scikit-optimize``\\n                - 'hyperopt' : ``pip install hyperopt``\\n                - 'optuna' : ``pip install optuna``\\n                - 'bohb' : ``pip install hpbandster ConfigSpace``\\n\\n            - 'optuna' possible values:\\n                - 'random' : randomized search\\n                - 'tpe' : Tree-structured Parzen Estimator search (default)\\n\\n        early_stopping: bool or str or object, default = False\\n            Use early stopping to stop fitting to a hyperparameter configuration\\n            if it performs poorly. Ignored if search_library is ``scikit-learn``, or\\n            if the estimator doesn't have partial_fit attribute.\\n            If False or None, early stopping will not be used.\\n            Can be either an object accepted by the search library or one of the\\n            following:\\n\\n            - 'asha' for Asynchronous Successive Halving Algorithm\\n            - 'hyperband' for Hyperband\\n            - 'median' for median stopping rule\\n            - If False or None, early stopping will not be used.\\n\\n            More info for Optuna - https://optuna.readthedocs.io/en/stable/reference/pruners.html\\n            More info for Ray Tune (tune-sklearn) - https://docs.ray.io/en/master/tune/api_docs/schedulers.html\\n\\n        early_stopping_max_iters: int, default = 10\\n            Maximum number of epochs to run for each sampled configuration.\\n            Ignored if early_stopping is False or None.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the performance doesn't\\n            improve by tune_model. This guarantees the returned object would perform at least\\n            equivalent to base estimator created using create_model or model returned by\\n            compare_models.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the tuner.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        return_tuner: bool, default = False\\n            If True, will return a tuple of (model, tuner_object). Otherwise,\\n            will return just the best model.\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        tuner_verbose: bool or in, default = True\\n            If True or above 0, will print messages from the tuner. Higher values\\n            print more messages. Ignored if verbose parameter is False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the optimizer.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained and tuned model object.\\n\\n        tuner_object\\n            Only if return_tuner parameter is True. The object used for tuning.\\n\\n        Notes\\n        -----\\n\\n        - If a StackingClassifier is passed, the hyperparameters of the meta model (final_estimator)\\n        will be tuned.\\n\\n        - If a VotingClassifier is passed, the weights will be tuned.\\n\\n        Warnings\\n        --------\\n\\n        - Using 'Grid' search algorithm with default parameter grids may result in very\\n        long computation.\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing tune_model()')\n    self.logger.info(f'tune_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if type(estimator) is str:\n        raise TypeError('The behavior of tune_model in version 1.0.1 is changed. Please pass trained model object.')\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(n_iter) is not int:\n        raise TypeError('n_iter parameter only accepts integer value.')\n    possible_early_stopping = ['asha', 'Hyperband', 'Median']\n    if isinstance(early_stopping, str) and early_stopping not in possible_early_stopping:\n        raise TypeError(f\"early_stopping parameter must be one of {', '.join(possible_early_stopping)}\")\n    if type(early_stopping_max_iters) is not int:\n        raise TypeError('early_stopping_max_iters parameter only accepts integer value.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    possible_search_libraries = ['scikit-learn', 'scikit-optimize', 'tune-sklearn', 'optuna']\n    search_library = search_library.lower()\n    if search_library not in possible_search_libraries:\n        raise ValueError(f\"search_library parameter must be one of {', '.join(possible_search_libraries)}\")\n    if search_library == 'scikit-optimize':\n        _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n        import skopt\n        if not search_algorithm:\n            search_algorithm = 'bayesian'\n        possible_search_algorithms = ['bayesian']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-optimize' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    elif search_library == 'tune-sklearn':\n        _check_soft_dependencies('tune_sklearn', extra='tuners', severity='error', install_name='tune-sklearn ray[tune]')\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid', 'bayesian', 'hyperopt', 'optuna', 'bohb']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'tune-sklearn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n        if search_algorithm == 'bohb':\n            _check_soft_dependencies('ConfigSpace', extra=None, severity='error')\n            _check_soft_dependencies('hpbandster', extra=None, severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'hyperopt':\n            _check_soft_dependencies('hyperopt', extra='tuners', severity='error')\n            _check_soft_dependencies('ray', extra='tuners', severity='error', install_name='ray[tune]')\n        elif search_algorithm == 'bayesian':\n            _check_soft_dependencies('skopt', extra='tuners', severity='error', install_name='scikit-optimize')\n            import skopt\n        elif search_algorithm == 'optuna':\n            _check_soft_dependencies('optuna', extra='tuners', severity='error')\n            import optuna\n    elif search_library == 'optuna':\n        _check_soft_dependencies('optuna', extra='tuners', severity='error')\n        import optuna\n        if not search_algorithm:\n            search_algorithm = 'tpe'\n        possible_search_algorithms = ['random', 'tpe']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'optuna' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    else:\n        if not search_algorithm:\n            search_algorithm = 'random'\n        possible_search_algorithms = ['random', 'grid']\n        if search_algorithm not in possible_search_algorithms:\n            raise ValueError(f\"For 'scikit-learn' search_algorithm parameter must be one of {', '.join(possible_search_algorithms)}\")\n    if custom_scorer is not None:\n        optimize = custom_scorer\n        warnings.warn('custom_scorer parameter will be depreciated, use optimize instead', DeprecationWarning, stacklevel=2)\n    if isinstance(optimize, str):\n        optimize = self._get_metric_by_name_or_id(optimize)\n        if optimize is None:\n            raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n        if self.is_multiclass:\n            if not optimize.is_multiclass:\n                raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    else:\n        self.logger.info(f'optimize set to user defined function {optimize}')\n    if type(verbose) is not bool:\n        raise TypeError('verbose parameter can only take argument as True or False.')\n    if type(return_tuner) is not bool:\n        raise TypeError('return_tuner parameter can only take argument as True or False.')\n    if not verbose:\n        tuner_verbose = 0\n    if type(tuner_verbose) not in (bool, int):\n        raise TypeError('tuner_verbose parameter must be a bool or an int.')\n    tuner_verbose = int(tuner_verbose)\n    if tuner_verbose < 0:\n        tuner_verbose = 0\n    elif tuner_verbose > 2:\n        tuner_verbose = 2\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 3 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    import logging\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    data_X = self.X_train\n    data_y = self.y_train\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    is_stacked_model = False\n    if isinstance(estimator, Pipeline):\n        estimator = self._get_final_model_from_pipeline(estimator)\n    if hasattr(estimator, 'final_estimator'):\n        self.logger.info('Model is stacked, using the definition of the meta-model')\n        is_stacked_model = True\n        estimator_id = self._get_model_id(estimator.final_estimator)\n    else:\n        estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        if custom_grid is None:\n            raise ValueError(\"When passing a model not in PyCaret's model library, the custom_grid parameter must be provided.\")\n        estimator_name = self._get_model_name(estimator)\n        estimator_definition = None\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    if estimator_definition is None or estimator_definition.tunable is None:\n        model = clone(estimator)\n    else:\n        self.logger.info('Model has a special tunable class, using that')\n        model = clone(estimator_definition.tunable(**estimator.get_params()))\n    base_estimator = model\n    display.update_monitor(2, estimator_name)\n    display.move_progress()\n    self.logger.info('Declaring metric variables')\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Searching Hyperparameters')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Defining Hyperparameters')\n    from pycaret.internal.tunable import VotingClassifier, VotingRegressor\n\n    def total_combinations_in_grid(grid):\n        nc = 1\n\n        def get_iter(x):\n            if isinstance(x, dict):\n                return x.values()\n            return x\n        for v in get_iter(grid):\n            if isinstance(v, dict):\n                for v2 in get_iter(v):\n                    nc *= len(v2)\n            else:\n                nc *= len(v)\n        return nc\n    if custom_grid is not None:\n        if not isinstance(custom_grid, dict):\n            raise TypeError(f'custom_grid must be a dict, got {type(custom_grid)}.')\n        param_grid = custom_grid\n        if not (search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random'))):\n            param_grid = {k: CategoricalDistribution(v) if isinstance(v, Iterable) else v for (k, v) in param_grid.items()}\n        elif any((isinstance(v, Distribution) for (k, v) in param_grid.items())):\n            raise TypeError(f\"For the combination of search_library {search_library} and search_algorithm {search_algorithm}, PyCaret Distribution objects are not supported. Pass a list or other object supported by the search library (in most cases, an object with a 'rvs' function).\")\n    elif search_library == 'scikit-learn' or (search_library == 'tune-sklearn' and (search_algorithm == 'grid' or search_algorithm == 'random')):\n        param_grid = estimator_definition.tune_grid\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': np.arange(0.01, 1, 0.01) for (i, e) in enumerate(base_estimator.estimators)}\n        if search_algorithm != 'grid':\n            tc = total_combinations_in_grid(param_grid)\n            if tc <= n_iter:\n                self.logger.info(f'{n_iter} is bigger than total combinations {tc}, setting search algorithm to grid')\n                search_algorithm = 'grid'\n    else:\n        param_grid = estimator_definition.tune_distribution\n        if isinstance(base_estimator, (VotingClassifier, VotingRegressor)):\n            param_grid = {f'weight_{i}': UniformDistribution(1e-09, 1) for (i, e) in enumerate(base_estimator.estimators)}\n    if not param_grid:\n        raise ValueError('parameter grid for tuning is empty. If passing custom_grid, make sure that it is not empty. If not passing custom_grid, the passed estimator does not have a built-in tuning grid.')\n    suffixes = []\n    if is_stacked_model:\n        self.logger.info('Stacked model passed, will tune meta model hyperparameters')\n        suffixes.append('final_estimator')\n    gc.collect()\n    with estimator_pipeline(self.pipeline, model) as pipeline_with_model:\n        extra_params = {}\n        fit_kwargs = get_pipeline_fit_kwargs(pipeline_with_model, fit_kwargs)\n        actual_estimator_label = get_pipeline_estimator_label(pipeline_with_model)\n        suffixes.append(actual_estimator_label)\n        suffixes = '__'.join(reversed(suffixes))\n        param_grid = {f'{suffixes}__{k}': v for (k, v) in param_grid.items()}\n        if estimator_definition is not None:\n            search_kwargs = {**estimator_definition.tune_args, **kwargs}\n            n_jobs = self.gpu_n_jobs_param if estimator_definition.is_gpu_enabled else self.n_jobs_param\n        else:\n            search_kwargs = {}\n            n_jobs = self.n_jobs_param\n        if custom_grid is not None:\n            self.logger.info(f'custom_grid: {param_grid}')\n        from sklearn.gaussian_process import GaussianProcessClassifier\n        if isinstance(pipeline_with_model.steps[-1][1], GaussianProcessClassifier):\n            n_jobs = 1\n        self.logger.info(f'Tuning with n_jobs={n_jobs}')\n\n        def get_optuna_tpe_sampler():\n            try:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True, constant_liar=True)\n            except TypeError:\n                tpe_sampler = optuna.samplers.TPESampler(seed=self.seed, multivariate=True)\n            return tpe_sampler\n        if search_library == 'optuna':\n            logging.getLogger('optuna').setLevel(logging.WARNING)\n            pruner_translator = {'asha': optuna.pruners.SuccessiveHalvingPruner(), 'hyperband': optuna.pruners.HyperbandPruner(), 'median': optuna.pruners.MedianPruner(), False: optuna.pruners.NopPruner(), None: optuna.pruners.NopPruner()}\n            pruner = early_stopping\n            if pruner in pruner_translator:\n                pruner = pruner_translator[early_stopping]\n            sampler_translator = {'tpe': get_optuna_tpe_sampler(), 'random': optuna.samplers.RandomSampler(seed=self.seed)}\n            sampler = sampler_translator[search_algorithm]\n            try:\n                param_grid = get_optuna_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            study = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n            self.logger.info('Initializing optuna.integration.OptunaSearchCV')\n            model_grid = optuna.integration.OptunaSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, cv=fold, enable_pruning=early_stopping and can_early_stop(pipeline_with_model, True, False, False, param_grid), max_iter=early_stopping_max_iters, n_jobs=n_jobs, n_trials=n_iter, random_state=self.seed, scoring=optimize, study=study, refit=False, verbose=tuner_verbose, error_score='raise', **search_kwargs)\n        elif search_library == 'tune-sklearn':\n            early_stopping_translator = {'asha': 'ASHAScheduler', 'hyperband': 'HyperBandScheduler', 'median': 'MedianStoppingRule'}\n            if early_stopping in early_stopping_translator:\n                early_stopping = early_stopping_translator[early_stopping]\n            do_early_stop = early_stopping and can_early_stop(pipeline_with_model, True, True, True, param_grid)\n            if not do_early_stop and search_algorithm == 'bohb':\n                raise ValueError(\"'bohb' requires early_stopping = True and the estimator to support early stopping (has partial_fit, warm_start or is an XGBoost model).\")\n            elif early_stopping and can_early_stop(pipeline_with_model, False, True, False, param_grid):\n                if 'actual_estimator__n_estimators' in param_grid:\n                    if custom_grid is None:\n                        extra_params['actual_estimator__n_estimators'] = pipeline_with_model.get_params()['actual_estimator__n_estimators']\n                        param_grid.pop('actual_estimator__n_estimators')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n                if 'actual_estimator__max_iter' in param_grid:\n                    if custom_grid is None:\n                        param_grid.pop('actual_estimator__max_iter')\n                    else:\n                        raise ValueError('parameter grid cannot contain n_estimators or max_iter if early_stopping is True and the model is warm started. Use early_stopping_max_iters params to set the upper bound of n_estimators or max_iter.')\n            from tune_sklearn import TuneGridSearchCV, TuneSearchCV\n            with true_warm_start(pipeline_with_model) if do_early_stop else nullcontext():\n                if search_algorithm == 'grid':\n                    self.logger.info('Initializing tune_sklearn.TuneGridSearchCV')\n                    model_grid = TuneGridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, early_stopping=do_early_stop, scoring=optimize, cv=fold, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=False, verbose=tuner_verbose, pipeline_auto_early_stop=True, **search_kwargs)\n                else:\n                    if search_algorithm == 'hyperopt':\n                        try:\n                            param_grid = get_hyperopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bayesian':\n                        try:\n                            param_grid = get_skopt_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm == 'bohb':\n                        try:\n                            param_grid = get_CS_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    elif search_algorithm != 'random':\n                        try:\n                            param_grid = get_tune_distributions(param_grid)\n                        except Exception:\n                            self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                            self.logger.warning(traceback.format_exc())\n                    self.logger.info(f'Initializing tune_sklearn.TuneSearchCV, {search_algorithm}')\n                    if search_algorithm == 'optuna' and 'sampler' not in search_kwargs:\n                        import optuna\n                        search_kwargs['sampler'] = get_optuna_tpe_sampler()\n                    model_grid = TuneSearchCV(estimator=pipeline_with_model, search_optimization=search_algorithm, param_distributions=param_grid, n_trials=n_iter, early_stopping=do_early_stop, scoring=optimize, cv=fold, random_state=self.seed, max_iters=early_stopping_max_iters, n_jobs=n_jobs, use_gpu=self.gpu_param, refit=True, verbose=tuner_verbose, pipeline_auto_early_stop=True, search_kwargs=search_kwargs)\n        elif search_library == 'scikit-optimize':\n            try:\n                param_grid = get_skopt_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            self.logger.info('Initializing skopt.BayesSearchCV')\n            model_grid = skopt.BayesSearchCV(estimator=pipeline_with_model, search_spaces=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        else:\n            import sklearn.model_selection._search\n            try:\n                param_grid = get_base_distributions(param_grid)\n            except Exception:\n                self.logger.warning(\"Couldn't convert param_grid to specific library distributions. Exception:\")\n                self.logger.warning(traceback.format_exc())\n            if search_algorithm == 'grid':\n                self.logger.info('Initializing GridSearchCV')\n                model_grid = sklearn.model_selection._search.GridSearchCV(estimator=pipeline_with_model, param_grid=param_grid, scoring=optimize, cv=fold, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n            else:\n                self.logger.info('Initializing RandomizedSearchCV')\n                model_grid = sklearn.model_selection._search.RandomizedSearchCV(estimator=pipeline_with_model, param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, random_state=self.seed, refit=False, n_jobs=n_jobs, verbose=tuner_verbose, **search_kwargs)\n        if search_library == 'scikit-learn':\n            with patch('sklearn.model_selection._search.sample_without_replacement', pycaret.internal.patches.sklearn._mp_sample_without_replacement):\n                with patch('sklearn.model_selection._search.ParameterGrid.__getitem__', pycaret.internal.patches.sklearn._mp_ParameterGrid_getitem):\n                    model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        else:\n            model_grid.fit(data_X, data_y, groups=groups, **fit_kwargs)\n        best_params = model_grid.best_params_\n        self.logger.info(f'best_params: {best_params}')\n        best_params = {**best_params, **extra_params}\n        if actual_estimator_label:\n            best_params = {k.replace(f'{actual_estimator_label}__', ''): v for (k, v) in best_params.items()}\n        cv_results = None\n        try:\n            cv_results = model_grid.cv_results_\n        except Exception:\n            self.logger.warning(\"Couldn't get cv_results from model_grid. Exception:\")\n            self.logger.warning(traceback.format_exc())\n    display.move_progress()\n    self.logger.info('Hyperparameter search completed')\n    if isinstance(model, TunableMixin):\n        self.logger.info('Getting base sklearn object from tunable')\n        model = clone(model)\n        model.set_params(**best_params)\n        best_params = {k: v for (k, v) in model.get_params().items() if k in model.get_base_sklearn_params().keys()}\n        model = model.get_base_sklearn_object()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (best_model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, groups=groups, fit_kwargs=fit_kwargs, return_train_score=return_train_score, **best_params)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    if choose_better:\n        new_best_model = self._choose_better([estimator, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_best_model is not best_model:\n            msg = 'Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        best_model = new_best_model\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'tune_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='tune_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, tune_cv_results=cv_results, display=display)\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(best_model))\n    self.logger.info('tune_model() successfully completed......................................')\n    gc.collect()\n    if return_tuner:\n        return (best_model, model_grid)\n    return best_model"
        ]
    },
    {
        "func_name": "ensemble_model",
        "original": "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    \"\"\"\n        This function ensembles the trained base estimator using the method defined in\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\n\n        This function returns a trained model object.\n\n        Model must be created using create_model() or tune_model().\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> dt = create_model('dt')\n        >>> ensembled_dt = ensemble_model(dt)\n\n        This will return an ensembled Decision Tree model using 'Bagging'.\n\n        Parameters\n        ----------\n        estimator : object, default = None\n\n        method: str, default = 'Bagging'\n            Bagging method will create an ensemble meta-estimator that fits base\n            classifiers each on random subsets of the original dataset. The other\n            available method is 'Boosting' which will create a meta-estimators by\n            fitting a classifier on the original dataset and then fits additional\n            copies of the classifier on the same dataset but where the weights of\n            incorrectly classified instances are adjusted such that subsequent\n            classifiers focus more on difficult cases.\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        n_estimators: integer, default = 10\n            The number of base estimators in the ensemble.\n            In case of perfect fit, the learning procedure is stopped early.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        choose_better: bool, default = False\n            When set to set to True, base estimator is returned when the metric doesn't\n            improve by ensemble_model. This guarantees the returned object would perform\n            at least equivalent to base estimator created using create_model or model\n            returned by compare_models.\n\n        optimize: str, default = 'Accuracy'\n            Only used when choose_better is set to True. optimize parameter is used\n            to compare ensembled model with base estimator. Values accepted in\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\n            'Kappa', 'MCC'.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are also returned.\n\n        model\n            Trained ensembled model object.\n\n        Warnings\n        --------\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\n        as zero (0.0).\n\n\n        \"\"\"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model",
        "mutated": [
            "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function ensembles the trained base estimator using the method defined in\\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        Model must be created using create_model() or tune_model().\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> ensembled_dt = ensemble_model(dt)\\n\\n        This will return an ensembled Decision Tree model using 'Bagging'.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        method: str, default = 'Bagging'\\n            Bagging method will create an ensemble meta-estimator that fits base\\n            classifiers each on random subsets of the original dataset. The other\\n            available method is 'Boosting' which will create a meta-estimators by\\n            fitting a classifier on the original dataset and then fits additional\\n            copies of the classifier on the same dataset but where the weights of\\n            incorrectly classified instances are adjusted such that subsequent\\n            classifiers focus more on difficult cases.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        n_estimators: integer, default = 10\\n            The number of base estimators in the ensemble.\\n            In case of perfect fit, the learning procedure is stopped early.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained ensembled model object.\\n\\n        Warnings\\n        --------\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model",
            "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function ensembles the trained base estimator using the method defined in\\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        Model must be created using create_model() or tune_model().\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> ensembled_dt = ensemble_model(dt)\\n\\n        This will return an ensembled Decision Tree model using 'Bagging'.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        method: str, default = 'Bagging'\\n            Bagging method will create an ensemble meta-estimator that fits base\\n            classifiers each on random subsets of the original dataset. The other\\n            available method is 'Boosting' which will create a meta-estimators by\\n            fitting a classifier on the original dataset and then fits additional\\n            copies of the classifier on the same dataset but where the weights of\\n            incorrectly classified instances are adjusted such that subsequent\\n            classifiers focus more on difficult cases.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        n_estimators: integer, default = 10\\n            The number of base estimators in the ensemble.\\n            In case of perfect fit, the learning procedure is stopped early.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained ensembled model object.\\n\\n        Warnings\\n        --------\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model",
            "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function ensembles the trained base estimator using the method defined in\\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        Model must be created using create_model() or tune_model().\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> ensembled_dt = ensemble_model(dt)\\n\\n        This will return an ensembled Decision Tree model using 'Bagging'.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        method: str, default = 'Bagging'\\n            Bagging method will create an ensemble meta-estimator that fits base\\n            classifiers each on random subsets of the original dataset. The other\\n            available method is 'Boosting' which will create a meta-estimators by\\n            fitting a classifier on the original dataset and then fits additional\\n            copies of the classifier on the same dataset but where the weights of\\n            incorrectly classified instances are adjusted such that subsequent\\n            classifiers focus more on difficult cases.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        n_estimators: integer, default = 10\\n            The number of base estimators in the ensemble.\\n            In case of perfect fit, the learning procedure is stopped early.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained ensembled model object.\\n\\n        Warnings\\n        --------\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model",
            "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function ensembles the trained base estimator using the method defined in\\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        Model must be created using create_model() or tune_model().\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> ensembled_dt = ensemble_model(dt)\\n\\n        This will return an ensembled Decision Tree model using 'Bagging'.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        method: str, default = 'Bagging'\\n            Bagging method will create an ensemble meta-estimator that fits base\\n            classifiers each on random subsets of the original dataset. The other\\n            available method is 'Boosting' which will create a meta-estimators by\\n            fitting a classifier on the original dataset and then fits additional\\n            copies of the classifier on the same dataset but where the weights of\\n            incorrectly classified instances are adjusted such that subsequent\\n            classifiers focus more on difficult cases.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        n_estimators: integer, default = 10\\n            The number of base estimators in the ensemble.\\n            In case of perfect fit, the learning procedure is stopped early.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained ensembled model object.\\n\\n        Warnings\\n        --------\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model",
            "def ensemble_model(self, estimator, method: str='Bagging', fold: Optional[Union[int, Any]]=None, n_estimators: int=10, round: int=4, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function ensembles the trained base estimator using the method defined in\\n        'method' parameter (default = 'Bagging'). The output prints a score grid that shows\\n        Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by fold (default = 10 Fold).\\n\\n        This function returns a trained model object.\\n\\n        Model must be created using create_model() or tune_model().\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> ensembled_dt = ensemble_model(dt)\\n\\n        This will return an ensembled Decision Tree model using 'Bagging'.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = None\\n\\n        method: str, default = 'Bagging'\\n            Bagging method will create an ensemble meta-estimator that fits base\\n            classifiers each on random subsets of the original dataset. The other\\n            available method is 'Boosting' which will create a meta-estimators by\\n            fitting a classifier on the original dataset and then fits additional\\n            copies of the classifier on the same dataset but where the weights of\\n            incorrectly classified instances are adjusted such that subsequent\\n            classifiers focus more on difficult cases.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        n_estimators: integer, default = 10\\n            The number of base estimators in the ensemble.\\n            In case of perfect fit, the learning procedure is stopped early.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained ensembled model object.\\n\\n        Warnings\\n        --------\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing ensemble_model()')\n    self.logger.info(f'ensemble_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if not hasattr(estimator, 'fit'):\n        raise ValueError(f'Estimator {estimator} does not have the required fit() method.')\n    available_method = ['Bagging', 'Boosting']\n    if method not in available_method:\n        raise ValueError(\"Method parameter only accepts two values 'Bagging' or 'Boosting'.\")\n    if method == 'Boosting':\n        boosting_model_definition = self._all_models_internal['ada']\n        check_model = estimator\n        try:\n            check_model = boosting_model_definition.class_def(check_model, n_estimators=n_estimators, **boosting_model_definition.args)\n            with redirect_output(self.logger):\n                check_model.fit(self.X_train_transformed, self.y_train_transformed)\n        except Exception:\n            raise TypeError(\"Estimator not supported for the Boosting method. Change the estimator or method to 'Bagging'.\")\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(n_estimators) is not int:\n        raise TypeError('n_estimators parameter only accepts integer value.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    display.move_progress()\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    self.logger.info('Checking base model')\n    _estimator_ = estimator\n    estimator_id = self._get_model_id(estimator)\n    if estimator_id is None:\n        estimator_name = self._get_model_name(estimator)\n        self.logger.info('A custom model has been passed')\n    else:\n        estimator_definition = self._all_models_internal[estimator_id]\n        estimator_name = estimator_definition.name\n    self.logger.info(f'Base model : {estimator_name}')\n    display.update_monitor(2, estimator_name)\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Selecting Estimator')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    model = get_estimator_from_meta_estimator(_estimator_)\n    self.logger.info('Importing untrained ensembler')\n    if method == 'Bagging':\n        self.logger.info('Ensemble method set to Bagging')\n        bagging_model_definition = self._all_models_internal['Bagging']\n        model = bagging_model_definition.class_def(model, bootstrap=True, n_estimators=n_estimators, **bagging_model_definition.args)\n    else:\n        self.logger.info('Ensemble method set to Boosting')\n        boosting_model_definition = self._all_models_internal['ada']\n        model = boosting_model_definition.class_def(model, n_estimators=n_estimators, **boosting_model_definition.args)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    best_model = model\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'ensemble_model')\n        self._log_model(model=best_model, model_results=model_results, score_dict=avgs_dict_log, source='ensemble_model', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([_estimator_, (best_model, model_results)], compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not best_model:\n            msg = 'Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('ensemble_model() successfully completed......................................')\n    gc.collect()\n    return model"
        ]
    },
    {
        "func_name": "blend_models",
        "original": "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    \"\"\"\n        This function creates a Soft Voting / Majority Rule classifier for all the\n        estimators in the model library (excluding the few when turbo is True) or\n        for specific trained estimators passed as a list in estimator_list param.\n        It scores it using Cross Validation. The output prints a score\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\n        fold (default CV = 10 Folds).\n\n        This function returns a trained model object.\n\n        Example\n        -------\n        >>> lr = create_model('lr')\n        >>> rf = create_model('rf')\n        >>> knn = create_model('knn')\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\n\n        This will create a VotingClassifier of lr, rf and knn.\n\n        Parameters\n        ----------\n        estimator_list : list of objects\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        choose_better: bool, default = False\n            When set to set to True, base estimator is returned when the metric doesn't\n            improve by ensemble_model. This guarantees the returned object would perform\n            at least equivalent to base estimator created using create_model or model\n            returned by compare_models.\n\n        optimize: str, default = 'Accuracy'\n            Only used when choose_better is set to True. optimize parameter is used\n            to compare ensembled model with base estimator. Values accepted in\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\n            'Kappa', 'MCC'.\n\n        method: str, default = 'auto'\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\n            the class label based on the argmax of the sums of the predicted probabilities,\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\n\n        weights: list, default = None\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are also returned.\n\n        model\n            Trained Voting Classifier model object.\n\n        Warnings\n        --------\n        - When passing estimator_list with method set to 'soft'. All the models in the\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\n        support the predict_proba and hence an exception will be raised.\n\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\n        that does not support the predict_proba function will be dropped from the estimator\n        list.\n\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\n        zero (0.0).\n\n\n\n        \"\"\"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model",
        "mutated": [
            "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function creates a Soft Voting / Majority Rule classifier for all the\\n        estimators in the model library (excluding the few when turbo is True) or\\n        for specific trained estimators passed as a list in estimator_list param.\\n        It scores it using Cross Validation. The output prints a score\\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\\n        fold (default CV = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> lr = create_model('lr')\\n        >>> rf = create_model('rf')\\n        >>> knn = create_model('knn')\\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\\n\\n        This will create a VotingClassifier of lr, rf and knn.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        method: str, default = 'auto'\\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\\n            the class label based on the argmax of the sums of the predicted probabilities,\\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\\n\\n        weights: list, default = None\\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained Voting Classifier model object.\\n\\n        Warnings\\n        --------\\n        - When passing estimator_list with method set to 'soft'. All the models in the\\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\\n        support the predict_proba and hence an exception will be raised.\\n\\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\\n        that does not support the predict_proba function will be dropped from the estimator\\n        list.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\\n        zero (0.0).\\n\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function creates a Soft Voting / Majority Rule classifier for all the\\n        estimators in the model library (excluding the few when turbo is True) or\\n        for specific trained estimators passed as a list in estimator_list param.\\n        It scores it using Cross Validation. The output prints a score\\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\\n        fold (default CV = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> lr = create_model('lr')\\n        >>> rf = create_model('rf')\\n        >>> knn = create_model('knn')\\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\\n\\n        This will create a VotingClassifier of lr, rf and knn.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        method: str, default = 'auto'\\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\\n            the class label based on the argmax of the sums of the predicted probabilities,\\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\\n\\n        weights: list, default = None\\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained Voting Classifier model object.\\n\\n        Warnings\\n        --------\\n        - When passing estimator_list with method set to 'soft'. All the models in the\\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\\n        support the predict_proba and hence an exception will be raised.\\n\\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\\n        that does not support the predict_proba function will be dropped from the estimator\\n        list.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\\n        zero (0.0).\\n\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function creates a Soft Voting / Majority Rule classifier for all the\\n        estimators in the model library (excluding the few when turbo is True) or\\n        for specific trained estimators passed as a list in estimator_list param.\\n        It scores it using Cross Validation. The output prints a score\\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\\n        fold (default CV = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> lr = create_model('lr')\\n        >>> rf = create_model('rf')\\n        >>> knn = create_model('knn')\\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\\n\\n        This will create a VotingClassifier of lr, rf and knn.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        method: str, default = 'auto'\\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\\n            the class label based on the argmax of the sums of the predicted probabilities,\\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\\n\\n        weights: list, default = None\\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained Voting Classifier model object.\\n\\n        Warnings\\n        --------\\n        - When passing estimator_list with method set to 'soft'. All the models in the\\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\\n        support the predict_proba and hence an exception will be raised.\\n\\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\\n        that does not support the predict_proba function will be dropped from the estimator\\n        list.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\\n        zero (0.0).\\n\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function creates a Soft Voting / Majority Rule classifier for all the\\n        estimators in the model library (excluding the few when turbo is True) or\\n        for specific trained estimators passed as a list in estimator_list param.\\n        It scores it using Cross Validation. The output prints a score\\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\\n        fold (default CV = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> lr = create_model('lr')\\n        >>> rf = create_model('rf')\\n        >>> knn = create_model('knn')\\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\\n\\n        This will create a VotingClassifier of lr, rf and knn.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        method: str, default = 'auto'\\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\\n            the class label based on the argmax of the sums of the predicted probabilities,\\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\\n\\n        weights: list, default = None\\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained Voting Classifier model object.\\n\\n        Warnings\\n        --------\\n        - When passing estimator_list with method set to 'soft'. All the models in the\\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\\n        support the predict_proba and hence an exception will be raised.\\n\\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\\n        that does not support the predict_proba function will be dropped from the estimator\\n        list.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\\n        zero (0.0).\\n\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def blend_models(self, estimator_list: list, fold: Optional[Union[int, Any]]=None, round: int=4, choose_better: bool=False, optimize: str='Accuracy', method: str='auto', weights: Optional[List[float]]=None, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function creates a Soft Voting / Majority Rule classifier for all the\\n        estimators in the model library (excluding the few when turbo is True) or\\n        for specific trained estimators passed as a list in estimator_list param.\\n        It scores it using Cross Validation. The output prints a score\\n        grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa and MCC by\\n        fold (default CV = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> lr = create_model('lr')\\n        >>> rf = create_model('rf')\\n        >>> knn = create_model('knn')\\n        >>> blend_three = blend_models(estimator_list = [lr,rf,knn])\\n\\n        This will create a VotingClassifier of lr, rf and knn.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        method: str, default = 'auto'\\n            'hard' uses predicted class labels for majority rule voting. 'soft', predicts\\n            the class label based on the argmax of the sums of the predicted probabilities,\\n            which is recommended for an ensemble of well-calibrated classifiers. Default value,\\n            'auto', will try to use 'soft' and fall back to 'hard' if the former is not supported.\\n\\n        weights: list, default = None\\n            Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting)\\n            or class probabilities before averaging (soft voting). Uses uniform weights if None.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained Voting Classifier model object.\\n\\n        Warnings\\n        --------\\n        - When passing estimator_list with method set to 'soft'. All the models in the\\n        estimator_list must support predict_proba function. 'svm' and 'ridge' does not\\n        support the predict_proba and hence an exception will be raised.\\n\\n        - When estimator_list is set to 'All' and method is forced to 'soft', estimators\\n        that does not support the predict_proba function will be dropped from the estimator\\n        list.\\n\\n        - If target variable is multiclass (more than 2 classes), AUC will be returned as\\n        zero (0.0).\\n\\n\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing blend_models()')\n    self.logger.info(f'blend_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        available_method = ['mean', 'median', 'min', 'max', 'gmean']\n    else:\n        available_method = ['auto', 'soft', 'hard', 'mean', 'median', 'voting']\n    if method not in available_method:\n        raise ValueError(f'Method parameter only accepts the following values: {available_method}. See Docstring for details.')\n    if not self._ml_usecase == MLUsecase.TIME_SERIES:\n        for i in estimator_list:\n            if not hasattr(i, 'fit'):\n                raise ValueError(f'Estimator {i} does not have the required fit() method.')\n            if self._ml_usecase == MLUsecase.CLASSIFICATION:\n                if method != 'hard':\n                    for i in estimator_list:\n                        if not hasattr(i, 'predict_proba'):\n                            if method != 'auto':\n                                raise TypeError(f\"Estimator list contains estimator {i} that doesn't support probabilities and method is forced to 'soft'. Either change the method or drop the estimator.\")\n                            else:\n                                self.logger.info(f\"Estimator {i} doesn't support probabilities, falling back to 'hard'.\")\n                                method = 'hard'\n                                break\n                    if method == 'auto':\n                        method = 'soft'\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    if weights is not None:\n        num_estimators = len(estimator_list)\n        if len(weights) != num_estimators:\n            raise ValueError('weights parameter must have the same length as the estimator_list.')\n        if not all((isinstance(x, int) or isinstance(x, float) for x in weights)):\n            raise TypeError('weights must contain only ints or floats.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        pass\n    else:\n        fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    self.logger.info('Importing libraries')\n    np.random.seed(self.seed)\n    self.logger.info('Copying training dataset')\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    if self._ml_usecase == MLUsecase.TIME_SERIES:\n        voting_model_definition = self._all_models_internal['ensemble_forecaster']\n    else:\n        voting_model_definition = self._all_models_internal['Voting']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = voting_model_definition.class_def(estimators=estimator_list, voting=method, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        model = voting_model_definition.class_def(forecasters=estimator_list, aggfunc=method, weights=weights, n_jobs=self.gpu_n_jobs_param)\n    else:\n        model = voting_model_definition.class_def(estimators=estimator_list, n_jobs=self.gpu_n_jobs_param, weights=weights)\n    display.update_monitor(2, voting_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'blend_models')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='blend_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('blend_models() successfully completed......................................')\n    gc.collect()\n    return model"
        ]
    },
    {
        "func_name": "stack_models",
        "original": "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    \"\"\"\n        This function trains a meta model and scores it using Cross Validation.\n        The predictions from the base level models as passed in the estimator_list parameter\n        are used as input features for the meta model. The restacking parameter controls\n        the ability to expose raw features to the meta model when set to True\n        (default = False).\n\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\n        F1, Kappa and MCC by fold (default = 10 Folds).\n\n        This function returns a trained model object.\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> dt = create_model('dt')\n        >>> rf = create_model('rf')\n        >>> ada = create_model('ada')\n        >>> ridge = create_model('ridge')\n        >>> knn = create_model('knn')\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\n\n        This will create a meta model that will use the predictions of all the\n        models provided in estimator_list param. By default, the meta model is\n        Logistic Regression but can be changed with meta_model param.\n\n        Parameters\n        ----------\n        estimator_list : list of objects\n\n        meta_model : object, default = None\n            If set to None, Logistic Regression is used as a meta model.\n\n        fold: int or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\n            parameter of the ``setup`` function is used. When an integer is passed,\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\n            ``setup`` function.\n\n        fold: integer or scikit-learn compatible CV generator, default = None\n            Controls cross-validation. If None, will use the CV generator defined in setup().\n            If integer, will use KFold CV with that many folds.\n            When cross_validation is False, this parameter is ignored.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        method: string, default = 'auto'\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\n            'decision_function' or 'predict' in that order.\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\n            If the method is not implemented by the estimator, it will raise an error.\n\n        restack: bool, default = False\n            When restack is set to True, raw data will be exposed to meta model when\n            making predictions, otherwise when False, only the predicted label or\n            probabilities is passed to meta model when making final predictions.\n\n        choose_better: bool, default = False\n            When set to set to True, base estimator is returned when the metric doesn't\n            improve by ensemble_model. This guarantees the returned object would perform\n            at least equivalent to base estimator created using create_model or model\n            returned by compare_models.\n\n        optimize: str, default = 'Accuracy'\n            Only used when choose_better is set to True. optimize parameter is used\n            to compare ensembled model with base estimator. Values accepted in\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\n            'Kappa', 'MCC'.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        verbose: bool, default = True\n            Score grid is not printed when verbose is set to False.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        Returns\n        -------\n        score_grid\n            A table containing the scores of the model across the kfolds.\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\n            Kappa and MCC. Mean and standard deviation of the scores across\n            the folds are also returned.\n\n        model\n            Trained model object.\n\n        Warnings\n        --------\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\n        as zero (0.0).\n\n        \"\"\"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model",
        "mutated": [
            "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function trains a meta model and scores it using Cross Validation.\\n        The predictions from the base level models as passed in the estimator_list parameter\\n        are used as input features for the meta model. The restacking parameter controls\\n        the ability to expose raw features to the meta model when set to True\\n        (default = False).\\n\\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> rf = create_model('rf')\\n        >>> ada = create_model('ada')\\n        >>> ridge = create_model('ridge')\\n        >>> knn = create_model('knn')\\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\\n\\n        This will create a meta model that will use the predictions of all the\\n        models provided in estimator_list param. By default, the meta model is\\n        Logistic Regression but can be changed with meta_model param.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        meta_model : object, default = None\\n            If set to None, Logistic Regression is used as a meta model.\\n\\n        fold: int or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\\n            parameter of the ``setup`` function is used. When an integer is passed,\\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\\n            ``setup`` function.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        method: string, default = 'auto'\\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\\n            'decision_function' or 'predict' in that order.\\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\\n            If the method is not implemented by the estimator, it will raise an error.\\n\\n        restack: bool, default = False\\n            When restack is set to True, raw data will be exposed to meta model when\\n            making predictions, otherwise when False, only the predicted label or\\n            probabilities is passed to meta model when making final predictions.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained model object.\\n\\n        Warnings\\n        --------\\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function trains a meta model and scores it using Cross Validation.\\n        The predictions from the base level models as passed in the estimator_list parameter\\n        are used as input features for the meta model. The restacking parameter controls\\n        the ability to expose raw features to the meta model when set to True\\n        (default = False).\\n\\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> rf = create_model('rf')\\n        >>> ada = create_model('ada')\\n        >>> ridge = create_model('ridge')\\n        >>> knn = create_model('knn')\\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\\n\\n        This will create a meta model that will use the predictions of all the\\n        models provided in estimator_list param. By default, the meta model is\\n        Logistic Regression but can be changed with meta_model param.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        meta_model : object, default = None\\n            If set to None, Logistic Regression is used as a meta model.\\n\\n        fold: int or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\\n            parameter of the ``setup`` function is used. When an integer is passed,\\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\\n            ``setup`` function.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        method: string, default = 'auto'\\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\\n            'decision_function' or 'predict' in that order.\\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\\n            If the method is not implemented by the estimator, it will raise an error.\\n\\n        restack: bool, default = False\\n            When restack is set to True, raw data will be exposed to meta model when\\n            making predictions, otherwise when False, only the predicted label or\\n            probabilities is passed to meta model when making final predictions.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained model object.\\n\\n        Warnings\\n        --------\\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function trains a meta model and scores it using Cross Validation.\\n        The predictions from the base level models as passed in the estimator_list parameter\\n        are used as input features for the meta model. The restacking parameter controls\\n        the ability to expose raw features to the meta model when set to True\\n        (default = False).\\n\\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> rf = create_model('rf')\\n        >>> ada = create_model('ada')\\n        >>> ridge = create_model('ridge')\\n        >>> knn = create_model('knn')\\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\\n\\n        This will create a meta model that will use the predictions of all the\\n        models provided in estimator_list param. By default, the meta model is\\n        Logistic Regression but can be changed with meta_model param.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        meta_model : object, default = None\\n            If set to None, Logistic Regression is used as a meta model.\\n\\n        fold: int or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\\n            parameter of the ``setup`` function is used. When an integer is passed,\\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\\n            ``setup`` function.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        method: string, default = 'auto'\\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\\n            'decision_function' or 'predict' in that order.\\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\\n            If the method is not implemented by the estimator, it will raise an error.\\n\\n        restack: bool, default = False\\n            When restack is set to True, raw data will be exposed to meta model when\\n            making predictions, otherwise when False, only the predicted label or\\n            probabilities is passed to meta model when making final predictions.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained model object.\\n\\n        Warnings\\n        --------\\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function trains a meta model and scores it using Cross Validation.\\n        The predictions from the base level models as passed in the estimator_list parameter\\n        are used as input features for the meta model. The restacking parameter controls\\n        the ability to expose raw features to the meta model when set to True\\n        (default = False).\\n\\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> rf = create_model('rf')\\n        >>> ada = create_model('ada')\\n        >>> ridge = create_model('ridge')\\n        >>> knn = create_model('knn')\\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\\n\\n        This will create a meta model that will use the predictions of all the\\n        models provided in estimator_list param. By default, the meta model is\\n        Logistic Regression but can be changed with meta_model param.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        meta_model : object, default = None\\n            If set to None, Logistic Regression is used as a meta model.\\n\\n        fold: int or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\\n            parameter of the ``setup`` function is used. When an integer is passed,\\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\\n            ``setup`` function.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        method: string, default = 'auto'\\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\\n            'decision_function' or 'predict' in that order.\\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\\n            If the method is not implemented by the estimator, it will raise an error.\\n\\n        restack: bool, default = False\\n            When restack is set to True, raw data will be exposed to meta model when\\n            making predictions, otherwise when False, only the predicted label or\\n            probabilities is passed to meta model when making final predictions.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained model object.\\n\\n        Warnings\\n        --------\\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model",
            "def stack_models(self, estimator_list: list, meta_model=None, meta_model_fold: Optional[Union[int, Any]]=5, fold: Optional[Union[int, Any]]=None, round: int=4, method: str='auto', restack: bool=False, choose_better: bool=False, optimize: str='Accuracy', fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, probability_threshold: Optional[float]=None, verbose: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function trains a meta model and scores it using Cross Validation.\\n        The predictions from the base level models as passed in the estimator_list parameter\\n        are used as input features for the meta model. The restacking parameter controls\\n        the ability to expose raw features to the meta model when set to True\\n        (default = False).\\n\\n        The output prints the score grid that shows Accuracy, AUC, Recall, Precision,\\n        F1, Kappa and MCC by fold (default = 10 Folds).\\n\\n        This function returns a trained model object.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> rf = create_model('rf')\\n        >>> ada = create_model('ada')\\n        >>> ridge = create_model('ridge')\\n        >>> knn = create_model('knn')\\n        >>> stacked_models = stack_models(estimator_list=[dt,rf,ada,ridge,knn])\\n\\n        This will create a meta model that will use the predictions of all the\\n        models provided in estimator_list param. By default, the meta model is\\n        Logistic Regression but can be changed with meta_model param.\\n\\n        Parameters\\n        ----------\\n        estimator_list : list of objects\\n\\n        meta_model : object, default = None\\n            If set to None, Logistic Regression is used as a meta model.\\n\\n        fold: int or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, the CV generator in the ``fold_strategy``\\n            parameter of the ``setup`` function is used. When an integer is passed,\\n            it is interpreted as the 'n_splits' parameter of the CV generator in the\\n            ``setup`` function.\\n\\n        fold: integer or scikit-learn compatible CV generator, default = None\\n            Controls cross-validation. If None, will use the CV generator defined in setup().\\n            If integer, will use KFold CV with that many folds.\\n            When cross_validation is False, this parameter is ignored.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        method: string, default = 'auto'\\n            - if \u2018auto\u2019, it will try to invoke, for each estimator, 'predict_proba',\\n            'decision_function' or 'predict' in that order.\\n            - otherwise, one of 'predict_proba', 'decision_function' or 'predict'.\\n            If the method is not implemented by the estimator, it will raise an error.\\n\\n        restack: bool, default = False\\n            When restack is set to True, raw data will be exposed to meta model when\\n            making predictions, otherwise when False, only the predicted label or\\n            probabilities is passed to meta model when making final predictions.\\n\\n        choose_better: bool, default = False\\n            When set to set to True, base estimator is returned when the metric doesn't\\n            improve by ensemble_model. This guarantees the returned object would perform\\n            at least equivalent to base estimator created using create_model or model\\n            returned by compare_models.\\n\\n        optimize: str, default = 'Accuracy'\\n            Only used when choose_better is set to True. optimize parameter is used\\n            to compare ensembled model with base estimator. Values accepted in\\n            optimize parameter are 'Accuracy', 'AUC', 'Recall', 'Precision', 'F1',\\n            'Kappa', 'MCC'.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        verbose: bool, default = True\\n            Score grid is not printed when verbose is set to False.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns\\n        -------\\n        score_grid\\n            A table containing the scores of the model across the kfolds.\\n            Scoring metrics used are Accuracy, AUC, Recall, Precision, F1,\\n            Kappa and MCC. Mean and standard deviation of the scores across\\n            the folds are also returned.\\n\\n        model\\n            Trained model object.\\n\\n        Warnings\\n        --------\\n        -  If target variable is multiclass (more than 2 classes), AUC will be returned\\n        as zero (0.0).\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing stack_models()')\n    self.logger.info(f'stack_models({function_params_str})')\n    self.logger.info('Checking exceptions')\n    runtime_start = time.time()\n    if not fit_kwargs:\n        fit_kwargs = {}\n    for i in estimator_list:\n        if not hasattr(i, 'fit'):\n            raise ValueError(f'Estimator {i} does not have the required fit() method.')\n    if meta_model is not None:\n        if not hasattr(meta_model, 'fit'):\n            raise ValueError(f'Meta Model {meta_model} does not have the required fit() method.')\n    if fold is not None and (not (type(fold) is int or is_sklearn_cv_generator(fold))):\n        raise TypeError('fold parameter must be either None, an integer or a scikit-learn compatible CV generator object.')\n    if type(round) is not int:\n        raise TypeError('Round parameter only accepts integer value.')\n    available_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n    if method not in available_method:\n        raise ValueError(\"Method parameter not acceptable. It only accepts 'auto', 'predict_proba', 'decision_function', 'predict'.\")\n    if type(restack) is not bool:\n        raise TypeError('Restack parameter can only take argument as True or False.')\n    if type(verbose) is not bool:\n        raise TypeError('Verbose parameter can only take argument as True or False.')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    '\\n\\n        ERROR HANDLING ENDS HERE\\n\\n        '\n    fold = self._get_cv_splitter(fold)\n    groups = self._get_groups(groups)\n    self.logger.info('Defining meta model')\n    if meta_model is None:\n        estimator = 'lr'\n        meta_model_definition = self._all_models_internal[estimator]\n        meta_model_args = meta_model_definition.args\n        meta_model = meta_model_definition.class_def(**meta_model_args)\n    else:\n        meta_model = clone(get_estimator_from_meta_estimator(meta_model))\n    progress_args = {'max': 2 + 4}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    np.random.seed(self.seed)\n    compare_dimension = optimize.display_name\n    optimize = optimize.scorer\n    display.move_progress()\n    '\\n        MONITOR UPDATE STARTS\\n        '\n    display.update_monitor(1, 'Compiling Estimators')\n    '\\n        MONITOR UPDATE ENDS\\n        '\n    self.logger.info('Getting model names')\n    estimator_dict = {}\n    for x in estimator_list:\n        x = get_estimator_from_meta_estimator(x)\n        name = self._get_model_name(x)\n        suffix = 1\n        original_name = name\n        while name in estimator_dict:\n            name = f'{original_name}_{suffix}'\n            suffix += 1\n        estimator_dict[name] = x\n    estimator_list = list(estimator_dict.items())\n    self.logger.info(estimator_list)\n    stacking_model_definition = self._all_models_internal['Stacking']\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, stack_method=method, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    else:\n        model = stacking_model_definition.class_def(estimators=estimator_list, final_estimator=meta_model, cv=meta_model_fold, n_jobs=self.gpu_n_jobs_param, passthrough=restack)\n    display.update_monitor(2, stacking_model_definition.name)\n    display.move_progress()\n    self.logger.info('SubProcess create_model() called ==================================')\n    (model, model_fit_time) = self._create_model(estimator=model, system=False, display=display, fold=fold, round=round, fit_kwargs=fit_kwargs, groups=groups, probability_threshold=probability_threshold, return_train_score=return_train_score)\n    model_results = self.pull()\n    self.logger.info('SubProcess create_model() end ==================================')\n    runtime_end = time.time()\n    runtime = np.array(runtime_end - runtime_start).round(2)\n    if self.logging_param:\n        indices = self._get_return_train_score_indices_for_logging(return_train_score=return_train_score)\n        avgs_dict_log = {k: v for (k, v) in model_results.loc[indices].items()}\n        self.logging_param.log_model_comparison(model_results, 'stack_model')\n        self._log_model(model=model, model_results=model_results, score_dict=avgs_dict_log, source='stack_models', runtime=runtime, model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, display=display)\n    if choose_better:\n        new_model = self._choose_better([(model, model_results)] + estimator_list, compare_dimension, fold, groups=groups, fit_kwargs=fit_kwargs, display=display)\n        if new_model is not model:\n            msg = 'Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).'\n            if verbose:\n                print(msg)\n            self.logger.info(msg)\n        model = new_model\n    model_results = self._highlight_and_round_model_results(model_results, return_train_score, round)\n    display.display(model_results)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(model))\n    self.logger.info('stack_models() successfully completed......................................')\n    gc.collect()\n    return model"
        ]
    },
    {
        "func_name": "summary",
        "original": "def summary(show: bool=True):\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot",
        "mutated": [
            "def summary(show: bool=True):\n    if False:\n        i = 10\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot",
            "def summary(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot",
            "def summary(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot",
            "def summary(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot",
            "def summary(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    try:\n        assert len(shap_values) == 2\n        shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n    except Exception:\n        shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return shap_plot"
        ]
    },
    {
        "func_name": "correlation",
        "original": "def correlation(show: bool=True):\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None",
        "mutated": [
            "def correlation(show: bool=True):\n    if False:\n        i = 10\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None",
            "def correlation(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None",
            "def correlation(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None",
            "def correlation(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None",
            "def correlation(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n        dependence = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        dependence = feature\n    self.logger.info('Creating TreeExplainer')\n    explainer = shap.TreeExplainer(model)\n    self.logger.info('Compiling shap values')\n    shap_values = explainer.shap_values(test_X)\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.png'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        plt.savefig(plot_filename, bbox_inches='tight')\n        plt.close()\n    return None"
        ]
    },
    {
        "func_name": "reason",
        "original": "def reason(show: bool=True):\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot",
        "mutated": [
            "def reason(show: bool=True):\n    if False:\n        i = 10\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot",
            "def reason(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot",
            "def reason(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot",
            "def reason(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot",
            "def reason(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shap_plot = None\n    if model_id in shap_models_type1:\n        self.logger.info('model type detected: type 1')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            if model_id == 'lightgbm':\n                self.logger.info('model type detected: LGBMClassifier')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n            else:\n                self.logger.info('model type detected: Unknown')\n                shap_values = explainer.shap_values(data_for_prediction)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n    elif model_id in shap_models_type2:\n        self.logger.info('model type detected: type 2')\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        shap.initjs()\n        if observation is None:\n            self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n        else:\n            row_to_show = observation\n            data_for_prediction = test_X.iloc[row_to_show]\n            shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n    if save:\n        plot_filename = f'SHAP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        shap.save_html(plot_filename, shap_plot)\n    return shap_plot"
        ]
    },
    {
        "func_name": "pdp",
        "original": "def pdp(show: bool=True):\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot",
        "mutated": [
            "def pdp(show: bool=True):\n    if False:\n        i = 10\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot",
            "def pdp(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot",
            "def pdp(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot",
            "def pdp(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot",
            "def pdp(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.logger.info('Checking feature parameter passed')\n    if feature is None:\n        self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n        pdp_feature = test_X.columns[0]\n    else:\n        self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n        pdp_feature = feature\n    from interpret.blackbox import PartialDependence\n    try:\n        pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n    pdp_global = pdp.explain_global()\n    pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PDP {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pdp_plot, plot_filename)\n    return pdp_plot"
        ]
    },
    {
        "func_name": "msa",
        "original": "def msa(show: bool=True):\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot",
        "mutated": [
            "def msa(show: bool=True):\n    if False:\n        i = 10\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot",
            "def msa(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot",
            "def msa(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot",
            "def msa(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot",
            "def msa(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from interpret.blackbox import MorrisSensitivity\n    try:\n        msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n    except TypeError:\n        try:\n            msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n        except AttributeError:\n            msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n    msa_global = msa.explain_global()\n    msa_plot = msa_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'MSA {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(msa_plot, plot_filename)\n    return msa_plot"
        ]
    },
    {
        "func_name": "pfi",
        "original": "def pfi(show: bool=True):\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot",
        "mutated": [
            "def pfi(show: bool=True):\n    if False:\n        i = 10\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot",
            "def pfi(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot",
            "def pfi(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot",
            "def pfi(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot",
            "def pfi(show: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from interpret.ext.blackbox import PFIExplainer\n    pfi = PFIExplainer(model)\n    pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n    pfi_plot = pfi_global.visualize()\n    if save:\n        import plotly.io as pio\n        plot_filename = f'PFI {plot}.html'\n        if not isinstance(save, bool):\n            plot_filename = os.path.join(save, plot_filename)\n        self.logger.info(f\"Saving '{plot_filename}'\")\n        pio.write_html(pfi_plot, plot_filename)\n    return pfi_plot"
        ]
    },
    {
        "func_name": "interpret_model",
        "original": "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    \"\"\"\n        This function takes a trained model object and returns an interpretation plot\n        based on the test / hold-out set. It only supports tree based algorithms.\n\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\n        which is a unified approach to explain the output of any machine learning model.\n        SHAP connects game theory with local explanations.\n\n        For more information : https://shap.readthedocs.io/en/latest/\n\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> dt = create_model('dt')\n        >>> interpret_model(dt)\n\n        This will return a summary interpretation plot of Decision Tree model.\n\n        Parameters\n        ----------\n        estimator : object, default = none\n            A trained model object to be passed as an estimator. Only tree-based\n            models are accepted when plot type is 'summary', 'correlation', or\n            'reason'. 'pdp' plot is model agnostic.\n\n        plot : str, default = 'summary'\n            Abbreviation of type of plot. The current list of plots supported\n            are (Plot - Name):\n            * 'summary' - Summary Plot using SHAP\n            * 'correlation' - Dependence Plot using SHAP\n            * 'reason' - Force Plot using SHAP\n            * 'pdp' - Partial Dependence Plot\n            * 'msa' - Morris Sensitivity Analysis\n            * 'pfi' - Permutation Feature Importance\n\n        feature: str, default = None\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\n            By default feature is set to None which means the first column of the\n            dataset will be used as a variable. A feature parameter must be passed\n            to change this.\n\n        observation: integer, default = None\n            This parameter only comes into effect when plot is set to 'reason'. If no\n            observation number is provided, it will return an analysis of all observations\n            with the option to select the feature on x and y axes through drop down\n            interactivity. For analysis at the sample level, an observation parameter must\n            be passed with the index value of the observation in test / hold-out set.\n\n        use_train_data: bool, default = False\n            When set to true, train data will be used for plots, instead\n            of test data.\n\n        X_new_sample: pd.DataFrame, default = None\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\n            The sample must have the same columns as the raw input train data, and it is transformed\n            by the preprocessing pipeline automatically before plotting.\n\n        y_new_sample: pd.DataFrame, default = None\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\n            The sample must have the same columns as the raw input label data, and it is transformed\n            by the preprocessing pipeline automatically before plotting.\n\n        save: string or bool, default = False\n            When set to True, Plot is saved as a 'png' file in current working directory.\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\n\n        **kwargs:\n            Additional keyword arguments to pass to the plot.\n\n        Returns\n        -------\n        Visual_Plot\n            Returns the visual plot.\n            Returns the interactive JS plot when plot = 'reason'.\n\n        Warnings\n        --------\n        - interpret_model doesn't support multiclass problems.\n\n        \"\"\"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot",
        "mutated": [
            "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    if False:\n        i = 10\n    \"\\n        This function takes a trained model object and returns an interpretation plot\\n        based on the test / hold-out set. It only supports tree based algorithms.\\n\\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\\n        which is a unified approach to explain the output of any machine learning model.\\n        SHAP connects game theory with local explanations.\\n\\n        For more information : https://shap.readthedocs.io/en/latest/\\n\\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> interpret_model(dt)\\n\\n        This will return a summary interpretation plot of Decision Tree model.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object to be passed as an estimator. Only tree-based\\n            models are accepted when plot type is 'summary', 'correlation', or\\n            'reason'. 'pdp' plot is model agnostic.\\n\\n        plot : str, default = 'summary'\\n            Abbreviation of type of plot. The current list of plots supported\\n            are (Plot - Name):\\n            * 'summary' - Summary Plot using SHAP\\n            * 'correlation' - Dependence Plot using SHAP\\n            * 'reason' - Force Plot using SHAP\\n            * 'pdp' - Partial Dependence Plot\\n            * 'msa' - Morris Sensitivity Analysis\\n            * 'pfi' - Permutation Feature Importance\\n\\n        feature: str, default = None\\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\\n            By default feature is set to None which means the first column of the\\n            dataset will be used as a variable. A feature parameter must be passed\\n            to change this.\\n\\n        observation: integer, default = None\\n            This parameter only comes into effect when plot is set to 'reason'. If no\\n            observation number is provided, it will return an analysis of all observations\\n            with the option to select the feature on x and y axes through drop down\\n            interactivity. For analysis at the sample level, an observation parameter must\\n            be passed with the index value of the observation in test / hold-out set.\\n\\n        use_train_data: bool, default = False\\n            When set to true, train data will be used for plots, instead\\n            of test data.\\n\\n        X_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input train data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        y_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input label data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        save: string or bool, default = False\\n            When set to True, Plot is saved as a 'png' file in current working directory.\\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the plot.\\n\\n        Returns\\n        -------\\n        Visual_Plot\\n            Returns the visual plot.\\n            Returns the interactive JS plot when plot = 'reason'.\\n\\n        Warnings\\n        --------\\n        - interpret_model doesn't support multiclass problems.\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot",
            "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function takes a trained model object and returns an interpretation plot\\n        based on the test / hold-out set. It only supports tree based algorithms.\\n\\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\\n        which is a unified approach to explain the output of any machine learning model.\\n        SHAP connects game theory with local explanations.\\n\\n        For more information : https://shap.readthedocs.io/en/latest/\\n\\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> interpret_model(dt)\\n\\n        This will return a summary interpretation plot of Decision Tree model.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object to be passed as an estimator. Only tree-based\\n            models are accepted when plot type is 'summary', 'correlation', or\\n            'reason'. 'pdp' plot is model agnostic.\\n\\n        plot : str, default = 'summary'\\n            Abbreviation of type of plot. The current list of plots supported\\n            are (Plot - Name):\\n            * 'summary' - Summary Plot using SHAP\\n            * 'correlation' - Dependence Plot using SHAP\\n            * 'reason' - Force Plot using SHAP\\n            * 'pdp' - Partial Dependence Plot\\n            * 'msa' - Morris Sensitivity Analysis\\n            * 'pfi' - Permutation Feature Importance\\n\\n        feature: str, default = None\\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\\n            By default feature is set to None which means the first column of the\\n            dataset will be used as a variable. A feature parameter must be passed\\n            to change this.\\n\\n        observation: integer, default = None\\n            This parameter only comes into effect when plot is set to 'reason'. If no\\n            observation number is provided, it will return an analysis of all observations\\n            with the option to select the feature on x and y axes through drop down\\n            interactivity. For analysis at the sample level, an observation parameter must\\n            be passed with the index value of the observation in test / hold-out set.\\n\\n        use_train_data: bool, default = False\\n            When set to true, train data will be used for plots, instead\\n            of test data.\\n\\n        X_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input train data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        y_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input label data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        save: string or bool, default = False\\n            When set to True, Plot is saved as a 'png' file in current working directory.\\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the plot.\\n\\n        Returns\\n        -------\\n        Visual_Plot\\n            Returns the visual plot.\\n            Returns the interactive JS plot when plot = 'reason'.\\n\\n        Warnings\\n        --------\\n        - interpret_model doesn't support multiclass problems.\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot",
            "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function takes a trained model object and returns an interpretation plot\\n        based on the test / hold-out set. It only supports tree based algorithms.\\n\\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\\n        which is a unified approach to explain the output of any machine learning model.\\n        SHAP connects game theory with local explanations.\\n\\n        For more information : https://shap.readthedocs.io/en/latest/\\n\\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> interpret_model(dt)\\n\\n        This will return a summary interpretation plot of Decision Tree model.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object to be passed as an estimator. Only tree-based\\n            models are accepted when plot type is 'summary', 'correlation', or\\n            'reason'. 'pdp' plot is model agnostic.\\n\\n        plot : str, default = 'summary'\\n            Abbreviation of type of plot. The current list of plots supported\\n            are (Plot - Name):\\n            * 'summary' - Summary Plot using SHAP\\n            * 'correlation' - Dependence Plot using SHAP\\n            * 'reason' - Force Plot using SHAP\\n            * 'pdp' - Partial Dependence Plot\\n            * 'msa' - Morris Sensitivity Analysis\\n            * 'pfi' - Permutation Feature Importance\\n\\n        feature: str, default = None\\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\\n            By default feature is set to None which means the first column of the\\n            dataset will be used as a variable. A feature parameter must be passed\\n            to change this.\\n\\n        observation: integer, default = None\\n            This parameter only comes into effect when plot is set to 'reason'. If no\\n            observation number is provided, it will return an analysis of all observations\\n            with the option to select the feature on x and y axes through drop down\\n            interactivity. For analysis at the sample level, an observation parameter must\\n            be passed with the index value of the observation in test / hold-out set.\\n\\n        use_train_data: bool, default = False\\n            When set to true, train data will be used for plots, instead\\n            of test data.\\n\\n        X_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input train data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        y_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input label data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        save: string or bool, default = False\\n            When set to True, Plot is saved as a 'png' file in current working directory.\\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the plot.\\n\\n        Returns\\n        -------\\n        Visual_Plot\\n            Returns the visual plot.\\n            Returns the interactive JS plot when plot = 'reason'.\\n\\n        Warnings\\n        --------\\n        - interpret_model doesn't support multiclass problems.\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot",
            "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function takes a trained model object and returns an interpretation plot\\n        based on the test / hold-out set. It only supports tree based algorithms.\\n\\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\\n        which is a unified approach to explain the output of any machine learning model.\\n        SHAP connects game theory with local explanations.\\n\\n        For more information : https://shap.readthedocs.io/en/latest/\\n\\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> interpret_model(dt)\\n\\n        This will return a summary interpretation plot of Decision Tree model.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object to be passed as an estimator. Only tree-based\\n            models are accepted when plot type is 'summary', 'correlation', or\\n            'reason'. 'pdp' plot is model agnostic.\\n\\n        plot : str, default = 'summary'\\n            Abbreviation of type of plot. The current list of plots supported\\n            are (Plot - Name):\\n            * 'summary' - Summary Plot using SHAP\\n            * 'correlation' - Dependence Plot using SHAP\\n            * 'reason' - Force Plot using SHAP\\n            * 'pdp' - Partial Dependence Plot\\n            * 'msa' - Morris Sensitivity Analysis\\n            * 'pfi' - Permutation Feature Importance\\n\\n        feature: str, default = None\\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\\n            By default feature is set to None which means the first column of the\\n            dataset will be used as a variable. A feature parameter must be passed\\n            to change this.\\n\\n        observation: integer, default = None\\n            This parameter only comes into effect when plot is set to 'reason'. If no\\n            observation number is provided, it will return an analysis of all observations\\n            with the option to select the feature on x and y axes through drop down\\n            interactivity. For analysis at the sample level, an observation parameter must\\n            be passed with the index value of the observation in test / hold-out set.\\n\\n        use_train_data: bool, default = False\\n            When set to true, train data will be used for plots, instead\\n            of test data.\\n\\n        X_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input train data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        y_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input label data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        save: string or bool, default = False\\n            When set to True, Plot is saved as a 'png' file in current working directory.\\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the plot.\\n\\n        Returns\\n        -------\\n        Visual_Plot\\n            Returns the visual plot.\\n            Returns the interactive JS plot when plot = 'reason'.\\n\\n        Warnings\\n        --------\\n        - interpret_model doesn't support multiclass problems.\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot",
            "def interpret_model(self, estimator, plot: str='summary', feature: Optional[str]=None, observation: Optional[int]=None, use_train_data: bool=False, X_new_sample: Optional[pd.DataFrame]=None, y_new_sample: Optional[pd.DataFrame]=None, save: Union[str, bool]=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function takes a trained model object and returns an interpretation plot\\n        based on the test / hold-out set. It only supports tree based algorithms.\\n\\n        This function is implemented based on the SHAP (SHapley Additive exPlanations),\\n        which is a unified approach to explain the output of any machine learning model.\\n        SHAP connects game theory with local explanations.\\n\\n        For more information : https://shap.readthedocs.io/en/latest/\\n\\n        For Partial Dependence Plot : https://github.com/SauceCat/PDPbox\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> dt = create_model('dt')\\n        >>> interpret_model(dt)\\n\\n        This will return a summary interpretation plot of Decision Tree model.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object to be passed as an estimator. Only tree-based\\n            models are accepted when plot type is 'summary', 'correlation', or\\n            'reason'. 'pdp' plot is model agnostic.\\n\\n        plot : str, default = 'summary'\\n            Abbreviation of type of plot. The current list of plots supported\\n            are (Plot - Name):\\n            * 'summary' - Summary Plot using SHAP\\n            * 'correlation' - Dependence Plot using SHAP\\n            * 'reason' - Force Plot using SHAP\\n            * 'pdp' - Partial Dependence Plot\\n            * 'msa' - Morris Sensitivity Analysis\\n            * 'pfi' - Permutation Feature Importance\\n\\n        feature: str, default = None\\n            This parameter is only needed when plot = 'correlation' or 'pdp'.\\n            By default feature is set to None which means the first column of the\\n            dataset will be used as a variable. A feature parameter must be passed\\n            to change this.\\n\\n        observation: integer, default = None\\n            This parameter only comes into effect when plot is set to 'reason'. If no\\n            observation number is provided, it will return an analysis of all observations\\n            with the option to select the feature on x and y axes through drop down\\n            interactivity. For analysis at the sample level, an observation parameter must\\n            be passed with the index value of the observation in test / hold-out set.\\n\\n        use_train_data: bool, default = False\\n            When set to true, train data will be used for plots, instead\\n            of test data.\\n\\n        X_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input train data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        y_new_sample: pd.DataFrame, default = None\\n            Row from an out-of-sample dataframe (neither train nor test data) to be plotted.\\n            The sample must have the same columns as the raw input label data, and it is transformed\\n            by the preprocessing pipeline automatically before plotting.\\n\\n        save: string or bool, default = False\\n            When set to True, Plot is saved as a 'png' file in current working directory.\\n            When a path destination is given, Plot is saved as a 'png' file the given path to the directory of choice.\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the plot.\\n\\n        Returns\\n        -------\\n        Visual_Plot\\n            Returns the visual plot.\\n            Returns the interactive JS plot when plot = 'reason'.\\n\\n        Warnings\\n        --------\\n        - interpret_model doesn't support multiclass problems.\\n\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing interpret_model()')\n    self.logger.info(f'interpret_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    if plot in ['summary', 'correlation', 'reason']:\n        _check_soft_dependencies('shap', extra='analysis', severity='error')\n        import shap\n    if plot == 'pdp':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'msa':\n        _check_soft_dependencies('interpret', extra='analysis', severity='error')\n    if plot == 'pfi':\n        _check_soft_dependencies('interpret_community', extra=None, severity='error', install_name='interpret-community')\n    estimator = get_estimator_from_meta_estimator(estimator)\n    model_id = self._get_model_id(estimator)\n    shap_models = {k: v for (k, v) in self._all_models_internal.items() if v.shap}\n    shap_models_ids = set(shap_models.keys())\n    if plot in ['summary', 'correlation', 'reason'] and model_id not in shap_models_ids:\n        raise TypeError(f\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\")\n    allowed_types = ['summary', 'correlation', 'reason', 'pdp', 'msa', 'pfi']\n    if plot not in allowed_types:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(allowed_types) + str(None))}.\")\n    if X_new_sample is not None and (observation is not None or use_train_data):\n        raise ValueError(\"Specifying 'X_new_sample' and ('observation' or 'use_train_data') is ambiguous.\")\n    '\\n        Error Checking Ends here\\n\\n        '\n    if X_new_sample is not None:\n        test_X = self.pipeline.transform(X_new_sample)\n        if plot == 'pfi':\n            test_y = self.pipeline.transform(y_new_sample)\n    else:\n        if use_train_data:\n            test_X = self.X_train_transformed\n        else:\n            test_X = self.X_test_transformed\n        if plot == 'pfi':\n            if use_train_data:\n                test_y = self.y_train_transformed\n            else:\n                test_y = self.y_test_transformed\n    np.random.seed(self.seed)\n    model = estimator\n    shap_models_type1 = {k for (k, v) in shap_models.items() if v.shap == 'type1'}\n    shap_models_type2 = {k for (k, v) in shap_models.items() if v.shap == 'type2'}\n    self.logger.info(f'plot type: {plot}')\n    shap_plot = None\n\n    def summary(show: bool=True):\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        try:\n            assert len(shap_values) == 2\n            shap_plot = shap.summary_plot(shap_values[1], test_X, show=show, **kwargs)\n        except Exception:\n            shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return shap_plot\n\n    def correlation(show: bool=True):\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for correlation plot: {test_X.columns[0]}')\n            dependence = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            dependence = feature\n        self.logger.info('Creating TreeExplainer')\n        explainer = shap.TreeExplainer(model)\n        self.logger.info('Compiling shap values')\n        shap_values = explainer.shap_values(test_X)\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            shap.dependence_plot(dependence, shap_values[1], test_X, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            shap.dependence_plot(dependence, shap_values, test_X, show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.png'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            plt.savefig(plot_filename, bbox_inches='tight')\n            plt.close()\n        return None\n\n    def reason(show: bool=True):\n        shap_plot = None\n        if model_id in shap_models_type1:\n            self.logger.info('model type detected: type 1')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_values = explainer.shap_values(test_X)\n                shap.initjs()\n                shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], test_X, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                if model_id == 'lightgbm':\n                    self.logger.info('model type detected: LGBMClassifier')\n                    shap_values = explainer.shap_values(test_X)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[0][row_to_show], data_for_prediction, show=show, **kwargs)\n                else:\n                    self.logger.info('model type detected: Unknown')\n                    shap_values = explainer.shap_values(data_for_prediction)\n                    shap.initjs()\n                    shap_plot = shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, show=show, **kwargs)\n        elif model_id in shap_models_type2:\n            self.logger.info('model type detected: type 2')\n            self.logger.info('Creating TreeExplainer')\n            explainer = shap.TreeExplainer(model)\n            self.logger.info('Compiling shap values')\n            shap_values = explainer.shap_values(test_X)\n            shap.initjs()\n            if observation is None:\n                self.logger.warning('Observation set to None. Model agnostic plot will be rendered.')\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values, test_X, show=show, **kwargs)\n            else:\n                row_to_show = observation\n                data_for_prediction = test_X.iloc[row_to_show]\n                shap_plot = shap.force_plot(explainer.expected_value, shap_values[row_to_show, :], test_X.iloc[row_to_show, :], show=show, **kwargs)\n        if save:\n            plot_filename = f'SHAP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            shap.save_html(plot_filename, shap_plot)\n        return shap_plot\n\n    def pdp(show: bool=True):\n        self.logger.info('Checking feature parameter passed')\n        if feature is None:\n            self.logger.warning(f'No feature passed. Default value of feature used for pdp : {test_X.columns[0]}')\n            pdp_feature = test_X.columns[0]\n        else:\n            self.logger.warning(f'feature value passed. Feature used for correlation plot: {feature}')\n            pdp_feature = feature\n        from interpret.blackbox import PartialDependence\n        try:\n            pdp = PartialDependence(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                pdp = PartialDependence(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                pdp = PartialDependence(predict_fn=model.predict, data=test_X)\n        pdp_global = pdp.explain_global()\n        pdp_plot = pdp_global.visualize(list(test_X.columns).index(pdp_feature))\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PDP {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pdp_plot, plot_filename)\n        return pdp_plot\n\n    def msa(show: bool=True):\n        from interpret.blackbox import MorrisSensitivity\n        try:\n            msa = MorrisSensitivity(model=model, data=test_X.to_numpy(), feature_names=list(test_X.columns))\n        except TypeError:\n            try:\n                msa = MorrisSensitivity(predict_fn=model.predict_proba, data=test_X)\n            except AttributeError:\n                msa = MorrisSensitivity(predict_fn=model.predict, data=test_X)\n        msa_global = msa.explain_global()\n        msa_plot = msa_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'MSA {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(msa_plot, plot_filename)\n        return msa_plot\n\n    def pfi(show: bool=True):\n        from interpret.ext.blackbox import PFIExplainer\n        pfi = PFIExplainer(model)\n        pfi_global = pfi.explain_global(test_X, true_labels=test_y)\n        pfi_plot = pfi_global.visualize()\n        if save:\n            import plotly.io as pio\n            plot_filename = f'PFI {plot}.html'\n            if not isinstance(save, bool):\n                plot_filename = os.path.join(save, plot_filename)\n            self.logger.info(f\"Saving '{plot_filename}'\")\n            pio.write_html(pfi_plot, plot_filename)\n        return pfi_plot\n    shap_plot = locals()[plot](show=not save)\n    self.logger.info('Visual Rendered Successfully')\n    self.logger.info('interpret_model() successfully completed......................................')\n    gc.collect()\n    return shap_plot"
        ]
    },
    {
        "func_name": "filter_model_df_by_type",
        "original": "def filter_model_df_by_type(df):\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]",
        "mutated": [
            "def filter_model_df_by_type(df):\n    if False:\n        i = 10\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]",
            "def filter_model_df_by_type(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]",
            "def filter_model_df_by_type(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]",
            "def filter_model_df_by_type(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]",
            "def filter_model_df_by_type(df):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not type:\n        return df\n    return df[df.index.isin(model_type[type])]"
        ]
    },
    {
        "func_name": "models",
        "original": "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    \"\"\"\n        Returns table of models available in model library.\n\n        Example\n        -------\n        >>> _all_models = models()\n\n        This will return pandas dataframe with all available\n        models and their metadata.\n\n        Parameters\n        ----------\n        type : str, default = None\n            - linear : filters and only return linear models\n            - tree : filters and only return tree based models\n            - ensemble : filters and only return ensemble models\n\n        internal: bool, default = False\n            If True, will return extra columns and rows used internally.\n\n        raise_errors: bool, default = True\n            If False, will suppress all exceptions, ignoring models\n            that couldn't be created.\n\n        Returns\n        -------\n        pandas.DataFrame\n\n        \"\"\"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)",
        "mutated": [
            "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n    \"\\n        Returns table of models available in model library.\\n\\n        Example\\n        -------\\n        >>> _all_models = models()\\n\\n        This will return pandas dataframe with all available\\n        models and their metadata.\\n\\n        Parameters\\n        ----------\\n        type : str, default = None\\n            - linear : filters and only return linear models\\n            - tree : filters and only return tree based models\\n            - ensemble : filters and only return ensemble models\\n\\n        internal: bool, default = False\\n            If True, will return extra columns and rows used internally.\\n\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)",
            "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns table of models available in model library.\\n\\n        Example\\n        -------\\n        >>> _all_models = models()\\n\\n        This will return pandas dataframe with all available\\n        models and their metadata.\\n\\n        Parameters\\n        ----------\\n        type : str, default = None\\n            - linear : filters and only return linear models\\n            - tree : filters and only return tree based models\\n            - ensemble : filters and only return ensemble models\\n\\n        internal: bool, default = False\\n            If True, will return extra columns and rows used internally.\\n\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)",
            "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns table of models available in model library.\\n\\n        Example\\n        -------\\n        >>> _all_models = models()\\n\\n        This will return pandas dataframe with all available\\n        models and their metadata.\\n\\n        Parameters\\n        ----------\\n        type : str, default = None\\n            - linear : filters and only return linear models\\n            - tree : filters and only return tree based models\\n            - ensemble : filters and only return ensemble models\\n\\n        internal: bool, default = False\\n            If True, will return extra columns and rows used internally.\\n\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)",
            "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns table of models available in model library.\\n\\n        Example\\n        -------\\n        >>> _all_models = models()\\n\\n        This will return pandas dataframe with all available\\n        models and their metadata.\\n\\n        Parameters\\n        ----------\\n        type : str, default = None\\n            - linear : filters and only return linear models\\n            - tree : filters and only return tree based models\\n            - ensemble : filters and only return ensemble models\\n\\n        internal: bool, default = False\\n            If True, will return extra columns and rows used internally.\\n\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)",
            "def models(self, type: Optional[str]=None, internal: bool=False, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns table of models available in model library.\\n\\n        Example\\n        -------\\n        >>> _all_models = models()\\n\\n        This will return pandas dataframe with all available\\n        models and their metadata.\\n\\n        Parameters\\n        ----------\\n        type : str, default = None\\n            - linear : filters and only return linear models\\n            - tree : filters and only return tree based models\\n            - ensemble : filters and only return ensemble models\\n\\n        internal: bool, default = False\\n            If True, will return extra columns and rows used internally.\\n\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    model_type = {'linear': ['lr', 'ridge', 'svm', 'lasso', 'en', 'lar', 'llar', 'omp', 'br', 'ard', 'par', 'ransac', 'tr', 'huber', 'kr'], 'tree': ['dt'], 'ensemble': ['rf', 'et', 'gbc', 'gbr', 'xgboost', 'lightgbm', 'catboost', 'ada']}\n\n    def filter_model_df_by_type(df):\n        if not type:\n            return df\n        return df[df.index.isin(model_type[type])]\n    if type not in list(model_type) + [None]:\n        raise ValueError(f\"type parameter only accepts {', '.join(list(model_type) + str(None))}.\")\n    self.logger.info(f'gpu_param set to {self.gpu_param}')\n    (_, model_containers) = self._get_models(raise_errors)\n    rows = [v.get_dict(internal) for (k, v) in model_containers.items() if internal or not v.is_special]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    return filter_model_df_by_type(df)"
        ]
    },
    {
        "func_name": "get_metrics",
        "original": "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    \"\"\"\n        Returns table of metrics available.\n\n        Example\n        -------\n        >>> metrics = get_metrics()\n\n        This will return pandas dataframe with all available\n        metrics and their metadata.\n\n        Parameters\n        ----------\n        reset: bool, default = False\n            If True, will reset all changes made using add_metric() and get_metric().\n        include_custom: bool, default = True\n            Whether to include user added (custom) metrics or not.\n        raise_errors: bool, default = True\n            If False, will suppress all exceptions, ignoring models\n            that couldn't be created.\n\n        Returns\n        -------\n        pandas.DataFrame\n\n        \"\"\"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df",
        "mutated": [
            "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n    \"\\n        Returns table of metrics available.\\n\\n        Example\\n        -------\\n        >>> metrics = get_metrics()\\n\\n        This will return pandas dataframe with all available\\n        metrics and their metadata.\\n\\n        Parameters\\n        ----------\\n        reset: bool, default = False\\n            If True, will reset all changes made using add_metric() and get_metric().\\n        include_custom: bool, default = True\\n            Whether to include user added (custom) metrics or not.\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df",
            "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns table of metrics available.\\n\\n        Example\\n        -------\\n        >>> metrics = get_metrics()\\n\\n        This will return pandas dataframe with all available\\n        metrics and their metadata.\\n\\n        Parameters\\n        ----------\\n        reset: bool, default = False\\n            If True, will reset all changes made using add_metric() and get_metric().\\n        include_custom: bool, default = True\\n            Whether to include user added (custom) metrics or not.\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df",
            "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns table of metrics available.\\n\\n        Example\\n        -------\\n        >>> metrics = get_metrics()\\n\\n        This will return pandas dataframe with all available\\n        metrics and their metadata.\\n\\n        Parameters\\n        ----------\\n        reset: bool, default = False\\n            If True, will reset all changes made using add_metric() and get_metric().\\n        include_custom: bool, default = True\\n            Whether to include user added (custom) metrics or not.\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df",
            "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns table of metrics available.\\n\\n        Example\\n        -------\\n        >>> metrics = get_metrics()\\n\\n        This will return pandas dataframe with all available\\n        metrics and their metadata.\\n\\n        Parameters\\n        ----------\\n        reset: bool, default = False\\n            If True, will reset all changes made using add_metric() and get_metric().\\n        include_custom: bool, default = True\\n            Whether to include user added (custom) metrics or not.\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df",
            "def get_metrics(self, reset: bool=False, include_custom: bool=True, raise_errors: bool=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns table of metrics available.\\n\\n        Example\\n        -------\\n        >>> metrics = get_metrics()\\n\\n        This will return pandas dataframe with all available\\n        metrics and their metadata.\\n\\n        Parameters\\n        ----------\\n        reset: bool, default = False\\n            If True, will reset all changes made using add_metric() and get_metric().\\n        include_custom: bool, default = True\\n            Whether to include user added (custom) metrics or not.\\n        raise_errors: bool, default = True\\n            If False, will suppress all exceptions, ignoring models\\n            that couldn't be created.\\n\\n        Returns\\n        -------\\n        pandas.DataFrame\\n\\n        \"\n    if reset and (not self._setup_ran):\n        raise ValueError('setup() needs to be ran first.')\n    np.random.seed(self.seed)\n    if reset:\n        self._all_metrics = self._get_metrics(raise_errors=raise_errors)\n    metric_containers = self._all_metrics\n    rows = [v.get_dict() for (k, v) in metric_containers.items()]\n    df = pd.DataFrame(rows)\n    df.set_index('ID', inplace=True, drop=True)\n    if not include_custom:\n        df = df[df['Custom'] is False]\n    return df"
        ]
    },
    {
        "func_name": "add_metric",
        "original": "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    \"\"\"\n        Adds a custom metric to be used in all functions.\n\n        Parameters\n        ----------\n        id: str\n            Unique id for the metric.\n\n        name: str\n            Display name of the metric.\n\n        score_func: type\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\n\n        target: str, default = 'pred'\n            The target of the score function.\n            - 'pred' for the prediction table\n            - 'pred_proba' for pred_proba\n            - 'threshold' for decision_function or predict_proba\n\n        greater_is_better: bool, default = True\n            Whether score_func is a score function (default), meaning high is good,\n            or a loss function, meaning low is good. In the latter case, the\n            scorer object will sign-flip the outcome of the score_func.\n\n        multiclass: bool, default = True\n            Whether the metric supports multiclass problems.\n\n        **kwargs:\n            Arguments to be passed to score function.\n\n        Returns\n        -------\n        pandas.Series\n            The created row as Series.\n\n        \"\"\"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric",
        "mutated": [
            "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    if False:\n        i = 10\n    \"\\n        Adds a custom metric to be used in all functions.\\n\\n        Parameters\\n        ----------\\n        id: str\\n            Unique id for the metric.\\n\\n        name: str\\n            Display name of the metric.\\n\\n        score_func: type\\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\\n\\n        target: str, default = 'pred'\\n            The target of the score function.\\n            - 'pred' for the prediction table\\n            - 'pred_proba' for pred_proba\\n            - 'threshold' for decision_function or predict_proba\\n\\n        greater_is_better: bool, default = True\\n            Whether score_func is a score function (default), meaning high is good,\\n            or a loss function, meaning low is good. In the latter case, the\\n            scorer object will sign-flip the outcome of the score_func.\\n\\n        multiclass: bool, default = True\\n            Whether the metric supports multiclass problems.\\n\\n        **kwargs:\\n            Arguments to be passed to score function.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The created row as Series.\\n\\n        \"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric",
            "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Adds a custom metric to be used in all functions.\\n\\n        Parameters\\n        ----------\\n        id: str\\n            Unique id for the metric.\\n\\n        name: str\\n            Display name of the metric.\\n\\n        score_func: type\\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\\n\\n        target: str, default = 'pred'\\n            The target of the score function.\\n            - 'pred' for the prediction table\\n            - 'pred_proba' for pred_proba\\n            - 'threshold' for decision_function or predict_proba\\n\\n        greater_is_better: bool, default = True\\n            Whether score_func is a score function (default), meaning high is good,\\n            or a loss function, meaning low is good. In the latter case, the\\n            scorer object will sign-flip the outcome of the score_func.\\n\\n        multiclass: bool, default = True\\n            Whether the metric supports multiclass problems.\\n\\n        **kwargs:\\n            Arguments to be passed to score function.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The created row as Series.\\n\\n        \"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric",
            "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Adds a custom metric to be used in all functions.\\n\\n        Parameters\\n        ----------\\n        id: str\\n            Unique id for the metric.\\n\\n        name: str\\n            Display name of the metric.\\n\\n        score_func: type\\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\\n\\n        target: str, default = 'pred'\\n            The target of the score function.\\n            - 'pred' for the prediction table\\n            - 'pred_proba' for pred_proba\\n            - 'threshold' for decision_function or predict_proba\\n\\n        greater_is_better: bool, default = True\\n            Whether score_func is a score function (default), meaning high is good,\\n            or a loss function, meaning low is good. In the latter case, the\\n            scorer object will sign-flip the outcome of the score_func.\\n\\n        multiclass: bool, default = True\\n            Whether the metric supports multiclass problems.\\n\\n        **kwargs:\\n            Arguments to be passed to score function.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The created row as Series.\\n\\n        \"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric",
            "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Adds a custom metric to be used in all functions.\\n\\n        Parameters\\n        ----------\\n        id: str\\n            Unique id for the metric.\\n\\n        name: str\\n            Display name of the metric.\\n\\n        score_func: type\\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\\n\\n        target: str, default = 'pred'\\n            The target of the score function.\\n            - 'pred' for the prediction table\\n            - 'pred_proba' for pred_proba\\n            - 'threshold' for decision_function or predict_proba\\n\\n        greater_is_better: bool, default = True\\n            Whether score_func is a score function (default), meaning high is good,\\n            or a loss function, meaning low is good. In the latter case, the\\n            scorer object will sign-flip the outcome of the score_func.\\n\\n        multiclass: bool, default = True\\n            Whether the metric supports multiclass problems.\\n\\n        **kwargs:\\n            Arguments to be passed to score function.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The created row as Series.\\n\\n        \"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric",
            "def add_metric(self, id: str, name: str, score_func: type, target: str='pred', greater_is_better: bool=True, multiclass: bool=True, **kwargs) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Adds a custom metric to be used in all functions.\\n\\n        Parameters\\n        ----------\\n        id: str\\n            Unique id for the metric.\\n\\n        name: str\\n            Display name of the metric.\\n\\n        score_func: type\\n            Score function (or loss function) with signature score_func(y, y_pred, **kwargs).\\n\\n        target: str, default = 'pred'\\n            The target of the score function.\\n            - 'pred' for the prediction table\\n            - 'pred_proba' for pred_proba\\n            - 'threshold' for decision_function or predict_proba\\n\\n        greater_is_better: bool, default = True\\n            Whether score_func is a score function (default), meaning high is good,\\n            or a loss function, meaning low is good. In the latter case, the\\n            scorer object will sign-flip the outcome of the score_func.\\n\\n        multiclass: bool, default = True\\n            Whether the metric supports multiclass problems.\\n\\n        **kwargs:\\n            Arguments to be passed to score function.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The created row as Series.\\n\\n        \"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    if id in self._all_metrics:\n        raise ValueError('id already present in metrics dataframe.')\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        new_metric = pycaret.containers.metrics.classification.ClassificationMetricContainer(id=id, name=name, score_func=EncodedDecodedLabelsReplaceScoreFunc(score_func, get_pos_label(self.__dict__)), target=target, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_multiclass=bool(multiclass), is_custom=True)\n    elif self._ml_usecase == MLUsecase.TIME_SERIES:\n        new_metric = pycaret.containers.metrics.time_series.TimeSeriesMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    else:\n        new_metric = pycaret.containers.metrics.regression.RegressionMetricContainer(id=id, name=name, score_func=score_func, args=kwargs, display_name=name, greater_is_better=greater_is_better, is_custom=True)\n    self._all_metrics[id] = new_metric\n    new_metric = new_metric.get_dict()\n    new_metric = pd.Series(new_metric, name=id.replace(' ', '_')).drop('ID')\n    return new_metric"
        ]
    },
    {
        "func_name": "remove_metric",
        "original": "def remove_metric(self, name_or_id: str):\n    \"\"\"\n        Removes a metric used in all functions.\n\n        Parameters\n        ----------\n        name_or_id: str\n            Display name or ID of the metric.\n\n        \"\"\"\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")",
        "mutated": [
            "def remove_metric(self, name_or_id: str):\n    if False:\n        i = 10\n    '\\n        Removes a metric used in all functions.\\n\\n        Parameters\\n        ----------\\n        name_or_id: str\\n            Display name or ID of the metric.\\n\\n        '\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")",
            "def remove_metric(self, name_or_id: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Removes a metric used in all functions.\\n\\n        Parameters\\n        ----------\\n        name_or_id: str\\n            Display name or ID of the metric.\\n\\n        '\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")",
            "def remove_metric(self, name_or_id: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Removes a metric used in all functions.\\n\\n        Parameters\\n        ----------\\n        name_or_id: str\\n            Display name or ID of the metric.\\n\\n        '\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")",
            "def remove_metric(self, name_or_id: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Removes a metric used in all functions.\\n\\n        Parameters\\n        ----------\\n        name_or_id: str\\n            Display name or ID of the metric.\\n\\n        '\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")",
            "def remove_metric(self, name_or_id: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Removes a metric used in all functions.\\n\\n        Parameters\\n        ----------\\n        name_or_id: str\\n            Display name or ID of the metric.\\n\\n        '\n    if not self._setup_ran:\n        raise ValueError('setup() needs to be ran first.')\n    try:\n        self._all_metrics.pop(name_or_id)\n        return\n    except Exception:\n        pass\n    try:\n        k_to_remove = next((k for (k, v) in self._all_metrics.items() if v.name == name_or_id))\n        self._all_metrics.pop(k_to_remove)\n        return\n    except Exception:\n        pass\n    raise ValueError(f\"No metric 'Display Name' or 'ID' (index) {name_or_id} present in the metrics repository.\")"
        ]
    },
    {
        "func_name": "finalize_model",
        "original": "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    \"\"\"\n        This function fits the complete pipeline with the estimator on the\n        complete dataset passed during the setup() stage. The purpose of\n        this function is to prepare for final model deployment after\n        experimentation.\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> lr = create_model('lr')\n        >>> final_lr = finalize_model(lr)\n\n        This will return the final model object fitted to complete dataset.\n\n        Parameters\n        ----------\n        estimator : object, default = none\n            A trained model object should be passed as an estimator.\n\n        fit_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the fit method of the model.\n\n        groups: str or array-like, with shape (n_samples,), default = None\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\n            If string is passed, will use the data column with that name as the groups.\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\n            If None, will use the value set in fold_groups parameter in setup().\n\n        model_only : bool, default = False\n            Whether to return the complete fitted pipeline or only the fitted model.\n\n        experiment_custom_tags: dict, default = None\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\n\n        Returns\n        -------\n            Trained pipeline or model object fitted on complete dataset.\n\n        Warnings\n        --------\n        - If the model returned by finalize_model(), is used on predict_model() without\n        passing a new unseen dataset, then the information grid printed is misleading\n        as the model is trained on the complete dataset including test / hold-out sample.\n        Once finalize_model() is used, the model is considered ready for deployment and\n        should be used on new unseen dataset only.\n\n        \"\"\"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final",
        "mutated": [
            "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function fits the complete pipeline with the estimator on the\\n        complete dataset passed during the setup() stage. The purpose of\\n        this function is to prepare for final model deployment after\\n        experimentation.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> final_lr = finalize_model(lr)\\n\\n        This will return the final model object fitted to complete dataset.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object should be passed as an estimator.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        model_only : bool, default = False\\n            Whether to return the complete fitted pipeline or only the fitted model.\\n\\n        experiment_custom_tags: dict, default = None\\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\\n\\n        Returns\\n        -------\\n            Trained pipeline or model object fitted on complete dataset.\\n\\n        Warnings\\n        --------\\n        - If the model returned by finalize_model(), is used on predict_model() without\\n        passing a new unseen dataset, then the information grid printed is misleading\\n        as the model is trained on the complete dataset including test / hold-out sample.\\n        Once finalize_model() is used, the model is considered ready for deployment and\\n        should be used on new unseen dataset only.\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final",
            "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function fits the complete pipeline with the estimator on the\\n        complete dataset passed during the setup() stage. The purpose of\\n        this function is to prepare for final model deployment after\\n        experimentation.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> final_lr = finalize_model(lr)\\n\\n        This will return the final model object fitted to complete dataset.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object should be passed as an estimator.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        model_only : bool, default = False\\n            Whether to return the complete fitted pipeline or only the fitted model.\\n\\n        experiment_custom_tags: dict, default = None\\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\\n\\n        Returns\\n        -------\\n            Trained pipeline or model object fitted on complete dataset.\\n\\n        Warnings\\n        --------\\n        - If the model returned by finalize_model(), is used on predict_model() without\\n        passing a new unseen dataset, then the information grid printed is misleading\\n        as the model is trained on the complete dataset including test / hold-out sample.\\n        Once finalize_model() is used, the model is considered ready for deployment and\\n        should be used on new unseen dataset only.\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final",
            "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function fits the complete pipeline with the estimator on the\\n        complete dataset passed during the setup() stage. The purpose of\\n        this function is to prepare for final model deployment after\\n        experimentation.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> final_lr = finalize_model(lr)\\n\\n        This will return the final model object fitted to complete dataset.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object should be passed as an estimator.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        model_only : bool, default = False\\n            Whether to return the complete fitted pipeline or only the fitted model.\\n\\n        experiment_custom_tags: dict, default = None\\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\\n\\n        Returns\\n        -------\\n            Trained pipeline or model object fitted on complete dataset.\\n\\n        Warnings\\n        --------\\n        - If the model returned by finalize_model(), is used on predict_model() without\\n        passing a new unseen dataset, then the information grid printed is misleading\\n        as the model is trained on the complete dataset including test / hold-out sample.\\n        Once finalize_model() is used, the model is considered ready for deployment and\\n        should be used on new unseen dataset only.\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final",
            "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function fits the complete pipeline with the estimator on the\\n        complete dataset passed during the setup() stage. The purpose of\\n        this function is to prepare for final model deployment after\\n        experimentation.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> final_lr = finalize_model(lr)\\n\\n        This will return the final model object fitted to complete dataset.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object should be passed as an estimator.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        model_only : bool, default = False\\n            Whether to return the complete fitted pipeline or only the fitted model.\\n\\n        experiment_custom_tags: dict, default = None\\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\\n\\n        Returns\\n        -------\\n            Trained pipeline or model object fitted on complete dataset.\\n\\n        Warnings\\n        --------\\n        - If the model returned by finalize_model(), is used on predict_model() without\\n        passing a new unseen dataset, then the information grid printed is misleading\\n        as the model is trained on the complete dataset including test / hold-out sample.\\n        Once finalize_model() is used, the model is considered ready for deployment and\\n        should be used on new unseen dataset only.\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final",
            "def finalize_model(self, estimator, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, model_only: bool=False, experiment_custom_tags: Optional[Dict[str, Any]]=None) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function fits the complete pipeline with the estimator on the\\n        complete dataset passed during the setup() stage. The purpose of\\n        this function is to prepare for final model deployment after\\n        experimentation.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> final_lr = finalize_model(lr)\\n\\n        This will return the final model object fitted to complete dataset.\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object should be passed as an estimator.\\n\\n        fit_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the fit method of the model.\\n\\n        groups: str or array-like, with shape (n_samples,), default = None\\n            Optional Group labels for the samples used while splitting the dataset into train/test set.\\n            If string is passed, will use the data column with that name as the groups.\\n            Only used if a group based cross-validation generator is used (eg. GroupKFold).\\n            If None, will use the value set in fold_groups parameter in setup().\\n\\n        model_only : bool, default = False\\n            Whether to return the complete fitted pipeline or only the fitted model.\\n\\n        experiment_custom_tags: dict, default = None\\n            Dictionary of tag_name: String -> value: (String, but will be string-ified if\\n            not) passed to the mlflow.set_tags to add new custom tags for the experiment.\\n\\n        Returns\\n        -------\\n            Trained pipeline or model object fitted on complete dataset.\\n\\n        Warnings\\n        --------\\n        - If the model returned by finalize_model(), is used on predict_model() without\\n        passing a new unseen dataset, then the information grid printed is misleading\\n        as the model is trained on the complete dataset including test / hold-out sample.\\n        Once finalize_model() is used, the model is considered ready for deployment and\\n        should be used on new unseen dataset only.\\n\\n        \"\n    self._check_setup_ran()\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing finalize_model()')\n    self.logger.info(f'finalize_model({function_params_str})')\n    runtime_start = time.time()\n    display = CommonDisplay(verbose=False, html_param=self.html_param)\n    np.random.seed(self.seed)\n    self.logger.info(f'Finalizing {estimator}')\n    (pipeline_final, model_fit_time) = self._create_model(estimator=estimator, cross_validation=False, verbose=False, system=False, X_train_data=self.X, y_train_data=self.y, fit_kwargs=fit_kwargs or {}, predict=False, groups=self._get_groups(groups, data=self.X), add_to_model_list=False, model_only=False)\n    if self.logging_param:\n        self._log_model(model=pipeline_final, model_results=None, score_dict={}, source='finalize_model', runtime=np.array(time.time() - runtime_start).round(2), model_fit_time=model_fit_time, pipeline=self.pipeline, log_plots=self.log_plots_param, experiment_custom_tags=experiment_custom_tags, display=display)\n    self.logger.info(f'_master_model_container: {len(self._master_model_container)}')\n    self.logger.info(f'_display_container: {len(self._display_container)}')\n    self.logger.info(str(pipeline_final))\n    self.logger.info('finalize_model() successfully completed......................................')\n    gc.collect()\n    if model_only:\n        return self._get_final_model_from_pipeline(pipeline_final)\n    return pipeline_final"
        ]
    },
    {
        "func_name": "encode_labels",
        "original": "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels",
        "mutated": [
            "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if False:\n        i = 10\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels",
            "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels",
            "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels",
            "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels",
            "def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if label_encoder:\n        return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n    else:\n        return labels"
        ]
    },
    {
        "func_name": "predict_model",
        "original": "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    \"\"\"\n        This function is used to predict label and probability score on the new dataset\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\n        setup() is used to generate predictions.\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\n        >>> lr = create_model('lr')\n        >>> lr_predictions_holdout = predict_model(lr)\n\n        Parameters\n        ----------\n        estimator : object, default = none\n            A trained model object / pipeline should be passed as an estimator.\n\n        data : pandas.DataFrame\n            Shape (n_samples, n_features) where n_samples is the number of samples\n            and n_features is the number of features. All features used during training\n            must be present in the new dataset.\n\n        probability_threshold : float, default = None\n            Threshold used to convert probability values into binary outcome. By default\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\n            changed using probability_threshold param.\n\n        encoded_labels: Boolean, default = False\n            If True, will return labels encoded as an integer.\n\n        raw_score: bool, default = False\n            When set to True, scores for all labels will be returned.\n\n        round: integer, default = 4\n            Number of decimal places the metrics in the score grid will be rounded to.\n\n        verbose: bool, default = True\n            Holdout score grid is not printed when verbose is set to False.\n\n        preprocess: bool or 'features', default = True\n            Whether to preprocess unseen data. If 'features', will not\n            preprocess labels.\n\n        Returns\n        -------\n        Predictions\n            Predictions (label and score) columns are attached to the original\n            dataset and returned as pandas dataframe.\n\n        score_grid\n            A table containing the scoring metrics on hold-out / test set.\n\n        Warnings\n        --------\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\n        the version for inference.\n\n        \"\"\"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_",
        "mutated": [
            "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    if False:\n        i = 10\n    \"\\n        This function is used to predict label and probability score on the new dataset\\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\\n        setup() is used to generate predictions.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> lr_predictions_holdout = predict_model(lr)\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object / pipeline should be passed as an estimator.\\n\\n        data : pandas.DataFrame\\n            Shape (n_samples, n_features) where n_samples is the number of samples\\n            and n_features is the number of features. All features used during training\\n            must be present in the new dataset.\\n\\n        probability_threshold : float, default = None\\n            Threshold used to convert probability values into binary outcome. By default\\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\\n            changed using probability_threshold param.\\n\\n        encoded_labels: Boolean, default = False\\n            If True, will return labels encoded as an integer.\\n\\n        raw_score: bool, default = False\\n            When set to True, scores for all labels will be returned.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        verbose: bool, default = True\\n            Holdout score grid is not printed when verbose is set to False.\\n\\n        preprocess: bool or 'features', default = True\\n            Whether to preprocess unseen data. If 'features', will not\\n            preprocess labels.\\n\\n        Returns\\n        -------\\n        Predictions\\n            Predictions (label and score) columns are attached to the original\\n            dataset and returned as pandas dataframe.\\n\\n        score_grid\\n            A table containing the scoring metrics on hold-out / test set.\\n\\n        Warnings\\n        --------\\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\\n        the version for inference.\\n\\n        \"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_",
            "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function is used to predict label and probability score on the new dataset\\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\\n        setup() is used to generate predictions.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> lr_predictions_holdout = predict_model(lr)\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object / pipeline should be passed as an estimator.\\n\\n        data : pandas.DataFrame\\n            Shape (n_samples, n_features) where n_samples is the number of samples\\n            and n_features is the number of features. All features used during training\\n            must be present in the new dataset.\\n\\n        probability_threshold : float, default = None\\n            Threshold used to convert probability values into binary outcome. By default\\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\\n            changed using probability_threshold param.\\n\\n        encoded_labels: Boolean, default = False\\n            If True, will return labels encoded as an integer.\\n\\n        raw_score: bool, default = False\\n            When set to True, scores for all labels will be returned.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        verbose: bool, default = True\\n            Holdout score grid is not printed when verbose is set to False.\\n\\n        preprocess: bool or 'features', default = True\\n            Whether to preprocess unseen data. If 'features', will not\\n            preprocess labels.\\n\\n        Returns\\n        -------\\n        Predictions\\n            Predictions (label and score) columns are attached to the original\\n            dataset and returned as pandas dataframe.\\n\\n        score_grid\\n            A table containing the scoring metrics on hold-out / test set.\\n\\n        Warnings\\n        --------\\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\\n        the version for inference.\\n\\n        \"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_",
            "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function is used to predict label and probability score on the new dataset\\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\\n        setup() is used to generate predictions.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> lr_predictions_holdout = predict_model(lr)\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object / pipeline should be passed as an estimator.\\n\\n        data : pandas.DataFrame\\n            Shape (n_samples, n_features) where n_samples is the number of samples\\n            and n_features is the number of features. All features used during training\\n            must be present in the new dataset.\\n\\n        probability_threshold : float, default = None\\n            Threshold used to convert probability values into binary outcome. By default\\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\\n            changed using probability_threshold param.\\n\\n        encoded_labels: Boolean, default = False\\n            If True, will return labels encoded as an integer.\\n\\n        raw_score: bool, default = False\\n            When set to True, scores for all labels will be returned.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        verbose: bool, default = True\\n            Holdout score grid is not printed when verbose is set to False.\\n\\n        preprocess: bool or 'features', default = True\\n            Whether to preprocess unseen data. If 'features', will not\\n            preprocess labels.\\n\\n        Returns\\n        -------\\n        Predictions\\n            Predictions (label and score) columns are attached to the original\\n            dataset and returned as pandas dataframe.\\n\\n        score_grid\\n            A table containing the scoring metrics on hold-out / test set.\\n\\n        Warnings\\n        --------\\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\\n        the version for inference.\\n\\n        \"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_",
            "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function is used to predict label and probability score on the new dataset\\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\\n        setup() is used to generate predictions.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> lr_predictions_holdout = predict_model(lr)\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object / pipeline should be passed as an estimator.\\n\\n        data : pandas.DataFrame\\n            Shape (n_samples, n_features) where n_samples is the number of samples\\n            and n_features is the number of features. All features used during training\\n            must be present in the new dataset.\\n\\n        probability_threshold : float, default = None\\n            Threshold used to convert probability values into binary outcome. By default\\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\\n            changed using probability_threshold param.\\n\\n        encoded_labels: Boolean, default = False\\n            If True, will return labels encoded as an integer.\\n\\n        raw_score: bool, default = False\\n            When set to True, scores for all labels will be returned.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        verbose: bool, default = True\\n            Holdout score grid is not printed when verbose is set to False.\\n\\n        preprocess: bool or 'features', default = True\\n            Whether to preprocess unseen data. If 'features', will not\\n            preprocess labels.\\n\\n        Returns\\n        -------\\n        Predictions\\n            Predictions (label and score) columns are attached to the original\\n            dataset and returned as pandas dataframe.\\n\\n        score_grid\\n            A table containing the scoring metrics on hold-out / test set.\\n\\n        Warnings\\n        --------\\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\\n        the version for inference.\\n\\n        \"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_",
            "def predict_model(self, estimator, data: Optional[pd.DataFrame]=None, probability_threshold: Optional[float]=None, encoded_labels: bool=False, raw_score: bool=False, round: int=4, verbose: bool=True, ml_usecase: Optional[MLUsecase]=None, preprocess: Union[bool, str]=True) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function is used to predict label and probability score on the new dataset\\n        using a trained estimator. New unseen data can be passed to data parameter as pandas\\n        Dataframe. If data is not passed, the test / hold-out set separated at the time of\\n        setup() is used to generate predictions.\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> experiment_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> lr_predictions_holdout = predict_model(lr)\\n\\n        Parameters\\n        ----------\\n        estimator : object, default = none\\n            A trained model object / pipeline should be passed as an estimator.\\n\\n        data : pandas.DataFrame\\n            Shape (n_samples, n_features) where n_samples is the number of samples\\n            and n_features is the number of features. All features used during training\\n            must be present in the new dataset.\\n\\n        probability_threshold : float, default = None\\n            Threshold used to convert probability values into binary outcome. By default\\n            the probability threshold for all binary classifiers is 0.5 (50%). This can be\\n            changed using probability_threshold param.\\n\\n        encoded_labels: Boolean, default = False\\n            If True, will return labels encoded as an integer.\\n\\n        raw_score: bool, default = False\\n            When set to True, scores for all labels will be returned.\\n\\n        round: integer, default = 4\\n            Number of decimal places the metrics in the score grid will be rounded to.\\n\\n        verbose: bool, default = True\\n            Holdout score grid is not printed when verbose is set to False.\\n\\n        preprocess: bool or 'features', default = True\\n            Whether to preprocess unseen data. If 'features', will not\\n            preprocess labels.\\n\\n        Returns\\n        -------\\n        Predictions\\n            Predictions (label and score) columns are attached to the original\\n            dataset and returned as pandas dataframe.\\n\\n        score_grid\\n            A table containing the scoring metrics on hold-out / test set.\\n\\n        Warnings\\n        --------\\n        - The behavior of the predict_model is changed in version 2.1 without backward compatibility.\\n        As such, the pipelines trained using the version (<= 2.0), may not work for inference\\n        with version >= 2.1. You can either retrain your models with a newer version or downgrade\\n        the version for inference.\\n\\n        \"\n\n    def encode_labels(label_encoder, labels: pd.Series) -> pd.Series:\n        if label_encoder:\n            return pd.Series(data=label_encoder.transform(labels), name=labels.name, index=labels.index)\n        else:\n            return labels\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items() if k != 'data'])\n    self.logger.info('Initializing predict_model()')\n    self.logger.info(f'predict_model({function_params_str})')\n    self.logger.info('Checking exceptions')\n    '\\n        exception checking starts here\\n        '\n    if ml_usecase is None:\n        ml_usecase = self._ml_usecase\n    if data is None and (not self._setup_ran):\n        raise ValueError('data parameter may not be None without running setup() first.')\n    if probability_threshold is not None:\n        allowed_types = [int, float]\n        if type(probability_threshold) not in allowed_types or probability_threshold > 1 or probability_threshold < 0:\n            raise TypeError('probability_threshold parameter only accepts value between 0 to 1.')\n    '\\n        exception checking ends here\\n        '\n    self.logger.info('Preloading libraries')\n    try:\n        np.random.seed(self.seed)\n        display = CommonDisplay(verbose=verbose, html_param=self.html_param)\n    except Exception:\n        display = CommonDisplay(verbose=False, html_param=False)\n    if isinstance(estimator, skPipeline):\n        if not hasattr(estimator, 'feature_names_in_'):\n            raise ValueError('If estimator is a Pipeline, it must implement `feature_names_in_`.')\n        pipeline = copy(estimator)\n        final_step = pipeline.steps[-1]\n        estimator = final_step[-1]\n        pipeline.steps = pipeline.steps[:-1]\n    elif not self._setup_ran:\n        raise ValueError('If estimator is not a Pipeline, you must run setup() first.')\n    else:\n        pipeline = self.pipeline\n        final_step = None\n    X_columns = pipeline.feature_names_in_[:-1]\n    y_name = pipeline.feature_names_in_[-1]\n    y_test_ = None\n    if data is None:\n        (X_test_, y_test_) = (self.X_test_transformed, self.y_test_transformed)\n        X_test_untransformed = self.X_test[self.X_test.index.isin(X_test_.index)]\n        y_test_untransformed = self.y_test[self.y_test.index.isin(y_test_.index)]\n    else:\n        if y_name in data.columns:\n            data = self._set_index(self._prepare_dataset(data, y_name))\n            target = data[y_name]\n            data = data.drop(y_name, axis=1)\n        else:\n            data = self._set_index(self._prepare_dataset(data))\n            target = None\n        X_test_untransformed = data\n        y_test_untransformed = target\n        data = data[X_columns]\n        if preprocess:\n            X_test_ = pipeline.transform(X=data, y=target if preprocess != 'features' else None)\n            if final_step:\n                pipeline.steps.append(final_step)\n            if isinstance(X_test_, tuple):\n                (X_test_, y_test_) = X_test_\n            elif target is not None:\n                y_test_ = target\n        else:\n            X_test_ = data\n            y_test_ = target\n        X_test_untransformed = X_test_untransformed[X_test_untransformed.index.isin(X_test_.index)]\n        if target is not None:\n            y_test_untransformed = y_test_untransformed[y_test_untransformed.index.isin(X_test_.index)]\n    if isinstance(estimator, CustomProbabilityThresholdClassifier):\n        if probability_threshold is None:\n            probability_threshold = estimator.probability_threshold\n        estimator = get_estimator_from_meta_estimator(estimator)\n    pred = np.nan_to_num(estimator.predict(X_test_))\n    pred = pipeline.inverse_transform(pred)\n    label_encoder = get_label_encoder(pipeline)\n    if isinstance(pred, pd.Series):\n        pred = pred.values\n    try:\n        score = estimator.predict_proba(X_test_)\n        if len(np.unique(pred)) <= 2:\n            pred_prob = score[:, 1]\n        else:\n            pred_prob = score\n    except Exception:\n        score = None\n        pred_prob = None\n    y_test_metrics = y_test_untransformed\n    if probability_threshold is not None and pred_prob is not None:\n        try:\n            pred = (pred_prob >= probability_threshold).astype(int)\n            if label_encoder:\n                pred = label_encoder.inverse_transform(pred)\n        except Exception:\n            pass\n    if pred_prob is None:\n        pred_prob = pred\n    df_score = None\n    if y_test_ is not None and self._setup_ran:\n        full_name = self._get_model_name(estimator)\n        metrics = self._calculate_metrics(y_test_metrics, pred, pred_prob)\n        df_score = pd.DataFrame(metrics, index=[0])\n        df_score.insert(0, 'Model', full_name)\n        df_score = df_score.round(round)\n        display.display(df_score.style.format(precision=round))\n    if ml_usecase == MLUsecase.CLASSIFICATION:\n        try:\n            pred = pred.astype(int)\n        except Exception:\n            pass\n    label = pd.DataFrame(pred, columns=[LABEL_COLUMN], index=X_test_untransformed.index)\n    if encoded_labels:\n        label[LABEL_COLUMN] = encode_labels(label_encoder, label[LABEL_COLUMN])\n    old_index = X_test_untransformed.index\n    X_test_ = pd.concat([X_test_untransformed, y_test_untransformed, label], axis=1)\n    X_test_.index = old_index\n    if score is not None:\n        if not raw_score:\n            if label_encoder:\n                pred = label_encoder.transform(pred)\n            score = pd.DataFrame(data=[s[pred[i]] for (i, s) in enumerate(score)], index=X_test_.index, columns=[SCORE_COLUMN])\n        else:\n            if not encoded_labels:\n                if label_encoder:\n                    columns = label_encoder.classes_\n                else:\n                    columns = range(score.shape[1])\n            else:\n                columns = range(score.shape[1])\n            score = pd.DataFrame(data=score, index=X_test_.index, columns=[f'{SCORE_COLUMN}_{col}' for col in columns])\n        score = score.round(round)\n        X_test_ = pd.concat((X_test_, score), axis=1)\n    if df_score is not None:\n        self._display_container.append(df_score)\n    gc.collect()\n    return X_test_"
        ]
    },
    {
        "func_name": "get_leaderboard",
        "original": "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    \"\"\"\n        generates leaderboard for all models run in current run.\n        \"\"\"\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results",
        "mutated": [
            "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    if False:\n        i = 10\n    '\\n        generates leaderboard for all models run in current run.\\n        '\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results",
            "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        generates leaderboard for all models run in current run.\\n        '\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results",
            "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        generates leaderboard for all models run in current run.\\n        '\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results",
            "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        generates leaderboard for all models run in current run.\\n        '\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results",
            "def get_leaderboard(self, finalize_models: bool=False, model_only: bool=False, fit_kwargs: Optional[dict]=None, groups: Optional[Union[str, Any]]=None, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        generates leaderboard for all models run in current run.\\n        '\n    model_container = self._master_model_container\n    progress_args = {'max': len(model_container) + 1}\n    timestampStr = datetime.datetime.now().strftime('%H:%M:%S')\n    monitor_rows = [['Initiated', '. . . . . . . . . . . . . . . . . .', timestampStr], ['Status', '. . . . . . . . . . . . . . . . . .', 'Loading Dependencies'], ['Estimator', '. . . . . . . . . . . . . . . . . .', 'Compiling Library']]\n    display = CommonDisplay(verbose=verbose, html_param=self.html_param, progress_args=progress_args, monitor_rows=monitor_rows)\n    result_container_mean = []\n    finalized_models = []\n    display.update_monitor(1, 'Finalizing models' if finalize_models else 'Collecting models')\n    for (i, model_results_tuple) in enumerate(model_container):\n        model_results = model_results_tuple['scores']\n        model = model_results_tuple['model']\n        try:\n            mean_scores = model_results.loc[['Mean']]\n        except KeyError:\n            continue\n        model_name = self._get_model_name(model)\n        mean_scores['Index'] = i\n        mean_scores['Model Name'] = model_name\n        display.update_monitor(2, model_name)\n        if finalize_models:\n            model = self.finalize_model(model, fit_kwargs=fit_kwargs, groups=groups, model_only=model_only)\n        else:\n            model = deepcopy(model)\n            if not is_fitted(model):\n                (model, _) = self._create_model(estimator=model, verbose=False, system=False, fit_kwargs=fit_kwargs, groups=groups, add_to_model_list=False)\n            if not model_only:\n                pipeline = deepcopy(self.pipeline)\n                pipeline.steps.append(['trained_model', model])\n                model = pipeline\n        display.move_progress()\n        finalized_models.append(model)\n        result_container_mean.append(mean_scores)\n    display.update_monitor(1, 'Creating dataframe')\n    results = pd.concat(result_container_mean)\n    results['Model'] = list(range(len(results)))\n    results['Model'] = results['Model'].astype('object')\n    model_loc = results.columns.get_loc('Model')\n    for x in range(len(results)):\n        results.iat[x, model_loc] = finalized_models[x]\n    rearranged_columns = list(results.columns)\n    rearranged_columns.remove('Model')\n    rearranged_columns.remove('Model Name')\n    rearranged_columns = ['Model Name', 'Model'] + rearranged_columns\n    results = results[rearranged_columns]\n    results.set_index('Index', inplace=True, drop=True)\n    display.close()\n    return results"
        ]
    },
    {
        "func_name": "check_fairness",
        "original": "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    \"\"\"\n        There are many approaches to conceptualizing fairness. This function follows\n        the approach known as group fairness, which asks: Which groups of individuals\n        are at risk for experiencing harms. This function provides fairness-related\n        metrics between different groups (also called subpopulation).\n\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> income = get_data('income')\n        >>> from pycaret.classification import *\n        >>> exp_name = setup(data = income,  target = 'income >50K')\n        >>> lr = create_model('lr')\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\n\n\n        estimator: scikit-learn compatible object\n            Trained model object\n\n\n        sensitive_features: list\n            List of column names as present in the original dataset before any\n            transformations.\n\n\n        plot_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the matplotlib plot.\n\n\n        Returns:\n            pandas.DataFrame\n\n        \"\"\"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)",
        "mutated": [
            "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    if False:\n        i = 10\n    \"\\n        There are many approaches to conceptualizing fairness. This function follows\\n        the approach known as group fairness, which asks: Which groups of individuals\\n        are at risk for experiencing harms. This function provides fairness-related\\n        metrics between different groups (also called subpopulation).\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> income = get_data('income')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = income,  target = 'income >50K')\\n        >>> lr = create_model('lr')\\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        sensitive_features: list\\n            List of column names as present in the original dataset before any\\n            transformations.\\n\\n\\n        plot_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the matplotlib plot.\\n\\n\\n        Returns:\\n            pandas.DataFrame\\n\\n        \"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)",
            "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        There are many approaches to conceptualizing fairness. This function follows\\n        the approach known as group fairness, which asks: Which groups of individuals\\n        are at risk for experiencing harms. This function provides fairness-related\\n        metrics between different groups (also called subpopulation).\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> income = get_data('income')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = income,  target = 'income >50K')\\n        >>> lr = create_model('lr')\\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        sensitive_features: list\\n            List of column names as present in the original dataset before any\\n            transformations.\\n\\n\\n        plot_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the matplotlib plot.\\n\\n\\n        Returns:\\n            pandas.DataFrame\\n\\n        \"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)",
            "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        There are many approaches to conceptualizing fairness. This function follows\\n        the approach known as group fairness, which asks: Which groups of individuals\\n        are at risk for experiencing harms. This function provides fairness-related\\n        metrics between different groups (also called subpopulation).\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> income = get_data('income')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = income,  target = 'income >50K')\\n        >>> lr = create_model('lr')\\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        sensitive_features: list\\n            List of column names as present in the original dataset before any\\n            transformations.\\n\\n\\n        plot_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the matplotlib plot.\\n\\n\\n        Returns:\\n            pandas.DataFrame\\n\\n        \"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)",
            "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        There are many approaches to conceptualizing fairness. This function follows\\n        the approach known as group fairness, which asks: Which groups of individuals\\n        are at risk for experiencing harms. This function provides fairness-related\\n        metrics between different groups (also called subpopulation).\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> income = get_data('income')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = income,  target = 'income >50K')\\n        >>> lr = create_model('lr')\\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        sensitive_features: list\\n            List of column names as present in the original dataset before any\\n            transformations.\\n\\n\\n        plot_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the matplotlib plot.\\n\\n\\n        Returns:\\n            pandas.DataFrame\\n\\n        \"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)",
            "def check_fairness(self, estimator, sensitive_features: list, plot_kwargs: dict={}):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        There are many approaches to conceptualizing fairness. This function follows\\n        the approach known as group fairness, which asks: Which groups of individuals\\n        are at risk for experiencing harms. This function provides fairness-related\\n        metrics between different groups (also called subpopulation).\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> income = get_data('income')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = income,  target = 'income >50K')\\n        >>> lr = create_model('lr')\\n        >>> lr_fairness = check_fairness(lr, sensitive_features = ['sex', 'race'])\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        sensitive_features: list\\n            List of column names as present in the original dataset before any\\n            transformations.\\n\\n\\n        plot_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the matplotlib plot.\\n\\n\\n        Returns:\\n            pandas.DataFrame\\n\\n        \"\n    _check_soft_dependencies('fairlearn', extra='analysis', severity='error')\n    from fairlearn.metrics import MetricFrame, count, selection_rate\n    all_metrics = self.get_metrics()[['Name', 'Score Function', 'Args']].set_index('Name')\n    metric_dict = {}\n    metric_dict['Samples'] = count\n    for i in all_metrics.index:\n        metric_dict[i] = partial(all_metrics.loc[i][0], **all_metrics.loc[i][1])\n    if self._ml_usecase == MLUsecase.CLASSIFICATION:\n        metric_dict['Selection Rate'] = selection_rate\n    y_pred = self.predict_model(estimator)[LABEL_COLUMN]\n    y_true = self.y_test\n    try:\n        multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    except Exception:\n        if MLUsecase.CLASSIFICATION:\n            metric_dict.pop('AUC')\n            multi_metric = MetricFrame(metrics=metric_dict, y_true=y_true, y_pred=y_pred, sensitive_features=self.X_test[sensitive_features])\n    multi_metric.by_group.plot.bar(subplots=True, layout=[3, 3], legend=False, figsize=[16, 8], title='Performance Metrics by Sensitive Features', **plot_kwargs)\n    return pd.DataFrame(multi_metric.by_group)"
        ]
    },
    {
        "func_name": "compare_score",
        "original": "def compare_score(new, best):\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best",
        "mutated": [
            "def compare_score(new, best):\n    if False:\n        i = 10\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best",
            "def compare_score(new, best):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best",
            "def compare_score(new, best):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best",
            "def compare_score(new, best):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best",
            "def compare_score(new, best):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not best:\n        return True\n    if greater_is_better:\n        return new > best\n    else:\n        return new < best"
        ]
    },
    {
        "func_name": "automl",
        "original": "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    \"\"\"\n        This function returns the best model out of all models created in\n        current active environment based on metric defined in optimize parameter.\n\n        Parameters\n        ----------\n        optimize : str, default = 'Accuracy'\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\n            'F1', 'Kappa', and 'MCC'.\n\n        use_holdout: bool, default = False\n            When set to True, metrics are evaluated on holdout set instead of CV.\n\n        turbo: bool, default = True\n            When set to True and use_holdout is False, only models created with default fold\n            parameter will be considered. If set to False, models created with a non-default\n            fold parameter will be scored again using default fold settings, so that they can be\n            compared.\n\n        return_train_score: bool, default = False\n            If False, returns the CV Validation scores only.\n            If True, returns the CV training scores along with the CV validation scores.\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\n            training score with a low corresponding CV validation score indicates overfitting.\n\n        Returns:\n            Trained Model\n        \"\"\"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model",
        "mutated": [
            "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n    \"\\n        This function returns the best model out of all models created in\\n        current active environment based on metric defined in optimize parameter.\\n\\n        Parameters\\n        ----------\\n        optimize : str, default = 'Accuracy'\\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\\n            'F1', 'Kappa', and 'MCC'.\\n\\n        use_holdout: bool, default = False\\n            When set to True, metrics are evaluated on holdout set instead of CV.\\n\\n        turbo: bool, default = True\\n            When set to True and use_holdout is False, only models created with default fold\\n            parameter will be considered. If set to False, models created with a non-default\\n            fold parameter will be scored again using default fold settings, so that they can be\\n            compared.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns:\\n            Trained Model\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model",
            "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function returns the best model out of all models created in\\n        current active environment based on metric defined in optimize parameter.\\n\\n        Parameters\\n        ----------\\n        optimize : str, default = 'Accuracy'\\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\\n            'F1', 'Kappa', and 'MCC'.\\n\\n        use_holdout: bool, default = False\\n            When set to True, metrics are evaluated on holdout set instead of CV.\\n\\n        turbo: bool, default = True\\n            When set to True and use_holdout is False, only models created with default fold\\n            parameter will be considered. If set to False, models created with a non-default\\n            fold parameter will be scored again using default fold settings, so that they can be\\n            compared.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns:\\n            Trained Model\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model",
            "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function returns the best model out of all models created in\\n        current active environment based on metric defined in optimize parameter.\\n\\n        Parameters\\n        ----------\\n        optimize : str, default = 'Accuracy'\\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\\n            'F1', 'Kappa', and 'MCC'.\\n\\n        use_holdout: bool, default = False\\n            When set to True, metrics are evaluated on holdout set instead of CV.\\n\\n        turbo: bool, default = True\\n            When set to True and use_holdout is False, only models created with default fold\\n            parameter will be considered. If set to False, models created with a non-default\\n            fold parameter will be scored again using default fold settings, so that they can be\\n            compared.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns:\\n            Trained Model\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model",
            "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function returns the best model out of all models created in\\n        current active environment based on metric defined in optimize parameter.\\n\\n        Parameters\\n        ----------\\n        optimize : str, default = 'Accuracy'\\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\\n            'F1', 'Kappa', and 'MCC'.\\n\\n        use_holdout: bool, default = False\\n            When set to True, metrics are evaluated on holdout set instead of CV.\\n\\n        turbo: bool, default = True\\n            When set to True and use_holdout is False, only models created with default fold\\n            parameter will be considered. If set to False, models created with a non-default\\n            fold parameter will be scored again using default fold settings, so that they can be\\n            compared.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns:\\n            Trained Model\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model",
            "def automl(self, optimize: str='Accuracy', use_holdout: bool=False, turbo: bool=True, return_train_score: bool=False) -> Any:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function returns the best model out of all models created in\\n        current active environment based on metric defined in optimize parameter.\\n\\n        Parameters\\n        ----------\\n        optimize : str, default = 'Accuracy'\\n            Other values you can pass in optimize parameter are 'AUC', 'Recall', 'Precision',\\n            'F1', 'Kappa', and 'MCC'.\\n\\n        use_holdout: bool, default = False\\n            When set to True, metrics are evaluated on holdout set instead of CV.\\n\\n        turbo: bool, default = True\\n            When set to True and use_holdout is False, only models created with default fold\\n            parameter will be considered. If set to False, models created with a non-default\\n            fold parameter will be scored again using default fold settings, so that they can be\\n            compared.\\n\\n        return_train_score: bool, default = False\\n            If False, returns the CV Validation scores only.\\n            If True, returns the CV training scores along with the CV validation scores.\\n            This is useful when the user wants to do bias-variance tradeoff. A high CV\\n            training score with a low corresponding CV validation score indicates overfitting.\\n\\n        Returns:\\n            Trained Model\\n        \"\n    function_params_str = ', '.join([f'{k}={v}' for (k, v) in locals().items()])\n    self.logger.info('Initializing automl()')\n    self.logger.info(f'automl({function_params_str})')\n    optimize = self._get_metric_by_name_or_id(optimize)\n    if optimize is None:\n        raise ValueError('Optimize method not supported. See docstring for list of available parameters.')\n    if self.is_multiclass:\n        if not optimize.is_multiclass:\n            raise TypeError('Optimization metric not supported for multiclass problems. See docstring for list of other optimization parameters.')\n    if type(return_train_score) is not bool:\n        raise TypeError('return_train_score can only take argument as True or False')\n    compare_dimension = optimize.display_name\n    greater_is_better = optimize.greater_is_better\n    optimize = optimize.scorer\n    best_model = None\n    best_score = None\n\n    def compare_score(new, best):\n        if not best:\n            return True\n        if greater_is_better:\n            return new > best\n        else:\n            return new < best\n    if use_holdout:\n        self.logger.info('Model Selection Basis : Holdout set')\n        for i in self._master_model_container:\n            self.logger.info(f'Checking model {i}')\n            model = i['model']\n            try:\n                self.predict_model(model, verbose=False)\n            except Exception:\n                self.logger.warning(f'Model {model} is not fitted, running create_model')\n                (model, _) = self._create_model(estimator=model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                self.predict_model(model, verbose=False)\n            p = self.pull(pop=True)\n            p = p[compare_dimension][0]\n            if compare_score(p, best_score):\n                best_model = model\n                best_score = p\n    else:\n        self.logger.info('Model Selection Basis : CV Results on Training set')\n        for i in range(len(self._master_model_container)):\n            model = self._master_model_container[i]\n            scores = None\n            if model['cv'] is not self.fold_generator:\n                if turbo or self._is_unsupervised():\n                    continue\n                self._create_model(estimator=model['model'], system=False, verbose=False, cross_validation=True, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n                scores = self.pull(pop=True)\n                self._master_model_container.pop()\n            self.logger.info(f'Checking model {i}')\n            if scores is None:\n                scores = model['scores']\n            r = scores[compare_dimension][-2:][0]\n            if compare_score(r, best_score):\n                best_model = model['model']\n                best_score = r\n    (automl_model, _) = self._create_model(estimator=best_model, system=False, verbose=False, cross_validation=False, predict=False, groups=self.fold_groups_param, return_train_score=return_train_score)\n    gc.collect()\n    self.logger.info(str(automl_model))\n    self.logger.info('automl() successfully completed......................................')\n    return automl_model"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(*dict_input):\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()",
        "mutated": [
            "def predict(*dict_input):\n    if False:\n        i = 10\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()",
            "def predict(*dict_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()",
            "def predict(*dict_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()",
            "def predict(*dict_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()",
            "def predict(*dict_input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_df = pd.DataFrame.from_dict([dict_input])\n    input_df.columns = list(self.X.columns)\n    return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()"
        ]
    },
    {
        "func_name": "create_app",
        "original": "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    \"\"\"\n        This function creates a basic gradio app for inference.\n        It will later be expanded for other app types such as\n        Streamlit.\n\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> from pycaret.classification import *\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\n        >>> lr = create_model('lr')\n        >>> create_app(lr)\n\n\n        estimator: scikit-learn compatible object\n            Trained model object\n\n\n        app_kwargs: dict, default = {} (empty dict)\n            arguments to be passed to app class.\n\n\n        Returns:\n            None\n        \"\"\"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()",
        "mutated": [
            "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    if False:\n        i = 10\n    \"\\n        This function creates a basic gradio app for inference.\\n        It will later be expanded for other app types such as\\n        Streamlit.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> create_app(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        app_kwargs: dict, default = {} (empty dict)\\n            arguments to be passed to app class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()",
            "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function creates a basic gradio app for inference.\\n        It will later be expanded for other app types such as\\n        Streamlit.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> create_app(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        app_kwargs: dict, default = {} (empty dict)\\n            arguments to be passed to app class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()",
            "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function creates a basic gradio app for inference.\\n        It will later be expanded for other app types such as\\n        Streamlit.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> create_app(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        app_kwargs: dict, default = {} (empty dict)\\n            arguments to be passed to app class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()",
            "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function creates a basic gradio app for inference.\\n        It will later be expanded for other app types such as\\n        Streamlit.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> create_app(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        app_kwargs: dict, default = {} (empty dict)\\n            arguments to be passed to app class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()",
            "def create_app(self, estimator, app_kwargs: Optional[dict]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function creates a basic gradio app for inference.\\n        It will later be expanded for other app types such as\\n        Streamlit.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> create_app(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        app_kwargs: dict, default = {} (empty dict)\\n            arguments to be passed to app class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('gradio', extra='mlops', severity='error')\n    import gradio as gr\n    all_inputs = []\n    app_kwargs = app_kwargs or {}\n    for i in self.X.columns:\n        if i in self._fxs['Categorical'] or i in self._fxs['Ordinal']:\n            all_inputs.append(gr.inputs.Dropdown(list(self.X[i].unique()), label=i))\n        else:\n            all_inputs.append(gr.inputs.Textbox(label=i))\n\n    def predict(*dict_input):\n        input_df = pd.DataFrame.from_dict([dict_input])\n        input_df.columns = list(self.X.columns)\n        return self.predict_model(estimator, data=input_df, **self._create_app_predict_kwargs).iloc[0].to_dict()\n    return gr.Interface(fn=predict, inputs=all_inputs, outputs='text', live=False, **app_kwargs).launch()"
        ]
    },
    {
        "func_name": "dashboard",
        "original": "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    \"\"\"\n        This function generates the interactive dashboard for a trained model. The\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\n\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> from pycaret.classification import *\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\n        >>> lr = create_model('lr')\n        >>> dashboard(lr)\n\n\n        estimator: scikit-learn compatible object\n            Trained model object\n\n\n        display_format: str, default = 'dash'\n            Render mode for the dashboard. The default is set to ``dash`` which will\n            render a dashboard in browser. There are four possible options:\n\n            - 'dash' - displays the dashboard in browser\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\n\n\n        dashboard_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\n\n\n        run_kwargs: dict, default = {} (empty dict)\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\n\n\n        **kwargs:\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\n            ``RegressionExplainer`` class.\n\n\n        Returns:\n            None\n        \"\"\"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')",
        "mutated": [
            "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    if False:\n        i = 10\n    \"\\n        This function generates the interactive dashboard for a trained model. The\\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> dashboard(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        display_format: str, default = 'dash'\\n            Render mode for the dashboard. The default is set to ``dash`` which will\\n            render a dashboard in browser. There are four possible options:\\n\\n            - 'dash' - displays the dashboard in browser\\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\\n\\n\\n        dashboard_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\\n\\n\\n        run_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\\n\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\\n            ``RegressionExplainer`` class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')",
            "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function generates the interactive dashboard for a trained model. The\\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> dashboard(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        display_format: str, default = 'dash'\\n            Render mode for the dashboard. The default is set to ``dash`` which will\\n            render a dashboard in browser. There are four possible options:\\n\\n            - 'dash' - displays the dashboard in browser\\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\\n\\n\\n        dashboard_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\\n\\n\\n        run_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\\n\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\\n            ``RegressionExplainer`` class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')",
            "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function generates the interactive dashboard for a trained model. The\\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> dashboard(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        display_format: str, default = 'dash'\\n            Render mode for the dashboard. The default is set to ``dash`` which will\\n            render a dashboard in browser. There are four possible options:\\n\\n            - 'dash' - displays the dashboard in browser\\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\\n\\n\\n        dashboard_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\\n\\n\\n        run_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\\n\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\\n            ``RegressionExplainer`` class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')",
            "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function generates the interactive dashboard for a trained model. The\\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> dashboard(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        display_format: str, default = 'dash'\\n            Render mode for the dashboard. The default is set to ``dash`` which will\\n            render a dashboard in browser. There are four possible options:\\n\\n            - 'dash' - displays the dashboard in browser\\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\\n\\n\\n        dashboard_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\\n\\n\\n        run_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\\n\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\\n            ``RegressionExplainer`` class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')",
            "def dashboard(self, estimator, display_format: str='dash', dashboard_kwargs: Optional[Dict[str, Any]]=None, run_kwargs: Optional[Dict[str, Any]]=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function generates the interactive dashboard for a trained model. The\\n        dashboard is implemented using ExplainerDashboard (explainerdashboard.readthedocs.io)\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> lr = create_model('lr')\\n        >>> dashboard(lr)\\n\\n\\n        estimator: scikit-learn compatible object\\n            Trained model object\\n\\n\\n        display_format: str, default = 'dash'\\n            Render mode for the dashboard. The default is set to ``dash`` which will\\n            render a dashboard in browser. There are four possible options:\\n\\n            - 'dash' - displays the dashboard in browser\\n            - 'inline' - displays the dashboard in the jupyter notebook cell.\\n            - 'jupyterlab' - displays the dashboard in jupyterlab pane.\\n            - 'external' - displays the dashboard in a separate tab. (use in Colab)\\n\\n\\n        dashboard_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``ExplainerDashboard`` class.\\n\\n\\n        run_kwargs: dict, default = {} (empty dict)\\n            Dictionary of arguments passed to the ``run`` method of ``ExplainerDashboard``.\\n\\n\\n        **kwargs:\\n            Additional keyword arguments to pass to the ``ClassifierExplainer`` or\\n            ``RegressionExplainer`` class.\\n\\n\\n        Returns:\\n            None\\n        \"\n    _check_soft_dependencies('explainerdashboard', extra='analysis', severity='error')"
        ]
    },
    {
        "func_name": "check_drift",
        "original": "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    \"\"\"\n        This function generates a drift report file using the\n        evidently library.\n\n\n        Example\n        -------\n        >>> from pycaret.datasets import get_data\n        >>> juice = get_data('juice')\n        >>> from pycaret.classification import *\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\n        >>> check_drift()\n\n\n        reference_data: Optional[pd.DataFrame] = None\n            Reference data. If not specified, will use training data.\n            Must be specified if ``setup()`` has not been run.\n\n\n        current_data: Optional[pd.DataFrame] = None\n            Current data. If not specified, will use test data.\n            Must be specified if ``setup()`` has not been run.\n\n\n        target: Optional[str] = None\n            Name of the target column. If not specified, will use\n            the column specified in ``setup()``.\n            Must be specified if ``setup()`` has not been run.\n\n\n        numeric_features: Optional[List[str]] = None\n            Names of numeric columns. If not specified, will use\n            the columns specified/inferred in ``setup()``, or\n            all non-categorical and non-date columns otherwise.\n\n\n        categorical_features: Optional[List[str]] = None\n            Names of categorical columns. If not specified, will use\n            the columns specified/inferred in ``setup()``.\n\n\n        date_features: Optional[List[str]] = None\n            Names of date columns. If not specified, will use\n            the columns specified/inferred in ``setup()``.\n\n\n        filename: Optional[str] = None\n            Path to save the generated HTML file to. If not specified,\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\n\n\n        Returns:\n            Path the generated HTML file was saved to.\n        \"\"\"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename",
        "mutated": [
            "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    if False:\n        i = 10\n    \"\\n        This function generates a drift report file using the\\n        evidently library.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> check_drift()\\n\\n\\n        reference_data: Optional[pd.DataFrame] = None\\n            Reference data. If not specified, will use training data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        current_data: Optional[pd.DataFrame] = None\\n            Current data. If not specified, will use test data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        target: Optional[str] = None\\n            Name of the target column. If not specified, will use\\n            the column specified in ``setup()``.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        numeric_features: Optional[List[str]] = None\\n            Names of numeric columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``, or\\n            all non-categorical and non-date columns otherwise.\\n\\n\\n        categorical_features: Optional[List[str]] = None\\n            Names of categorical columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        date_features: Optional[List[str]] = None\\n            Names of date columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        filename: Optional[str] = None\\n            Path to save the generated HTML file to. If not specified,\\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\\n\\n\\n        Returns:\\n            Path the generated HTML file was saved to.\\n        \"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename",
            "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        This function generates a drift report file using the\\n        evidently library.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> check_drift()\\n\\n\\n        reference_data: Optional[pd.DataFrame] = None\\n            Reference data. If not specified, will use training data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        current_data: Optional[pd.DataFrame] = None\\n            Current data. If not specified, will use test data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        target: Optional[str] = None\\n            Name of the target column. If not specified, will use\\n            the column specified in ``setup()``.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        numeric_features: Optional[List[str]] = None\\n            Names of numeric columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``, or\\n            all non-categorical and non-date columns otherwise.\\n\\n\\n        categorical_features: Optional[List[str]] = None\\n            Names of categorical columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        date_features: Optional[List[str]] = None\\n            Names of date columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        filename: Optional[str] = None\\n            Path to save the generated HTML file to. If not specified,\\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\\n\\n\\n        Returns:\\n            Path the generated HTML file was saved to.\\n        \"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename",
            "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        This function generates a drift report file using the\\n        evidently library.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> check_drift()\\n\\n\\n        reference_data: Optional[pd.DataFrame] = None\\n            Reference data. If not specified, will use training data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        current_data: Optional[pd.DataFrame] = None\\n            Current data. If not specified, will use test data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        target: Optional[str] = None\\n            Name of the target column. If not specified, will use\\n            the column specified in ``setup()``.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        numeric_features: Optional[List[str]] = None\\n            Names of numeric columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``, or\\n            all non-categorical and non-date columns otherwise.\\n\\n\\n        categorical_features: Optional[List[str]] = None\\n            Names of categorical columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        date_features: Optional[List[str]] = None\\n            Names of date columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        filename: Optional[str] = None\\n            Path to save the generated HTML file to. If not specified,\\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\\n\\n\\n        Returns:\\n            Path the generated HTML file was saved to.\\n        \"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename",
            "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        This function generates a drift report file using the\\n        evidently library.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> check_drift()\\n\\n\\n        reference_data: Optional[pd.DataFrame] = None\\n            Reference data. If not specified, will use training data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        current_data: Optional[pd.DataFrame] = None\\n            Current data. If not specified, will use test data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        target: Optional[str] = None\\n            Name of the target column. If not specified, will use\\n            the column specified in ``setup()``.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        numeric_features: Optional[List[str]] = None\\n            Names of numeric columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``, or\\n            all non-categorical and non-date columns otherwise.\\n\\n\\n        categorical_features: Optional[List[str]] = None\\n            Names of categorical columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        date_features: Optional[List[str]] = None\\n            Names of date columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        filename: Optional[str] = None\\n            Path to save the generated HTML file to. If not specified,\\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\\n\\n\\n        Returns:\\n            Path the generated HTML file was saved to.\\n        \"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename",
            "def check_drift(self, reference_data: Optional[pd.DataFrame]=None, current_data: Optional[pd.DataFrame]=None, target: Optional[str]=None, numeric_features: Optional[List[str]]=None, categorical_features: Optional[List[str]]=None, date_features: Optional[List[str]]=None, filename: Optional[str]=None) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        This function generates a drift report file using the\\n        evidently library.\\n\\n\\n        Example\\n        -------\\n        >>> from pycaret.datasets import get_data\\n        >>> juice = get_data('juice')\\n        >>> from pycaret.classification import *\\n        >>> exp_name = setup(data = juice,  target = 'Purchase')\\n        >>> check_drift()\\n\\n\\n        reference_data: Optional[pd.DataFrame] = None\\n            Reference data. If not specified, will use training data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        current_data: Optional[pd.DataFrame] = None\\n            Current data. If not specified, will use test data.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        target: Optional[str] = None\\n            Name of the target column. If not specified, will use\\n            the column specified in ``setup()``.\\n            Must be specified if ``setup()`` has not been run.\\n\\n\\n        numeric_features: Optional[List[str]] = None\\n            Names of numeric columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``, or\\n            all non-categorical and non-date columns otherwise.\\n\\n\\n        categorical_features: Optional[List[str]] = None\\n            Names of categorical columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        date_features: Optional[List[str]] = None\\n            Names of date columns. If not specified, will use\\n            the columns specified/inferred in ``setup()``.\\n\\n\\n        filename: Optional[str] = None\\n            Path to save the generated HTML file to. If not specified,\\n            will default to '[EXPERIMENT_NAME]_[TIMESTAMP]_Drift_Report.html'.\\n\\n\\n        Returns:\\n            Path the generated HTML file was saved to.\\n        \"\n    _check_soft_dependencies('evidently', extra='mlops', severity='error')\n    if self._setup_ran:\n        reference_data = self.train if reference_data is None else reference_data\n        current_data = self.test if current_data is None else current_data\n        target = self.target_param if target is None else target\n        numeric_features = numeric_features or self._fxs['Numeric']\n        categorical_features = categorical_features or self._fxs['Categorical']\n        date_features = date_features or self._fxs['Date']\n    none_vars = [k for (k, v) in {'reference_data': reference_data, 'current_data': current_data, 'target': target}.items() if v is None]\n    if none_vars:\n        raise ValueError(f\"The following variables couldn't have been inferred from the experiment state: {none_vars}.If you have not ran `setup()`, you must pass all of the argumentsabove.\")\n    date_features = date_features or []\n    categorical_features = categorical_features or []\n    if numeric_features is None:\n        numeric_features = list(set(reference_data.columns).difference(categorical_features).difference(date_features))\n    from evidently.dashboard import Dashboard\n    from evidently.pipeline.column_mapping import ColumnMapping\n    from evidently.tabs import CatTargetDriftTab, DataDriftTab\n    column_mapping = ColumnMapping()\n    column_mapping.target = target\n    column_mapping.prediction = None\n    column_mapping.datetime = None\n    column_mapping.numerical_features = numeric_features\n    column_mapping.categorical_features = categorical_features\n    column_mapping.datetime_features = date_features\n    if target not in reference_data.columns or target not in current_data.columns:\n        raise ValueError(f'Both dataset must contain a label column {target} in order to create a drift report.')\n    dashboard = Dashboard(tabs=[DataDriftTab(), CatTargetDriftTab()])\n    dashboard.calculate(reference_data, current_data, column_mapping=column_mapping)\n    filename = filename or f'{self.exp_name_log}_{int(time.time())}_Drift_Report.html'\n    dashboard.save(filename)\n    return filename"
        ]
    },
    {
        "func_name": "load_experiment",
        "original": "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    \"\"\"\n        Load an experiment saved with ``save_experiment`` from path\n        or file.\n\n        The data (and test data) is NOT saved with the experiment\n        and will need to be specified again.\n\n\n        path_or_file: str or BinaryIO (file pointer)\n            The path/file pointer to load the experiment from.\n            The pickle file must be created through ``save_experiment``.\n\n\n        data: dataframe-like\n            Data set with shape (n_samples, n_features), where n_samples is the\n            number of samples and n_features is the number of features. If data\n            is not a pandas dataframe, it's converted to one using default column\n            names.\n\n\n        data_func: Callable[[], DATAFRAME_LIKE] = None\n            The function that generate ``data`` (the dataframe-like input). This\n            is useful when the dataset is large, and you need parallel operations\n            such as ``compare_models``. It can avoid broadcasting large dataset\n            from driver to workers. Notice one and only one of ``data`` and\n            ``data_func`` must be set.\n\n\n        test_data: dataframe-like or None, default = None\n            If not None, test_data is used as a hold-out set and `train_size` parameter\n            is ignored. The columns of data and test_data must match.\n\n\n        preprocess_data: bool, default = True\n            If True, the data will be preprocessed again (through running ``setup``\n            internally). If False, the data will not be preprocessed. This means\n            you can save the value of the ``data`` attribute of an experiment\n            separately, and then load it separately and pass it here with\n            ``preprocess_data`` set to False. This is an advanced feature.\n            We recommend leaving it set to True and passing the same data\n            as passed to the initial ``setup`` call.\n\n\n        **cloudpickle_kwargs:\n            Kwargs to pass to the ``cloudpickle.load`` call.\n\n\n        Returns:\n            loaded experiment\n\n        \"\"\"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)",
        "mutated": [
            "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    if False:\n        i = 10\n    \"\\n        Load an experiment saved with ``save_experiment`` from path\\n        or file.\\n\\n        The data (and test data) is NOT saved with the experiment\\n        and will need to be specified again.\\n\\n\\n        path_or_file: str or BinaryIO (file pointer)\\n            The path/file pointer to load the experiment from.\\n            The pickle file must be created through ``save_experiment``.\\n\\n\\n        data: dataframe-like\\n            Data set with shape (n_samples, n_features), where n_samples is the\\n            number of samples and n_features is the number of features. If data\\n            is not a pandas dataframe, it's converted to one using default column\\n            names.\\n\\n\\n        data_func: Callable[[], DATAFRAME_LIKE] = None\\n            The function that generate ``data`` (the dataframe-like input). This\\n            is useful when the dataset is large, and you need parallel operations\\n            such as ``compare_models``. It can avoid broadcasting large dataset\\n            from driver to workers. Notice one and only one of ``data`` and\\n            ``data_func`` must be set.\\n\\n\\n        test_data: dataframe-like or None, default = None\\n            If not None, test_data is used as a hold-out set and `train_size` parameter\\n            is ignored. The columns of data and test_data must match.\\n\\n\\n        preprocess_data: bool, default = True\\n            If True, the data will be preprocessed again (through running ``setup``\\n            internally). If False, the data will not be preprocessed. This means\\n            you can save the value of the ``data`` attribute of an experiment\\n            separately, and then load it separately and pass it here with\\n            ``preprocess_data`` set to False. This is an advanced feature.\\n            We recommend leaving it set to True and passing the same data\\n            as passed to the initial ``setup`` call.\\n\\n\\n        **cloudpickle_kwargs:\\n            Kwargs to pass to the ``cloudpickle.load`` call.\\n\\n\\n        Returns:\\n            loaded experiment\\n\\n        \"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)",
            "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Load an experiment saved with ``save_experiment`` from path\\n        or file.\\n\\n        The data (and test data) is NOT saved with the experiment\\n        and will need to be specified again.\\n\\n\\n        path_or_file: str or BinaryIO (file pointer)\\n            The path/file pointer to load the experiment from.\\n            The pickle file must be created through ``save_experiment``.\\n\\n\\n        data: dataframe-like\\n            Data set with shape (n_samples, n_features), where n_samples is the\\n            number of samples and n_features is the number of features. If data\\n            is not a pandas dataframe, it's converted to one using default column\\n            names.\\n\\n\\n        data_func: Callable[[], DATAFRAME_LIKE] = None\\n            The function that generate ``data`` (the dataframe-like input). This\\n            is useful when the dataset is large, and you need parallel operations\\n            such as ``compare_models``. It can avoid broadcasting large dataset\\n            from driver to workers. Notice one and only one of ``data`` and\\n            ``data_func`` must be set.\\n\\n\\n        test_data: dataframe-like or None, default = None\\n            If not None, test_data is used as a hold-out set and `train_size` parameter\\n            is ignored. The columns of data and test_data must match.\\n\\n\\n        preprocess_data: bool, default = True\\n            If True, the data will be preprocessed again (through running ``setup``\\n            internally). If False, the data will not be preprocessed. This means\\n            you can save the value of the ``data`` attribute of an experiment\\n            separately, and then load it separately and pass it here with\\n            ``preprocess_data`` set to False. This is an advanced feature.\\n            We recommend leaving it set to True and passing the same data\\n            as passed to the initial ``setup`` call.\\n\\n\\n        **cloudpickle_kwargs:\\n            Kwargs to pass to the ``cloudpickle.load`` call.\\n\\n\\n        Returns:\\n            loaded experiment\\n\\n        \"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)",
            "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Load an experiment saved with ``save_experiment`` from path\\n        or file.\\n\\n        The data (and test data) is NOT saved with the experiment\\n        and will need to be specified again.\\n\\n\\n        path_or_file: str or BinaryIO (file pointer)\\n            The path/file pointer to load the experiment from.\\n            The pickle file must be created through ``save_experiment``.\\n\\n\\n        data: dataframe-like\\n            Data set with shape (n_samples, n_features), where n_samples is the\\n            number of samples and n_features is the number of features. If data\\n            is not a pandas dataframe, it's converted to one using default column\\n            names.\\n\\n\\n        data_func: Callable[[], DATAFRAME_LIKE] = None\\n            The function that generate ``data`` (the dataframe-like input). This\\n            is useful when the dataset is large, and you need parallel operations\\n            such as ``compare_models``. It can avoid broadcasting large dataset\\n            from driver to workers. Notice one and only one of ``data`` and\\n            ``data_func`` must be set.\\n\\n\\n        test_data: dataframe-like or None, default = None\\n            If not None, test_data is used as a hold-out set and `train_size` parameter\\n            is ignored. The columns of data and test_data must match.\\n\\n\\n        preprocess_data: bool, default = True\\n            If True, the data will be preprocessed again (through running ``setup``\\n            internally). If False, the data will not be preprocessed. This means\\n            you can save the value of the ``data`` attribute of an experiment\\n            separately, and then load it separately and pass it here with\\n            ``preprocess_data`` set to False. This is an advanced feature.\\n            We recommend leaving it set to True and passing the same data\\n            as passed to the initial ``setup`` call.\\n\\n\\n        **cloudpickle_kwargs:\\n            Kwargs to pass to the ``cloudpickle.load`` call.\\n\\n\\n        Returns:\\n            loaded experiment\\n\\n        \"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)",
            "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Load an experiment saved with ``save_experiment`` from path\\n        or file.\\n\\n        The data (and test data) is NOT saved with the experiment\\n        and will need to be specified again.\\n\\n\\n        path_or_file: str or BinaryIO (file pointer)\\n            The path/file pointer to load the experiment from.\\n            The pickle file must be created through ``save_experiment``.\\n\\n\\n        data: dataframe-like\\n            Data set with shape (n_samples, n_features), where n_samples is the\\n            number of samples and n_features is the number of features. If data\\n            is not a pandas dataframe, it's converted to one using default column\\n            names.\\n\\n\\n        data_func: Callable[[], DATAFRAME_LIKE] = None\\n            The function that generate ``data`` (the dataframe-like input). This\\n            is useful when the dataset is large, and you need parallel operations\\n            such as ``compare_models``. It can avoid broadcasting large dataset\\n            from driver to workers. Notice one and only one of ``data`` and\\n            ``data_func`` must be set.\\n\\n\\n        test_data: dataframe-like or None, default = None\\n            If not None, test_data is used as a hold-out set and `train_size` parameter\\n            is ignored. The columns of data and test_data must match.\\n\\n\\n        preprocess_data: bool, default = True\\n            If True, the data will be preprocessed again (through running ``setup``\\n            internally). If False, the data will not be preprocessed. This means\\n            you can save the value of the ``data`` attribute of an experiment\\n            separately, and then load it separately and pass it here with\\n            ``preprocess_data`` set to False. This is an advanced feature.\\n            We recommend leaving it set to True and passing the same data\\n            as passed to the initial ``setup`` call.\\n\\n\\n        **cloudpickle_kwargs:\\n            Kwargs to pass to the ``cloudpickle.load`` call.\\n\\n\\n        Returns:\\n            loaded experiment\\n\\n        \"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)",
            "@classmethod\ndef load_experiment(cls, path_or_file: Union[str, os.PathLike, BinaryIO], data: Optional[DATAFRAME_LIKE]=None, data_func: Optional[Callable[[], DATAFRAME_LIKE]]=None, test_data: Optional[DATAFRAME_LIKE]=None, preprocess_data: bool=True, **cloudpickle_kwargs) -> '_SupervisedExperiment':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Load an experiment saved with ``save_experiment`` from path\\n        or file.\\n\\n        The data (and test data) is NOT saved with the experiment\\n        and will need to be specified again.\\n\\n\\n        path_or_file: str or BinaryIO (file pointer)\\n            The path/file pointer to load the experiment from.\\n            The pickle file must be created through ``save_experiment``.\\n\\n\\n        data: dataframe-like\\n            Data set with shape (n_samples, n_features), where n_samples is the\\n            number of samples and n_features is the number of features. If data\\n            is not a pandas dataframe, it's converted to one using default column\\n            names.\\n\\n\\n        data_func: Callable[[], DATAFRAME_LIKE] = None\\n            The function that generate ``data`` (the dataframe-like input). This\\n            is useful when the dataset is large, and you need parallel operations\\n            such as ``compare_models``. It can avoid broadcasting large dataset\\n            from driver to workers. Notice one and only one of ``data`` and\\n            ``data_func`` must be set.\\n\\n\\n        test_data: dataframe-like or None, default = None\\n            If not None, test_data is used as a hold-out set and `train_size` parameter\\n            is ignored. The columns of data and test_data must match.\\n\\n\\n        preprocess_data: bool, default = True\\n            If True, the data will be preprocessed again (through running ``setup``\\n            internally). If False, the data will not be preprocessed. This means\\n            you can save the value of the ``data`` attribute of an experiment\\n            separately, and then load it separately and pass it here with\\n            ``preprocess_data`` set to False. This is an advanced feature.\\n            We recommend leaving it set to True and passing the same data\\n            as passed to the initial ``setup`` call.\\n\\n\\n        **cloudpickle_kwargs:\\n            Kwargs to pass to the ``cloudpickle.load`` call.\\n\\n\\n        Returns:\\n            loaded experiment\\n\\n        \"\n    return cls._load_experiment(path_or_file, cloudpickle_kwargs=cloudpickle_kwargs, preprocess_data=preprocess_data, data=data, data_func=data_func, test_data=test_data)"
        ]
    },
    {
        "func_name": "X",
        "original": "@property\n@abstractmethod\ndef X(self):\n    \"\"\"Feature set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X(self):\n    if False:\n        i = 10\n    'Feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Feature set.'\n    pass"
        ]
    },
    {
        "func_name": "dataset_transformed",
        "original": "@property\n@abstractmethod\ndef dataset_transformed(self):\n    \"\"\"Transformed dataset.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef dataset_transformed(self):\n    if False:\n        i = 10\n    'Transformed dataset.'\n    pass",
            "@property\n@abstractmethod\ndef dataset_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed dataset.'\n    pass",
            "@property\n@abstractmethod\ndef dataset_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed dataset.'\n    pass",
            "@property\n@abstractmethod\ndef dataset_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed dataset.'\n    pass",
            "@property\n@abstractmethod\ndef dataset_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed dataset.'\n    pass"
        ]
    },
    {
        "func_name": "X_train_transformed",
        "original": "@property\n@abstractmethod\ndef X_train_transformed(self):\n    \"\"\"Transformed feature set of the training set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X_train_transformed(self):\n    if False:\n        i = 10\n    'Transformed feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed feature set of the training set.'\n    pass"
        ]
    },
    {
        "func_name": "train_transformed",
        "original": "@property\n@abstractmethod\ndef train_transformed(self):\n    \"\"\"Transformed training set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef train_transformed(self):\n    if False:\n        i = 10\n    'Transformed training set.'\n    pass",
            "@property\n@abstractmethod\ndef train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed training set.'\n    pass",
            "@property\n@abstractmethod\ndef train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed training set.'\n    pass",
            "@property\n@abstractmethod\ndef train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed training set.'\n    pass",
            "@property\n@abstractmethod\ndef train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed training set.'\n    pass"
        ]
    },
    {
        "func_name": "X_transformed",
        "original": "@property\n@abstractmethod\ndef X_transformed(self):\n    \"\"\"Transformed feature set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X_transformed(self):\n    if False:\n        i = 10\n    'Transformed feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed feature set.'\n    pass",
            "@property\n@abstractmethod\ndef X_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed feature set.'\n    pass"
        ]
    },
    {
        "func_name": "y",
        "original": "@property\ndef y(self):\n    \"\"\"Target column.\"\"\"\n    return self.dataset[self.target_param]",
        "mutated": [
            "@property\ndef y(self):\n    if False:\n        i = 10\n    'Target column.'\n    return self.dataset[self.target_param]",
            "@property\ndef y(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Target column.'\n    return self.dataset[self.target_param]",
            "@property\ndef y(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Target column.'\n    return self.dataset[self.target_param]",
            "@property\ndef y(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Target column.'\n    return self.dataset[self.target_param]",
            "@property\ndef y(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Target column.'\n    return self.dataset[self.target_param]"
        ]
    },
    {
        "func_name": "X_train",
        "original": "@property\n@abstractmethod\ndef X_train(self):\n    \"\"\"Feature set of the training set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X_train(self):\n    if False:\n        i = 10\n    'Feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Feature set of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef X_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Feature set of the training set.'\n    pass"
        ]
    },
    {
        "func_name": "X_test",
        "original": "@property\n@abstractmethod\ndef X_test(self):\n    \"\"\"Feature set of the test set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X_test(self):\n    if False:\n        i = 10\n    'Feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Feature set of the test set.'\n    pass"
        ]
    },
    {
        "func_name": "train",
        "original": "@property\ndef train(self):\n    \"\"\"Training set.\"\"\"\n    return self.dataset.loc[self.idx[0], :]",
        "mutated": [
            "@property\ndef train(self):\n    if False:\n        i = 10\n    'Training set.'\n    return self.dataset.loc[self.idx[0], :]",
            "@property\ndef train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Training set.'\n    return self.dataset.loc[self.idx[0], :]",
            "@property\ndef train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Training set.'\n    return self.dataset.loc[self.idx[0], :]",
            "@property\ndef train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Training set.'\n    return self.dataset.loc[self.idx[0], :]",
            "@property\ndef train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Training set.'\n    return self.dataset.loc[self.idx[0], :]"
        ]
    },
    {
        "func_name": "test",
        "original": "@property\n@abstractmethod\ndef test(self):\n    \"\"\"Test set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef test(self):\n    if False:\n        i = 10\n    'Test set.'\n    pass",
            "@property\n@abstractmethod\ndef test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test set.'\n    pass",
            "@property\n@abstractmethod\ndef test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test set.'\n    pass",
            "@property\n@abstractmethod\ndef test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test set.'\n    pass",
            "@property\n@abstractmethod\ndef test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test set.'\n    pass"
        ]
    },
    {
        "func_name": "y_train",
        "original": "@property\ndef y_train(self):\n    \"\"\"Target column of the training set.\"\"\"\n    return self.train[self.target_param]",
        "mutated": [
            "@property\ndef y_train(self):\n    if False:\n        i = 10\n    'Target column of the training set.'\n    return self.train[self.target_param]",
            "@property\ndef y_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Target column of the training set.'\n    return self.train[self.target_param]",
            "@property\ndef y_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Target column of the training set.'\n    return self.train[self.target_param]",
            "@property\ndef y_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Target column of the training set.'\n    return self.train[self.target_param]",
            "@property\ndef y_train(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Target column of the training set.'\n    return self.train[self.target_param]"
        ]
    },
    {
        "func_name": "y_test",
        "original": "@property\ndef y_test(self):\n    \"\"\"Target column of the test set.\"\"\"\n    return self.test[self.target_param]",
        "mutated": [
            "@property\ndef y_test(self):\n    if False:\n        i = 10\n    'Target column of the test set.'\n    return self.test[self.target_param]",
            "@property\ndef y_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Target column of the test set.'\n    return self.test[self.target_param]",
            "@property\ndef y_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Target column of the test set.'\n    return self.test[self.target_param]",
            "@property\ndef y_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Target column of the test set.'\n    return self.test[self.target_param]",
            "@property\ndef y_test(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Target column of the test set.'\n    return self.test[self.target_param]"
        ]
    },
    {
        "func_name": "test_transformed",
        "original": "@property\n@abstractmethod\ndef test_transformed(self):\n    \"\"\"Transformed test set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef test_transformed(self):\n    if False:\n        i = 10\n    'Transformed test set.'\n    pass",
            "@property\n@abstractmethod\ndef test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed test set.'\n    pass",
            "@property\n@abstractmethod\ndef test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed test set.'\n    pass",
            "@property\n@abstractmethod\ndef test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed test set.'\n    pass",
            "@property\n@abstractmethod\ndef test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed test set.'\n    pass"
        ]
    },
    {
        "func_name": "y_transformed",
        "original": "@property\n@abstractmethod\ndef y_transformed(self):\n    \"\"\"Transformed target column.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef y_transformed(self):\n    if False:\n        i = 10\n    'Transformed target column.'\n    pass",
            "@property\n@abstractmethod\ndef y_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed target column.'\n    pass",
            "@property\n@abstractmethod\ndef y_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed target column.'\n    pass",
            "@property\n@abstractmethod\ndef y_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed target column.'\n    pass",
            "@property\n@abstractmethod\ndef y_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed target column.'\n    pass"
        ]
    },
    {
        "func_name": "X_test_transformed",
        "original": "@property\n@abstractmethod\ndef X_test_transformed(self):\n    \"\"\"Transformed feature set of the test set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef X_test_transformed(self):\n    if False:\n        i = 10\n    'Transformed feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed feature set of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef X_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed feature set of the test set.'\n    pass"
        ]
    },
    {
        "func_name": "y_train_transformed",
        "original": "@property\n@abstractmethod\ndef y_train_transformed(self):\n    \"\"\"Transformed target column of the training set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef y_train_transformed(self):\n    if False:\n        i = 10\n    'Transformed target column of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef y_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed target column of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef y_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed target column of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef y_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed target column of the training set.'\n    pass",
            "@property\n@abstractmethod\ndef y_train_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed target column of the training set.'\n    pass"
        ]
    },
    {
        "func_name": "y_test_transformed",
        "original": "@property\n@abstractmethod\ndef y_test_transformed(self):\n    \"\"\"Transformed target column of the test set.\"\"\"\n    pass",
        "mutated": [
            "@property\n@abstractmethod\ndef y_test_transformed(self):\n    if False:\n        i = 10\n    'Transformed target column of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef y_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transformed target column of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef y_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transformed target column of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef y_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transformed target column of the test set.'\n    pass",
            "@property\n@abstractmethod\ndef y_test_transformed(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transformed target column of the test set.'\n    pass"
        ]
    }
]