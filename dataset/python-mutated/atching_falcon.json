[
    {
        "func_name": "falcon_forward_with_flash_attn",
        "original": "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    head_mask, alibi & output_attention are not supported.\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\n    \"\"\"\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)",
        "mutated": [
            "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    if False:\n        i = 10\n    '\\n    head_mask, alibi & output_attention are not supported.\\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\\n    '\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)",
            "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    head_mask, alibi & output_attention are not supported.\\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\\n    '\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)",
            "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    head_mask, alibi & output_attention are not supported.\\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\\n    '\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)",
            "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    head_mask, alibi & output_attention are not supported.\\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\\n    '\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)",
            "def falcon_forward_with_flash_attn(self, flash_attn: nn.Module, hidden_states: torch.Tensor, alibi: Optional[torch.Tensor], attention_mask: torch.Tensor, layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]]=None, head_mask: Optional[torch.Tensor]=None, use_cache: bool=False, output_attentions: bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    head_mask, alibi & output_attention are not supported.\\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\\n    '\n    assert head_mask is None\n    assert alibi is None\n    assert not output_attentions\n    fused_qkv = self.query_key_value(hidden_states)\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n    (batch_size, query_length, _, _) = query_layer.shape\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    (query_layer, key_layer) = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n    if layer_past is not None:\n        (past_key, past_value) = layer_past\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n    output_tensor = self.dense(attn_output)\n    return (output_tensor, present)"
        ]
    }
]