[
    {
        "func_name": "get_file_ignore",
        "original": "def get_file_ignore() -> FileIgnore:\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore",
        "mutated": [
            "def get_file_ignore() -> FileIgnore:\n    if False:\n        i = 10\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore",
            "def get_file_ignore() -> FileIgnore:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore",
            "def get_file_ignore() -> FileIgnore:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore",
            "def get_file_ignore() -> FileIgnore:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore",
            "def get_file_ignore() -> FileIgnore:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    TEMPLATES_DIR = Path(__file__).parent / 'templates'\n    try:\n        workdir = Path.cwd()\n    except FileNotFoundError:\n        workdir = Path.home()\n        logger.warn(f'Current working directory does not exist! Instead checking {workdir} for .semgrepignore files')\n    if 'SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE' in environ:\n        semgrepignore_path = Path(environ['SEMGREP_R2C_INTERNAL_EXPLICIT_SEMGREPIGNORE']).resolve()\n        logger.verbose('Using explicit semgrepignore file from environment variable')\n    else:\n        semgrepignore_path = Path(workdir / IGNORE_FILE_NAME)\n        if not semgrepignore_path.is_file():\n            logger.verbose('No .semgrepignore found. Using default .semgrepignore rules. See the docs for the list of default ignores: https://semgrep.dev/docs/cli-usage/#ignoring-files')\n            semgrepignore_path = TEMPLATES_DIR / IGNORE_FILE_NAME\n        else:\n            logger.verbose('using path ignore rules from user provided .semgrepignore')\n    with semgrepignore_path.open() as f:\n        file_ignore = FileIgnore.from_unprocessed_patterns(base_path=workdir, patterns=Parser(file_path=semgrepignore_path, base_path=workdir).parse(f))\n    return file_ignore"
        ]
    },
    {
        "func_name": "file_ignore_to_ignore_profiles",
        "original": "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}",
        "mutated": [
            "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    if False:\n        i = 10\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}",
            "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}",
            "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}",
            "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}",
            "def file_ignore_to_ignore_profiles(file_ignore: FileIgnore) -> Dict[str, FileIgnore]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {out.SAST().kind: file_ignore, out.SCA().kind: file_ignore, out.Secrets().kind: FileIgnore(file_ignore.base_path, frozenset())}"
        ]
    },
    {
        "func_name": "remove_matches_in_baseline",
        "original": "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    \"\"\"\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\n    \"\"\"\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule",
        "mutated": [
            "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    if False:\n        i = 10\n    '\\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\\n    '\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule",
            "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\\n    '\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule",
            "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\\n    '\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule",
            "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\\n    '\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule",
            "def remove_matches_in_baseline(head_matches_by_rule: RuleMatchMap, baseline_matches_by_rule: RuleMatchMap, file_renames: Dict[str, Path]) -> RuleMatchMap:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Remove the matches in head_matches_by_rule that also occur in baseline_matches_by_rule\\n    '\n    logger.verbose('Removing matches that exist in baseline scan')\n    kept_matches_by_rule: RuleMatchMap = {}\n    num_removed = 0\n    for (rule, matches) in head_matches_by_rule.items():\n        if len(matches) == 0:\n            continue\n        baseline_matches = {match.ci_unique_key for match in baseline_matches_by_rule.get(rule, [])}\n        kept_matches_by_rule[rule] = [match for match in matches if match.get_path_changed_ci_unique_key(file_renames) not in baseline_matches]\n        num_removed += len(matches) - len(kept_matches_by_rule[rule])\n    logger.verbose(f\"Removed {unit_str(num_removed, 'finding')} that were in baseline scan\")\n    return kept_matches_by_rule"
        ]
    },
    {
        "func_name": "run_rules",
        "original": "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)",
        "mutated": [
            "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if False:\n        i = 10\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)",
            "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)",
            "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)",
            "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)",
            "def run_rules(filtered_rules: List[Rule], target_manager: TargetManager, core_runner: CoreRunner, output_handler: OutputHandler, dump_command_for_core: bool, time_flag: bool, matching_explanations: bool, engine_type: EngineType, run_secrets: bool=False, disable_secrets_validation: bool=False, target_mode_config: Optional[TargetModeConfig]=None, *, with_code_rules: bool=True, with_supply_chain: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], OutputExtra, Dict[str, List[FoundDependency]], List[DependencyParserError], List[Plan]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not target_mode_config:\n        target_mode_config = TargetModeConfig.whole_scan()\n    cli_ux = get_state().get_cli_ux_flavor()\n    plans = scan_report.print_scan_status(filtered_rules, target_manager, target_mode_config, cli_ux=cli_ux, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    (join_rules, rest_of_the_rules) = partition(filtered_rules, lambda rule: rule.mode == JOIN_MODE)\n    dependency_aware_rules = [r for r in rest_of_the_rules if r.project_depends_on]\n    (dependency_only_rules, rest_of_the_rules) = partition(rest_of_the_rules, lambda rule: not rule.should_run_on_semgrep_core)\n    (rule_matches_by_rule, semgrep_errors, output_extra) = core_runner.invoke_semgrep_core(target_manager, rest_of_the_rules, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config)\n    if join_rules:\n        import semgrep.join_rule as join_rule\n        for rule in join_rules:\n            (join_rule_matches, join_rule_errors) = join_rule.run_join_rule(rule.raw, [target.path for target in target_manager.targets])\n            join_rule_matches_set = RuleMatchSet(rule)\n            for m in join_rule_matches:\n                join_rule_matches_set.add(m)\n            join_rule_matches_by_rule = {Rule.from_json(rule.raw): list(join_rule_matches_set)}\n            rule_matches_by_rule.update(join_rule_matches_by_rule)\n            output_handler.handle_semgrep_errors(join_rule_errors)\n    dependencies = {}\n    dependency_parser_errors = []\n    if len(dependency_aware_rules) > 0:\n        from semgrep.dependency_aware_rule import generate_unreachable_sca_findings, generate_reachable_sca_findings\n        for rule in dependency_aware_rules:\n            if rule.should_run_on_semgrep_core:\n                (dep_rule_matches, dep_rule_errors, already_reachable) = generate_reachable_sca_findings(rule_matches_by_rule.get(rule, []), rule, target_manager)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, already_reachable)\n                rule_matches_by_rule[rule].extend(dep_rule_matches)\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n            else:\n                (dep_rule_matches, dep_rule_errors) = generate_unreachable_sca_findings(rule, target_manager, lambda p, d: False)\n                rule_matches_by_rule[rule] = dep_rule_matches\n                output_handler.handle_semgrep_errors(dep_rule_errors)\n        for ecosystem in ECOSYSTEM_TO_LOCKFILES.keys():\n            for lockfile in target_manager.get_lockfiles(ecosystem):\n                output_extra.all_targets.add(lockfile)\n                (deps, parse_errors) = parse_lockfile_path(lockfile)\n                dependencies[str(lockfile)] = deps\n                dependency_parser_errors.extend(parse_errors)\n    return (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans)"
        ]
    },
    {
        "func_name": "run_scan",
        "original": "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)",
        "mutated": [
            "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    if False:\n        i = 10\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)",
            "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)",
            "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)",
            "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)",
            "def run_scan(*, diff_depth: int=DEFAULT_DIFF_DEPTH, dump_command_for_core: bool=False, time_flag: bool=False, matching_explanations: bool=False, engine_type: EngineType=EngineType.OSS, run_secrets: bool=False, disable_secrets_validation: bool=False, output_handler: OutputHandler, target: Sequence[str], pattern: Optional[str], lang: Optional[str], configs: Sequence[str], no_rewrite_rule_ids: bool=False, jobs: Optional[int]=None, include: Optional[Sequence[str]]=None, exclude: Optional[Sequence[str]]=None, exclude_rule: Optional[Sequence[str]]=None, strict: bool=False, autofix: bool=False, replacement: Optional[str]=None, dryrun: bool=False, disable_nosem: bool=False, no_git_ignore: bool=False, respect_rule_paths: bool=True, timeout: int=DEFAULT_TIMEOUT, max_memory: int=0, interfile_timeout: int=0, max_target_bytes: int=0, timeout_threshold: int=0, skip_unknown_extensions: bool=False, allow_untrusted_validators: bool=False, severity: Optional[Sequence[str]]=None, optimizations: str='none', baseline_commit: Optional[str]=None, baseline_commit_is_mergebase: bool=False, dump_contributions: bool=False) -> Tuple[RuleMatchMap, List[SemgrepError], Set[Path], FileTargetingLog, List[Rule], ProfileManager, OutputExtra, Collection[out.MatchSeverity], Dict[str, List[FoundDependency]], List[DependencyParserError], out.Contributions, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logger.debug(f'semgrep version {__VERSION__}')\n    if 'SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE' in environ:\n        recursion_limit_increase = int(environ['SEMGREP_PYTHON_RECURSION_LIMIT_INCREASE'])\n    else:\n        recursion_limit_increase = 500\n    setrecursionlimit(getrecursionlimit() + recursion_limit_increase)\n    if include is None:\n        include = []\n    if exclude is None:\n        exclude = []\n    if exclude_rule is None:\n        exclude_rule = []\n    project_url = get_project_url()\n    profiler = ProfileManager()\n    rule_start_time = time.time()\n    (configs_obj, config_errors) = get_config(pattern, lang, configs, replacement=replacement, project_url=project_url)\n    all_rules = configs_obj.get_rules(no_rewrite_rule_ids)\n    profiler.save('config_time', rule_start_time)\n    with_code_rules = configs_obj.with_code_rules\n    with_supply_chain = configs_obj.with_supply_chain\n    missed_rule_count = configs_obj.missed_rule_count\n    metrics = get_state().metrics\n    if metrics.is_enabled:\n        metrics.add_project_url(project_url)\n        metrics.add_integration_name(environ.get('SEMGREP_INTEGRATION_NAME'))\n        metrics.add_configs(configs)\n        metrics.add_engine_type(engine_type)\n        metrics.add_is_diff_scan(baseline_commit is not None)\n        if engine_type.is_pro:\n            metrics.add_diff_depth(diff_depth)\n    if not severity:\n        shown_severities = DEFAULT_SHOWN_SEVERITIES\n        filtered_rules = all_rules\n    else:\n        shown_severities = {out.MatchSeverity.from_json(s) for s in severity}\n        filtered_rules = [rule for rule in all_rules if rule.severity in shown_severities]\n    filtered_rules = filter_exclude_rule(filtered_rules, exclude_rule)\n    output_handler.handle_semgrep_errors(config_errors)\n    if config_errors and strict:\n        raise SemgrepError(f\"Ran with --strict and got {unit_str(len(config_errors), 'error')} while loading configs\", code=MISSING_CONFIG_EXIT_CODE)\n    if not pattern:\n        config_id_if_single = list(configs_obj.valid.keys())[0] if len(configs_obj.valid) == 1 else ''\n        invalid_msg = f\"({unit_str(len(config_errors), 'invalid config file')})\" if len(config_errors) else ''\n        logger.verbose(f\"running {len(filtered_rules)} rules from {unit_str(len(configs_obj.valid), 'config')} {config_id_if_single} {invalid_msg}\".strip())\n        if len(config_errors) > 0:\n            raise SemgrepError(f'invalid configuration file found ({len(config_errors)} configs were invalid)', code=MISSING_CONFIG_EXIT_CODE)\n        if len(configs_obj.valid) == 0:\n            raise SemgrepError('No config given. Run with `--config auto` or see https://semgrep.dev/docs/running-rules/ for instructions on running with a specific config\\n', code=MISSING_CONFIG_EXIT_CODE)\n    baseline_handler = None\n    if baseline_commit:\n        try:\n            baseline_handler = BaselineHandler(baseline_commit, is_mergebase=baseline_commit_is_mergebase)\n        except Exception as e:\n            raise SemgrepError(e)\n    respect_git_ignore = not no_git_ignore\n    try:\n        target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=target, respect_git_ignore=respect_git_ignore, respect_rule_paths=respect_rule_paths, baseline_handler=baseline_handler, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n    except FilesNotFoundError as e:\n        raise SemgrepError(e)\n    target_mode_config = TargetModeConfig.whole_scan()\n    if baseline_handler is not None:\n        if engine_type.is_interfile:\n            target_mode_config = TargetModeConfig.pro_diff_scan(target_manager.get_all_files(), diff_depth)\n        else:\n            target_mode_config = TargetModeConfig.diff_scan()\n    core_start_time = time.time()\n    core_runner = CoreRunner(jobs=jobs, engine_type=engine_type, timeout=timeout, max_memory=max_memory, interfile_timeout=interfile_timeout, timeout_threshold=timeout_threshold, optimizations=optimizations, allow_untrusted_validators=allow_untrusted_validators, respect_rule_paths=respect_rule_paths)\n    if dump_contributions:\n        contributions = get_contributions(engine_type)\n    else:\n        contributions = out.Contributions([])\n    (experimental_rules, unexperimental_rules) = partition(filtered_rules, lambda rule: isinstance(rule.severity.value, out.Experiment))\n    logger.verbose('Rules:')\n    for ruleid in sorted((rule.id for rule in unexperimental_rules)):\n        logger.verbose(f'- {ruleid}')\n    if len(experimental_rules) > 0:\n        logger.verbose('Experimental Rules:')\n        for ruleid in sorted((rule.id for rule in experimental_rules)):\n            logger.verbose(f'- {ruleid}')\n    (rule_matches_by_rule, semgrep_errors, output_extra, dependencies, dependency_parser_errors, plans) = run_rules(filtered_rules, target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, target_mode_config, with_code_rules=with_code_rules, with_supply_chain=with_supply_chain)\n    profiler.save('core_time', core_start_time)\n    output_handler.handle_semgrep_errors(semgrep_errors)\n    paths_with_matches = list({match.path for matches in rule_matches_by_rule.values() for match in matches})\n    findings_count = sum((len([match for match in matches if not match.from_transient_scan]) for matches in rule_matches_by_rule.values()))\n    if baseline_handler:\n        logger.info(f\"  Current version has {unit_str(findings_count, 'finding')}.\")\n        logger.info('')\n        baseline_targets: Set[Path] = set(paths_with_matches).union(set(baseline_handler.status.renamed.values())) - set(baseline_handler.status.added)\n        if not paths_with_matches:\n            logger.info('Skipping baseline scan, because there are no current findings.')\n        elif not baseline_targets:\n            logger.info(\"Skipping baseline scan, because all current findings are in files that didn't exist in the baseline commit.\")\n        else:\n            logger.info(f\"Creating git worktree from '{baseline_commit}' to scan baseline.\")\n            baseline_handler.print_git_log()\n            logger.info('')\n            try:\n                with baseline_handler.baseline_context():\n                    baseline_target_strings = target\n                    baseline_target_mode_config = target_mode_config\n                    if target_mode_config.is_pro_diff_scan:\n                        baseline_target_mode_config = TargetModeConfig.pro_diff_scan(frozenset((t for t in target_mode_config.get_diff_targets() if t.exists() and (not t.is_symlink()))), target_mode_config.get_diff_depth())\n                    else:\n                        baseline_target_strings = [str(t) for t in baseline_targets if t.exists() and (not t.is_symlink())]\n                    baseline_target_manager = TargetManager(includes=include, excludes=exclude, max_target_bytes=max_target_bytes, target_strings=baseline_target_strings, respect_git_ignore=respect_git_ignore, allow_unknown_extensions=not skip_unknown_extensions, ignore_profiles=file_ignore_to_ignore_profiles(get_file_ignore()))\n                    (baseline_rule_matches_by_rule, baseline_semgrep_errors, _, _, _, _plans) = run_rules([rule for (rule, matches) in rule_matches_by_rule.items() if matches], baseline_target_manager, core_runner, output_handler, dump_command_for_core, time_flag, matching_explanations, engine_type, run_secrets, disable_secrets_validation, baseline_target_mode_config)\n                    rule_matches_by_rule = remove_matches_in_baseline(rule_matches_by_rule, baseline_rule_matches_by_rule, baseline_handler.status.renamed)\n                    output_handler.handle_semgrep_errors(baseline_semgrep_errors)\n            except Exception as e:\n                raise SemgrepError(e)\n    ignores_start_time = time.time()\n    keep_ignored = disable_nosem or output_handler.formatter.keep_ignores()\n    (filtered_matches_by_rule, nosem_errors) = process_ignores(rule_matches_by_rule, keep_ignored=keep_ignored, strict=strict)\n    profiler.save('ignores_time', ignores_start_time)\n    output_handler.handle_semgrep_errors(nosem_errors)\n    profiler.save('total_time', rule_start_time)\n    if metrics.is_enabled:\n        metrics.add_rules(filtered_rules, output_extra.core.time)\n        metrics.add_max_memory_bytes(output_extra.core.time)\n        metrics.add_targets(output_extra.all_targets, output_extra.core.time)\n        metrics.add_findings(filtered_matches_by_rule)\n        metrics.add_errors(semgrep_errors)\n        metrics.add_profiling(profiler)\n        metrics.add_parse_rates(output_extra.parsing_data)\n        metrics.add_interfile_languages_used(output_extra.core.interfile_languages_used)\n    if autofix:\n        apply_fixes(filtered_matches_by_rule.kept, dryrun)\n    renamed_targets = set(baseline_handler.status.renamed.values() if baseline_handler else [])\n    executed_rule_count = sum((max(0, len(plan.rules) - len(plan.unused_rules)) for plan in plans))\n    return (filtered_matches_by_rule.kept, semgrep_errors, renamed_targets, target_manager.ignore_log, filtered_rules, profiler, output_extra, shown_severities, dependencies, dependency_parser_errors, contributions, executed_rule_count, missed_rule_count)"
        ]
    },
    {
        "func_name": "run_scan_and_return_json",
        "original": "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    \"\"\"\n    Return Semgrep results of 'config' on 'targets' as a dict|str\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\n    \"\"\"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())",
        "mutated": [
            "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    if False:\n        i = 10\n    \"\\n    Return Semgrep results of 'config' on 'targets' as a dict|str\\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\\n    \"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())",
            "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Return Semgrep results of 'config' on 'targets' as a dict|str\\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\\n    \"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())",
            "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Return Semgrep results of 'config' on 'targets' as a dict|str\\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\\n    \"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())",
            "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Return Semgrep results of 'config' on 'targets' as a dict|str\\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\\n    \"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())",
            "def run_scan_and_return_json(config: Path, targets: List[Path], output_settings: Optional[OutputSettings]=None, **kwargs: Any) -> Union[Dict[str, Any], str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Return Semgrep results of 'config' on 'targets' as a dict|str\\n    Uses default arguments of 'run_scan.run_scan' unless overwritten with 'kwargs'\\n    \"\n    if output_settings is None:\n        output_settings = OutputSettings(output_format=OutputFormat.JSON)\n    StringIO()\n    output_handler = OutputHandler(output_settings)\n    (filtered_matches_by_rule, _, _, _, filtered_rules, profiler, output_extra, shown_severities, _, _, _, _, _) = run_scan(output_handler=output_handler, target=[str(t) for t in targets], pattern='', lang='', configs=[str(config)], **kwargs)\n    output_handler.rules = frozenset(filtered_rules)\n    output_handler.rule_matches = [m for ms in filtered_matches_by_rule.values() for m in ms]\n    output_handler.profiler = profiler\n    output_handler.severities = shown_severities\n    output_handler.explanations = output_extra.core.explanations\n    output_handler.extra = output_extra\n    return json.loads(output_handler._build_output())"
        ]
    }
]