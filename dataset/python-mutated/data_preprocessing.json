[
    {
        "func_name": "_filter_index_sort",
        "original": "def _filter_index_sort(raw_rating_path, cache_path):\n    \"\"\"Read in data CSV, and output structured data.\n\n  This function reads in the raw CSV of positive items, and performs three\n  preprocessing transformations:\n\n  1)  Filter out all users who have not rated at least a certain number\n      of items. (Typically 20 items)\n\n  2)  Zero index the users and items such that the largest user_id is\n      `num_users - 1` and the largest item_id is `num_items - 1`\n\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\n      This allows the dataframe to be sliced by user in-place, and for the last\n      item to be selected simply by calling the `-1` index of a user's slice.\n\n  While all of these transformations are performed by Pandas (and are therefore\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\n  MapReduce pattern to parallel process the dataset adds significant complexity\n  for no computational gain. For a larger dataset parallelizing this\n  preprocessing could yield speedups. (Also, this preprocessing step is only\n  performed once for an entire run.\n\n  Args:\n    raw_rating_path: The path to the CSV which contains the raw dataset.\n    cache_path: The path to the file where results of this function are saved.\n\n  Returns:\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\n    item IDs.\n  \"\"\"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)",
        "mutated": [
            "def _filter_index_sort(raw_rating_path, cache_path):\n    if False:\n        i = 10\n    \"Read in data CSV, and output structured data.\\n\\n  This function reads in the raw CSV of positive items, and performs three\\n  preprocessing transformations:\\n\\n  1)  Filter out all users who have not rated at least a certain number\\n      of items. (Typically 20 items)\\n\\n  2)  Zero index the users and items such that the largest user_id is\\n      `num_users - 1` and the largest item_id is `num_items - 1`\\n\\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\\n      This allows the dataframe to be sliced by user in-place, and for the last\\n      item to be selected simply by calling the `-1` index of a user's slice.\\n\\n  While all of these transformations are performed by Pandas (and are therefore\\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\\n  MapReduce pattern to parallel process the dataset adds significant complexity\\n  for no computational gain. For a larger dataset parallelizing this\\n  preprocessing could yield speedups. (Also, this preprocessing step is only\\n  performed once for an entire run.\\n\\n  Args:\\n    raw_rating_path: The path to the CSV which contains the raw dataset.\\n    cache_path: The path to the file where results of this function are saved.\\n\\n  Returns:\\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\\n    item IDs.\\n  \"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)",
            "def _filter_index_sort(raw_rating_path, cache_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Read in data CSV, and output structured data.\\n\\n  This function reads in the raw CSV of positive items, and performs three\\n  preprocessing transformations:\\n\\n  1)  Filter out all users who have not rated at least a certain number\\n      of items. (Typically 20 items)\\n\\n  2)  Zero index the users and items such that the largest user_id is\\n      `num_users - 1` and the largest item_id is `num_items - 1`\\n\\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\\n      This allows the dataframe to be sliced by user in-place, and for the last\\n      item to be selected simply by calling the `-1` index of a user's slice.\\n\\n  While all of these transformations are performed by Pandas (and are therefore\\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\\n  MapReduce pattern to parallel process the dataset adds significant complexity\\n  for no computational gain. For a larger dataset parallelizing this\\n  preprocessing could yield speedups. (Also, this preprocessing step is only\\n  performed once for an entire run.\\n\\n  Args:\\n    raw_rating_path: The path to the CSV which contains the raw dataset.\\n    cache_path: The path to the file where results of this function are saved.\\n\\n  Returns:\\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\\n    item IDs.\\n  \"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)",
            "def _filter_index_sort(raw_rating_path, cache_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Read in data CSV, and output structured data.\\n\\n  This function reads in the raw CSV of positive items, and performs three\\n  preprocessing transformations:\\n\\n  1)  Filter out all users who have not rated at least a certain number\\n      of items. (Typically 20 items)\\n\\n  2)  Zero index the users and items such that the largest user_id is\\n      `num_users - 1` and the largest item_id is `num_items - 1`\\n\\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\\n      This allows the dataframe to be sliced by user in-place, and for the last\\n      item to be selected simply by calling the `-1` index of a user's slice.\\n\\n  While all of these transformations are performed by Pandas (and are therefore\\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\\n  MapReduce pattern to parallel process the dataset adds significant complexity\\n  for no computational gain. For a larger dataset parallelizing this\\n  preprocessing could yield speedups. (Also, this preprocessing step is only\\n  performed once for an entire run.\\n\\n  Args:\\n    raw_rating_path: The path to the CSV which contains the raw dataset.\\n    cache_path: The path to the file where results of this function are saved.\\n\\n  Returns:\\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\\n    item IDs.\\n  \"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)",
            "def _filter_index_sort(raw_rating_path, cache_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Read in data CSV, and output structured data.\\n\\n  This function reads in the raw CSV of positive items, and performs three\\n  preprocessing transformations:\\n\\n  1)  Filter out all users who have not rated at least a certain number\\n      of items. (Typically 20 items)\\n\\n  2)  Zero index the users and items such that the largest user_id is\\n      `num_users - 1` and the largest item_id is `num_items - 1`\\n\\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\\n      This allows the dataframe to be sliced by user in-place, and for the last\\n      item to be selected simply by calling the `-1` index of a user's slice.\\n\\n  While all of these transformations are performed by Pandas (and are therefore\\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\\n  MapReduce pattern to parallel process the dataset adds significant complexity\\n  for no computational gain. For a larger dataset parallelizing this\\n  preprocessing could yield speedups. (Also, this preprocessing step is only\\n  performed once for an entire run.\\n\\n  Args:\\n    raw_rating_path: The path to the CSV which contains the raw dataset.\\n    cache_path: The path to the file where results of this function are saved.\\n\\n  Returns:\\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\\n    item IDs.\\n  \"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)",
            "def _filter_index_sort(raw_rating_path, cache_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Read in data CSV, and output structured data.\\n\\n  This function reads in the raw CSV of positive items, and performs three\\n  preprocessing transformations:\\n\\n  1)  Filter out all users who have not rated at least a certain number\\n      of items. (Typically 20 items)\\n\\n  2)  Zero index the users and items such that the largest user_id is\\n      `num_users - 1` and the largest item_id is `num_items - 1`\\n\\n  3)  Sort the dataframe by user_id, with timestamp as a secondary sort key.\\n      This allows the dataframe to be sliced by user in-place, and for the last\\n      item to be selected simply by calling the `-1` index of a user's slice.\\n\\n  While all of these transformations are performed by Pandas (and are therefore\\n  single-threaded), they only take ~2 minutes, and the overhead to apply a\\n  MapReduce pattern to parallel process the dataset adds significant complexity\\n  for no computational gain. For a larger dataset parallelizing this\\n  preprocessing could yield speedups. (Also, this preprocessing step is only\\n  performed once for an entire run.\\n\\n  Args:\\n    raw_rating_path: The path to the CSV which contains the raw dataset.\\n    cache_path: The path to the file where results of this function are saved.\\n\\n  Returns:\\n    A filtered, zero-index remapped, sorted dataframe, a dict mapping raw user\\n    IDs to regularized user IDs, and a dict mapping raw item IDs to regularized\\n    item IDs.\\n  \"\n    valid_cache = tf.io.gfile.exists(cache_path)\n    if valid_cache:\n        with tf.io.gfile.GFile(cache_path, 'rb') as f:\n            cached_data = pickle.load(f)\n        for key in _EXPECTED_CACHE_KEYS:\n            if key not in cached_data:\n                valid_cache = False\n        if not valid_cache:\n            logging.info('Removing stale raw data cache file.')\n            tf.io.gfile.remove(cache_path)\n    if valid_cache:\n        data = cached_data\n    else:\n        with tf.io.gfile.GFile(raw_rating_path) as f:\n            df = pd.read_csv(f)\n        grouped = df.groupby(movielens.USER_COLUMN)\n        df = grouped.filter(lambda x: len(x) >= rconst.MIN_NUM_RATINGS)\n        original_users = df[movielens.USER_COLUMN].unique()\n        original_items = df[movielens.ITEM_COLUMN].unique()\n        logging.info('Generating user_map and item_map...')\n        user_map = {user: index for (index, user) in enumerate(original_users)}\n        item_map = {item: index for (index, item) in enumerate(original_items)}\n        df[movielens.USER_COLUMN] = df[movielens.USER_COLUMN].apply(lambda user: user_map[user])\n        df[movielens.ITEM_COLUMN] = df[movielens.ITEM_COLUMN].apply(lambda item: item_map[item])\n        num_users = len(original_users)\n        num_items = len(original_items)\n        mlperf_helper.ncf_print(key=mlperf_helper.TAGS.PREPROC_HP_NUM_EVAL, value=rconst.NUM_EVAL_NEGATIVES)\n        assert num_users <= np.iinfo(rconst.USER_DTYPE).max\n        assert num_items <= np.iinfo(rconst.ITEM_DTYPE).max\n        assert df[movielens.USER_COLUMN].max() == num_users - 1\n        assert df[movielens.ITEM_COLUMN].max() == num_items - 1\n        logging.info('Sorting by user, timestamp...')\n        df.sort_values(by=movielens.TIMESTAMP_COLUMN, inplace=True)\n        df.sort_values([movielens.USER_COLUMN, movielens.TIMESTAMP_COLUMN], inplace=True, kind='mergesort')\n        df = df.reset_index()\n        grouped = df.groupby(movielens.USER_COLUMN, group_keys=False)\n        (eval_df, train_df) = (grouped.tail(1), grouped.apply(lambda x: x.iloc[:-1]))\n        data = {rconst.TRAIN_USER_KEY: train_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.TRAIN_ITEM_KEY: train_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.EVAL_USER_KEY: eval_df[movielens.USER_COLUMN].values.astype(rconst.USER_DTYPE), rconst.EVAL_ITEM_KEY: eval_df[movielens.ITEM_COLUMN].values.astype(rconst.ITEM_DTYPE), rconst.USER_MAP: user_map, rconst.ITEM_MAP: item_map, 'create_time': time.time()}\n        logging.info('Writing raw data cache.')\n        with tf.io.gfile.GFile(cache_path, 'wb') as f:\n            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n    return (data, valid_cache)"
        ]
    },
    {
        "func_name": "instantiate_pipeline",
        "original": "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    \"\"\"Load and digest data CSV into a usable form.\n\n  Args:\n    dataset: The name of the dataset to be used.\n    data_dir: The root directory of the dataset.\n    params: dict of parameters for the run.\n    constructor_type: The name of the constructor subclass that should be used\n      for the input pipeline.\n    deterministic: Tell the data constructor to produce deterministically.\n    epoch_dir: Directory in which to store the training epochs.\n    generate_data_offline: Boolean, whether current pipeline is done offline\n      or while training.\n  \"\"\"\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)",
        "mutated": [
            "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    if False:\n        i = 10\n    'Load and digest data CSV into a usable form.\\n\\n  Args:\\n    dataset: The name of the dataset to be used.\\n    data_dir: The root directory of the dataset.\\n    params: dict of parameters for the run.\\n    constructor_type: The name of the constructor subclass that should be used\\n      for the input pipeline.\\n    deterministic: Tell the data constructor to produce deterministically.\\n    epoch_dir: Directory in which to store the training epochs.\\n    generate_data_offline: Boolean, whether current pipeline is done offline\\n      or while training.\\n  '\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)",
            "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load and digest data CSV into a usable form.\\n\\n  Args:\\n    dataset: The name of the dataset to be used.\\n    data_dir: The root directory of the dataset.\\n    params: dict of parameters for the run.\\n    constructor_type: The name of the constructor subclass that should be used\\n      for the input pipeline.\\n    deterministic: Tell the data constructor to produce deterministically.\\n    epoch_dir: Directory in which to store the training epochs.\\n    generate_data_offline: Boolean, whether current pipeline is done offline\\n      or while training.\\n  '\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)",
            "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load and digest data CSV into a usable form.\\n\\n  Args:\\n    dataset: The name of the dataset to be used.\\n    data_dir: The root directory of the dataset.\\n    params: dict of parameters for the run.\\n    constructor_type: The name of the constructor subclass that should be used\\n      for the input pipeline.\\n    deterministic: Tell the data constructor to produce deterministically.\\n    epoch_dir: Directory in which to store the training epochs.\\n    generate_data_offline: Boolean, whether current pipeline is done offline\\n      or while training.\\n  '\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)",
            "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load and digest data CSV into a usable form.\\n\\n  Args:\\n    dataset: The name of the dataset to be used.\\n    data_dir: The root directory of the dataset.\\n    params: dict of parameters for the run.\\n    constructor_type: The name of the constructor subclass that should be used\\n      for the input pipeline.\\n    deterministic: Tell the data constructor to produce deterministically.\\n    epoch_dir: Directory in which to store the training epochs.\\n    generate_data_offline: Boolean, whether current pipeline is done offline\\n      or while training.\\n  '\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)",
            "def instantiate_pipeline(dataset, data_dir, params, constructor_type=None, deterministic=False, epoch_dir=None, generate_data_offline=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load and digest data CSV into a usable form.\\n\\n  Args:\\n    dataset: The name of the dataset to be used.\\n    data_dir: The root directory of the dataset.\\n    params: dict of parameters for the run.\\n    constructor_type: The name of the constructor subclass that should be used\\n      for the input pipeline.\\n    deterministic: Tell the data constructor to produce deterministically.\\n    epoch_dir: Directory in which to store the training epochs.\\n    generate_data_offline: Boolean, whether current pipeline is done offline\\n      or while training.\\n  '\n    logging.info('Beginning data preprocessing.')\n    st = timeit.default_timer()\n    raw_rating_path = os.path.join(data_dir, dataset, movielens.RATINGS_FILE)\n    cache_path = os.path.join(data_dir, dataset, rconst.RAW_CACHE_FILE)\n    (raw_data, _) = _filter_index_sort(raw_rating_path, cache_path)\n    (user_map, item_map) = (raw_data['user_map'], raw_data['item_map'])\n    (num_users, num_items) = DATASET_TO_NUM_USERS_AND_ITEMS[dataset]\n    if num_users != len(user_map):\n        raise ValueError('Expected to find {} users, but found {}'.format(num_users, len(user_map)))\n    if num_items != len(item_map):\n        raise ValueError('Expected to find {} items, but found {}'.format(num_items, len(item_map)))\n    producer = data_pipeline.get_constructor(constructor_type or 'materialized')(maximum_number_epochs=params['train_epochs'], num_users=num_users, num_items=num_items, user_map=user_map, item_map=item_map, train_pos_users=raw_data[rconst.TRAIN_USER_KEY], train_pos_items=raw_data[rconst.TRAIN_ITEM_KEY], train_batch_size=params['batch_size'], batches_per_train_step=params['batches_per_step'], num_train_negatives=params['num_neg'], eval_pos_users=raw_data[rconst.EVAL_USER_KEY], eval_pos_items=raw_data[rconst.EVAL_ITEM_KEY], eval_batch_size=params['eval_batch_size'], batches_per_eval_step=params['batches_per_step'], stream_files=params['stream_files'], deterministic=deterministic, epoch_dir=epoch_dir, create_data_offline=generate_data_offline)\n    run_time = timeit.default_timer() - st\n    logging.info('Data preprocessing complete. Time: {:.1f} sec.'.format(run_time))\n    print(producer)\n    return (num_users, num_items, producer)"
        ]
    }
]