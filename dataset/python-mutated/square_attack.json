[
    {
        "func_name": "__init__",
        "original": "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    \"\"\"\n        Create a :class:`.SquareAttack` instance.\n\n        :param estimator: An trained estimator.\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\n        :param loss: The loss function which the attack should use for optimization.\n        :param max_iter: Maximum number of iterations.\n        :param eps: Maximum perturbation that the attacker can introduce.\n        :param p_init: Initial fraction of elements.\n        :param nb_restarts: Number of restarts.\n        :param batch_size: Batch size for estimator evaluations.\n        :param verbose: Show progress bars.\n        \"\"\"\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
        "mutated": [
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n    '\\n        Create a :class:`.SquareAttack` instance.\\n\\n        :param estimator: An trained estimator.\\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\\n        :param loss: The loss function which the attack should use for optimization.\\n        :param max_iter: Maximum number of iterations.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param p_init: Initial fraction of elements.\\n        :param nb_restarts: Number of restarts.\\n        :param batch_size: Batch size for estimator evaluations.\\n        :param verbose: Show progress bars.\\n        '\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a :class:`.SquareAttack` instance.\\n\\n        :param estimator: An trained estimator.\\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\\n        :param loss: The loss function which the attack should use for optimization.\\n        :param max_iter: Maximum number of iterations.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param p_init: Initial fraction of elements.\\n        :param nb_restarts: Number of restarts.\\n        :param batch_size: Batch size for estimator evaluations.\\n        :param verbose: Show progress bars.\\n        '\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a :class:`.SquareAttack` instance.\\n\\n        :param estimator: An trained estimator.\\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\\n        :param loss: The loss function which the attack should use for optimization.\\n        :param max_iter: Maximum number of iterations.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param p_init: Initial fraction of elements.\\n        :param nb_restarts: Number of restarts.\\n        :param batch_size: Batch size for estimator evaluations.\\n        :param verbose: Show progress bars.\\n        '\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a :class:`.SquareAttack` instance.\\n\\n        :param estimator: An trained estimator.\\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\\n        :param loss: The loss function which the attack should use for optimization.\\n        :param max_iter: Maximum number of iterations.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param p_init: Initial fraction of elements.\\n        :param nb_restarts: Number of restarts.\\n        :param batch_size: Batch size for estimator evaluations.\\n        :param verbose: Show progress bars.\\n        '\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, estimator: 'CLASSIFIER_TYPE', norm: Union[int, float, str]=np.inf, adv_criterion: Union[Callable[[np.ndarray, np.ndarray], bool], None]=None, loss: Union[Callable[[np.ndarray, np.ndarray], np.ndarray], None]=None, max_iter: int=100, eps: float=0.3, p_init: float=0.8, nb_restarts: int=1, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a :class:`.SquareAttack` instance.\\n\\n        :param estimator: An trained estimator.\\n        :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\\n        :param adv_criterion: The criterion which the attack should use in determining adversariality.\\n        :param loss: The loss function which the attack should use for optimization.\\n        :param max_iter: Maximum number of iterations.\\n        :param eps: Maximum perturbation that the attacker can introduce.\\n        :param p_init: Initial fraction of elements.\\n        :param nb_restarts: Number of restarts.\\n        :param batch_size: Batch size for estimator evaluations.\\n        :param verbose: Show progress bars.\\n        '\n    super().__init__(estimator=estimator)\n    self.norm = norm\n    if adv_criterion is not None:\n        self.adv_criterion = adv_criterion\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.adv_criterion = lambda y_pred, y: np.argmax(y_pred, axis=1) != np.argmax(y, axis=1)\n    else:\n        raise ValueError('No acceptable adversarial criterion available.')\n    if loss is not None:\n        self.loss = loss\n    elif isinstance(self.estimator, ClassifierMixin):\n        self.loss = self._get_logits_diff\n    else:\n        raise ValueError('No acceptable loss available.')\n    self.max_iter = max_iter\n    self.eps = eps\n    self.p_init = p_init\n    self.nb_restarts = nb_restarts\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()"
        ]
    },
    {
        "func_name": "_get_logits_diff",
        "original": "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]",
        "mutated": [
            "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]",
            "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]",
            "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]",
            "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]",
            "def _get_logits_diff(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    y_pred = self.estimator.predict(x, batch_size=self.batch_size)\n    logit_correct = np.take_along_axis(y_pred, np.expand_dims(np.argmax(y, axis=1), axis=1), axis=1)\n    logit_highest_incorrect = np.take_along_axis(y_pred, np.expand_dims(np.argsort(y_pred, axis=1)[:, -2], axis=1), axis=1)\n    return (logit_correct - logit_highest_incorrect)[:, 0]"
        ]
    },
    {
        "func_name": "_get_percentage_of_elements",
        "original": "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]",
        "mutated": [
            "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    if False:\n        i = 10\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]",
            "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]",
            "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]",
            "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]",
            "def _get_percentage_of_elements(self, i_iter: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    i_p = i_iter / self.max_iter\n    intervals = [0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n    p_ratio = [1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32, 1 / 64, 1 / 128, 1 / 256, 1 / 512]\n    i_ratio = bisect.bisect_left(intervals, i_p)\n    return self.p_init * p_ratio[i_ratio]"
        ]
    },
    {
        "func_name": "_get_perturbation",
        "original": "def _get_perturbation(height):\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta",
        "mutated": [
            "def _get_perturbation(height):\n    if False:\n        i = 10\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta",
            "def _get_perturbation(height):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta",
            "def _get_perturbation(height):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta",
            "def _get_perturbation(height):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta",
            "def _get_perturbation(height):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    delta = np.zeros([height, height])\n    gaussian_perturbation = np.zeros([height // 2, height])\n    x_c = height // 4\n    y_c = height // 2\n    for i_y in range(y_c):\n        gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n        x_c -= 1\n        y_c -= 1\n    gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n    delta[:height // 2] = gaussian_perturbation\n    delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n    delta /= np.sqrt(np.sum(delta ** 2))\n    if random.random() > 0.5:\n        delta = np.transpose(delta)\n    if random.random() > 0.5:\n        delta = -delta\n    return delta"
        ]
    },
    {
        "func_name": "generate",
        "original": "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    \"\"\"\n        Generate adversarial samples and return them in an array.\n\n        :param x: An array with the original inputs.\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\n                  (nb_samples,). Only provide this parameter if you'd like to use true labels when crafting adversarial\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\n        :return: An array holding the adversarial examples.\n        \"\"\"\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv",
        "mutated": [
            "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Generate adversarial samples and return them in an array.\\n\\n        :param x: An array with the original inputs.\\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\\n                  (nb_samples,). Only provide this parameter if you\\'d like to use true labels when crafting adversarial\\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\\n        :return: An array holding the adversarial examples.\\n        '\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv",
            "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Generate adversarial samples and return them in an array.\\n\\n        :param x: An array with the original inputs.\\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\\n                  (nb_samples,). Only provide this parameter if you\\'d like to use true labels when crafting adversarial\\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\\n        :return: An array holding the adversarial examples.\\n        '\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv",
            "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Generate adversarial samples and return them in an array.\\n\\n        :param x: An array with the original inputs.\\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\\n                  (nb_samples,). Only provide this parameter if you\\'d like to use true labels when crafting adversarial\\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\\n        :return: An array holding the adversarial examples.\\n        '\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv",
            "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Generate adversarial samples and return them in an array.\\n\\n        :param x: An array with the original inputs.\\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\\n                  (nb_samples,). Only provide this parameter if you\\'d like to use true labels when crafting adversarial\\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\\n        :return: An array holding the adversarial examples.\\n        '\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv",
            "def generate(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Generate adversarial samples and return them in an array.\\n\\n        :param x: An array with the original inputs.\\n        :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\\n                  (nb_samples,). Only provide this parameter if you\\'d like to use true labels when crafting adversarial\\n                  samples. Otherwise, model predictions are used as labels to avoid the \"label leaking\" effect\\n                  (explained in this paper: https://arxiv.org/abs/1611.01236). Default is `None`.\\n        :return: An array holding the adversarial examples.\\n        '\n    if x.ndim != 4:\n        raise ValueError('Unrecognized input dimension. Attack can only be applied to image data.')\n    x_adv = x.astype(ART_NUMPY_DTYPE)\n    if isinstance(self.estimator, ClassifierMixin):\n        if y is not None:\n            y = check_and_transform_label_format(y, nb_classes=self.estimator.nb_classes)\n    if y is None:\n        logger.info('Using model predictions as true labels.')\n        y = self.estimator.predict(x, batch_size=self.batch_size)\n        if isinstance(self.estimator, ClassifierMixin):\n            y = get_labels_np_array(y)\n    if isinstance(self.estimator, ClassifierMixin):\n        if self.estimator.nb_classes == 2 and y.shape[1] == 1:\n            raise ValueError('This attack has not yet been tested for binary classification with a single output classifier.')\n    if self.estimator.channels_first:\n        channels = x.shape[1]\n        height = x.shape[2]\n        width = x.shape[3]\n    else:\n        height = x.shape[1]\n        width = x.shape[2]\n        channels = x.shape[3]\n    for _ in trange(self.nb_restarts, desc='SquareAttack - restarts', disable=not self.verbose):\n        y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n        sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n        if np.sum(sample_is_robust) == 0:\n            break\n        x_robust = x[sample_is_robust]\n        y_robust = y[sample_is_robust]\n        sample_loss_init = self.loss(x_robust, y_robust)\n        if self.norm in [np.inf, 'inf']:\n            if self.estimator.channels_first:\n                size = (x_robust.shape[0], channels, 1, width)\n            else:\n                size = (x_robust.shape[0], 1, width, channels)\n            x_robust_new = np.clip(x_robust + self.eps * np.random.choice([-1, 1], size=size), a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 1)\n                height_mid = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                delta_new = np.zeros(self.estimator.input_shape)\n                if self.estimator.channels_first:\n                    delta_new[:, height_mid:height_mid + height_tile, width_start:width_start + height_tile] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[channels, 1, 1])\n                else:\n                    delta_new[height_mid:height_mid + height_tile, width_start:width_start + height_tile, :] = np.random.choice([-2 * self.eps, 2 * self.eps], size=[1, 1, channels])\n                x_robust_new = x_robust + delta_new\n                x_robust_new = np.minimum(np.maximum(x_robust_new, x_init - self.eps), x_init + self.eps)\n                x_robust_new = np.clip(x_robust_new, a_min=self.estimator.clip_values[0], a_max=self.estimator.clip_values[1]).astype(ART_NUMPY_DTYPE)\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n        elif self.norm == 2:\n            n_tiles = 5\n            height_tile = height // n_tiles\n\n            def _get_perturbation(height):\n                delta = np.zeros([height, height])\n                gaussian_perturbation = np.zeros([height // 2, height])\n                x_c = height // 4\n                y_c = height // 2\n                for i_y in range(y_c):\n                    gaussian_perturbation[max(x_c, 0):min(x_c + (2 * i_y + 1), height // 2), max(0, y_c):min(y_c + (2 * i_y + 1), height)] += 1.0 / (i_y + 1) ** 2\n                    x_c -= 1\n                    y_c -= 1\n                gaussian_perturbation /= np.sqrt(np.sum(gaussian_perturbation ** 2))\n                delta[:height // 2] = gaussian_perturbation\n                delta[height // 2:height // 2 + gaussian_perturbation.shape[0]] = -gaussian_perturbation\n                delta /= np.sqrt(np.sum(delta ** 2))\n                if random.random() > 0.5:\n                    delta = np.transpose(delta)\n                if random.random() > 0.5:\n                    delta = -delta\n                return delta\n            delta_init = np.zeros(x_robust.shape, dtype=ART_NUMPY_DTYPE)\n            height_start = 0\n            for _ in range(n_tiles):\n                width_start = 0\n                for _ in range(n_tiles):\n                    if self.estimator.channels_first:\n                        perturbation_size = (1, 1, height_tile, height_tile)\n                        random_size = (x_robust.shape[0], channels, 1, 1)\n                    else:\n                        perturbation_size = (1, height_tile, height_tile, 1)\n                        random_size = (x_robust.shape[0], 1, 1, channels)\n                    perturbation = _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_size)\n                    if self.estimator.channels_first:\n                        delta_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] += perturbation\n                    else:\n                        delta_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] += perturbation\n                    width_start += height_tile\n                height_start += height_tile\n            x_robust_new = np.clip(x_robust + delta_init / np.sqrt(np.sum(delta_init ** 2, axis=(1, 2, 3), keepdims=True)) * self.eps, self.estimator.clip_values[0], self.estimator.clip_values[1])\n            sample_loss_new = self.loss(x_robust_new, y_robust)\n            loss_improved = sample_loss_new - sample_loss_init < 0.0\n            x_robust[loss_improved] = x_robust_new[loss_improved]\n            x_adv[sample_is_robust] = x_robust\n            for i_iter in trange(self.max_iter, desc='SquareAttack - iterations', leave=False, disable=not self.verbose):\n                percentage_of_elements = self._get_percentage_of_elements(i_iter)\n                y_pred = self.estimator.predict(x_adv, batch_size=self.batch_size)\n                sample_is_robust = np.logical_not(self.adv_criterion(y_pred, y))\n                if np.sum(sample_is_robust) == 0:\n                    break\n                x_robust = x_adv[sample_is_robust]\n                x_init = x[sample_is_robust]\n                y_robust = y[sample_is_robust]\n                sample_loss_init = self.loss(x_robust, y_robust)\n                delta_x_robust_init = x_robust - x_init\n                height_tile = max(int(round(math.sqrt(percentage_of_elements * height * width))), 3)\n                if height_tile % 2 == 0:\n                    height_tile += 1\n                height_tile_2 = height_tile\n                height_start = np.random.randint(0, height - height_tile)\n                width_start = np.random.randint(0, width - height_tile)\n                new_deltas_mask = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] ** 2, axis=(2, 3), keepdims=True))\n                else:\n                    new_deltas_mask[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = 1.0\n                    w_1_norm = np.sqrt(np.sum(delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] ** 2, axis=(1, 2), keepdims=True))\n                height_2_start = np.random.randint(0, height - height_tile_2)\n                width_2_start = np.random.randint(0, width - height_tile_2)\n                new_deltas_mask_2 = np.zeros(x_init.shape)\n                if self.estimator.channels_first:\n                    new_deltas_mask_2[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 1.0\n                else:\n                    new_deltas_mask_2[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 1.0\n                norms_x_robust = np.sqrt(np.sum((x_robust - x_init) ** 2, axis=(1, 2, 3), keepdims=True))\n                w_norm = np.sqrt(np.sum((delta_x_robust_init * np.maximum(new_deltas_mask, new_deltas_mask_2)) ** 2, axis=(1, 2, 3), keepdims=True))\n                if self.estimator.channels_first:\n                    new_deltas_size = [x_init.shape[0], channels, height_tile, height_tile]\n                    random_choice_size = [x_init.shape[0], channels, 1, 1]\n                    perturbation_size = (1, 1, height_tile, height_tile)\n                else:\n                    new_deltas_size = [x_init.shape[0], height_tile, height_tile, channels]\n                    random_choice_size = [x_init.shape[0], 1, 1, channels]\n                    perturbation_size = (1, height_tile, height_tile, 1)\n                delta_new = np.ones(new_deltas_size) * _get_perturbation(height_tile).reshape(perturbation_size) * np.random.choice([-1, 1], size=random_choice_size)\n                if self.estimator.channels_first:\n                    delta_new += delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] / np.maximum(1e-09, w_1_norm)\n                else:\n                    delta_new += delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] / np.maximum(1e-09, w_1_norm)\n                diff_norm = (self.eps * np.ones(delta_new.shape)) ** 2 - norms_x_robust ** 2\n                diff_norm[diff_norm < 0.0] = 0.0\n                if self.estimator.channels_first:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(2, 3), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, :, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2] = 0.0\n                    delta_x_robust_init[:, :, height_start:height_start + height_tile, width_start:width_start + height_tile] = delta_new\n                else:\n                    delta_new /= np.sqrt(np.sum(delta_new ** 2, axis=(1, 2), keepdims=True)) * np.sqrt(diff_norm / channels + w_norm ** 2)\n                    delta_x_robust_init[:, height_2_start:height_2_start + height_tile_2, width_2_start:width_2_start + height_tile_2, :] = 0.0\n                    delta_x_robust_init[:, height_start:height_start + height_tile, width_start:width_start + height_tile, :] = delta_new\n                x_robust_new = np.clip(x_init + self.eps * delta_x_robust_init / np.sqrt(np.sum(delta_x_robust_init ** 2, axis=(1, 2, 3), keepdims=True)), self.estimator.clip_values[0], self.estimator.clip_values[1])\n                sample_loss_new = self.loss(x_robust_new, y_robust)\n                loss_improved = sample_loss_new - sample_loss_init < 0.0\n                x_robust[loss_improved] = x_robust_new[loss_improved]\n                x_adv[sample_is_robust] = x_robust\n    return x_adv"
        ]
    },
    {
        "func_name": "_check_params",
        "original": "def _check_params(self) -> None:\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')",
        "mutated": [
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.norm not in [1, 2, np.inf, 'inf']:\n        raise ValueError('The argument norm has to be either 1, 2, np.inf, or \"inf\".')\n    if not isinstance(self.max_iter, int) or self.max_iter <= 0:\n        raise ValueError('The argument max_iter has to be of type int and larger than zero.')\n    if not isinstance(self.eps, (int, float)) or self.eps <= 0.0:\n        raise ValueError('The argument eps has to be either of type int or float and larger than zero.')\n    if not isinstance(self.p_init, (int, float)) or self.p_init <= 0.0 or self.p_init >= 1.0:\n        raise ValueError('The argument p_init has to be either of type int or float and in range [0, 1].')\n    if not isinstance(self.nb_restarts, int) or self.nb_restarts <= 0:\n        raise ValueError('The argument nb_restarts has to be of type int and larger than zero.')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('The argument batch_size has to be of type int and larger than zero.')\n    if not isinstance(self.verbose, bool):\n        raise ValueError('The argument `verbose` has to be of type bool.')"
        ]
    }
]