[
    {
        "func_name": "__init__",
        "original": "def __init__(self, config):\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None",
        "mutated": [
            "def __init__(self, config):\n    if False:\n        i = 10\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None",
            "def __init__(self, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._skip_env_checking = True\n    if config.get('simplex_actions', False):\n        self.action_space = Simplex((2,))\n    else:\n        self.action_space = Box(0.0, 1.0, (1,))\n    self.observation_space = Box(0.0, 1.0, (1,))\n    self.max_steps = config.get('max_steps', 100)\n    self.state = None\n    self.steps = None"
        ]
    },
    {
        "func_name": "reset",
        "original": "def reset(self, *, seed=None, options=None):\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})",
        "mutated": [
            "def reset(self, *, seed=None, options=None):\n    if False:\n        i = 10\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})",
            "def reset(self, *, seed=None, options=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})",
            "def reset(self, *, seed=None, options=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})",
            "def reset(self, *, seed=None, options=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})",
            "def reset(self, *, seed=None, options=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.state = self.observation_space.sample()\n    self.steps = 0\n    return (self.state, {})"
        ]
    },
    {
        "func_name": "step",
        "original": "def step(self, action):\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})",
        "mutated": [
            "def step(self, action):\n    if False:\n        i = 10\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})",
            "def step(self, action):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})",
            "def step(self, action):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})",
            "def step(self, action):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})",
            "def step(self, action):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.steps += 1\n    [rew] = 1.0 - np.abs(np.max(action) - self.state)\n    terminated = False\n    truncated = self.steps >= self.max_steps\n    self.state = self.observation_space.sample()\n    return (self.state, rew, terminated, truncated, {})"
        ]
    },
    {
        "func_name": "setUpClass",
        "original": "@classmethod\ndef setUpClass(cls) -> None:\n    ray.init()",
        "mutated": [
            "@classmethod\ndef setUpClass(cls) -> None:\n    if False:\n        i = 10\n    ray.init()",
            "@classmethod\ndef setUpClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ray.init()",
            "@classmethod\ndef setUpClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ray.init()",
            "@classmethod\ndef setUpClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ray.init()",
            "@classmethod\ndef setUpClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ray.init()"
        ]
    },
    {
        "func_name": "tearDownClass",
        "original": "@classmethod\ndef tearDownClass(cls) -> None:\n    ray.shutdown()",
        "mutated": [
            "@classmethod\ndef tearDownClass(cls) -> None:\n    if False:\n        i = 10\n    ray.shutdown()",
            "@classmethod\ndef tearDownClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ray.shutdown()",
            "@classmethod\ndef tearDownClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ray.shutdown()",
            "@classmethod\ndef tearDownClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ray.shutdown()",
            "@classmethod\ndef tearDownClass(cls) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ray.shutdown()"
        ]
    },
    {
        "func_name": "test_ddpg_compilation",
        "original": "def test_ddpg_compilation(self):\n    \"\"\"Test whether DDPG can be built with both frameworks.\"\"\"\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()",
        "mutated": [
            "def test_ddpg_compilation(self):\n    if False:\n        i = 10\n    'Test whether DDPG can be built with both frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()",
            "def test_ddpg_compilation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test whether DDPG can be built with both frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()",
            "def test_ddpg_compilation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test whether DDPG can be built with both frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()",
            "def test_ddpg_compilation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test whether DDPG can be built with both frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()",
            "def test_ddpg_compilation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test whether DDPG can be built with both frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0).rollouts(num_rollout_workers=0, num_envs_per_worker=2).exploration(exploration_config={'random_timesteps': 100})\n    num_iterations = 1\n    for _ in framework_iterator(config, with_eager_tracing=True):\n        algo = config.build(env='Pendulum-v1')\n        for i in range(num_iterations):\n            results = algo.train()\n            check_train_results(results)\n            print(results)\n        check_compute_single_action(algo)\n        pol = algo.get_policy()\n        if config.framework_str == 'tf':\n            a = pol.get_session().run(pol.global_step)\n        else:\n            a = pol.global_step\n        check(convert_to_numpy(a), 500)\n        algo.stop()"
        ]
    },
    {
        "func_name": "test_ddpg_exploration_and_with_random_prerun",
        "original": "def test_ddpg_exploration_and_with_random_prerun(self):\n    \"\"\"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\"\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()",
        "mutated": [
            "def test_ddpg_exploration_and_with_random_prerun(self):\n    if False:\n        i = 10\n    \"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()",
            "def test_ddpg_exploration_and_with_random_prerun(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()",
            "def test_ddpg_exploration_and_with_random_prerun(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()",
            "def test_ddpg_exploration_and_with_random_prerun(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()",
            "def test_ddpg_exploration_and_with_random_prerun(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Tests DDPG's Exploration (w/ random actions for n timesteps).\"\n    core_config = DDPGConfig().environment('Pendulum-v1').rollouts(num_rollout_workers=0).training(num_steps_sampled_before_learning_starts=0)\n    obs = np.array([0.0, 0.1, -0.1])\n    for _ in framework_iterator(core_config):\n        config = copy.deepcopy(core_config)\n        algo = config.build()\n        a_ = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 2)\n            check(a, a_)\n        actions = []\n        for i in range(50):\n            actions.append(algo.compute_single_action(obs))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 52)\n        check(np.std(actions), 0.0, false=True)\n        algo.stop()\n        config.exploration(exploration_config={'random_timesteps': 50, 'ou_base_scale': 0.001, 'initial_scale': 0.001, 'final_scale': 0.001})\n        algo = config.build()\n        deterministic_action = algo.compute_single_action(obs, explore=False)\n        check(convert_to_numpy(algo.get_policy().global_timestep), 1)\n        random_a = []\n        for i in range(1, 50):\n            random_a.append(algo.compute_single_action(obs, explore=True))\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 1)\n            check(random_a[-1], deterministic_action, false=True)\n        self.assertTrue(np.std(random_a) > 0.5)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=True)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 51)\n            check(a, deterministic_action, rtol=0.1)\n        for i in range(50):\n            a = algo.compute_single_action(obs, explore=False)\n            check(convert_to_numpy(algo.get_policy().global_timestep), i + 101)\n            check(a, deterministic_action)\n        algo.stop()"
        ]
    },
    {
        "func_name": "test_ddpg_loss_function",
        "original": "def test_ddpg_loss_function(self):\n    \"\"\"Tests DDPG loss function results across all frameworks.\"\"\"\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()",
        "mutated": [
            "def test_ddpg_loss_function(self):\n    if False:\n        i = 10\n    'Tests DDPG loss function results across all frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()",
            "def test_ddpg_loss_function(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Tests DDPG loss function results across all frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()",
            "def test_ddpg_loss_function(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Tests DDPG loss function results across all frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()",
            "def test_ddpg_loss_function(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Tests DDPG loss function results across all frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()",
            "def test_ddpg_loss_function(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Tests DDPG loss function results across all frameworks.'\n    config = DDPGConfig().training(num_steps_sampled_before_learning_starts=0)\n    config.seed = 42\n    config.num_rollout_workers = 0\n    config.twin_q = True\n    config.use_huber = True\n    config.huber_threshold = 1.0\n    config.gamma = 0.99\n    config.l2_reg = 1e-10\n    config.replay_buffer_config = {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}\n    config.num_steps_sampled_before_learning_starts = 0\n    config.actor_hiddens = [10]\n    config.critic_hiddens = [10]\n    config.min_time_s_per_iteration = 0\n    config.min_sample_timesteps_per_iteration = 100\n    map_ = {'default_policy/actor_hidden_0/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_1/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_1/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_1/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_1/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias', 'default_policy/actor_hidden_0_1/kernel': 'policy_model.action_0._model.0.weight', 'default_policy/actor_hidden_0_1/bias': 'policy_model.action_0._model.0.bias', 'default_policy/actor_out_1/kernel': 'policy_model.action_out._model.0.weight', 'default_policy/actor_out_1/bias': 'policy_model.action_out._model.0.bias', 'default_policy/sequential_2/q_hidden_0/kernel': 'q_model.q_hidden_0._model.0.weight', 'default_policy/sequential_2/q_hidden_0/bias': 'q_model.q_hidden_0._model.0.bias', 'default_policy/sequential_2/q_out/kernel': 'q_model.q_out._model.0.weight', 'default_policy/sequential_2/q_out/bias': 'q_model.q_out._model.0.bias', 'default_policy/sequential_3/twin_q_hidden_0/kernel': 'twin_q_model.twin_q_hidden_0._model.0.weight', 'default_policy/sequential_3/twin_q_hidden_0/bias': 'twin_q_model.twin_q_hidden_0._model.0.bias', 'default_policy/sequential_3/twin_q_out/kernel': 'twin_q_model.twin_q_out._model.0.weight', 'default_policy/sequential_3/twin_q_out/bias': 'twin_q_model.twin_q_out._model.0.bias'}\n    env = SimpleEnv\n    batch_size = 100\n    obs_size = (batch_size, 1)\n    actions = np.random.random(size=(batch_size, 1))\n    input_ = self._get_batch_helper(obs_size, actions, batch_size)\n    prev_fw_loss = weights_dict = None\n    (expect_c, expect_a, expect_t) = (None, None, None)\n    tf_updated_weights = []\n    tf_inputs = []\n    for (fw, sess) in framework_iterator(config, frameworks=('tf', 'torch'), session=True):\n        algo = config.build(env=env)\n        policy = algo.get_policy()\n        p_sess = None\n        if sess:\n            p_sess = policy.get_session()\n        if weights_dict is None:\n            assert fw == 'tf'\n            weights_dict_list = policy.model.variables() + policy.target_model.variables()\n            with p_sess.graph.as_default():\n                collector = ray.experimental.tf_utils.TensorFlowVariables([], p_sess, weights_dict_list)\n                weights_dict = collector.get_weights()\n        else:\n            assert fw == 'torch'\n            model_dict = self._translate_weights_to_torch(weights_dict, map_)\n            policy.model.load_state_dict(model_dict)\n            policy.target_model.load_state_dict(model_dict)\n        if fw == 'torch':\n            input_ = policy._lazy_tensor_dict(input_)\n            input_ = {k: input_[k] for k in input_.keys()}\n        if expect_c is None:\n            (expect_c, expect_a, expect_t) = self._ddpg_loss_helper(input_, weights_dict, sorted(weights_dict.keys()), fw, gamma=config.gamma, huber_threshold=config.huber_threshold, l2_reg=config.l2_reg, sess=sess)\n        if fw == 'tf':\n            (c, a, t, tf_c_grads, tf_a_grads) = p_sess.run([policy.critic_loss, policy.actor_loss, policy.td_error, policy._critic_optimizer.compute_gradients(policy.critic_loss, policy.model.q_variables()), policy._actor_optimizer.compute_gradients(policy.actor_loss, policy.model.policy_variables())], feed_dict=policy._get_loss_inputs_dict(input_, shuffle=False))\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            tf_c_grads = [g for (g, v) in tf_c_grads]\n            tf_a_grads = [g for (g, v) in tf_a_grads]\n        elif fw == 'torch':\n            policy.loss(policy.model, None, input_)\n            (c, a, t) = (policy.get_tower_stats('critic_loss')[0], policy.get_tower_stats('actor_loss')[0], policy.get_tower_stats('td_error')[0])\n            check(c, expect_c)\n            check(a, expect_a)\n            check(t, expect_t)\n            policy._actor_optimizer.zero_grad()\n            assert all((v.grad is None for v in policy.model.q_variables()))\n            assert all((v.grad is None for v in policy.model.policy_variables()))\n            a.backward()\n            assert not any((v.grad is None for v in policy.model.q_variables()[:4]))\n            assert all((v.grad is None for v in policy.model.q_variables()[4:]))\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.policy_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.policy_variables()))\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            policy._critic_optimizer.zero_grad()\n            assert all((v.grad is None or torch.mean(v.grad) == 0.0 for v in policy.model.q_variables()))\n            assert all((v.grad is None or torch.min(v.grad) == 0.0 for v in policy.model.q_variables()))\n            c.backward()\n            assert not all((torch.mean(v.grad) == 0 for v in policy.model.q_variables()))\n            assert not all((torch.min(v.grad) == 0 for v in policy.model.q_variables()))\n            torch_c_grads = [v.grad for v in policy.model.q_variables()]\n            for (tf_g, torch_g) in zip(tf_c_grads, torch_c_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n            torch_a_grads = [v.grad for v in policy.model.policy_variables()]\n            for (tf_g, torch_g) in zip(tf_a_grads, torch_a_grads):\n                if tf_g.shape != torch_g.shape:\n                    check(tf_g, np.transpose(torch_g.cpu()))\n                else:\n                    check(tf_g, torch_g)\n        if prev_fw_loss is not None:\n            check(c, prev_fw_loss[0])\n            check(a, prev_fw_loss[1])\n            check(t, prev_fw_loss[2])\n        prev_fw_loss = (c, a, t)\n        for update_iteration in range(6):\n            print('train iteration {}'.format(update_iteration))\n            if fw == 'tf':\n                in_ = self._get_batch_helper(obs_size, actions, batch_size)\n                tf_inputs.append(in_)\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                updated_weights = policy.get_weights()\n                if tf_updated_weights:\n                    check(updated_weights['default_policy/actor_hidden_0/kernel'], tf_updated_weights[-1]['default_policy/actor_hidden_0/kernel'], false=True)\n                tf_updated_weights.append(updated_weights)\n            else:\n                tf_weights = tf_updated_weights[update_iteration]\n                in_ = tf_inputs[update_iteration]\n                buf = algo.local_replay_buffer\n                patch_buffer_with_fake_sampling_method(buf, in_)\n                algo.train()\n                for tf_key in tf_weights.keys():\n                    tf_var = tf_weights[tf_key]\n                    if re.search('actor_out_1|actor_hidden_0_1|sequential_[23]', tf_key):\n                        torch_var = policy.target_model.state_dict()[map_[tf_key]]\n                    else:\n                        torch_var = policy.model.state_dict()[map_[tf_key]]\n                    if tf_var.shape != torch_var.shape:\n                        check(tf_var, np.transpose(torch_var.cpu()), atol=0.1)\n                    else:\n                        check(tf_var, torch_var, atol=0.1)\n        algo.stop()"
        ]
    },
    {
        "func_name": "_get_batch_helper",
        "original": "def _get_batch_helper(self, obs_size, actions, batch_size):\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})",
        "mutated": [
            "def _get_batch_helper(self, obs_size, actions, batch_size):\n    if False:\n        i = 10\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})",
            "def _get_batch_helper(self, obs_size, actions, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})",
            "def _get_batch_helper(self, obs_size, actions, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})",
            "def _get_batch_helper(self, obs_size, actions, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})",
            "def _get_batch_helper(self, obs_size, actions, batch_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return SampleBatch({SampleBatch.CUR_OBS: np.random.random(size=obs_size), SampleBatch.ACTIONS: actions, SampleBatch.REWARDS: np.random.random(size=(batch_size,)), SampleBatch.TERMINATEDS: np.random.choice([True, False], size=(batch_size,)), SampleBatch.NEXT_OBS: np.random.random(size=obs_size), 'weights': np.ones(shape=(batch_size,))})"
        ]
    },
    {
        "func_name": "_ddpg_loss_helper",
        "original": "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    \"\"\"Emulates DDPG loss functions for tf and torch.\"\"\"\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)",
        "mutated": [
            "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    if False:\n        i = 10\n    'Emulates DDPG loss functions for tf and torch.'\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)",
            "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Emulates DDPG loss functions for tf and torch.'\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)",
            "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Emulates DDPG loss functions for tf and torch.'\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)",
            "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Emulates DDPG loss functions for tf and torch.'\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)",
            "def _ddpg_loss_helper(self, train_batch, weights, ks, fw, gamma, huber_threshold, l2_reg, sess):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Emulates DDPG loss functions for tf and torch.'\n    model_out_t = train_batch[SampleBatch.CUR_OBS]\n    target_model_out_tp1 = train_batch[SampleBatch.NEXT_OBS]\n    policy_t = sigmoid(2.0 * fc(relu(fc(model_out_t, weights[ks[1]], weights[ks[0]], framework=fw)), weights[ks[5]], weights[ks[4]], framework=fw))\n    policy_tp1 = sigmoid(2.0 * fc(relu(fc(target_model_out_tp1, weights[ks[3]], weights[ks[2]], framework=fw)), weights[ks[7]], weights[ks[6]], framework=fw))\n    policy_tp1_smoothed = policy_tp1\n    q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    twin_q_t = fc(relu(fc(np.concatenate([model_out_t, train_batch[SampleBatch.ACTIONS]], -1), weights[ks[13]], weights[ks[12]], framework=fw)), weights[ks[15]], weights[ks[14]], framework=fw)\n    q_t_det_policy = fc(relu(fc(np.concatenate([model_out_t, policy_t], -1), weights[ks[9]], weights[ks[8]], framework=fw)), weights[ks[11]], weights[ks[10]], framework=fw)\n    q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[17]], weights[ks[16]], framework=fw)), weights[ks[19]], weights[ks[18]], framework=fw)\n    twin_q_tp1 = fc(relu(fc(np.concatenate([target_model_out_tp1, policy_tp1_smoothed], -1), weights[ks[21]], weights[ks[20]], framework=fw)), weights[ks[23]], weights[ks[22]], framework=fw)\n    q_t_selected = np.squeeze(q_t, axis=-1)\n    twin_q_t_selected = np.squeeze(twin_q_t, axis=-1)\n    q_tp1 = np.minimum(q_tp1, twin_q_tp1)\n    q_tp1_best = np.squeeze(q_tp1, axis=-1)\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    if fw == 'torch':\n        dones = dones.float().numpy()\n        rewards = rewards.numpy()\n    q_tp1_best_masked = (1.0 - dones) * q_tp1_best\n    q_t_selected_target = rewards + gamma * q_tp1_best_masked\n    td_error = q_t_selected - q_t_selected_target\n    twin_td_error = twin_q_t_selected - q_t_selected_target\n    errors = huber_loss(td_error, huber_threshold) + huber_loss(twin_td_error, huber_threshold)\n    critic_loss = np.mean(errors)\n    actor_loss = -np.mean(q_t_det_policy)\n    for (name, var) in weights.items():\n        if re.match('default_policy/actor_(hidden_0|out)/kernel', name):\n            actor_loss += l2_reg * l2_loss(var)\n        elif re.match('default_policy/sequential(_1)?/\\\\w+/kernel', name):\n            critic_loss += l2_reg * l2_loss(var)\n    return (critic_loss, actor_loss, td_error)"
        ]
    },
    {
        "func_name": "_translate_weights_to_torch",
        "original": "def _translate_weights_to_torch(self, weights_dict, map_):\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict",
        "mutated": [
            "def _translate_weights_to_torch(self, weights_dict, map_):\n    if False:\n        i = 10\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict",
            "def _translate_weights_to_torch(self, weights_dict, map_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict",
            "def _translate_weights_to_torch(self, weights_dict, map_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict",
            "def _translate_weights_to_torch(self, weights_dict, map_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict",
            "def _translate_weights_to_torch(self, weights_dict, map_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_dict = {map_[k]: convert_to_torch_tensor(np.transpose(v) if re.search('kernel', k) else v) for (k, v) in weights_dict.items() if re.search('default_policy/(actor_(hidden_0|out)|sequential(_1)?)/', k)}\n    model_dict['policy_model.action_out_squashed.low_action'] = convert_to_torch_tensor(np.array([0.0]))\n    model_dict['policy_model.action_out_squashed.action_range'] = convert_to_torch_tensor(np.array([1.0]))\n    return model_dict"
        ]
    }
]