[
    {
        "func_name": "preprocess_function",
        "original": "def preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs",
        "mutated": [
            "def preprocess_function(examples):\n    if False:\n        i = 10\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs",
            "def preprocess_function(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs",
            "def preprocess_function(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs",
            "def preprocess_function(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs",
            "def preprocess_function(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n    labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n    labels = labels['input_ids']\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs['labels'] = labels\n    return model_inputs"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    accelerator = Accelerator()\n    model_name_or_path = 't5-base'\n    batch_size = 8\n    text_column = 'sentence'\n    label_column = 'label'\n    max_length = 64\n    lr = 0.001\n    num_epochs = 1\n    base_path = 'temp/data/FinancialPhraseBank-v1.0'\n    peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n    model = get_peft_model(model, peft_config)\n    accelerator.print(model.print_trainable_parameters())\n    dataset = load_dataset('json', data_files={'train': os.path.join(base_path, 'financial_phrase_bank_train.jsonl'), 'validation': os.path.join(base_path, 'financial_phrase_bank_val.jsonl')})\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    def preprocess_function(examples):\n        inputs = examples[text_column]\n        targets = examples[label_column]\n        model_inputs = tokenizer(inputs, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n        labels = tokenizer(targets, max_length=2, padding='max_length', truncation=True, return_tensors='pt')\n        labels = labels['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        model_inputs['labels'] = labels\n        return model_inputs\n    with accelerator.main_process_first():\n        processed_datasets = dataset.map(preprocess_function, batched=True, num_proc=1, remove_columns=dataset['train'].column_names, load_from_cache_file=False, desc='Running tokenizer on dataset')\n    train_dataset = processed_datasets['train']\n    eval_dataset = processed_datasets['validation']\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n    if getattr(accelerator.state, 'fsdp_plugin', None) is not None:\n        accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n    (model, train_dataloader, eval_dataloader, optimizer, lr_scheduler) = accelerator.prepare(model, train_dataloader, eval_dataloader, optimizer, lr_scheduler)\n    accelerator.print(model)\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for (step, batch) in enumerate(tqdm(train_dataloader)):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for (step, batch) in enumerate(tqdm(eval_dataloader)):\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        accelerator.print(f'epoch={epoch!r}: train_ppl={train_ppl!r} train_epoch_loss={train_epoch_loss!r} eval_ppl={eval_ppl!r} eval_epoch_loss={eval_epoch_loss!r}')\n        correct = 0\n        total = 0\n        for (pred, true) in zip(eval_preds, dataset['validation'][label_column]):\n            if pred.strip() == true.strip():\n                correct += 1\n            total += 1\n        accuracy = correct / total * 100\n        accelerator.print(f'accuracy={accuracy!r}')\n        accelerator.print(f'eval_preds[:10]={eval_preds[:10]!r}')\n        accelerator.print(f\"dataset['validation'][label_column][:10]={dataset['validation'][label_column][:10]!r}\")\n        accelerator.wait_for_everyone()\n        model.push_to_hub('smangrul/' + f'{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}'.replace('/', '_'), state_dict=accelerator.get_state_dict(model), use_auth_token=True)\n        accelerator.wait_for_everyone()"
        ]
    }
]