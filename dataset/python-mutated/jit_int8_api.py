def PytorchJITINT8Model(model, calib_data, q_config=None, input_sample=None, channels_last=False, thread_num=None, jit_strict=True, jit_method=None, enable_onednn=False, example_kwarg_inputs=None):
    if False:
        while True:
            i = 10
    '\n    :param model: the model(nn.module) to be transform if from_load is False\n           the accelerated model if from_load is True.\n    :param calib_data: calibration data is required for static quantization.\n    :param q_config: We support 2 types of input here:\n\n           | 1. Qconfig (https://pytorch.org/docs/stable/generated/torch.quantization.\n           | qconfig.QConfig.html#qconfig) is the configuration for how we insert\n           | observers for a particular operator. Quantization preparation function\n           | will instantiate observers multiple times for each of the layers.\n           |\n           | 2. QConfigMapping (https://pytorch.org/docs/stable/generated/torch.ao.\n           | quantization.qconfig_mapping.QConfigMapping.html#qconfigmapping)\n           | (recommended) is a collection of quantization configurations, user\n           | can set the qconfig for each operator (torch op calls, functional\n           | calls, module calls) in the model through qconfig_mapping.\n\n    :param input_sample: torch tensor indicate the data sample to be used\n           for tracing.\n    :param channels_last: if set model and data to be channels-last mode.\n    :param thread_num: the thread num allocated for this model.\n    :param from_load: this will only be set by _load method.\n    :param jit_strict: Whether recording your mutable container types.\n    :param jit_method: use ``jit.trace`` or ``jit.script`` to convert a model\n           to TorchScript.\n    :param enable_onednn: Whether to use PyTorch JIT graph fuser based on\n           oneDNN Graph API, which provides a flexible API for aggressive\n           fusion. Default to ``False``.\n    :param example_kwarg_inputs: keyword arguments of example inputs that will be passed\n           to ``torch.jit.trace``. Default to None. Either this argument or input_sample\n           should be specified when use_jit is ``True`` and torch > 2.0,\n           sotherwise will be ignored.\n    '
    from .jit_int8_model import PytorchJITINT8Model
    return PytorchJITINT8Model(model, calib_data, q_config=q_config, input_sample=input_sample, channels_last=channels_last, thread_num=thread_num, jit_strict=jit_strict, jit_method=jit_method, enable_onednn=enable_onednn, example_kwarg_inputs=example_kwarg_inputs)

def load_pytorchjitint8_model(path):
    if False:
        print('Hello World!')
    from .jit_int8_model import PytorchJITINT8Model
    return PytorchJITINT8Model._load(path)