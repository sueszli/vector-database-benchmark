[
    {
        "func_name": "getattr_recursive",
        "original": "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj",
        "mutated": [
            "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    if False:\n        i = 10\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj",
            "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj",
            "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj",
            "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj",
            "@compatibility(is_backward_compatible=False)\ndef getattr_recursive(obj, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for layer in name.split('.'):\n        if hasattr(obj, layer):\n            obj = getattr(obj, layer)\n        else:\n            return None\n    return obj"
        ]
    },
    {
        "func_name": "setattr_recursive",
        "original": "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)",
        "mutated": [
            "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if False:\n        i = 10\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)",
            "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)",
            "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)",
            "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)",
            "@compatibility(is_backward_compatible=False)\ndef setattr_recursive(obj, attr, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if '.' not in attr:\n        setattr(obj, attr, value)\n    else:\n        layer = attr.split('.')\n        setattr_recursive(getattr(obj, layer[0]), '.'.join(layer[1:]), value)"
        ]
    },
    {
        "func_name": "flatten",
        "original": "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r",
        "mutated": [
            "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    if False:\n        i = 10\n    '\\n        Stores nodes in x to a list and returns the list.\\n        '\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r",
            "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Stores nodes in x to a list and returns the list.\\n        '\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r",
            "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Stores nodes in x to a list and returns the list.\\n        '\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r",
            "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Stores nodes in x to a list and returns the list.\\n        '\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r",
            "def flatten(x: torch.fx.node.Argument) -> NodeList:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Stores nodes in x to a list and returns the list.\\n        '\n    r: NodeList = []\n    map_arg(x, r.append)\n    return r"
        ]
    },
    {
        "func_name": "remap_func",
        "original": "def remap_func(x):\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]",
        "mutated": [
            "def remap_func(x):\n    if False:\n        i = 10\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]",
            "def remap_func(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]",
            "def remap_func(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]",
            "def remap_func(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]",
            "def remap_func(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if x.op == 'get_attr':\n        if x not in comp.getattr_maps:\n            comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n        return comp.getattr_maps[x]\n    if x.op != 'placeholder' and node_to_component[x] == comp:\n        return node_remapping[x]\n    if x not in comp.orig_inputs:\n        comp.orig_inputs.append(x)\n        placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n        placeholder.meta = copy.copy(x.meta)\n        comp.input_placeholders.append(placeholder)\n        used_in_main[x] = None\n    return comp.input_placeholders[comp.orig_inputs.index(x)]"
        ]
    },
    {
        "func_name": "split_by_tags",
        "original": "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    \"\"\"\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\n    the initial submodules in the order of \"a\", \"b\", \"c\".\n\n    To set a tag:\n    gm.graph.nodes[idx].tag = \"mytag\"\n\n    This will result in all nodes with the same tag being extracted and placed in their\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\n    and output nodes are created when needed while get_attr nodes get copied to submodules\n    where they are used.\n\n    Given the following module def:\n\n    class SimpleModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear1 = torch.nn.Linear(...)\n            self.linear2 = torch.nn.Linear(...)\n            self.linear3 = torch.nn.Linear(...)\n\n        def forward(self, in1, in2):\n            r1 = self.linear1(in1)\n            r2 = self.linear2(in2)\n            r3 = torch.cat([r1, r2])\n            return self.linear3(r3)\n\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\n\n    ro:\n    def forward(self, in1):\n        self = self.root\n        linear1 = self.linear1(in1)\n        return linear1\n\n    main:\n    def forward(self, in2, linear1):\n        self = self.root\n        linear2 = self.linear2(in2)\n        cat_1 = torch.cat([linear1, linear2])\n        linear3 = self.linear3(cat_1)\n        return linear3\n\n    main:\n    def forward(self, in1, in2):\n        self = self.root\n        ro_0 = self.ro_0(in1)\n        main_1 = self.main_1(in2, ro_0)\n        return main_1\n\n    Returns:\n        split_gm: torch fx graph after split\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\n            after split for call_module and get_attr.\n    \"\"\"\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm",
        "mutated": [
            "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    if False:\n        i = 10\n    '\\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\\n    the initial submodules in the order of \"a\", \"b\", \"c\".\\n\\n    To set a tag:\\n    gm.graph.nodes[idx].tag = \"mytag\"\\n\\n    This will result in all nodes with the same tag being extracted and placed in their\\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\\n    and output nodes are created when needed while get_attr nodes get copied to submodules\\n    where they are used.\\n\\n    Given the following module def:\\n\\n    class SimpleModule(torch.nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.linear1 = torch.nn.Linear(...)\\n            self.linear2 = torch.nn.Linear(...)\\n            self.linear3 = torch.nn.Linear(...)\\n\\n        def forward(self, in1, in2):\\n            r1 = self.linear1(in1)\\n            r2 = self.linear2(in2)\\n            r3 = torch.cat([r1, r2])\\n            return self.linear3(r3)\\n\\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\\n\\n    ro:\\n    def forward(self, in1):\\n        self = self.root\\n        linear1 = self.linear1(in1)\\n        return linear1\\n\\n    main:\\n    def forward(self, in2, linear1):\\n        self = self.root\\n        linear2 = self.linear2(in2)\\n        cat_1 = torch.cat([linear1, linear2])\\n        linear3 = self.linear3(cat_1)\\n        return linear3\\n\\n    main:\\n    def forward(self, in1, in2):\\n        self = self.root\\n        ro_0 = self.ro_0(in1)\\n        main_1 = self.main_1(in2, ro_0)\\n        return main_1\\n\\n    Returns:\\n        split_gm: torch fx graph after split\\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\\n            after split for call_module and get_attr.\\n    '\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm",
            "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\\n    the initial submodules in the order of \"a\", \"b\", \"c\".\\n\\n    To set a tag:\\n    gm.graph.nodes[idx].tag = \"mytag\"\\n\\n    This will result in all nodes with the same tag being extracted and placed in their\\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\\n    and output nodes are created when needed while get_attr nodes get copied to submodules\\n    where they are used.\\n\\n    Given the following module def:\\n\\n    class SimpleModule(torch.nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.linear1 = torch.nn.Linear(...)\\n            self.linear2 = torch.nn.Linear(...)\\n            self.linear3 = torch.nn.Linear(...)\\n\\n        def forward(self, in1, in2):\\n            r1 = self.linear1(in1)\\n            r2 = self.linear2(in2)\\n            r3 = torch.cat([r1, r2])\\n            return self.linear3(r3)\\n\\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\\n\\n    ro:\\n    def forward(self, in1):\\n        self = self.root\\n        linear1 = self.linear1(in1)\\n        return linear1\\n\\n    main:\\n    def forward(self, in2, linear1):\\n        self = self.root\\n        linear2 = self.linear2(in2)\\n        cat_1 = torch.cat([linear1, linear2])\\n        linear3 = self.linear3(cat_1)\\n        return linear3\\n\\n    main:\\n    def forward(self, in1, in2):\\n        self = self.root\\n        ro_0 = self.ro_0(in1)\\n        main_1 = self.main_1(in2, ro_0)\\n        return main_1\\n\\n    Returns:\\n        split_gm: torch fx graph after split\\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\\n            after split for call_module and get_attr.\\n    '\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm",
            "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\\n    the initial submodules in the order of \"a\", \"b\", \"c\".\\n\\n    To set a tag:\\n    gm.graph.nodes[idx].tag = \"mytag\"\\n\\n    This will result in all nodes with the same tag being extracted and placed in their\\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\\n    and output nodes are created when needed while get_attr nodes get copied to submodules\\n    where they are used.\\n\\n    Given the following module def:\\n\\n    class SimpleModule(torch.nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.linear1 = torch.nn.Linear(...)\\n            self.linear2 = torch.nn.Linear(...)\\n            self.linear3 = torch.nn.Linear(...)\\n\\n        def forward(self, in1, in2):\\n            r1 = self.linear1(in1)\\n            r2 = self.linear2(in2)\\n            r3 = torch.cat([r1, r2])\\n            return self.linear3(r3)\\n\\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\\n\\n    ro:\\n    def forward(self, in1):\\n        self = self.root\\n        linear1 = self.linear1(in1)\\n        return linear1\\n\\n    main:\\n    def forward(self, in2, linear1):\\n        self = self.root\\n        linear2 = self.linear2(in2)\\n        cat_1 = torch.cat([linear1, linear2])\\n        linear3 = self.linear3(cat_1)\\n        return linear3\\n\\n    main:\\n    def forward(self, in1, in2):\\n        self = self.root\\n        ro_0 = self.ro_0(in1)\\n        main_1 = self.main_1(in2, ro_0)\\n        return main_1\\n\\n    Returns:\\n        split_gm: torch fx graph after split\\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\\n            after split for call_module and get_attr.\\n    '\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm",
            "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\\n    the initial submodules in the order of \"a\", \"b\", \"c\".\\n\\n    To set a tag:\\n    gm.graph.nodes[idx].tag = \"mytag\"\\n\\n    This will result in all nodes with the same tag being extracted and placed in their\\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\\n    and output nodes are created when needed while get_attr nodes get copied to submodules\\n    where they are used.\\n\\n    Given the following module def:\\n\\n    class SimpleModule(torch.nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.linear1 = torch.nn.Linear(...)\\n            self.linear2 = torch.nn.Linear(...)\\n            self.linear3 = torch.nn.Linear(...)\\n\\n        def forward(self, in1, in2):\\n            r1 = self.linear1(in1)\\n            r2 = self.linear2(in2)\\n            r3 = torch.cat([r1, r2])\\n            return self.linear3(r3)\\n\\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\\n\\n    ro:\\n    def forward(self, in1):\\n        self = self.root\\n        linear1 = self.linear1(in1)\\n        return linear1\\n\\n    main:\\n    def forward(self, in2, linear1):\\n        self = self.root\\n        linear2 = self.linear2(in2)\\n        cat_1 = torch.cat([linear1, linear2])\\n        linear3 = self.linear3(cat_1)\\n        return linear3\\n\\n    main:\\n    def forward(self, in1, in2):\\n        self = self.root\\n        ro_0 = self.ro_0(in1)\\n        main_1 = self.main_1(in2, ro_0)\\n        return main_1\\n\\n    Returns:\\n        split_gm: torch fx graph after split\\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\\n            after split for call_module and get_attr.\\n    '\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm",
            "@compatibility(is_backward_compatible=False)\ndef split_by_tags(gm: torch.fx.GraphModule, tags: List[str], return_fqn_mapping: bool=False, GraphModuleCls: Type[torch.fx.GraphModule]=torch.fx.GraphModule) -> Union[torch.fx.GraphModule, Tuple[torch.fx.GraphModule, Dict[str, str]]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Splits a GraphModule using tags on its graph nodes. We honor the order of\\n    tags. For example, we have tags = [\"a\", \"b\", \"c\"], the function will create\\n    the initial submodules in the order of \"a\", \"b\", \"c\".\\n\\n    To set a tag:\\n    gm.graph.nodes[idx].tag = \"mytag\"\\n\\n    This will result in all nodes with the same tag being extracted and placed in their\\n    own submodule. For placeholder, output and get_attr node, the tag is ignored. placeholder\\n    and output nodes are created when needed while get_attr nodes get copied to submodules\\n    where they are used.\\n\\n    Given the following module def:\\n\\n    class SimpleModule(torch.nn.Module):\\n        def __init__(self):\\n            super().__init__()\\n            self.linear1 = torch.nn.Linear(...)\\n            self.linear2 = torch.nn.Linear(...)\\n            self.linear3 = torch.nn.Linear(...)\\n\\n        def forward(self, in1, in2):\\n            r1 = self.linear1(in1)\\n            r2 = self.linear2(in2)\\n            r3 = torch.cat([r1, r2])\\n            return self.linear3(r3)\\n\\n    Marking the node corresponding to in1 with the tag sc.REQUEST_ONLY.lower() results in the following split:\\n\\n    ro:\\n    def forward(self, in1):\\n        self = self.root\\n        linear1 = self.linear1(in1)\\n        return linear1\\n\\n    main:\\n    def forward(self, in2, linear1):\\n        self = self.root\\n        linear2 = self.linear2(in2)\\n        cat_1 = torch.cat([linear1, linear2])\\n        linear3 = self.linear3(cat_1)\\n        return linear3\\n\\n    main:\\n    def forward(self, in1, in2):\\n        self = self.root\\n        ro_0 = self.ro_0(in1)\\n        main_1 = self.main_1(in2, ro_0)\\n        return main_1\\n\\n    Returns:\\n        split_gm: torch fx graph after split\\n        orig_to_split_fqn_mapping: a map between the original fqn and the fqn\\n            after split for call_module and get_attr.\\n    '\n\n    def flatten(x: torch.fx.node.Argument) -> NodeList:\n        \"\"\"\n        Stores nodes in x to a list and returns the list.\n        \"\"\"\n        r: NodeList = []\n        map_arg(x, r.append)\n        return r\n    node_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    node_to_component: Dict[torch.fx.Node, Component] = {}\n    tag_to_component: Dict[str, Component] = {}\n    all_components: List[Component] = []\n    used_in_main: Dict[torch.fx.Node, None] = {}\n    main_g = torch.fx.Graph()\n    main_remapping: Dict[torch.fx.Node, torch.fx.Node] = {}\n    output_node: Optional[torch.fx.Node] = None\n    for tag in tags:\n        comp = Component(torch.fx.Graph(), len(all_components), f'{tag}')\n        all_components.append(comp)\n        tag_to_component[tag] = comp\n    for node in gm.graph.nodes:\n        if node.op == 'output':\n            if output_node is not None:\n                raise RuntimeError('Multiple output nodes in graph!')\n            output_node = node\n            continue\n        if node.op == 'placeholder':\n            main_remapping[node] = main_g.placeholder(node.name, type_expr=node.type)\n            main_remapping[node].meta = copy.copy(node.meta)\n            continue\n        if node.op == 'get_attr':\n            continue\n        assert hasattr(node, 'tag')\n        upstream_components = [node_to_component[x] for x in flatten(node.args) + flatten(node.kwargs) if x.op not in {'placeholder', 'get_attr'}]\n        comp = tag_to_component[node.tag]\n        node_to_component[node] = comp\n        mx = max((c.order for c in upstream_components), default=0)\n        assert comp.order >= mx\n\n        def remap_func(x):\n            if x.op == 'get_attr':\n                if x not in comp.getattr_maps:\n                    comp.getattr_maps[x] = comp.graph.get_attr(x.target, type_expr=x.type)\n                return comp.getattr_maps[x]\n            if x.op != 'placeholder' and node_to_component[x] == comp:\n                return node_remapping[x]\n            if x not in comp.orig_inputs:\n                comp.orig_inputs.append(x)\n                placeholder = comp.graph.placeholder(x.name, type_expr=x.type)\n                placeholder.meta = copy.copy(x.meta)\n                comp.input_placeholders.append(placeholder)\n                used_in_main[x] = None\n            return comp.input_placeholders[comp.orig_inputs.index(x)]\n        n = comp.graph.node_copy(node, remap_func)\n        n.tag = node.tag\n        node_remapping[node] = n\n        node_to_component[n] = comp\n    if output_node is None:\n        raise RuntimeError('Graph had no output node!')\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            main_remapping[x] = main_g.get_attr(x.name, type_expr=x.type)\n        else:\n            used_in_main[x] = None\n    for n in used_in_main:\n        if n.op != 'placeholder':\n            node_to_component[n].orig_outputs.append(n)\n    orig_to_split_fqn_mapping: Dict[str, str] = {}\n    for comp in all_components:\n        outs = tuple(map(node_remapping.__getitem__, comp.orig_outputs))\n        comp.graph.output(outs[0] if len(outs) == 1 else outs)\n        (comp.gm, comp_orig_to_split_fqn_mapping) = lift_subgraph_as_module(gm, subgraph=comp.graph, comp_name=comp.name)\n        orig_to_split_fqn_mapping.update(comp_orig_to_split_fqn_mapping)\n        main_node = main_g.call_module(comp.name, args=tuple(map(main_remapping.__getitem__, comp.orig_inputs)), kwargs=None)\n        if len(outs) == 1:\n            main_remapping[comp.orig_outputs[0]] = main_node\n        else:\n            for (i, o) in enumerate(comp.orig_outputs):\n                main_remapping[o] = torch.fx.Proxy(main_node)[i].node\n    main_g.output(map_arg(output_node.args[0], main_remapping.__getitem__))\n    main_root = HolderModule({comp.name: comp.gm for comp in all_components})\n    for x in flatten(output_node.args[0]):\n        if x.op == 'get_attr':\n            setattr(main_root, x.name, getattr_recursive(gm, x.target))\n    result_gm = GraphModuleCls(main_root, main_g)\n    if return_fqn_mapping:\n        return (result_gm, orig_to_split_fqn_mapping)\n    return result_gm"
        ]
    }
]