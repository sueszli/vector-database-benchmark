[
    {
        "func_name": "expected_alignment_from_p_choose",
        "original": "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    \"\"\"\n    Calculating expected alignment for from stepwise probability\n\n    Reference:\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\n    https://arxiv.org/pdf/1704.00784.pdf\n\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\n    a_ij = p_ij q_ij\n\n    Parallel solution:\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\n\n    ============================================================\n    Expected input size\n    p_choose: bsz, tgt_len, src_len\n    \"\"\"\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha",
        "mutated": [
            "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    if False:\n        i = 10\n    '\\n    Calculating expected alignment for from stepwise probability\\n\\n    Reference:\\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\\n    https://arxiv.org/pdf/1704.00784.pdf\\n\\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\\n    a_ij = p_ij q_ij\\n\\n    Parallel solution:\\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\\n\\n    ============================================================\\n    Expected input size\\n    p_choose: bsz, tgt_len, src_len\\n    '\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha",
            "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculating expected alignment for from stepwise probability\\n\\n    Reference:\\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\\n    https://arxiv.org/pdf/1704.00784.pdf\\n\\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\\n    a_ij = p_ij q_ij\\n\\n    Parallel solution:\\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\\n\\n    ============================================================\\n    Expected input size\\n    p_choose: bsz, tgt_len, src_len\\n    '\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha",
            "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculating expected alignment for from stepwise probability\\n\\n    Reference:\\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\\n    https://arxiv.org/pdf/1704.00784.pdf\\n\\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\\n    a_ij = p_ij q_ij\\n\\n    Parallel solution:\\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\\n\\n    ============================================================\\n    Expected input size\\n    p_choose: bsz, tgt_len, src_len\\n    '\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha",
            "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculating expected alignment for from stepwise probability\\n\\n    Reference:\\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\\n    https://arxiv.org/pdf/1704.00784.pdf\\n\\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\\n    a_ij = p_ij q_ij\\n\\n    Parallel solution:\\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\\n\\n    ============================================================\\n    Expected input size\\n    p_choose: bsz, tgt_len, src_len\\n    '\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha",
            "def expected_alignment_from_p_choose(p_choose: Tensor, padding_mask: Optional[Tensor]=None, eps: float=1e-06):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculating expected alignment for from stepwise probability\\n\\n    Reference:\\n    Online and Linear-Time Attention by Enforcing Monotonic Alignments\\n    https://arxiv.org/pdf/1704.00784.pdf\\n\\n    q_ij = (1 \u2212 p_{ij\u22121})q_{ij\u22121} + a+{i\u22121j}\\n    a_ij = p_ij q_ij\\n\\n    Parallel solution:\\n    ai = p_i * cumprod(1 \u2212 pi) * cumsum(a_i / cumprod(1 \u2212 pi))\\n\\n    ============================================================\\n    Expected input size\\n    p_choose: bsz, tgt_len, src_len\\n    '\n    prob_check(p_choose)\n    (bsz, tgt_len, src_len) = p_choose.size()\n    dtype = p_choose.dtype\n    p_choose = p_choose.float()\n    if padding_mask is not None:\n        p_choose = p_choose.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if p_choose.is_cuda:\n        p_choose = p_choose.contiguous()\n        from alignment_train_cuda_binding import alignment_train_cuda as alignment_train\n    else:\n        from alignment_train_cpu_binding import alignment_train_cpu as alignment_train\n    alpha = p_choose.new_zeros([bsz, tgt_len, src_len])\n    alignment_train(p_choose, alpha, eps)\n    alpha = alpha.type(dtype)\n    prob_check(alpha)\n    return alpha"
        ]
    },
    {
        "func_name": "expected_soft_attention",
        "original": "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    \"\"\"\n    Function to compute expected soft attention for\n    monotonic infinite lookback attention from\n    expected alignment and soft energy.\n\n    Reference:\n    Monotonic Chunkwise Attention\n    https://arxiv.org/abs/1712.05382\n\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\n    https://arxiv.org/abs/1906.05218\n\n    alpha: bsz, tgt_len, src_len\n    soft_energy: bsz, tgt_len, src_len\n    padding_mask: bsz, src_len\n    left_padding: bool\n    \"\"\"\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta",
        "mutated": [
            "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    if False:\n        i = 10\n    '\\n    Function to compute expected soft attention for\\n    monotonic infinite lookback attention from\\n    expected alignment and soft energy.\\n\\n    Reference:\\n    Monotonic Chunkwise Attention\\n    https://arxiv.org/abs/1712.05382\\n\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    soft_energy: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta",
            "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Function to compute expected soft attention for\\n    monotonic infinite lookback attention from\\n    expected alignment and soft energy.\\n\\n    Reference:\\n    Monotonic Chunkwise Attention\\n    https://arxiv.org/abs/1712.05382\\n\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    soft_energy: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta",
            "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Function to compute expected soft attention for\\n    monotonic infinite lookback attention from\\n    expected alignment and soft energy.\\n\\n    Reference:\\n    Monotonic Chunkwise Attention\\n    https://arxiv.org/abs/1712.05382\\n\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    soft_energy: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta",
            "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Function to compute expected soft attention for\\n    monotonic infinite lookback attention from\\n    expected alignment and soft energy.\\n\\n    Reference:\\n    Monotonic Chunkwise Attention\\n    https://arxiv.org/abs/1712.05382\\n\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    soft_energy: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta",
            "def expected_soft_attention(alpha: Tensor, soft_energy: Tensor, padding_mask: Optional[Tensor]=None, chunk_size: Optional[int]=None, eps: float=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Function to compute expected soft attention for\\n    monotonic infinite lookback attention from\\n    expected alignment and soft energy.\\n\\n    Reference:\\n    Monotonic Chunkwise Attention\\n    https://arxiv.org/abs/1712.05382\\n\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    soft_energy: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    if padding_mask is not None:\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n        soft_energy = soft_energy.masked_fill(padding_mask.unsqueeze(1), -float('inf'))\n    prob_check(alpha)\n    dtype = alpha.dtype\n    alpha = alpha.float()\n    soft_energy = soft_energy.float()\n    soft_energy = soft_energy - soft_energy.max(dim=2, keepdim=True)[0]\n    exp_soft_energy = torch.exp(soft_energy) + eps\n    if chunk_size is not None:\n        beta = exp_soft_energy * moving_sum(alpha / (eps + moving_sum(exp_soft_energy, chunk_size, 1)), 1, chunk_size)\n    else:\n        inner_items = alpha / (eps + torch.cumsum(exp_soft_energy, dim=2))\n        beta = exp_soft_energy * torch.cumsum(inner_items.flip(dims=[2]), dim=2).flip(dims=[2])\n    if padding_mask is not None:\n        beta = beta.masked_fill(padding_mask.unsqueeze(1).to(torch.bool), 0.0)\n    beta = beta.type(dtype)\n    beta = beta.clamp(0, 1)\n    prob_check(beta)\n    return beta"
        ]
    },
    {
        "func_name": "mass_preservation",
        "original": "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    \"\"\"\n    Function to compute the mass perservation for alpha.\n    This means that the residual weights of alpha will be assigned\n    to the last token.\n\n    Reference:\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\n    https://arxiv.org/abs/1906.05218\n\n    alpha: bsz, tgt_len, src_len\n    padding_mask: bsz, src_len\n    left_padding: bool\n    \"\"\"\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha",
        "mutated": [
            "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    if False:\n        i = 10\n    '\\n    Function to compute the mass perservation for alpha.\\n    This means that the residual weights of alpha will be assigned\\n    to the last token.\\n\\n    Reference:\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha",
            "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Function to compute the mass perservation for alpha.\\n    This means that the residual weights of alpha will be assigned\\n    to the last token.\\n\\n    Reference:\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha",
            "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Function to compute the mass perservation for alpha.\\n    This means that the residual weights of alpha will be assigned\\n    to the last token.\\n\\n    Reference:\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha",
            "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Function to compute the mass perservation for alpha.\\n    This means that the residual weights of alpha will be assigned\\n    to the last token.\\n\\n    Reference:\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha",
            "def mass_preservation(alpha: Tensor, padding_mask: Optional[Tensor]=None, left_padding: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Function to compute the mass perservation for alpha.\\n    This means that the residual weights of alpha will be assigned\\n    to the last token.\\n\\n    Reference:\\n    Monotonic Infinite Lookback Attention for Simultaneous Machine Translation\\n    https://arxiv.org/abs/1906.05218\\n\\n    alpha: bsz, tgt_len, src_len\\n    padding_mask: bsz, src_len\\n    left_padding: bool\\n    '\n    prob_check(alpha)\n    if padding_mask is not None:\n        if not left_padding:\n            assert not padding_mask[:, 0].any(), 'Find padding on the beginning of the sequence.'\n        alpha = alpha.masked_fill(padding_mask.unsqueeze(1), 0.0)\n    if left_padding or padding_mask is None:\n        residuals = 1 - alpha[:, :, :-1].sum(dim=-1).clamp(0, 1)\n        alpha[:, :, -1] = residuals\n    else:\n        (_, tgt_len, src_len) = alpha.size()\n        residuals = 1 - alpha.sum(dim=-1, keepdim=True).clamp(0, 1)\n        src_lens = src_len - padding_mask.sum(dim=1, keepdim=True)\n        src_lens = src_lens.expand(-1, tgt_len).contiguous()\n        residuals += alpha.gather(2, src_lens.unsqueeze(2) - 1)\n        alpha = alpha.scatter(2, src_lens.unsqueeze(2) - 1, residuals)\n        prob_check(alpha)\n    return alpha"
        ]
    }
]