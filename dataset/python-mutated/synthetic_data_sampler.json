[
    {
        "func_name": "sample_contextual_data",
        "original": "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    \"\"\"Samples independent Gaussian data.\n\n  There is nothing to learn here as the rewards do not depend on the context.\n\n  Args:\n    num_contexts: Number of contexts to sample.\n    dim_context: Dimension of the contexts.\n    num_actions: Number of arms for the multi-armed bandit.\n    sigma: Standard deviation of the independent Gaussian samples.\n\n  Returns:\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\n  \"\"\"\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)",
        "mutated": [
            "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    if False:\n        i = 10\n    'Samples independent Gaussian data.\\n\\n  There is nothing to learn here as the rewards do not depend on the context.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the independent Gaussian samples.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\\n  '\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)",
            "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Samples independent Gaussian data.\\n\\n  There is nothing to learn here as the rewards do not depend on the context.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the independent Gaussian samples.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\\n  '\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)",
            "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Samples independent Gaussian data.\\n\\n  There is nothing to learn here as the rewards do not depend on the context.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the independent Gaussian samples.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\\n  '\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)",
            "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Samples independent Gaussian data.\\n\\n  There is nothing to learn here as the rewards do not depend on the context.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the independent Gaussian samples.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\\n  '\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)",
            "def sample_contextual_data(num_contexts, dim_context, num_actions, sigma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Samples independent Gaussian data.\\n\\n  There is nothing to learn here as the rewards do not depend on the context.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the independent Gaussian samples.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context + num_actions] numpy array with the data.\\n  '\n    size_data = [num_contexts, dim_context + num_actions]\n    return np.random.normal(scale=sigma, size=size_data)"
        ]
    },
    {
        "func_name": "sample_linear_data",
        "original": "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    \"\"\"Samples data from linearly parameterized arms.\n\n  The reward for context X and arm j is given by X^T beta_j, for some latent\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\n\n  Args:\n    num_contexts: Number of contexts to sample.\n    dim_context: Dimension of the contexts.\n    num_actions: Number of arms for the multi-armed bandit.\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\n\n  Returns:\n    data: A [n, d+k] numpy array with the data.\n    betas: Latent parameters that determine expected reward for each arm.\n    opt: (optimal_rewards, optimal_actions) for all contexts.\n  \"\"\"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
        "mutated": [
            "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    if False:\n        i = 10\n    \"Samples data from linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [n, d+k] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Samples data from linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [n, d+k] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Samples data from linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [n, d+k] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Samples data from linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [n, d+k] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_linear_data(num_contexts, dim_context, num_actions, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Samples data from linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [n, d+k] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))"
        ]
    },
    {
        "func_name": "sample_sparse_linear_data",
        "original": "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    \"\"\"Samples data from sparse linearly parameterized arms.\n\n  The reward for context X and arm j is given by X^T beta_j, for some latent\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\n  Only s components out of d are non-zero for each arm's beta.\n\n  Args:\n    num_contexts: Number of contexts to sample.\n    dim_context: Dimension of the contexts.\n    num_actions: Number of arms for the multi-armed bandit.\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\n\n  Returns:\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\n    betas: Latent parameters that determine expected reward for each arm.\n    opt: (optimal_rewards, optimal_actions) for all contexts.\n  \"\"\"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
        "mutated": [
            "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    if False:\n        i = 10\n    \"Samples data from sparse linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n  Only s components out of d are non-zero for each arm's beta.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Samples data from sparse linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n  Only s components out of d are non-zero for each arm's beta.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Samples data from sparse linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n  Only s components out of d are non-zero for each arm's beta.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Samples data from sparse linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n  Only s components out of d are non-zero for each arm's beta.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))",
            "def sample_sparse_linear_data(num_contexts, dim_context, num_actions, sparse_dim, sigma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Samples data from sparse linearly parameterized arms.\\n\\n  The reward for context X and arm j is given by X^T beta_j, for some latent\\n  set of parameters {beta_j : j = 1, ..., k}. The beta's are sampled uniformly\\n  at random, the contexts are Gaussian, and sigma-noise is added to the rewards.\\n  Only s components out of d are non-zero for each arm's beta.\\n\\n  Args:\\n    num_contexts: Number of contexts to sample.\\n    dim_context: Dimension of the contexts.\\n    num_actions: Number of arms for the multi-armed bandit.\\n    sparse_dim: Dimension of the latent subspace (sparsity pattern dimension).\\n    sigma: Standard deviation of the additive noise. Set to zero for no noise.\\n\\n  Returns:\\n    data: A [num_contexts, dim_context+num_actions] numpy array with the data.\\n    betas: Latent parameters that determine expected reward for each arm.\\n    opt: (optimal_rewards, optimal_actions) for all contexts.\\n  \"\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    sparse_pattern = flatten([[(j, i) for j in np.random.choice(range(dim_context), sparse_dim, replace=False)] for i in range(num_actions)])\n    betas = np.random.uniform(-1, 1, (dim_context, num_actions))\n    mask = np.zeros((dim_context, num_actions))\n    for elt in sparse_pattern:\n        mask[elt] = 1\n    betas = np.multiply(betas, mask)\n    betas /= np.linalg.norm(betas, axis=0)\n    contexts = np.random.normal(size=[num_contexts, dim_context])\n    rewards = np.dot(contexts, betas)\n    opt_actions = np.argmax(rewards, axis=1)\n    rewards += np.random.normal(scale=sigma, size=rewards.shape)\n    opt_rewards = np.array([rewards[i, act] for (i, act) in enumerate(opt_actions)])\n    return (np.hstack((contexts, rewards)), betas, (opt_rewards, opt_actions))"
        ]
    },
    {
        "func_name": "sample_wheel_bandit_data",
        "original": "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    \"\"\"Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\n\n  Args:\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\n    delta: Exploration parameter: high reward in one region if norm above delta.\n    mean_v: Mean reward for each action if context norm is below delta.\n    std_v: Gaussian reward std for each action if context norm is below delta.\n    mu_large: Mean reward for optimal action if context norm is above delta.\n    std_large: Reward std for optimal action if context norm is above delta.\n\n  Returns:\n    dataset: Sampled matrix with n rows: (context, action rewards).\n    opt_vals: Vector of expected optimal (reward, action) for each context.\n  \"\"\"\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))",
        "mutated": [
            "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    if False:\n        i = 10\n    'Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\\n\\n  Args:\\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\\n    delta: Exploration parameter: high reward in one region if norm above delta.\\n    mean_v: Mean reward for each action if context norm is below delta.\\n    std_v: Gaussian reward std for each action if context norm is below delta.\\n    mu_large: Mean reward for optimal action if context norm is above delta.\\n    std_large: Reward std for optimal action if context norm is above delta.\\n\\n  Returns:\\n    dataset: Sampled matrix with n rows: (context, action rewards).\\n    opt_vals: Vector of expected optimal (reward, action) for each context.\\n  '\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))",
            "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\\n\\n  Args:\\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\\n    delta: Exploration parameter: high reward in one region if norm above delta.\\n    mean_v: Mean reward for each action if context norm is below delta.\\n    std_v: Gaussian reward std for each action if context norm is below delta.\\n    mu_large: Mean reward for optimal action if context norm is above delta.\\n    std_large: Reward std for optimal action if context norm is above delta.\\n\\n  Returns:\\n    dataset: Sampled matrix with n rows: (context, action rewards).\\n    opt_vals: Vector of expected optimal (reward, action) for each context.\\n  '\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))",
            "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\\n\\n  Args:\\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\\n    delta: Exploration parameter: high reward in one region if norm above delta.\\n    mean_v: Mean reward for each action if context norm is below delta.\\n    std_v: Gaussian reward std for each action if context norm is below delta.\\n    mu_large: Mean reward for optimal action if context norm is above delta.\\n    std_large: Reward std for optimal action if context norm is above delta.\\n\\n  Returns:\\n    dataset: Sampled matrix with n rows: (context, action rewards).\\n    opt_vals: Vector of expected optimal (reward, action) for each context.\\n  '\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))",
            "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\\n\\n  Args:\\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\\n    delta: Exploration parameter: high reward in one region if norm above delta.\\n    mean_v: Mean reward for each action if context norm is below delta.\\n    std_v: Gaussian reward std for each action if context norm is below delta.\\n    mu_large: Mean reward for optimal action if context norm is above delta.\\n    std_large: Reward std for optimal action if context norm is above delta.\\n\\n  Returns:\\n    dataset: Sampled matrix with n rows: (context, action rewards).\\n    opt_vals: Vector of expected optimal (reward, action) for each context.\\n  '\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))",
            "def sample_wheel_bandit_data(num_contexts, delta, mean_v, std_v, mu_large, std_large):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Samples from Wheel bandit game (see https://arxiv.org/abs/1802.09127).\\n\\n  Args:\\n    num_contexts: Number of points to sample, i.e. (context, action rewards).\\n    delta: Exploration parameter: high reward in one region if norm above delta.\\n    mean_v: Mean reward for each action if context norm is below delta.\\n    std_v: Gaussian reward std for each action if context norm is below delta.\\n    mu_large: Mean reward for optimal action if context norm is above delta.\\n    std_large: Reward std for optimal action if context norm is above delta.\\n\\n  Returns:\\n    dataset: Sampled matrix with n rows: (context, action rewards).\\n    opt_vals: Vector of expected optimal (reward, action) for each context.\\n  '\n    context_dim = 2\n    num_actions = 5\n    data = []\n    rewards = []\n    opt_actions = []\n    opt_rewards = []\n    while len(data) < num_contexts:\n        raw_data = np.random.uniform(-1, 1, (int(num_contexts / 3), context_dim))\n        for i in range(raw_data.shape[0]):\n            if np.linalg.norm(raw_data[i, :]) <= 1:\n                data.append(raw_data[i, :])\n    contexts = np.stack(data)[:num_contexts, :]\n    for i in range(num_contexts):\n        r = [np.random.normal(mean_v[j], std_v[j]) for j in range(num_actions)]\n        if np.linalg.norm(contexts[i, :]) >= delta:\n            r_big = np.random.normal(mu_large, std_large)\n            if contexts[i, 0] > 0:\n                if contexts[i, 1] > 0:\n                    r[0] = r_big\n                    opt_actions.append(0)\n                else:\n                    r[1] = r_big\n                    opt_actions.append(1)\n            elif contexts[i, 1] > 0:\n                r[2] = r_big\n                opt_actions.append(2)\n            else:\n                r[3] = r_big\n                opt_actions.append(3)\n        else:\n            opt_actions.append(np.argmax(mean_v))\n        opt_rewards.append(r[opt_actions[-1]])\n        rewards.append(r)\n    rewards = np.stack(rewards)\n    opt_rewards = np.array(opt_rewards)\n    opt_actions = np.array(opt_actions)\n    return (np.hstack((contexts, rewards)), (opt_rewards, opt_actions))"
        ]
    }
]