[
    {
        "func_name": "set_vars",
        "original": "def set_vars(self):\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
        "mutated": [
            "def set_vars(self):\n    if False:\n        i = 10\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = False\n    self.check_export_model_accuracy = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False"
        ]
    },
    {
        "func_name": "test_qat",
        "original": "def test_qat(self):\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)",
        "mutated": [
            "def test_qat(self):\n    if False:\n        i = 10\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)",
            "def test_qat(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)",
            "def test_qat(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)",
            "def test_qat(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)",
            "def test_qat(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.set_vars()\n    imperative_qat = ImperativeQuantAware(weight_quantize_type=self.weight_quantize_type, activation_quantize_type=self.activation_quantize_type, fuse_conv_bn=self.fuse_conv_bn, onnx_format=self.onnx_format)\n    with base.dygraph.guard():\n        conv1 = Conv2D(in_channels=3, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='replicate')\n        quant_conv1 = QuantizedConv2D(conv1)\n        data = np.random.uniform(-1, 1, [10, 3, 32, 32]).astype('float32')\n        quant_conv1(paddle.to_tensor(data))\n        conv_transpose = Conv2DTranspose(4, 6, (3, 3))\n        quant_conv_transpose = QuantizedConv2DTranspose(conv_transpose)\n        x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1.0, max=1.0)\n        quant_conv_transpose(x_var)\n        seed = 1\n        np.random.seed(seed)\n        paddle.static.default_main_program().random_seed = seed\n        paddle.static.default_startup_program().random_seed = seed\n        lenet = ImperativeLenet()\n        lenet = fix_model_dict(lenet)\n        imperative_qat.quantize(lenet)\n        adam = Adam(learning_rate=0.001, parameters=lenet.parameters())\n        train_reader = paddle.batch(paddle.dataset.mnist.train(), batch_size=32, drop_last=True)\n        test_reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=32)\n        epoch_num = 1\n        for epoch in range(epoch_num):\n            lenet.train()\n            for (batch_id, data) in enumerate(train_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc = paddle.metric.accuracy(out, label)\n                loss = paddle.nn.functional.cross_entropy(out, label, reduction='none', use_softmax=False)\n                avg_loss = paddle.mean(loss)\n                avg_loss.backward()\n                adam.minimize(avg_loss)\n                lenet.clear_gradients()\n                if batch_id % 100 == 0:\n                    _logger.info('Train | At epoch {} step {}: loss = {:}, acc= {:}'.format(epoch, batch_id, avg_loss.numpy(), acc.numpy()))\n                if batch_id == 500:\n                    break\n            lenet.eval()\n            eval_acc_top1_list = []\n            for (batch_id, data) in enumerate(test_reader()):\n                x_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n                y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n                img = paddle.to_tensor(x_data)\n                label = paddle.to_tensor(y_data)\n                out = lenet(img)\n                acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)\n                acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)\n                if batch_id % 100 == 0:\n                    eval_acc_top1_list.append(float(acc_top1.numpy()))\n                    _logger.info('Test | At epoch {} step {}: acc1 = {:}, acc5 = {:}'.format(epoch, batch_id, acc_top1.numpy(), acc_top5.numpy()))\n            eval_acc_top1 = sum(eval_acc_top1_list) / len(eval_acc_top1_list)\n            print('eval_acc_top1', eval_acc_top1)\n            self.assertTrue(eval_acc_top1 > 0.9, msg='The test acc {%f} is less than 0.9.' % eval_acc_top1)\n        data = next(test_reader())\n        test_data = np.array([x[0].reshape(1, 28, 28) for x in data]).astype('float32')\n        y_data = np.array([x[1] for x in data]).astype('int64').reshape(-1, 1)\n        test_img = paddle.to_tensor(test_data)\n        label = paddle.to_tensor(y_data)\n        lenet.eval()\n        fp32_out = lenet(test_img)\n        fp32_acc = paddle.metric.accuracy(fp32_out, label).numpy()\n    with tempfile.TemporaryDirectory(prefix='qat_save_path_') as tmpdir:\n        imperative_qat.save_quantized_model(layer=lenet, path=os.path.join(tmpdir, 'lenet'), input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n        print('Quantized model saved in %s' % tmpdir)\n        if core.is_compiled_with_cuda():\n            place = core.CUDAPlace(0)\n        else:\n            place = core.CPUPlace()\n        exe = paddle.static.Executor(place)\n        [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(tmpdir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n        (quant_out,) = exe.run(inference_program, feed={feed_target_names[0]: test_data}, fetch_list=fetch_targets)\n        paddle.disable_static()\n        quant_out = paddle.to_tensor(quant_out)\n        quant_acc = paddle.metric.accuracy(quant_out, label).numpy()\n        paddle.enable_static()\n        delta_value = fp32_acc - quant_acc\n        self.assertLessEqual(delta_value, self.diff_threshold)"
        ]
    },
    {
        "func_name": "set_vars",
        "original": "def set_vars(self):\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
        "mutated": [
            "def set_vars(self):\n    if False:\n        i = 10\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False",
            "def set_vars(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.weight_quantize_type = 'abs_max'\n    self.activation_quantize_type = 'moving_average_abs_max'\n    self.onnx_format = True\n    self.diff_threshold = 0.03125\n    self.fuse_conv_bn = False"
        ]
    }
]