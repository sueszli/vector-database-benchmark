[
    {
        "func_name": "test_dpr_modules",
        "original": "def test_dpr_modules():\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001",
        "mutated": [
            "def test_dpr_modules():\n    if False:\n        i = 10\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001",
            "def test_dpr_modules():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001",
            "def test_dpr_modules():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001",
            "def test_dpr_modules():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001",
            "def test_dpr_modules():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-question_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='facebook/dpr-ctx_encoder-single-nq-base', do_lower_case=True, use_fast=True)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', data_dir='data/retriever', train_filename='nq-train.json', dev_filename='nq-dev.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1)\n    question_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRQuestionEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    passage_language_model = DPREncoder(pretrained_model_name_or_path='bert-base-uncased', model_type='DPRContextEncoder', model_kwargs={'hidden_dropout_prob': 0, 'attention_probs_dropout_prob': 0})\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    (devices, _) = initialize_device_settings(use_cuda=True)\n    model = BiAdaptiveModel(language_model1=question_language_model, language_model2=passage_language_model, prediction_heads=[prediction_head], embeds_dropout_prob=0.0, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=devices[0])\n    model.connect_heads_with_processor(processor.tasks)\n    assert type(model) == BiAdaptiveModel\n    assert type(processor) == TextSimilarityProcessor\n    assert type(question_language_model) == DPREncoder\n    assert type(passage_language_model) == DPREncoder\n    assert list(model.named_parameters())[0][1][0, 0].item() - -0.010200000368058681 < 0.0001\n    d = {'query': 'big little lies season 2 how many episodes', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    features = {key: val.unsqueeze(0).to(devices[0]) for (key, val) in zip(tensor_names, dataset[0])}\n    assert torch.all(torch.eq(features['query_input_ids'][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 2161, 1016, 2129, 2116, 4178, 102])))\n    assert torch.all(torch.eq(features['passage_input_ids'][0][0][:10].cpu(), torch.tensor([101, 2502, 2210, 3658, 1006, 2694, 2186, 1007, 102, 2186])))\n    assert len(features['query_segment_ids'][0].nonzero()) == 0\n    assert len(features['passage_segment_ids'][0].nonzero()) == 0\n    assert torch.all(torch.eq(features['query_attention_mask'].nonzero()[:, 1].cpu(), torch.tensor(list(range(10)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][0].nonzero().cpu().squeeze(), torch.tensor(list(range(127)))))\n    assert torch.all(torch.eq(features['passage_attention_mask'][0][1].nonzero().cpu().squeeze(), torch.tensor(list(range(143)))))\n    features_query = {key.replace('query_', ''): value for (key, value) in features.items() if key.startswith('query_')}\n    features_passage = {key.replace('passage_', ''): value for (key, value) in features.items() if key.startswith('passage_')}\n    max_seq_len = features_passage.get('input_ids').shape[-1]\n    features_passage = {key: value.view(-1, max_seq_len) for (key, value) in features_passage.items()}\n    query_vector = model.language_model1(**features_query)[0]\n    passage_vector = model.language_model2(**features_passage)[0]\n    assert torch.all(torch.le(query_vector[0, :10].cpu() - torch.tensor([-0.2135, -0.4748, 0.0501, -0.043, -0.1747, -0.0441, 0.5638, 0.1405, 0.2285, 0.0893]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[0, :10].cpu() - torch.tensor([0.0557, -0.6836, -0.3645, -0.5566, 0.2034, -0.3656, 0.2969, -0.0555, 0.3405, -0.8691]), torch.ones((1, 10)) * 0.0001))\n    assert torch.all(torch.le(passage_vector[1, :10].cpu() - torch.tensor([-0.2006, -1.5002, -0.1897, -0.3421, -0.0405, -0.0471, -0.0306, 0.1156, 0.335, -0.3412]), torch.ones((1, 10)) * 0.0001))\n    embeddings = model(query_input_ids=features.get('query_input_ids', None), query_segment_ids=features.get('query_segment_ids', None), query_attention_mask=features.get('query_attention_mask', None), passage_input_ids=features.get('passage_input_ids', None), passage_segment_ids=features.get('passage_segment_ids', None), passage_attention_mask=features.get('passage_attention_mask', None))\n    (query_emb, passage_emb) = embeddings[0]\n    assert torch.all(torch.eq(query_emb.cpu(), query_vector.cpu()))\n    assert torch.all(torch.eq(passage_emb.cpu(), passage_vector.cpu()))\n    loss = model.logits_to_loss_per_head(embeddings, **features)\n    similarity_scores = model.prediction_heads[0]._embeddings_to_scores(query_emb, passage_emb).cpu()\n    assert torch.all(torch.le(similarity_scores - torch.tensor([[-0.0018311, -6.3016]]), torch.ones((1, 2)) * 0.0001))\n    assert loss[0].item() - 0.0018 <= 0.0001"
        ]
    },
    {
        "func_name": "test_dpr_processor",
        "original": "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0",
        "mutated": [
            "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    if False:\n        i = 10\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0",
            "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0",
            "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0",
            "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0",
            "@pytest.mark.parametrize('embed_title, passage_ids, passage_attns', [(True, passage_ids['titled'], passage_attention['titled']), (False, passage_ids['untitled'], passage_attention['untitled'])])\n@pytest.mark.parametrize('use_fast', [True, False])\n@pytest.mark.parametrize('num_hard_negatives, labels', [(1, labels1), (2, labels2)])\ndef test_dpr_processor(embed_title, passage_ids, passage_attns, use_fast, num_hard_negatives, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dict = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}, {'title': '', 'text': 'Director Radio Ia\u0219i); Drago\u0219-Liviu V\u00eelceanu; Mihnea-Adrian V\u00eelceanu; Nathalie-Teona', 'label': 'positive', 'external_id': 'b21eaeff-e08b-4548-b5e0-a280f6f4efef'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok, use_fast=use_fast)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok, use_fast=use_fast)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=embed_title, num_hard_negatives=num_hard_negatives, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    for (i, d) in enumerate(dict):\n        (__, ___, _, baskets) = processor.dataset_from_dicts(dicts=[d], return_baskets=True)\n        feat = baskets[0].samples[0].features\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_input_ids'][:10]), query_input_ids[i]))\n        assert len(torch.tensor(feat[0]['query_segment_ids']).nonzero()) == 0\n        assert torch.all(torch.eq(torch.tensor(feat[0]['query_attention_mask']).nonzero(), query_attention_mask[i]))\n        positive_indices = np.where(np.array(feat[0]['label_ids']) == 1)[0].item()\n        assert torch.all(torch.eq(torch.tensor(feat[0]['passage_input_ids'])[positive_indices, :10], passage_ids[i][positive_indices]))\n        for j in range(num_hard_negatives + 1):\n            assert torch.all(torch.eq(torch.tensor(feat[0]['passage_attention_mask'][j]).nonzero(), passage_attns[i][j]))\n        assert torch.all(torch.eq(torch.tensor(feat[0]['label_ids']), torch.tensor(labels[i])[:num_hard_negatives + 1]))\n        assert len(torch.tensor(feat[0]['passage_segment_ids']).nonzero()) == 0"
        ]
    },
    {
        "func_name": "test_dpr_problematic",
        "original": "def test_dpr_problematic():\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}",
        "mutated": [
            "def test_dpr_problematic():\n    if False:\n        i = 10\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}",
            "def test_dpr_problematic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}",
            "def test_dpr_problematic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}",
            "def test_dpr_problematic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}",
            "def test_dpr_problematic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    erroneous_dicts = [{'query': [1], 'answers': ['Framlingham Castle'], 'passages': [{'text': 'Castle on the Hill \"Castle on the Hill\" is a song by English singer-songwriter Ed Sheeran. It was released as a digital download on 6 January 2017 as one of the double lead singles from his third studio album \"\u00f7\" (2017), along with \"Shape of You\". \"Castle on the Hill\" was written and produced by Ed Sheeran and Benny Blanco. The song refers to Framlingham Castle in Sheeran\\'s home town. Released on the same day as \"Shape of You\", \"Castle on the Hill\" reached number two in a number of countries, including the UK, Australia and Germany, while \"Shape of', 'title': 'Castle on the Hill', 'label': 'positive', 'external_id': '19930582'}, {'text': 'crops so as to feed a struggling infant colony. Governor King began Government Farm 3 there on 8 July 1801, referring to it as \"Castle Hill\" on 1 March 1802. The majority of the convicts who worked the prison farm were Irish Catholics, many having been transported for seditious activity in 1798. The most notorious incident being the Battle of Vinegar Hill where around 39 were slaughtered. They were branded \"politicals\" and exiled for life, never to return. The first free settler in Castle Hill, a Frenchman Baron Verincourt de Clambe, in unusual circumstances received a grant of 200 acres', 'title': 'Castle Hill, New South Wales', 'label': 'hard_negative', 'external_id': '1977568'}, {'text': 'Tom Gleeson, proposed \"\"high on the peak of Castle Hill, overlooking the harbour\"\" would be a suitable location for the monument. Having arrived in Townsville, the monument was then placed in storage for a number of years. It was not until October 1947 that the Council discussed where to place the monument. A number of locations were considered: Castle Hill, the Botanic Gardens, in front of the Queens Hotel, the Anzac Memorial Park and the Railway Oval, but Castle Hill was ultimately the council\\'s choice. In February 1948, the Queensland Government gave its approval to the council to place the', 'title': 'Castle Hill, Townsville', 'label': 'hard_negative', 'external_id': '3643705'}]}, {'query': 'when did the royal family adopt the name windsor', 'answers': ['in 1917'], 'passages': [{'text2': 'House of Windsor The House of Windsor is the reigning royal house of the United Kingdom and the other Commonwealth realms. The dynasty is of German paternal descent and was originally a branch of the House of Saxe-Coburg and Gotha, itself derived from the House of Wettin, which succeeded the House of Hanover to the British monarchy following the death of Queen Victoria, wife of Albert, Prince Consort. The name was changed from \"Saxe-Coburg and Gotha\" to the English \"Windsor\" (from \"Windsor Castle\") in 1917 because of anti-German sentiment in the British Empire during World War I. There have been', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text2': \"2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious service of blessing at St George's Chapel. However, to conduct a civil marriage at Windsor Castle would oblige the venue to obtain a licence for civil marriages, which it did not have. A condition of such a licence is that the licensed venue must be available for a period of one year to anyone wishing to be married there, and as the royal family did not wish to make Windsor Castle available to the public for civil marriages, even just for one year,\", 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'query': 'what is a cat?', 'answers': ['animal', 'feline'], 'passages': [{'text': 'This is a <mask> sentence. Cats are good pets.', 'title': 'title with \"special characters\" ', 'label': 'positive', 'external_id': '0'}, {'text': '2nd text => More text about cats is good', 'title': '2nd title \\n', 'label': 'positive', 'external_id': '1'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, __, problematic_ids, ___) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert problematic_ids == {0, 1}"
        ]
    },
    {
        "func_name": "test_dpr_query_only",
        "original": "def test_dpr_query_only():\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']",
        "mutated": [
            "def test_dpr_query_only():\n    if False:\n        i = 10\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']",
            "def test_dpr_query_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']",
            "def test_dpr_query_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']",
            "def test_dpr_query_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']",
            "def test_dpr_query_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    erroneous_dicts = [{'query': 'where is castle on the hill based on', 'answers': ['Framlingham Castle']}, {'query': 'where is castle on the hill 2 based on', 'answers': ['Framlingham Castle 2']}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['query_input_ids', 'query_segment_ids', 'query_attention_mask']"
        ]
    },
    {
        "func_name": "test_dpr_context_only",
        "original": "def test_dpr_context_only():\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']",
        "mutated": [
            "def test_dpr_context_only():\n    if False:\n        i = 10\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']",
            "def test_dpr_context_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']",
            "def test_dpr_context_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']",
            "def test_dpr_context_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']",
            "def test_dpr_context_only():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    erroneous_dicts = [{'passages': [{'text': 'House of Windsor 2 The House of Windsor is the reigning royal house of the United', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent religious', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}, {'passages': [{'text': 'House of Windsor The House of Windsor is the reigning royal house of the', 'title': 'House of Windsor', 'label': 'positive', 'external_id': '1478954'}, {'text': '2005, and was to take place in a civil ceremony at Windsor Castle, with a subsequent', 'title': 'Camilla, Duchess of Cornwall', 'label': 'hard_negative', 'external_id': '1399730'}]}]\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    (_, tensor_names, problematic_ids, __) = processor.dataset_from_dicts(dicts=erroneous_dicts, return_baskets=True)\n    assert len(problematic_ids) == 0\n    assert tensor_names == ['passage_input_ids', 'passage_segment_ids', 'passage_attention_mask', 'label_ids']"
        ]
    },
    {
        "func_name": "test_dpr_processor_save_load",
        "original": "def test_dpr_processor_save_load(tmp_path):\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])",
        "mutated": [
            "def test_dpr_processor_save_load(tmp_path):\n    if False:\n        i = 10\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])",
            "def test_dpr_processor_save_load(tmp_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])",
            "def test_dpr_processor_save_load(tmp_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])",
            "def test_dpr_processor_save_load(tmp_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])",
            "def test_dpr_processor_save_load(tmp_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    d = {'query': 'big little lies season 2 how many episodes ?', 'passages': [{'title': 'Big Little Lies (TV series)', 'text': 'series garnered several accolades. It received 16 Emmy Award nominations and won eight, including Outstanding Limited Series and acting awards for Kidman, Skarsg\u00e5rd, and Dern. The trio also won Golden Globe Awards in addition to a Golden Globe Award for Best Miniseries or Television Film win for the series. Kidman and Skarsg\u00e5rd also received Screen Actors Guild Awards for their performances. Despite originally being billed as a miniseries, HBO renewed the series for a second season. Production on the second season began in March 2018 and is set to premiere in 2019. All seven episodes are being written by Kelley', 'label': 'positive', 'external_id': '18768923'}, {'title': 'Little People, Big World', 'text': 'final minutes of the season two-A finale, \"Farm Overload\". A crowd had gathered around Jacob, who was lying on the ground near the trebuchet. The first two episodes of season two-B focus on the accident, and how the local media reacted to it. The first season of \"Little People, Big World\" generated solid ratings for TLC (especially in the important 18\u201349 demographic), leading to the show\\'s renewal for a second season. Critical reviews of the series have been generally positive, citing the show\\'s positive portrayal of little people. Conversely, other reviews have claimed that the show has a voyeuristic bend', 'label': 'hard_negative', 'external_id': '7459116'}, {'title': 'Cormac McCarthy', 'text': 'chores of the house, Lee was asked by Cormac to also get a day job so he could focus on his novel writing. Dismayed with the situation, she moved to Wyoming, where she filed for divorce and landed her first job teaching. Cormac McCarthy is fluent in Spanish and lived in Ibiza, Spain, in the 1960s and later settled in El Paso, Texas, where he lived for nearly 20 years. In an interview with Richard B. Woodward from \"The New York Times\", \"McCarthy doesn\\'t drink anymore \u2013 he quit 16 years ago in El Paso, with one of his young', 'label': 'negative', 'passage_id': '2145653'}]}\n    query_tok = 'facebook/dpr-question_encoder-single-nq-base'\n    query_tokenizer = AutoTokenizer.from_pretrained(query_tok)\n    passage_tok = 'facebook/dpr-ctx_encoder-single-nq-base'\n    passage_tokenizer = AutoTokenizer.from_pretrained(passage_tok)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_query=256, max_seq_len_passage=256, data_dir='data/retriever', train_filename='nq-train.json', test_filename='nq-dev.json', embed_title=True, num_hard_negatives=1, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', shuffle_negatives=False)\n    save_dir = f'{tmp_path}/testsave/dpr_processor'\n    processor.save(save_dir=save_dir)\n    (dataset, __, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    loadedprocessor = TextSimilarityProcessor.load_from_dir(load_dir=save_dir)\n    (dataset2, __, _) = loadedprocessor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])"
        ]
    },
    {
        "func_name": "test_dpr_processor_save_load_non_bert_tokenizer",
        "original": "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    \"\"\"\n    This test compares 1) a model that was loaded from model hub with\n    2) a model from model hub that was saved to disk and then loaded from disk and\n    3) a model in FARM style that was saved to disk and then loaded from disk\n    \"\"\"\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])",
        "mutated": [
            "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    if False:\n        i = 10\n    '\\n    This test compares 1) a model that was loaded from model hub with\\n    2) a model from model hub that was saved to disk and then loaded from disk and\\n    3) a model in FARM style that was saved to disk and then loaded from disk\\n    '\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])",
            "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This test compares 1) a model that was loaded from model hub with\\n    2) a model from model hub that was saved to disk and then loaded from disk and\\n    3) a model in FARM style that was saved to disk and then loaded from disk\\n    '\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])",
            "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This test compares 1) a model that was loaded from model hub with\\n    2) a model from model hub that was saved to disk and then loaded from disk and\\n    3) a model in FARM style that was saved to disk and then loaded from disk\\n    '\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])",
            "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This test compares 1) a model that was loaded from model hub with\\n    2) a model from model hub that was saved to disk and then loaded from disk and\\n    3) a model in FARM style that was saved to disk and then loaded from disk\\n    '\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])",
            "@pytest.mark.parametrize('query_and_passage_model', [{'query': 'etalab-ia/dpr-question_encoder-fr_qa-camembert', 'passage': 'etalab-ia/dpr-ctx_encoder-fr_qa-camembert'}, {'query': 'deepset/gbert-base-germandpr-question_encoder', 'passage': 'deepset/gbert-base-germandpr-ctx_encoder'}, {'query': 'facebook/dpr-question_encoder-single-nq-base', 'passage': 'facebook/dpr-ctx_encoder-single-nq-base'}])\ndef test_dpr_processor_save_load_non_bert_tokenizer(tmp_path: Path, query_and_passage_model: Dict[str, str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This test compares 1) a model that was loaded from model hub with\\n    2) a model from model hub that was saved to disk and then loaded from disk and\\n    3) a model in FARM style that was saved to disk and then loaded from disk\\n    '\n    d = {'query': \"Comment s'appelle le portail open data du gouvernement?\", 'passages': [{'title': 'Etalab', 'text': \"Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes.\", 'label': 'positive', 'external_id': '1'}]}\n    query_embedding_model = query_and_passage_model['query']\n    passage_embedding_model = query_and_passage_model['passage']\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=query_embedding_model)\n    query_encoder = get_language_model(pretrained_model_name_or_path=query_embedding_model)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=passage_embedding_model)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=passage_embedding_model)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    loaded_query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir, use_fast=True)\n    loaded_query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    loaded_passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir, use_fast=True)\n    loaded_passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    loaded_processor = TextSimilarityProcessor(query_tokenizer=loaded_query_tokenizer, passage_tokenizer=loaded_passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    loaded_prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    loaded_model = BiAdaptiveModel(language_model1=loaded_query_encoder, language_model2=loaded_passage_encoder, prediction_heads=[loaded_prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    loaded_model.connect_heads_with_processor(loaded_processor.tasks, require_labels=False)\n    (dataset, tensor_names, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset.tensors[0], dataset2.tensors[0])\n    (dataset, tensor_names, _, __) = processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset, sampler=SequentialSampler(dataset), batch_size=16, tensor_names=tensor_names)\n    all_embeddings: Dict[str, Any] = {'query': [], 'passages': []}\n    model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings['passages']:\n        all_embeddings['passages'] = np.concatenate(all_embeddings['passages'])\n    if all_embeddings['query']:\n        all_embeddings['query'] = np.concatenate(all_embeddings['query'])\n    (dataset2, tensor_names2, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset2, sampler=SequentialSampler(dataset2), batch_size=16, tensor_names=tensor_names2)\n    all_embeddings2: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings2['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings2['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings2['passages']:\n        all_embeddings2['passages'] = np.concatenate(all_embeddings2['passages'])\n    if all_embeddings2['query']:\n        all_embeddings2['query'] = np.concatenate(all_embeddings2['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings2['query'][0])\n    save_dir = f'{tmp_path}/testsave/dpr_model'\n    query_encoder_dir = 'query_encoder'\n    passage_encoder_dir = 'passage_encoder'\n    loaded_model.save(Path(save_dir), lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n    loaded_query_tokenizer.save_pretrained(save_dir + f'/{query_encoder_dir}')\n    loaded_passage_tokenizer.save_pretrained(save_dir + f'/{passage_encoder_dir}')\n    query_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    query_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / query_encoder_dir)\n    passage_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    passage_encoder = get_language_model(pretrained_model_name_or_path=Path(save_dir) / passage_encoder_dir)\n    processor = TextSimilarityProcessor(query_tokenizer=query_tokenizer, passage_tokenizer=passage_tokenizer, max_seq_len_passage=256, max_seq_len_query=256, label_list=['hard_negative', 'positive'], metric='text_similarity_metric', embed_title=True, num_hard_negatives=0, num_positives=1)\n    prediction_head = TextSimilarityHead(similarity_function='dot_product')\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() and (os.getenv('HAYSTACK_MPS_ENABLED', 'true') != 'false'):\n        device = torch.device('mps')\n    else:\n        device = torch.device('cpu')\n    model = BiAdaptiveModel(language_model1=query_encoder, language_model2=passage_encoder, prediction_heads=[prediction_head], embeds_dropout_prob=0.1, lm1_output_types=['per_sequence'], lm2_output_types=['per_sequence'], device=device)\n    model.connect_heads_with_processor(processor.tasks, require_labels=False)\n    (dataset3, tensor_names3, _) = processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    (dataset2, tensor_names2, _) = loaded_processor.dataset_from_dicts(dicts=[d], return_baskets=False)\n    assert np.array_equal(dataset3.tensors[0], dataset2.tensors[0])\n    (dataset3, tensor_names3, _, __) = loaded_processor.dataset_from_dicts(dicts=[d], indices=list(range(len([d]))), return_baskets=True)\n    data_loader = NamedDataLoader(dataset=dataset3, sampler=SequentialSampler(dataset3), batch_size=16, tensor_names=tensor_names3)\n    all_embeddings3: Dict[str, Any] = {'query': [], 'passages': []}\n    loaded_model.eval()\n    for batch in tqdm(data_loader, desc='Creating Embeddings', unit=' Batches', disable=True):\n        batch = {key: batch[key].to(device) for key in batch}\n        with torch.inference_mode():\n            (query_embeddings, passage_embeddings) = loaded_model.forward(query_input_ids=batch.get('query_input_ids', None), query_segment_ids=batch.get('query_segment_ids', None), query_attention_mask=batch.get('query_attention_mask', None), passage_input_ids=batch.get('passage_input_ids', None), passage_segment_ids=batch.get('passage_segment_ids', None), passage_attention_mask=batch.get('passage_attention_mask', None))[0]\n            if query_embeddings is not None:\n                all_embeddings3['query'].append(query_embeddings.cpu().numpy())\n            if passage_embeddings is not None:\n                all_embeddings3['passages'].append(passage_embeddings.cpu().numpy())\n    if all_embeddings3['passages']:\n        all_embeddings3['passages'] = np.concatenate(all_embeddings3['passages'])\n    if all_embeddings3['query']:\n        all_embeddings3['query'] = np.concatenate(all_embeddings3['query'])\n    assert np.array_equal(all_embeddings['query'][0], all_embeddings3['query'][0])"
        ]
    }
]