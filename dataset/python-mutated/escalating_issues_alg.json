[
    {
        "func_name": "generate_issue_forecast",
        "original": "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    \"\"\"\n    Calculates daily issue spike limits, given an input dataset from snuba.\n\n    For issues with at least 14 days of history, we combine a weighted average of the last\n    7 days of hourly data with the observed variance over that time interval. We double the\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\n    The overall multiplier is calibrated to 5 standard deviations, although it is\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\n    or low variance.\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\n    standard deviation over the average. This is to get an understanding of how high or low the variance\n    is relative to the data. The CV is then placed into an exponential equation that outputs\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\n    have a very high variance.\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\n    :param data: Dict of Snuba query results - hourly data over past 7 days\n    :param start_time: datetime indicating the first hour to calc spike protection for\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\n    :return output: Dict containing a list of spike protection values\n    \"\"\"\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output",
        "mutated": [
            "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    if False:\n        i = 10\n    '\\n    Calculates daily issue spike limits, given an input dataset from snuba.\\n\\n    For issues with at least 14 days of history, we combine a weighted average of the last\\n    7 days of hourly data with the observed variance over that time interval. We double the\\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\\n    The overall multiplier is calibrated to 5 standard deviations, although it is\\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\\n    or low variance.\\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\\n    standard deviation over the average. This is to get an understanding of how high or low the variance\\n    is relative to the data. The CV is then placed into an exponential equation that outputs\\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\\n    have a very high variance.\\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\\n    :param data: Dict of Snuba query results - hourly data over past 7 days\\n    :param start_time: datetime indicating the first hour to calc spike protection for\\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\\n    :return output: Dict containing a list of spike protection values\\n    '\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output",
            "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculates daily issue spike limits, given an input dataset from snuba.\\n\\n    For issues with at least 14 days of history, we combine a weighted average of the last\\n    7 days of hourly data with the observed variance over that time interval. We double the\\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\\n    The overall multiplier is calibrated to 5 standard deviations, although it is\\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\\n    or low variance.\\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\\n    standard deviation over the average. This is to get an understanding of how high or low the variance\\n    is relative to the data. The CV is then placed into an exponential equation that outputs\\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\\n    have a very high variance.\\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\\n    :param data: Dict of Snuba query results - hourly data over past 7 days\\n    :param start_time: datetime indicating the first hour to calc spike protection for\\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\\n    :return output: Dict containing a list of spike protection values\\n    '\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output",
            "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculates daily issue spike limits, given an input dataset from snuba.\\n\\n    For issues with at least 14 days of history, we combine a weighted average of the last\\n    7 days of hourly data with the observed variance over that time interval. We double the\\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\\n    The overall multiplier is calibrated to 5 standard deviations, although it is\\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\\n    or low variance.\\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\\n    standard deviation over the average. This is to get an understanding of how high or low the variance\\n    is relative to the data. The CV is then placed into an exponential equation that outputs\\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\\n    have a very high variance.\\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\\n    :param data: Dict of Snuba query results - hourly data over past 7 days\\n    :param start_time: datetime indicating the first hour to calc spike protection for\\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\\n    :return output: Dict containing a list of spike protection values\\n    '\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output",
            "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculates daily issue spike limits, given an input dataset from snuba.\\n\\n    For issues with at least 14 days of history, we combine a weighted average of the last\\n    7 days of hourly data with the observed variance over that time interval. We double the\\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\\n    The overall multiplier is calibrated to 5 standard deviations, although it is\\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\\n    or low variance.\\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\\n    standard deviation over the average. This is to get an understanding of how high or low the variance\\n    is relative to the data. The CV is then placed into an exponential equation that outputs\\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\\n    have a very high variance.\\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\\n    :param data: Dict of Snuba query results - hourly data over past 7 days\\n    :param start_time: datetime indicating the first hour to calc spike protection for\\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\\n    :return output: Dict containing a list of spike protection values\\n    '\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output",
            "def generate_issue_forecast(data: GroupCount, start_time: datetime, alg_params: ThresholdVariables=standard_version) -> List[IssueForecast]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculates daily issue spike limits, given an input dataset from snuba.\\n\\n    For issues with at least 14 days of history, we combine a weighted average of the last\\n    7 days of hourly data with the observed variance over that time interval. We double the\\n    weight if historical observation falls on the same day of week to incorporate daily seasonality.\\n    The overall multiplier is calibrated to 5 standard deviations, although it is\\n    truncated to [5, 8] to avoid poor results in a timeseries with very high\\n    or low variance.\\n    In addition, we also calculate the cv (coefficient of variance) of the timeseries the past week, which is the ratio of the\\n    standard deviation over the average. This is to get an understanding of how high or low the variance\\n    is relative to the data. The CV is then placed into an exponential equation that outputs\\n    a multiplier inversely related to how high the cv is. The multiplier is bounded between 2 and 5. The\\n    ceilings for the next week are all the same - which is the maximum number of events in an hour over the\\n    past week multiplied by this multiplier. This calculation is to accound for bursty issues or those that\\n    have a very high variance.\\n    The final spike limit for each hour is set to the max of the bursty limit bound or the calculated limit.\\n    :param data: Dict of Snuba query results - hourly data over past 7 days\\n    :param start_time: datetime indicating the first hour to calc spike protection for\\n    :param alg_params: Threshold Variables dataclass with different ceiling versions\\n    :return output: Dict containing a list of spike protection values\\n    '\n    output: List[IssueForecast] = []\n    input_dates = [datetime.strptime(x, '%Y-%m-%dT%H:%M:%S%f%z') for x in data['intervals']]\n    output_dates = [start_time + timedelta(days=x) for x in range(14)]\n    ts_data = data['data']\n    if len(ts_data) == 0 or len(input_dates) == 0:\n        return output\n    ts_max = max(ts_data)\n    if len(ts_data) < 168:\n        for output_ts in output_dates:\n            output.append({'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': ts_max * 10})\n        return output\n    ts_avg = statistics.mean(ts_data)\n    ts_std_dev = statistics.stdev(ts_data)\n    ts_cv = ts_std_dev / ts_avg\n    regression_multiplier = min(max(alg_params.min_bursty_multiplier, 5 * math.e ** (-0.65 * ts_cv)), alg_params.max_bursty_multiplier)\n    limit_v1 = ts_max * regression_multiplier\n    ts_multiplier = min(max((ts_avg + alg_params.std_multiplier * ts_std_dev) / ts_avg, alg_params.min_spike_multiplier), alg_params.max_spike_multiplier)\n    baseline = ts_multiplier * ts_avg\n    for output_ts in output_dates:\n        weights = [1 + (input_ts.weekday() == output_ts.weekday()) for input_ts in input_dates]\n        numerator = sum([datum * weight for (datum, weight) in zip(ts_data, weights)])\n        wavg_limit = numerator / sum(weights)\n        limit_v2 = wavg_limit + baseline\n        forecast: IssueForecast = {'forecasted_date': output_ts.strftime('%Y-%m-%d'), 'forecasted_value': int(max(limit_v1, limit_v2))}\n        output.append(forecast)\n    return output"
        ]
    }
]