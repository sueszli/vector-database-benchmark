[
    {
        "func_name": "assert_valid_inputs_multiannotator",
        "original": "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    \"\"\"Validate format of multi-annotator labels\"\"\"\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')",
        "mutated": [
            "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    if False:\n        i = 10\n    'Validate format of multi-annotator labels'\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')",
            "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Validate format of multi-annotator labels'\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')",
            "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Validate format of multi-annotator labels'\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')",
            "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Validate format of multi-annotator labels'\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')",
            "def assert_valid_inputs_multiannotator(labels_multiannotator: np.ndarray, pred_probs: Optional[np.ndarray]=None, ensemble: bool=False, allow_single_label: bool=False, annotator_ids: Optional[pd.Index]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Validate format of multi-annotator labels'\n    if labels_multiannotator.ndim != 2:\n        raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n    if any([isinstance(label, str) for label in labels_multiannotator.ravel()]):\n        raise ValueError('Labels cannot be strings, they must be zero-indexed integers corresponding to class indices.')\n    nan_row_mask = np.isnan(labels_multiannotator).all(axis=1)\n    if nan_row_mask.any():\n        nan_rows = list(np.where(nan_row_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have rows with all NaN, each example must have at least one label.\\nExamples {nan_rows} do not have any labels.')\n    nan_col_mask = np.isnan(labels_multiannotator).all(axis=0)\n    if nan_col_mask.any():\n        if annotator_ids is not None:\n            nan_columns = list(annotator_ids[np.where(nan_col_mask)[0]])\n        else:\n            nan_columns = list(np.where(nan_col_mask)[0])\n        raise ValueError(f'labels_multiannotator cannot have columns with all NaN, each annotator must annotator at least one example.\\nAnnotators {nan_columns} did not label any examples.')\n    if not allow_single_label:\n        if labels_multiannotator.shape[1] <= 1:\n            raise ValueError('labels_multiannotator must have more than one column.\\nIf there is only one annotator, use cleanlab.rank.get_label_quality_scores instead')\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            raise ValueError('Each example only has one label, collapse the labels into a 1-D array and use cleanlab.rank.get_label_quality_scores instead')\n        if np.apply_along_axis(lambda s: np.array_equal(np.unique(s[~np.isnan(s)]), s[~np.isnan(s)]), axis=1, arr=labels_multiannotator).all():\n            warnings.warn('Annotators do not agree on any example. Check input data.')\n    all_labels_flatten = labels_multiannotator.ravel()\n    all_labels_flatten = all_labels_flatten[~np.isnan(all_labels_flatten)]\n    assert_valid_class_labels(all_labels_flatten, allow_one_class=True)\n    if pred_probs is not None:\n        if not isinstance(pred_probs, np.ndarray):\n            raise TypeError('pred_probs must be a numpy array.')\n        if ensemble:\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n            if pred_probs.shape[1] != len(labels_multiannotator):\n                raise ValueError('each pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[2]\n        else:\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n            if len(pred_probs) != len(labels_multiannotator):\n                raise ValueError('pred_probs and labels_multiannotator must have same length.')\n            num_classes = pred_probs.shape[1]\n        highest_class = np.nanmax(labels_multiannotator) + 1\n        if num_classes < highest_class:\n            raise ValueError(f'pred_probs must have at least {int(highest_class)} columns based on the largest class label which appears in labels_multiannotator. Perhaps some rarely-annotated classes were lost while establishing consensus labels used to train your classifier.')"
        ]
    },
    {
        "func_name": "assert_valid_pred_probs",
        "original": "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    \"\"\"Validate format of pred_probs for multiannotator active learning functions\"\"\"\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')",
        "mutated": [
            "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    if False:\n        i = 10\n    'Validate format of pred_probs for multiannotator active learning functions'\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')",
            "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Validate format of pred_probs for multiannotator active learning functions'\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')",
            "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Validate format of pred_probs for multiannotator active learning functions'\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')",
            "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Validate format of pred_probs for multiannotator active learning functions'\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')",
            "def assert_valid_pred_probs(pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None, ensemble: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Validate format of pred_probs for multiannotator active learning functions'\n    if pred_probs is None and pred_probs_unlabeled is None:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if ensemble:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 3:\n                error_message = 'pred_probs must be a 3d array.'\n                if pred_probs.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs array (ie. only one predictor), use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 3:\n                error_message = 'pred_probs_unlabeled must be a 3d array.'\n                if pred_probs_unlabeled.ndim == 2:\n                    error_message += ' If you have a 2d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[2] != pred_probs_unlabeled.shape[2]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')\n    else:\n        if pred_probs is not None:\n            if not isinstance(pred_probs, np.ndarray):\n                raise TypeError('pred_probs must be a numpy array.')\n            if pred_probs.ndim != 2:\n                error_message = 'pred_probs must be a 2d array.'\n                if pred_probs.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs array, use the ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs_unlabeled is not None:\n            if not isinstance(pred_probs_unlabeled, np.ndarray):\n                raise TypeError('pred_probs_unlabeled must be a numpy array.')\n            if pred_probs_unlabeled.ndim != 2:\n                error_message = 'pred_probs_unlabeled must be a 2d array.'\n                if pred_probs_unlabeled.ndim == 3:\n                    error_message += ' If you have a 3d pred_probs_unlabeled array, use the non-ensemble version of this function.'\n                raise ValueError(error_message)\n        if pred_probs is not None and pred_probs_unlabeled is not None:\n            if pred_probs.shape[1] != pred_probs_unlabeled.shape[1]:\n                raise ValueError('pred_probs and pred_probs_unlabeled must have the same number of classes')"
        ]
    },
    {
        "func_name": "format_multiannotator_labels",
        "original": "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    \"\"\"Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\n\n    Returns\n    -------\n    formatted_labels\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\n        cleanlab.multiannotator functions.\n\n    mapping\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\n    \"\"\"\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)",
        "mutated": [
            "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    if False:\n        i = 10\n    'Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\\n\\n    Returns\\n    -------\\n    formatted_labels\\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\\n        cleanlab.multiannotator functions.\\n\\n    mapping\\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\\n    '\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)",
            "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\\n\\n    Returns\\n    -------\\n    formatted_labels\\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\\n        cleanlab.multiannotator functions.\\n\\n    mapping\\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\\n    '\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)",
            "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\\n\\n    Returns\\n    -------\\n    formatted_labels\\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\\n        cleanlab.multiannotator functions.\\n\\n    mapping\\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\\n    '\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)",
            "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\\n\\n    Returns\\n    -------\\n    formatted_labels\\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\\n        cleanlab.multiannotator functions.\\n\\n    mapping\\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\\n    '\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)",
            "def format_multiannotator_labels(labels: LabelLike) -> Tuple[pd.DataFrame, dict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Takes an array of labels and formats it such that labels are in the set ``0, 1, ..., K-1``,\\n    where ``K`` is the number of classes. The labels are assigned based on lexicographic order.\\n\\n    Returns\\n    -------\\n    formatted_labels\\n        Returns pd.DataFrame of shape ``(N,M)``. The return labels will be properly formatted and can be passed to\\n        cleanlab.multiannotator functions.\\n\\n    mapping\\n        A dictionary showing the mapping of new to old labels, such that ``mapping[k]`` returns the name of the k-th class.\\n    '\n    if isinstance(labels, pd.DataFrame):\n        np_labels = labels.values\n    elif isinstance(labels, np.ndarray):\n        np_labels = labels\n    else:\n        raise TypeError('labels must be 2D numpy array or pandas DataFrame')\n    unique_labels = pd.unique(np_labels.ravel())\n    try:\n        unique_labels = unique_labels[~np.isnan(unique_labels)]\n        unique_labels.sort()\n    except TypeError:\n        nan_mask = np.array([l is np.NaN or l is pd.NA or l == 'nan' for l in unique_labels])\n        unique_labels = unique_labels[~nan_mask]\n        unique_labels.sort()\n    if unique_labels.dtype == 'float':\n        unique_labels = unique_labels.astype('int')\n    label_map = {label: i for (i, label) in enumerate(unique_labels)}\n    inverse_map = {i: label for (label, i) in label_map.items()}\n    if isinstance(labels, np.ndarray):\n        labels = pd.DataFrame(labels)\n    formatted_labels = labels.replace(label_map)\n    return (formatted_labels, inverse_map)"
        ]
    },
    {
        "func_name": "check_consensus_label_classes",
        "original": "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    \"\"\"Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)\"\"\"\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")",
        "mutated": [
            "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    if False:\n        i = 10\n    'Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)'\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")",
            "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)'\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")",
            "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)'\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")",
            "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)'\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")",
            "def check_consensus_label_classes(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_method: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Check if any classes no longer appear in the set of consensus labels (established using the consensus_method stated)'\n    unique_ma_labels = np.unique(labels_multiannotator)\n    unique_ma_labels = unique_ma_labels[~np.isnan(unique_ma_labels)]\n    labels_set_difference = set(unique_ma_labels) - set(consensus_label)\n    if len(labels_set_difference) > 0:\n        print(f\"CAUTION: Number of unique classes has been reduced from the original data when establishing consensus labels using consensus method '{consensus_method}', likely due to some classes being rarely annotated. If training a classifier on these consensus labels, it will never see any of the omitted classes unless you manually replace some of the consensus labels.\\nClasses in the original data but not in consensus labels: {list(map(int, labels_set_difference))}\")"
        ]
    },
    {
        "func_name": "compute_soft_cross_entropy",
        "original": "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    \"\"\"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\"\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy",
        "mutated": [
            "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    if False:\n        i = 10\n    \"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy",
            "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy",
            "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy",
            "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy",
            "def compute_soft_cross_entropy(labels_multiannotator: np.ndarray, pred_probs: np.ndarray) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Compute soft cross entropy between the annotators' empirical label distribution and model pred_probs\"\n    num_classes = get_num_classes(pred_probs=pred_probs)\n    empirical_label_distribution = np.full((len(labels_multiannotator), num_classes), np.NaN)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        empirical_label_distribution[i, :] = value_counts(labels_subset, num_classes=num_classes) / len(labels_subset)\n    clipped_pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    soft_cross_entropy = -np.sum(empirical_label_distribution * np.log(clipped_pred_probs), axis=1) / np.log(num_classes)\n    return soft_cross_entropy"
        ]
    },
    {
        "func_name": "find_best_temp_scaler",
        "original": "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    \"\"\"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\n    and model pred_probs\"\"\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp",
        "mutated": [
            "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    if False:\n        i = 10\n    \"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\\n    and model pred_probs\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp",
            "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\\n    and model pred_probs\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp",
            "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\\n    and model pred_probs\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp",
            "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\\n    and model pred_probs\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp",
            "def find_best_temp_scaler(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, coarse_search_range: list=[0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 8], fine_search_size: int=4) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Find the best temperature scaling factor that minimizes the soft cross entropy between the annotators' empirical label distribution\\n    and model pred_probs\"\n    soft_cross_entropy_coarse = np.full(len(coarse_search_range), np.NaN)\n    log_pred_probs = np.log(pred_probs, where=pred_probs > 0, out=np.full(pred_probs.shape, -np.inf))\n    for (i, curr_temp) in enumerate(coarse_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_coarse[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    min_entropy_ind = np.argmin(soft_cross_entropy_coarse)\n    fine_search_range = _set_fine_search_range(coarse_search_range, fine_search_size, min_entropy_ind)\n    soft_cross_entropy_fine = np.full(len(fine_search_range), np.NaN)\n    for (i, curr_temp) in enumerate(fine_search_range):\n        scaled_pred_probs = softmax(log_pred_probs, temperature=curr_temp, axis=1, shift=False)\n        soft_cross_entropy_fine[i] = np.mean(compute_soft_cross_entropy(labels_multiannotator, scaled_pred_probs))\n    best_temp = fine_search_range[np.argmin(soft_cross_entropy_fine)]\n    return best_temp"
        ]
    },
    {
        "func_name": "_set_fine_search_range",
        "original": "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range",
        "mutated": [
            "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    if False:\n        i = 10\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range",
            "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range",
            "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range",
            "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range",
            "def _set_fine_search_range(coarse_search_range: list, fine_search_size: int, min_entropy_ind: np.intp) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    fine_search_range = np.array([])\n    if min_entropy_ind != 0:\n        fine_search_range = np.append(np.linspace(coarse_search_range[min_entropy_ind - 1], coarse_search_range[min_entropy_ind], fine_search_size, endpoint=False), fine_search_range)\n    if min_entropy_ind != len(coarse_search_range) - 1:\n        fine_search_range = np.append(fine_search_range, np.linspace(coarse_search_range[min_entropy_ind], coarse_search_range[min_entropy_ind + 1], fine_search_size + 1, endpoint=True))\n    return fine_search_range"
        ]
    },
    {
        "func_name": "temp_scale_pred_probs",
        "original": "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    \"\"\"Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.\"\"\"\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs",
        "mutated": [
            "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    if False:\n        i = 10\n    'Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.'\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs",
            "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.'\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs",
            "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.'\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs",
            "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.'\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs",
            "def temp_scale_pred_probs(pred_probs: np.ndarray, temp: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Scales pred_probs by the given temperature factor. Temperature of <1 will sharpen the pred_probs while temperatures of >1 will smoothen it.'\n    pred_probs = np.clip(pred_probs, a_min=SMALL_CONST, a_max=None)\n    pred_probs = pred_probs / np.sum(pred_probs, axis=1)[:, np.newaxis]\n    scaled_pred_probs = softmax(np.log(pred_probs), temperature=temp, axis=1, shift=False)\n    scaled_pred_probs = scaled_pred_probs / np.sum(scaled_pred_probs, axis=1)[:, np.newaxis]\n    return scaled_pred_probs"
        ]
    }
]