"""Decoders for the speech model."""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import collections
import math
import lingvo.compat as tf
from lingvo.core import attention
from lingvo.core import base_decoder
from lingvo.core import base_layer
from lingvo.core import cluster_factory
from lingvo.core import layers
from lingvo.core import plot
from lingvo.core import py_utils
from lingvo.core import recurrent
from lingvo.core import rnn_cell
from lingvo.core import summary_utils
from lingvo.core import symbolic
from lingvo.tasks.asr import contextualizer_base
from lingvo.tasks.asr import decoder_utils
from lingvo.tasks.asr import fusion
from matplotlib import font_manager
import six
from six.moves import range

def _ToTensorArray(name, v, max_seq_length, clear_after_read=None):
    if False:
        while True:
            i = 10
    'Create TensorArray from v, of size max_seq_length.'
    ta = tf.TensorArray(v.dtype, max_seq_length, name=name, clear_after_read=clear_after_read)
    ta = ta.unstack(v)
    return ta

def _NewTensorArray(name, max_seq_length, dtype=None):
    if False:
        i = 10
        return i + 15
    'Create empty TensorArray which can store max_seq_length elements.'
    return tf.TensorArray(dtype, max_seq_length, name=name)

class AsrDecoderBase(base_decoder.BaseBeamSearchDecoder):
    """Base class for RNN-with-attention speech decoders.

  The decoder takes encoder_outputs, a NestedMap generated by the encoder, as
  input. The NestedMap is expected to contain the following fields:

    - 'encoded': encoded features
    - 'padding': padding for encoded features

  The decoder operates in a 'step-by-step' fashion. The model encapsulates all
  information which should persist from one step to the next in the
  DecoderStepState NestedMap, which provides a 'misc_states' NestedMap which can
  store arbitrary information required by the specific decoder sub-class.

  A 'step' in training consists of the following sequence of steps which compute
  the outputs from the decoder given the current input target (and the state of
  the model after making the previous predictions):

  1. Compute the input target at the current time step::

      cur_target_info = self.TargetsToBeFedAtCurrentDecodeStep(...)

  2. Update state and compute outputs by running SingleDecodeStep::

      step_outs, new_state = self.SingleDecodeStep(...)

  3. Update state based on the logits computed at this step::

      new_state = self.PostStepDecoderStateUpdate(old_state, logits)

  4. Display summaries based on the accumulated information across all steps::

      self.AddAdditionalDecoderSummaries(seq_out_tas)

  Sub-classes can customize behavior by implementing the following functions,
  which will modify the behavior of the decoder:

  For beam search decoder:
    - _InitBeamSearchStateCallback
    - _PreBeamSearchStepCallback
    - _PostBeamSearchStepCallback
  For EMBR training:
    - ComputeHypsWithBeamSearch

  - MiscZeroState: NestedMap which represents the initial state for the
    'misc_states' in the DecoderStepState. The default implementation returns
    an empty NestedMap.

  - SingleDecodeStep: This corresponds to the computation which happens in each
    step of the model. The function should return the outputs of the decoder
    as well as the updated state.

  - PostStepDecoderStateUpdate: A function which updates the DecoderStepState
    after the output logits from the decoder have been computed. By default,
    this returns the DecoderStepState unchanged.

  - TargetsToBeFedAtCurrentDecodeStep: Returns a TargetInfo namedtuple, which
    represents information about the targets which should be input at the
    current step, as well as the output label which should be predicted.
    The default implementation uses the values in the batched 'targets'
    provided by the InputGenerator.

  - AddAdditionalDecoderSummaries: A function which can be used to add any
    decoder specific information as part of the summaries displayed during
    training. By default this is a no-op.

  - CreateTargetInfoMisc: A function which can be used to store arbitrary
    information as required by a sub-classes in the target info arrays used
    to determine the current label at each step during training. By default,
    this creates an empty NestedMap.

  A few other functions that control how the decoder initializes and computes
  attention during the initial step, and during each step can also be
  modified, if need be:

  - _GetAttenContextDim: The dimensionality of the attention context vector.
  - _CreateAtten: Controls how the attention module is configured. Most
    subclasses will not have to change this unless it changes how attention
    works.
  - BaseZeroState: Returns initial state of RNNs, and attention.
  - _InitAttention: Initializes Tensors used by the attention module.
  - _GetInitialSeqStateTensorArrays: Get intitial tensor arrays for
    ComputePredictionsDynamic.
  - _GetNewAttenProbs: Update atten probs for a timestep and return the
    updated tensor array.
  """
    TargetInfo = collections.namedtuple('TargetInfo', ['id', 'label', 'weight', 'emb', 'padding', 'misc'])
    SequenceOutTensorArrays = collections.namedtuple('SequenceOutTensorArrays', ['rnn_outs', 'step_outs', 'atten_probs', 'logits', 'fusion', 'misc'])

    @classmethod
    def Params(cls):
        if False:
            for i in range(10):
                print('nop')
        p = super(AsrDecoderBase, cls).Params()
        p.Define('dropout_prob', 0.0, 'Prob at which we do dropout.')
        p.Define('emb', layers.EmbeddingLayer.Params(), 'Embedding layer params.')
        p.Define('emb_dim', 0, 'dimension of the embedding layer.')
        p.Define('label_smoothing', None, 'Label smoothing class.')
        p.Define('rnn_cell_tpl', rnn_cell.LSTMCellSimple.Params(), 'RNNCell params template. Can be a single param or a list of rnn_layers params, one for each layer.')
        p.Define('rnn_cell_dim', 0, 'size of the rnn cells.')
        p.Define('rnn_cell_hidden_dim', 0, 'internal size of the rnn cells. When set to > 0 it enables a projection layer at the output of the rnn cell (see call to SetRnnCellNodes).')
        p.Define('attention', attention.AdditiveAttention.Params(), 'Additive attention params.')
        p.Define('softmax', layers.SimpleFullSoftmax.Params(), 'Softmax params.')
        p.Define('softmax_uses_attention', True, 'Controls whether attention is fed to the softmax or not.')
        p.Define('source_dim', 0, 'Dimension of the source encodings.')
        p.Define('atten_context_dim', 0, 'Depth of the attention context vector output.')
        p.Define('attention_plot_font_properties', '', 'Adds font properties for the given file if set. Required for displaying east-Asian character sets on plot axes.')
        p.Define('rnn_layers', 1, 'Number of rnn layers.')
        p.Define('residual_start', 0, 'Start residual connections from this layer. For this and higher layers, the layer output is the sum of the RNN cell output and input; if the layer also normalizes its output, then the normalization is done over this sum. Set to 0 to disable residual connections.')
        p.Define('fusion', fusion.NullFusion.Params(), 'Fusion class params.')
        p.Define('parallel_iterations', 30, 'Max number of iterations to run in parallel for while loop.')
        p.Define('per_token_avg_loss', True, 'Use per-token average loss when set to True (default); when set to False use sequence average loss (sum logP across tokens in an output sequence) and average across all sequences in the batch.')
        p.Define('token_normalized_per_seq_loss', False, 'Whether or not to normalize the per-sequence loss by the sequence length.')
        p.Define('min_ground_truth_prob', 1.0, 'The min probability of using the ground truth as the previous prediction.')
        p.Define('min_prob_step', 1000000.0, 'Step to reach min_ground_truth_prob.')
        p.Define('prob_decay_start_step', 10000.0, 'The step to starts linearly decrease the probability of sampling ground truth.')
        p.Define('use_while_loop_based_unrolling', True, 'Whether or not to use while loop based unrolling for training. If false, we use a functional while based unrolling.')
        p.Define('logit_types', {'logits': 1.0}, 'A dict of logit_name -> loss_weight. logit_name must be a field in the predictions NestedMap. loss_weight should add up to 1.0.')
        p.Define('use_unnormalized_logits_as_log_probs', True, 'If true, decoder beam search may return unnormalized logits as log_probs. Used for backwards-compatibility.')
        p.Define('contextualizer', contextualizer_base.NullContextualizer.Params(), 'A contextualizer that can be usedto inject context into the decoder. The default NullContextualizer does not add parameters to the model nor changes the computation.')
        p.Define('focal_loss_alpha', None, 'The weighting factor alpha.')
        p.Define('focal_loss_gamma', None, 'Tunable focusing parameter.')
        vocab = 96
        p.emb_dim = 96
        p.emb.vocab_size = vocab
        p.emb.max_num_shards = 1
        p.emb.params_init = py_utils.WeightInit.Uniform(1.0)
        p.rnn_cell_dim = 256
        p.rnn_cell_tpl.params_init = py_utils.WeightInit.Uniform(0.1)
        p.attention.hidden_dim = 128
        p.attention.params_init = py_utils.WeightInit.UniformSqrtDim(math.sqrt(3.0))
        p.softmax.num_classes = vocab
        p.softmax.params_init = py_utils.WeightInit.Uniform(0.1)
        p.fusion.lm.vocab_size = vocab
        p.target_seq_len = 300
        p.source_dim = 512
        return p

    @classmethod
    def UpdateTargetVocabSize(cls, p, vocab_size, wpm_model=None):
        if False:
            while True:
                i = 10
        'Updates params with the vocab size and wpm model.\n\n    Args:\n      p: model params.\n      vocab_size: size of the vocabulary.\n      wpm_model: file name prefix pointing to a wordpiece model.\n\n    Returns:\n      Model params updated with the vocab size and wpm model.\n    '
        p.emb.vocab_size = vocab_size
        p.softmax.num_classes = vocab_size
        p.fusion.lm = p.fusion.lm.cls.UpdateTargetVocabSize(p.fusion.lm, vocab_size, wpm_model)
        return p

    @base_layer.initializer
    def __init__(self, params):
        if False:
            while True:
                i = 10
        params = params.Copy()
        if params.min_ground_truth_prob < 1:
            params.emb.on_ps = False
        super(AsrDecoderBase, self).__init__(params)
        p = self.params
        assert not p.packed_input, 'Packed inputs are not yet supported for AsrDecoderBase.'
        self._max_label_prob = 1 - p.min_ground_truth_prob
        self._decay_interval = p.min_prob_step - p.prob_decay_start_step
        if self._decay_interval <= 0:
            raise ValueError('min_prob_step (%d) <= prob_decay_start_step (%d)' % (p.min_prob_step, p.prob_decay_start_step))
        if p.attention_plot_font_properties:
            self._font_properties = font_manager.FontProperties(fname=p.attention_plot_font_properties)
        else:
            self._font_properties = font_manager.FontProperties()
        name = p.name
        with tf.variable_scope(name):
            self.CreateChild('contextualizer', p.contextualizer)
            atten_context_dim = self._GetAttenContextDim()
            assert symbolic.IsExpr(atten_context_dim) or atten_context_dim > 0
            p.emb.dtype = p.dtype
            p.emb.embedding_dim = p.emb_dim
            self.CreateChild('emb', p.emb)
            params_rnn_cells = []
            feat_dim = p.emb_dim
            for i in range(p.rnn_layers):
                if isinstance(p.rnn_cell_tpl, (list, tuple)):
                    assert len(p.rnn_cell_tpl) == p.rnn_layers
                    rnn_cell_params = p.rnn_cell_tpl[i].Copy()
                else:
                    rnn_cell_params = p.rnn_cell_tpl.Copy()
                rnn_cell_params.dtype = p.dtype
                rnn_cell_params.inputs_arity = 2
                decoder_utils.SetRnnCellNodes(p, rnn_cell_params)
                rnn_cell_params.num_input_nodes = feat_dim + atten_context_dim
                if i == 0:
                    rnn_cell_params.name = 'rnn_cell'
                else:
                    rnn_cell_params.name = 'rnn_cell_%d' % i
                feat_dim = rnn_cell_params.num_output_nodes
                params_rnn_cells.append(rnn_cell_params)
            self.CreateChildren('rnn_cell', params_rnn_cells)
            p.softmax.dtype = p.dtype
            p.softmax.input_dim = feat_dim
            if p.softmax_uses_attention:
                p.softmax.input_dim += atten_context_dim
            self.CreateChild('softmax', p.softmax)
            p.fusion.base_model_logits_dim = p.softmax.input_dim
            self.CreateChild('fusion', p.fusion)
            self._CreateAtten()
            if p.label_smoothing is not None:
                p.label_smoothing.name = 'smoother'
                if p.label_smoothing.num_classes == 0:
                    p.label_smoothing.num_classes = p.softmax.num_classes
                elif p.label_smoothing.num_classes != p.softmax.num_classes:
                    raise ValueError('label_smoothing.num_classes ({}) does not match softmax.num_classes ({})'.format(p.label_smoothing.num_classes, p.softmax.num_classes))
                self.CreateChild('smoother', p.label_smoothing)

    def _CreateAtten(self):
        if False:
            i = 10
            return i + 15
        p = self.params
        p.attention.dtype = p.dtype
        p.attention.source_dim = p.attention.source_dim or p.source_dim
        p.attention.query_dim = p.attention.query_dim or self.rnn_cell[0].params.num_output_nodes
        self.CreateChild('atten', p.attention)

    def _GetAttenContextDim(self):
        if False:
            print('Hello World!')
        p = self.params
        audio_context_dim = p.atten_context_dim if p.atten_context_dim else p.source_dim
        additional_context_dim = self.contextualizer.GetContextDim()
        return audio_context_dim + additional_context_dim

    def _ApplyDropout(self, theta, x_in, deterministic=False, extra_seed=None):
        if False:
            i = 10
            return i + 15
        p = self.params
        assert 0 <= p.dropout_prob and p.dropout_prob < 1.0
        if self.do_eval or p.dropout_prob == 0.0:
            return x_in
        if deterministic:
            seeds = py_utils.GenerateStepSeedPair(p, theta.global_step)
            if extra_seed:
                seeds += extra_seed
            return py_utils.DeterministicDropout(x_in, 1.0 - p.dropout_prob, seeds)
        else:
            seed = p.random_seed
            if seed and extra_seed:
                seed += extra_seed
            return tf.nn.dropout(x_in, 1.0 - p.dropout_prob, seed=seed)

    def _InitAttention(self, theta, encoder_outputs):
        if False:
            print('Hello World!')
        'Intializes attention and returns a NestedMap with those values.'
        packed_src = self.atten.InitForSourcePacked(theta.atten, encoder_outputs.encoded, encoder_outputs.encoded, encoder_outputs.padding)
        self.contextualizer.InitAttention(theta.contextualizer, packed_src)
        return packed_src

    def BaseZeroState(self, theta, encoder_outputs, bs, misc_zero_states, per_step_source_padding=None):
        if False:
            i = 10
            return i + 15
        'Returns initial state of RNNs, and attention.'
        p = self.params
        rnn_states = []
        for i in range(p.rnn_layers):
            rnn_states.append(self.rnn_cell[i].zero_state(theta.rnn_cell[i], bs))
        packed_src = self._InitAttention(theta, encoder_outputs)
        zero_atten_state = self.atten.ZeroAttentionState(tf.shape(encoder_outputs.padding)[0], bs)
        (atten_context, atten_probs, atten_states) = self.atten.ComputeContextVectorWithSource(theta.atten, packed_src, py_utils.Zeros([bs, self.rnn_cell[0].params.num_output_nodes], dtype=py_utils.FPropDtype(p)), zero_atten_state, per_step_source_padding=per_step_source_padding)
        atten_context = self.contextualizer.ZeroAttention(theta.contextualizer, bs, misc_zero_states, atten_context, packed_src)
        return (rnn_states, atten_context, atten_probs, atten_states, packed_src)

    def AddAdditionalDecoderSummaries(self, encoder_outputs, targets, seq_out_tas, softmax_input):
        if False:
            print('Hello World!')
        'Additional model-specific summaries which should be displayed.'
        pass

    def DecoderStepZeroState(self, theta, encoder_outputs, target_ids, bs):
        if False:
            return 10
        misc_zero_states = self.MiscZeroState(theta, encoder_outputs, target_ids, bs)
        (rnn_states, atten_context, atten_probs, atten_states, packed_src) = self.BaseZeroState(theta, encoder_outputs, bs, misc_zero_states)
        return (py_utils.NestedMap(rnn_states=rnn_states, atten_context=atten_context, atten_probs=atten_probs, atten_states=atten_states, fusion_states=self.fusion.zero_state(theta.fusion, bs), misc_states=misc_zero_states), packed_src)

    def _AddDecoderActivationsSummary(self, encoder_outputs, targets, atten_probs, rnn_outs, softmax_input, additional_atten_probs=None, target_alignments=None):
        if False:
            while True:
                i = 10
        'Adds summary about decoder activations.\n\n    For each of the args, a TensorArray can also be a Tensor representing\n    the stacked array.\n\n    Args:\n      encoder_outputs: a NestedMap computed by encoder.\n      targets: a NestedMap, usually input_batch.tgt.\n      atten_probs: a TensorArray of max_target_length elements, each of shape\n        [batch, max_source_length].\n      rnn_outs: a list of TensorArray, one for each RNN layer. Each\n        TensorArray has max_target_length elements, each of shape [batch,\n        rnn_output_dim].\n      softmax_input: a Tensor of shape [batch, max_target_length, vocab_size].\n      additional_atten_probs: an optional list of (name, TensorArray) to display\n        along with atten_probs.\n      target_alignments: an optional Tensor of shape [batch, max_target_length]\n        where every value is an int32 in the range of [1, max_source_length],\n        representing number of source frames by which a target label should be\n        emitted.\n\n    Returns:\n      A finalized figure.\n    '
        source_encs = encoder_outputs.encoded
        source_paddings = encoder_outputs.padding
        if not self.cluster.add_summary:
            return

        def _ToTensor(t):
            if False:
                print('Hello World!')
            return t.stack() if isinstance(t, tf.TensorArray) else t
        atten_probs = _ToTensor(atten_probs)
        rnn_outs = [_ToTensor(ta) for ta in rnn_outs]
        if additional_atten_probs:
            additional_atten_probs = [(name, _ToTensor(ta)) for (name, ta) in additional_atten_probs]
        num_cols = 2 + len(rnn_outs)
        fig = plot.MatplotlibFigureSummary('decoder_example', figsize=(2.3 * (3 + num_cols - 1), 6), max_outputs=1, subplot_grid_shape=(2, num_cols), gridspec_kwargs=dict(width_ratios=[3] + [1] * (num_cols - 1), height_ratios=(4, 1)))

        def PlotAttention(fig, axes, transcript, atten_probs, title):
            if False:
                print('Hello World!')
            plot.AddImage(fig, axes, atten_probs, title=title)
            axes.set_ylabel(plot.ToUnicode(transcript + '\nOutput token'), size='x-small', wrap=True, fontproperties=self._font_properties)
        index = 0
        if 'transcripts' not in targets:
            return
        transcript = targets.transcripts[:index + 1]
        srclen = tf.cast(tf.round(tf.reduce_sum(1 - source_paddings[:, index])), tf.int32)
        tgtlen = tf.cast(tf.round(tf.reduce_sum(1 - targets.paddings[index, :])), tf.int32)

        def PlotAttentionForOneExample(atten_probs, target_fig, title, alignments=None):
            if False:
                while True:
                    i = 10
            'Plots attention for one example.'
            tf.logging.info('Plotting attention for %s: %s %s', title, atten_probs.shape, alignments)
            atten_probs = atten_probs[:tgtlen, index, :srclen]
            if alignments is not None:
                alignment_positions = alignments[index, :tgtlen] - 1
                alignment_probs = tf.one_hot(alignment_positions, depth=srclen, axis=-1)
                atten_probs = 1 - tf.stack([atten_probs, tf.minimum(atten_probs + alignment_probs, 1.0), alignment_probs], axis=-1)
            probs = tf.expand_dims(atten_probs, 0)
            target_fig.AddSubplot([transcript, probs], PlotAttention, title=title)
        PlotAttentionForOneExample(atten_probs, fig, title=u'atten_probs')
        for i in range(len(rnn_outs)):
            rnn_out = tf.expand_dims(rnn_outs[i][:tgtlen, index, :], 0)
            fig.AddSubplot([rnn_out], title=u'rnn_outs/%d' % i)
        fig.AddSubplot([softmax_input[:index + 1, :tgtlen, :]], title=u'softmax_input')
        source_encs = tf.expand_dims(tf.transpose(source_encs[:srclen, index, :]), 0)
        fig.AddSubplot([source_encs], title=u'source_encs', xlabel=u'Encoder frame')
        finalized_fig = fig.Finalize()
        if additional_atten_probs:
            all_atten_probs = [('atten_probs', atten_probs)] + additional_atten_probs
            num_atten_images = len(all_atten_probs)
            atten_fig = plot.MatplotlibFigureSummary('decoder_attention', figsize=(6, 3 * num_atten_images), max_outputs=1)
            for (key, probs) in all_atten_probs:
                PlotAttentionForOneExample(probs, atten_fig, title=key, alignments=target_alignments)
            atten_fig.Finalize()
        return finalized_fig

    def _ComputeMetrics(self, logits, target_labels, target_weights, target_probs=None):
        if False:
            return 10
        'Compute loss and misc metrics.\n\n    Args:\n      logits: Tensor of shape [batch, time, num_classes].\n      target_labels: Tensor of shape [batch, time].\n      target_weights: Tensor of shape [batch, time].\n      target_probs: Tensor of shape [batch, time, num_classes].\n    Returns:\n      A (metrics, per_sequence_loss) pair.\n    '
        p = self.params
        target_weights_sum = tf.reduce_sum(target_weights)
        target_weights_sum_eps = target_weights_sum + 1e-06
        target_weights_batch = tf.reduce_sum(target_weights, 1)
        target_weights_batch_eps = target_weights_batch + 1e-06
        correct_preds = tf.cast(tf.equal(tf.argmax(logits, 2, output_type=tf.int32), target_labels), py_utils.FPropDtype(p))
        correct_next_preds = tf.reduce_sum(correct_preds * target_weights)
        accuracy = tf.identity(correct_next_preds / target_weights_sum_eps, name='fraction_of_correct_next_step_preds')
        per_example_loss = py_utils.SoftmaxCrossEntropyFocalLoss(logits=logits, label_ids=target_labels, label_probs=target_probs, alpha=p.focal_loss_alpha, gamma=p.focal_loss_gamma)
        per_sequence_loss = tf.reduce_sum(per_example_loss * target_weights, 1)
        per_example_batch = per_sequence_loss / target_weights_batch_eps
        per_token_avg_loss = tf.reduce_sum(per_sequence_loss) / target_weights_sum_eps
        if p.token_normalized_per_seq_loss:
            per_seq_length = tf.reduce_sum(target_weights, 1)
            per_sequence_loss /= per_seq_length + 0.001
        if p.per_token_avg_loss:
            loss = per_token_avg_loss
            loss_weight = target_weights_sum
        else:
            loss = tf.reduce_mean(per_sequence_loss)
            loss_weight = tf.shape(per_sequence_loss)[0]
        tf.add_to_collection('per_loss', per_example_batch)
        metrics = {'loss': (loss, loss_weight), 'log_pplx': (per_token_avg_loss, target_weights_sum), 'token_normed_prob': (tf.exp(-per_token_avg_loss), target_weights_sum)}
        metrics['fraction_of_correct_next_step_preds'] = (accuracy, target_weights_sum)
        return (metrics, per_sequence_loss)

    def InitDecoder(self, theta, encoder_outputs, dec_bs):
        if False:
            return 10
        (decoder_step_zero_state, packed_src) = self.DecoderStepZeroState(theta, encoder_outputs, tf.ones([dec_bs, 1], dtype=tf.int32) * self.params.target_sos_id, dec_bs)
        return (decoder_step_zero_state.rnn_states, decoder_step_zero_state.atten_context, decoder_step_zero_state.atten_probs, decoder_step_zero_state.atten_states, decoder_step_zero_state.fusion_states, decoder_step_zero_state.misc_states, packed_src)

    def _InitBeamSearchStateCallback(self, theta, encoder_outputs, num_hyps_per_beam):
        if False:
            print('Hello World!')
        raise NotImplementedError('_InitBeamSearchStateCallback')

    def _PreBeamSearchStepCallback(self, theta, encoder_outputs, step_ids, states, num_hyps_per_beam):
        if False:
            print('Hello World!')
        raise NotImplementedError('_PreBeamSearchStepCallback')

    def _PostBeamSearchStepCallback(self, theta, encoder_outputs, new_step_ids, states):
        if False:
            return 10
        raise NotImplementedError('_PostBeamSearchStepCallback')

    def ComputeLoss(self, theta, predictions, targets):
        if False:
            while True:
                i = 10
        "Computes loss metrics and per-sequence losses.\n\n    Args:\n      theta: A NestedMap object containing weights' values of this\n        layer and its children layers.\n      predictions: A NestedMap containing logits (and possibly other fields).\n      targets: A dict of string to tensors representing the targets one is\n          trying to predict. Each tensor in targets is of shape [batch, time].\n\n    Returns:\n      (metrics, per_sequence_loss), where metrics is a dictionary containing\n      metrics for the xent loss and prediction accuracy. per_sequence is a\n      dictionary containing 'loss', a (-log(p)) vector of size [bs].\n    "
        p = self.params
        with tf.name_scope(p.name):
            if 'probs' in targets:
                target_probs = targets.probs
            elif p.label_smoothing is not None:
                target_probs = self.smoother.FProp(theta.smoother, targets.paddings, targets.labels, targets.ids)
            else:
                target_probs = None
            merged_metrics = {}
            merged_per_sequence_loss = 0.0

            def AddToMetric(acc, scale, metric):
                if False:
                    print('Hello World!')
                assert len(acc) == 2
                assert len(metric) == 2
                return (acc[0] + scale * tf.cast(metric[0], py_utils.FPropDtype(p)), acc[1] + scale * tf.cast(metric[1], py_utils.FPropDtype(p)))
            for (logit_name, loss_weight) in six.iteritems(p.logit_types):
                (metrics, per_sequence_loss) = self._ComputeMetrics(getattr(predictions, logit_name), targets.labels, targets.weights, target_probs)
                for (k, v) in six.iteritems(metrics):
                    tf.logging.info('Merging metric %s: %s', k, v)
                    merged_metrics[k + '/' + logit_name] = v
                    if k not in merged_metrics:
                        merged_metrics[k] = (tf.zeros(shape=[], dtype=py_utils.FPropDtype(p)), tf.zeros(shape=[], dtype=py_utils.FPropDtype(p)))
                    merged_metrics[k] = AddToMetric(merged_metrics[k], loss_weight, v)
                merged_per_sequence_loss += loss_weight * per_sequence_loss
            return (merged_metrics, {'loss': merged_per_sequence_loss})

    def CreateTargetInfoMisc(self, targets):
        if False:
            while True:
                i = 10
        "Return a NestedMap corresponding to the 'misc' field in TargetInfo."
        if 'fst_bias_probs' in targets:
            return py_utils.NestedMap({'fst_bias_probs': targets.fst_bias_probs})
        else:
            return py_utils.NestedMap()

    def ComputePredictions(self, theta, encoder_outputs, targets):
        if False:
            i = 10
            return i + 15
        "Computes logits.\n\n    Args:\n      theta: A NestedMap object containing weights values of this layer and its\n        child layers.\n      encoder_outputs: a NestedMap computed by encoder.\n      targets: A dict of string to tensors representing the targets one is\n        trying to predict. Each tensor in targets is of shape [batch, time].\n\n    Returns:\n      A NestedMap object containing logit tensors as values, each of shape\n      [target_batch, max_target_length, vocab_size]. One of the keys must be\n      'logits'.\n    "
        assert getattr(encoder_outputs, 'src_segment_id', None) is None
        p = self.params
        self.contextualizer.SetContextMap(targets, theta.contextualizer)
        if 'weights' not in targets and 'paddings' in targets:
            targets.weights = 1.0 - targets.paddings
        if p.use_while_loop_based_unrolling:
            predictions = self.ComputePredictionsDynamic(theta, encoder_outputs, targets)
        else:
            predictions = self.ComputePredictionsFunctional(theta, encoder_outputs, targets)
        if encoder_outputs and isinstance(encoder_outputs.padding, tf.Tensor):
            predictions.source_enc_len = tf.reduce_sum(1 - encoder_outputs.padding, axis=0)
            if 'paddings' in targets:
                source_batch = py_utils.GetShape(encoder_outputs.padding)[1]
                target_batch = py_utils.GetShape(targets.paddings)[0]
                multiplier = target_batch // source_batch
                source_len = py_utils.RepeatDim(predictions.source_enc_len, multiplier, axis=0)
                target_len = tf.reduce_sum(1 - targets.paddings, axis=1)
                target_source_length_ratio = target_len / tf.maximum(source_len, 1.0)
                summary_utils.scalar('avg_target_source_length_ratio', tf.reduce_mean(target_source_length_ratio))
        return predictions

    def _GetInitialSeqStateTensorArrays(self, max_seq_length, decoder_step_state_zero_fusion_flat, decoder_step_state_zero_misc_flat):
        if False:
            i = 10
            return i + 15
        'Get intitial tensor arrays for ComputePredictionsDynamic.'
        p = self.params
        return AsrDecoder.SequenceOutTensorArrays(rnn_outs=[_NewTensorArray(name='rnn%d_outs' % i, max_seq_length=max_seq_length, dtype=py_utils.FPropDtype(p)) for i in range(p.rnn_layers)], step_outs=_NewTensorArray(name='step_outs', max_seq_length=max_seq_length, dtype=py_utils.FPropDtype(p)), atten_probs=_NewTensorArray(name='atten_probs', max_seq_length=max_seq_length, dtype=py_utils.FPropDtype(p)), logits=_NewTensorArray(name='logits', max_seq_length=max_seq_length, dtype=py_utils.FPropDtype(p)), fusion=[_NewTensorArray(name='fusion_states%d' % i, max_seq_length=max_seq_length, dtype=decoder_step_state_zero_fusion_flat[i].dtype) for i in range(len(decoder_step_state_zero_fusion_flat))], misc=[_NewTensorArray(name='misc_states%d' % i, max_seq_length=max_seq_length, dtype=decoder_step_state_zero_misc_flat[i].dtype) for i in range(len(decoder_step_state_zero_misc_flat))])

    def _GetNewAttenProbs(self, seq_out_tas, time, decoder_step_state):
        if False:
            return 10
        'Update atten probs for a timestep and return the updated tensor array.'
        return seq_out_tas.atten_probs.write(time, decoder_step_state.atten_probs)

    def _UpdateSequenceOutTensorArrays(self, decoder_step_state, time, step_outs, seq_out_tas):
        if False:
            for i in range(10):
                print('nop')
        'Update SequenceOutTensorArrays at each time step.'
        new_rnn_outs = []
        assert len(seq_out_tas.rnn_outs) == len(decoder_step_state.rnn_states)
        for i in range(len(seq_out_tas.rnn_outs)):
            new_rnn_outs.append(seq_out_tas.rnn_outs[i].write(time, decoder_step_state.rnn_states[i].m))
        new_logits_ta = seq_out_tas.logits.write(time, decoder_step_state.logits)
        new_step_outs_ta = seq_out_tas.step_outs.write(time, step_outs)
        new_atten_probs_ta = self._GetNewAttenProbs(seq_out_tas, time, decoder_step_state)
        new_seq_outs_fusion_states = []
        new_fusion_states_flat = decoder_step_state.fusion_states.Flatten()
        for i in range(len(new_fusion_states_flat)):
            new_seq_outs_fusion_states.append(seq_out_tas.fusion[i].write(time, new_fusion_states_flat[i]))
        new_seq_outs_misc_states = []
        new_misc_states_flat = decoder_step_state.misc_states.Flatten()
        for i in range(len(new_misc_states_flat)):
            new_seq_outs_misc_states.append(seq_out_tas.misc[i].write(time, new_misc_states_flat[i]))
        return AsrDecoder.SequenceOutTensorArrays(rnn_outs=new_rnn_outs, step_outs=new_step_outs_ta, atten_probs=new_atten_probs_ta, logits=new_logits_ta, fusion=new_seq_outs_fusion_states, misc=new_seq_outs_misc_states)

    def _GetAttenProbsFromSequenceOutTensorArrays(self, atten_probs):
        if False:
            i = 10
            return i + 15
        return tf.transpose(atten_probs.stack(), [1, 0, 2])

    def _GetPredictionFromSequenceOutTensorArrays(self, seq_out_tas):
        if False:
            while True:
                i = 10
        return py_utils.NestedMap(softmax_input=seq_out_tas.step_outs.stack(), logits=tf.transpose(seq_out_tas.logits.stack(), [1, 0, 2]), attention=py_utils.NestedMap(probs=self._GetAttenProbsFromSequenceOutTensorArrays(seq_out_tas.atten_probs)))

    def _GetInitialTargetInfo(self, targets, max_seq_length, target_embs):
        if False:
            for i in range(10):
                print('nop')
        return AsrDecoderBase.TargetInfo(id=_ToTensorArray('target_ids_ta', tf.transpose(targets.ids), max_seq_length, clear_after_read=False), label=_ToTensorArray('target_labels_ta', tf.transpose(targets.labels), max_seq_length, clear_after_read=False), weight=_ToTensorArray('target_weights_ta', tf.transpose(targets.weights), max_seq_length), emb=_ToTensorArray('target_embs_ta', tf.transpose(target_embs, [1, 0, 2]), max_seq_length), padding=_ToTensorArray('target_paddings_ta', tf.expand_dims(tf.transpose(targets.paddings), -1), max_seq_length), misc=self.CreateTargetInfoMisc(targets))

    def ComputePredictionsDynamic(self, theta, encoder_outputs, targets):
        if False:
            for i in range(10):
                print('nop')
        p = self.params
        with tf.name_scope(p.name):
            dec_bs = tf.shape(targets.ids)[0]
            max_seq_length = tf.shape(targets.ids)[1]
            target_embs = self.emb.EmbLookup(theta.emb, tf.reshape(targets.ids, [-1]))
            target_embs = tf.reshape(target_embs, [dec_bs, max_seq_length, p.emb_dim])
            target_embs = self._ApplyDropout(theta, target_embs)
            target_info_tas = self._GetInitialTargetInfo(targets, max_seq_length, target_embs)
            time = tf.constant(0, tf.int32)
            (decoder_step_state_zero, packed_src) = self.DecoderStepZeroState(theta, encoder_outputs, targets.ids, dec_bs)
            decoder_step_state_zero_fusion_flat = decoder_step_state_zero.fusion_states.Flatten()
            decoder_step_state_zero_misc_flat = decoder_step_state_zero.misc_states.Flatten()
            seq_out_tas = self._GetInitialSeqStateTensorArrays(max_seq_length, decoder_step_state_zero_fusion_flat, decoder_step_state_zero_misc_flat)

            def _LoopContinue(time, decoder_step_state, target_info_tas, seq_out_tas):
                if False:
                    for i in range(10):
                        print('nop')
                del decoder_step_state, target_info_tas, seq_out_tas
                return time < max_seq_length

            def _LoopBody(time, old_decoder_step_state, target_info_tas, seq_out_tas):
                if False:
                    print('Hello World!')
                'Computes decoder outputs and updates decoder_step_state.'
                cur_target_info = self.TargetsToBeFedAtCurrentDecodeStep(time, theta, old_decoder_step_state, target_info_tas, seq_out_tas)
                (step_outs, decoder_step_state) = self.SingleDecodeStep(theta, packed_src, cur_target_info, old_decoder_step_state)
                (step_outs, decoder_step_state.fusion_states) = self.fusion.FProp(theta.fusion, old_decoder_step_state.fusion_states, step_outs, cur_target_info.id, cur_target_info.padding)
                xent_loss = self.softmax.FProp(theta.softmax, [step_outs], class_weights=cur_target_info.weight, class_ids=cur_target_info.label)
                decoder_step_state = self.PostStepDecoderStateUpdate(decoder_step_state, xent_loss.logits)
                decoder_step_state.logits = self.fusion.ComputeLogitsWithLM(decoder_step_state.fusion_states, decoder_step_state.logits)
                new_seq_out_tas = self._UpdateSequenceOutTensorArrays(decoder_step_state, time, step_outs, seq_out_tas)
                del decoder_step_state.logits
                return (time + 1, decoder_step_state, target_info_tas, new_seq_out_tas)
            loop_vars = (time, decoder_step_state_zero, target_info_tas, seq_out_tas)
            shape_invariants = tf.nest.map_structure(lambda t: tf.TensorShape(None), loop_vars)
            (time, _, target_info_tas, seq_out_tas) = tf.while_loop(_LoopContinue, _LoopBody, loop_vars=loop_vars, shape_invariants=shape_invariants, parallel_iterations=p.parallel_iterations, swap_memory=False)
            softmax_input = seq_out_tas.step_outs.stack()
            softmax_input = tf.transpose(softmax_input, [1, 0, 2])
            self._AddDecoderActivationsSummary(encoder_outputs, targets, seq_out_tas.atten_probs, seq_out_tas.rnn_outs, softmax_input)
            self.AddAdditionalDecoderSummaries(encoder_outputs, targets, seq_out_tas, softmax_input)
            return self._GetPredictionFromSequenceOutTensorArrays(seq_out_tas)

    def ComputePredictionsFunctional(self, theta, encoder_outputs, targets):
        if False:
            i = 10
            return i + 15
        p = self.params
        assert p.min_ground_truth_prob == 1.0
        with tf.name_scope(p.name):
            dec_bs = tf.shape(targets.ids)[0]
            (state0, packed_src) = self.DecoderStepZeroState(theta, encoder_outputs, targets.ids, dec_bs)
            atten_context_dim = self._GetAttenContextDim()
            rnn_output_dim = self.rnn_cell[-1].params.num_output_nodes
            out_dim = rnn_output_dim + atten_context_dim
            state0.step_outs = py_utils.Zeros([dec_bs, out_dim], dtype=py_utils.FPropDtype(p))
            target_embs = self.emb.EmbLookup(theta.emb, targets.ids)
            target_embs = self._ApplyDropout(theta, target_embs)
            inputs = py_utils.NestedMap(id=tf.transpose(targets.ids), label=tf.transpose(targets.labels), weight=tf.transpose(targets.weights), emb=tf.transpose(target_embs, [1, 0, 2]), padding=tf.expand_dims(tf.transpose(targets.paddings), -1), misc=self.CreateTargetInfoMisc(targets))
            theta_no_fusion = theta.copy()
            del theta_no_fusion.fusion
            recurrent_theta = py_utils.NestedMap(theta=theta_no_fusion, packed_src=packed_src)
            state0_no_fusion = state0.copy()
            del state0_no_fusion.fusion_states

            def RnnStep(recurrent_theta, state0, inputs):
                if False:
                    return 10
                'Computes one rnn step.'
                with tf.name_scope('single_decode_step'):
                    (step_outs, state1) = self.SingleDecodeStep(recurrent_theta.theta, recurrent_theta.packed_src, inputs, state0, use_deterministic_random=True)
                    state1.step_outs = step_outs
                state1 = self.PostStepDecoderStateUpdate(state1, inputs.label)
                return (state1, py_utils.NestedMap())
            (accumulated_states, _) = recurrent.Recurrent(recurrent_theta, state0_no_fusion, inputs, RnnStep)
            if not p.softmax_uses_attention:
                (step_out, _) = tf.split(accumulated_states.step_outs, [rnn_output_dim, atten_context_dim], axis=-1)
            else:
                step_out = accumulated_states.step_outs
            (softmax_input, state0.fusion_states) = self.fusion.FProp(theta.fusion, state0.fusion_states, step_out, inputs.id, inputs.padding, inputs.misc)
            seq_logits = self._ComputeLogits(theta, softmax_input)
            atten_states = accumulated_states.atten_states
            if isinstance(atten_states, py_utils.NestedMap):
                additional_atten_probs = sorted([(name, tensor) for (name, tensor) in atten_states.FlattenItems() if name.endswith('probs')])
            else:
                additional_atten_probs = []
            rnn_outs = [cell.GetOutput(accumulated_states.rnn_states[i]) for (i, cell) in enumerate(self.rnn_cell)]
            self._AddDecoderActivationsSummary(encoder_outputs, targets, accumulated_states.atten_probs, rnn_outs, softmax_input, additional_atten_probs=additional_atten_probs, target_alignments=getattr(targets, 'alignments', None))
            adjusted_logits = self.fusion.ComputeLogitsWithLM(state0.fusion_states, seq_logits)
            predictions = py_utils.NestedMap(logits_without_bias=tf.transpose(seq_logits, [1, 0, 2]), logits=tf.transpose(adjusted_logits, [1, 0, 2]), softmax_input=softmax_input)
            attention_map = py_utils.NestedMap(probs=accumulated_states.atten_probs)
            for (k, v) in additional_atten_probs:
                attention_map[k] = v
            predictions.attention = attention_map.Transform(lambda x: tf.transpose(x, [1, 0, 2]))
            return predictions

    def _ComputeLogits(self, theta, softmax_input):
        if False:
            while True:
                i = 10
        if isinstance(self.softmax, layers.ConvSoftmax):
            return self.softmax.Logits(theta.softmax, softmax_input)
        else:
            xent_loss = self.softmax.FProp(theta.softmax, [softmax_input], class_weights=tf.ones(shape=tf.shape(softmax_input)[:-1], dtype=softmax_input.dtype), class_ids=tf.ones(shape=tf.shape(softmax_input)[:-1], dtype=tf.int32))
            return xent_loss.logits

    def SingleDecodeStep(self, theta, packed_src, cur_target_info, decoder_step_state, per_step_src_padding=None, use_deterministic_random=False):
        if False:
            i = 10
            return i + 15
        "Computes one 'step' of computation for the decoder.\n\n    Must be implemented by sub-classes. Residual connections must also be taken\n    care of in sub-classes.\n\n    Args:\n      theta: A NestedMap object containing weights' values of this\n        layer and its children layers.\n      packed_src: A NestedMap to represent the packed source tensors generated\n        by the attention model.\n      cur_target_info: TargetInfo namedtuple, which represents the targets\n        which represents information about the target at this step. It is up\n        to the various sub-classes to determine how to process the current\n        target.\n      decoder_step_state: DecoderStepState which encapsulates the state of the\n        decoder before computing outputs at the current step.\n      per_step_src_padding: Optional padding to be applied to the source_encs\n        which overrides the default padding in source_paddings. Used, for\n        example, by the Neural Transducer (NT) decoder.\n      use_deterministic_random: whether to use deterministic random numbers when\n        needed. Must be set to True if called from functional recurrent.\n\n    Returns:\n      A tuple (step_out, new_decoder_state) which represent the outputs of the\n      decoder (usually logits), and the new decoder state after processing the\n      current step.\n    "
        raise NotImplementedError('Must be implemented by sub-classes.')

    def MiscZeroState(self, theta, encoder_outputs, target_ids, bs):
        if False:
            i = 10
            return i + 15
        'Returns initial state for other miscellaneous states, if any.'
        del encoder_outputs
        misc_zero_state = py_utils.NestedMap()
        p = self.params
        if self._max_label_prob > 0:
            misc_zero_state.prev_predicted_ids = tf.reshape(target_ids[:, 0], [bs])
            step = tf.cast(theta.global_step, tf.float32)
            sampling_p = (step - p.prob_decay_start_step) / self._decay_interval
            groundtruth_p = 1 - self._max_label_prob * sampling_p
            groundtruth_p = tf.maximum(groundtruth_p, p.min_ground_truth_prob)
            groundtruth_p = tf.minimum(groundtruth_p, 1.0)
            summary_utils.scalar('ground_truth_sampling_probability', groundtruth_p)
            misc_zero_state.groundtruth_p = groundtruth_p
        return misc_zero_state

    def TargetsToBeFedAtCurrentDecodeStep(self, time, theta, decoder_step_state, target_info_tas, seq_out_tas):
        if False:
            while True:
                i = 10
        del seq_out_tas
        target_id = target_info_tas.id.read(time)
        label = target_info_tas.label.read(time)
        weight = tf.squeeze(target_info_tas.weight.read(time))
        emb = target_info_tas.emb.read(time)
        padding = target_info_tas.padding.read(time)
        misc = py_utils.NestedMap()
        if self._max_label_prob > 0:
            dec_bs = tf.shape(decoder_step_state.misc_states.prev_predicted_ids)[0]
            pick_groundtruth = tf.less(tf.random_uniform([dec_bs], seed=self.params.random_seed), decoder_step_state.misc_states.groundtruth_p)
            emb = tf.where(pick_groundtruth, target_info_tas.emb.read(time), self.emb.EmbLookup(theta.emb, tf.stop_gradient(decoder_step_state.misc_states.prev_predicted_ids)))
            target_id = tf.where(pick_groundtruth, target_info_tas.id.read(time), decoder_step_state.misc_states.prev_predicted_ids)
        return AsrDecoderBase.TargetInfo(id=target_id, label=label, weight=weight, emb=emb, padding=padding, misc=misc)

    def PostStepDecoderStateUpdate(self, decoder_step_state, logits=None):
        if False:
            return 10
        'Update decoder states and logits after SingleDecodeStep.\n\n    Args:\n      decoder_step_state: A NestedMap object which encapsulates decoder states.\n      logits: a tensor, predicted logits.\n\n    Returns:\n      decoder_step_state.\n\n    Raises:\n      ValueError: if scheduled sampling is used for functional unrolling or\n                  if logits is None for while loop based unrolling.\n    '
        if not self.params.use_while_loop_based_unrolling:
            if self.params.min_ground_truth_prob < 1.0:
                raise ValueError('SS is not yet supported')
        else:
            if logits is None:
                raise ValueError('logits cannot be None')
            decoder_step_state.logits = logits
            if self._max_label_prob > 0:
                bs = tf.shape(logits)[0]
                log_probs = tf.nn.log_softmax(logits)
                log_prob_sample = tf.multinomial(log_probs, 1, seed=self.params.random_seed)
                pred_ids = tf.reshape(tf.cast(log_prob_sample, tf.int32), [bs])
                decoder_step_state.misc_states.prev_predicted_ids = pred_ids
        return decoder_step_state

class AsrDecoder(AsrDecoderBase):
    """Step-by-step decoder with LM fusion."""

    @classmethod
    def Params(cls):
        if False:
            return 10
        p = super(AsrDecoder, cls).Params()
        return p

    def AddAdditionalDecoderSummaries(self, encoder_outputs, targets, seq_out_tas, softmax_input):
        if False:
            for i in range(10):
                print('nop')
        'Add summaries not covered by the default activations summaries.\n\n    Args:\n      encoder_outputs: a NestedMap computed by encoder.\n      targets: a NestedMap containing target info.\n      seq_out_tas: a SequenceOutTensorArrays.\n      softmax_input: a tensor of shape [batch, time, vocab_size].\n    '
        if cluster_factory.Current().add_summary:
            self.fusion.AddAdditionalDecoderSummaries(encoder_outputs.encoded, encoder_outputs.padding, targets, seq_out_tas, softmax_input)

    def _ComputeAttention(self, theta, rnn_out, packed_src, attention_state, per_step_src_padding=None, query_segment_id=None):
        if False:
            for i in range(10):
                print('nop')
        "Runs attention and computes context vector.\n\n    Can be overridden by a child class if attention is computed differently.\n\n    Args:\n      theta: A NestedMap object containing weights for the attention layers.\n        Expects a member named 'atten'.\n      rnn_out: A Tensor of shape [batch_size, query_dim]; output of the\n        first layer of decoder RNN, which is the query vector used for\n        attention.\n      packed_src: A NestedMap returned by self.atten.InitForSourcePacked.\n      attention_state: The attention state computed at the previous timestep.\n        Varies with the type of attention, but is usually a Tensor or a\n        NestedMap of Tensors of shape [batch_size, <state_dim>].\n      per_step_src_padding: Source sequence padding to apply at this step.\n      query_segment_id: a tensor of shape [batch_size].\n\n    Returns:\n      A tuple of 3 tensors:\n\n      - The attention context vector: shaped [batch_size, context_dim].\n      - The attention probability vector: shaped [batch_size, seq_len]\n      - The attention state: A Tensor or a NestedMap of Tensors of shape\n        [batch_size, <state_dim>].\n    "
        return self.atten.ComputeContextVectorWithSource(theta.atten, packed_src, rnn_out, attention_state=attention_state, per_step_source_padding=per_step_src_padding, query_segment_id=query_segment_id)

    def SingleDecodeStep(self, theta, packed_src, cur_target_info, decoder_step_state, per_step_src_padding=None, use_deterministic_random=False):
        if False:
            print('Hello World!')
        "Decode one step.\n\n    Note that the implementation of attention here follows the model in\n    https://arxiv.org/pdf/1609.08144.pdf, detailed more in\n    https://arxiv.org/pdf/1703.08581.pdf.\n\n    Args:\n      theta: A NestedMap object containing weights' values of this\n        layer and its children layers.\n      packed_src: A NestedMap to represent the packed source tensors generated\n        by the attention model.\n      cur_target_info: TargetInfo namedtuple, which represents the targets\n        which represents information about the target at this step. It is up\n        to the various sub-classes to determine how to process the current\n        target.\n      decoder_step_state: DecoderStepState which encapsulates the state of the\n        decoder before computing outputs at the current step.\n      per_step_src_padding: Optional padding to be applied to the source_encs\n        which overrides the default padding in source_paddings. Used, for\n        example, by the Neural Transducer (NT) decoder.\n      use_deterministic_random: whether to use deterministic random numbers when\n        needed. Must be set to True if called from functional recurrent.\n\n    Returns:\n      A tuple (step_out, new_decoder_state) which represent the outputs of the\n      decoder (usually logits), and the new decoder state after processing the\n      current step.\n    "
        misc_states = decoder_step_state.misc_states
        new_rnn_states = []
        (new_rnn_states_0, _) = self.rnn_cell[0].FProp(theta.rnn_cell[0], decoder_step_state.rnn_states[0], py_utils.NestedMap(act=[cur_target_info.emb, decoder_step_state.atten_context], padding=cur_target_info.padding))
        new_rnn_states.append(new_rnn_states_0)
        rnn_out = self.rnn_cell[0].GetOutput(new_rnn_states_0)
        (new_atten_context, new_atten_probs, new_atten_states) = self._ComputeAttention(theta, rnn_out, packed_src, decoder_step_state.atten_states, per_step_src_padding=per_step_src_padding)
        new_atten_context = self.contextualizer.QueryAttention(theta.contextualizer, rnn_out, misc_states, new_atten_context, packed_src)
        for (i, cell) in enumerate(self.rnn_cell[1:], 1):
            (new_rnn_states_i, _) = cell.FProp(theta.rnn_cell[i], decoder_step_state.rnn_states[i], py_utils.NestedMap(act=[rnn_out, new_atten_context], padding=cur_target_info.padding))
            new_rnn_states.append(new_rnn_states_i)
            new_rnn_out = cell.GetOutput(new_rnn_states_i)
            new_rnn_out = self._ApplyDropout(theta, new_rnn_out, deterministic=use_deterministic_random, extra_seed=i * 1000)
            if i + 1 >= self.params.residual_start > 0:
                rnn_out += new_rnn_out
            else:
                rnn_out = new_rnn_out
        step_out = tf.concat([rnn_out, new_atten_context], 1)
        return (step_out, py_utils.NestedMap(rnn_states=new_rnn_states, atten_context=new_atten_context, atten_probs=new_atten_probs, atten_states=new_atten_states, misc_states=misc_states))

    def _GetNumHypsForBeamSearch(self, source_encs, num_hyps_per_beam):
        if False:
            i = 10
            return i + 15
        'Returns number of hypothesis times batch_size.\n\n    This function can be overridden by a child class if the total number of\n    hyps are to be computed in a different way, e.g., when the format of inputs\n    change.\n    Args:\n      source_encs: A Tensor of [time, batch, dim] with source encodings.\n      num_hyps_per_beam: Int, the number of hypothesis per example in the beam.\n    Returns:\n      A Tensor with value batch * num_hyps_per_beam.\n    '
        return tf.shape(source_encs)[1] * num_hyps_per_beam

    def _PostProcessAttenProbsForBeamSearch(self, atten_probs):
        if False:
            i = 10
            return i + 15
        'Returns the attention probabilities after optional post processing.\n\n    This is a noop for the base class. But this function can be overridden\n    by a child class, e.g., when the format of probabilities change.\n    Args:\n      atten_probs: A Tensor of [batch, source_len] dimension with atten probs.\n    Returns:\n      A Tensor with processed atten_probs. The same as input in this case.\n    '
        return atten_probs

    def _InitBeamSearchStateCallback(self, theta, encoder_outputs, num_hyps_per_beam):
        if False:
            for i in range(10):
                print('nop')
        p = self.params
        num_hyps = self._GetNumHypsForBeamSearch(encoder_outputs.encoded, num_hyps_per_beam)
        (rnn_states, atten_context, atten_probs, atten_states, fusion_states, misc_states, packed_src) = self.InitDecoder(theta, encoder_outputs, num_hyps)
        del packed_src
        atten_probs = self._PostProcessAttenProbsForBeamSearch(atten_probs)
        all_atten_states = py_utils.NestedMap({'atten_context': atten_context, 'atten_probs': atten_probs, 'atten_states': atten_states})
        initial_results = py_utils.NestedMap({'log_probs': tf.nn.log_softmax(tf.zeros([num_hyps, p.softmax.num_classes], dtype=py_utils.FPropDtype(p))), 'atten_probs': atten_probs})
        other_states = py_utils.NestedMap({'rnn_states': rnn_states, 'all_atten_states': all_atten_states, 'fusion_states': fusion_states, 'misc_states': misc_states})
        return (initial_results, other_states)

    def _PreBeamSearchStepCallback(self, theta, encoder_outputs, step_ids, states, num_hyps_per_beam):
        if False:
            i = 10
            return i + 15
        p = self.params
        step_paddings = tf.zeros(tf.shape(step_ids), dtype=p.dtype)
        embs = self.emb.EmbLookup(theta.emb, tf.reshape(step_ids, [-1]))
        prev_rnn_states = states.rnn_states
        prev_atten_states = states.all_atten_states.atten_states
        prev_atten_context = states.all_atten_states.atten_context
        prev_atten_probs = states.all_atten_states.atten_probs
        prev_fusion_states = states.fusion_states
        prev_misc_states = states.misc_states
        prev_decoder_step_state = py_utils.NestedMap(rnn_states=prev_rnn_states, atten_context=prev_atten_context, atten_probs=prev_atten_probs, atten_states=prev_atten_states, misc_states=prev_misc_states)
        cur_target_info = AsrDecoderBase.TargetInfo(id=tf.reshape(step_ids, [-1]), label=None, weight=None, emb=embs, padding=step_paddings, misc=py_utils.NestedMap())
        packed_src = self._InitAttention(theta, encoder_outputs)
        (step_out, new_decoder_step_state) = self.SingleDecodeStep(theta, packed_src, cur_target_info=cur_target_info, decoder_step_state=prev_decoder_step_state)
        (atten_context, atten_probs, rnn_states, atten_states, misc_states) = (new_decoder_step_state.atten_context, new_decoder_step_state.atten_probs, new_decoder_step_state.rnn_states, new_decoder_step_state.atten_states, new_decoder_step_state.misc_states)
        if p.softmax_uses_attention:
            softmax_input = step_out
        else:
            atten_context_dim = self._GetAttenContextDim()
            rnn_output_dim = self.rnn_cell[-1].params.num_output_nodes
            (softmax_input, _) = tf.split(step_out, [rnn_output_dim, atten_context_dim], axis=-1)
        (softmax_input, fusion_states) = self.fusion.FProp(theta.fusion, prev_fusion_states, softmax_input, cur_target_info.id, cur_target_info.padding)
        logits = self._ComputeLogits(theta, softmax_input)
        logits = self.fusion.ComputeLogitsWithLM(fusion_states, logits, is_eval=True)
        if p.use_unnormalized_logits_as_log_probs:
            log_probs = logits
        else:
            log_probs = tf.nn.log_softmax(logits)
        atten_probs = self._PostProcessAttenProbsForBeamSearch(atten_probs)
        bs_results = py_utils.NestedMap({'atten_probs': atten_probs, 'log_probs': log_probs})
        all_atten_states = py_utils.NestedMap({'atten_context': atten_context, 'atten_probs': atten_probs, 'atten_states': atten_states})
        new_states = py_utils.NestedMap({'rnn_states': rnn_states, 'all_atten_states': all_atten_states, 'fusion_states': fusion_states, 'misc_states': misc_states})
        return (bs_results, new_states)

    def _PostBeamSearchStepCallback(self, theta, encoder_outputs, new_step_ids, states):
        if False:
            while True:
                i = 10
        del encoder_outputs, new_step_ids
        return states