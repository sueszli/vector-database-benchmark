[
    {
        "func_name": "_init_learn",
        "original": "def _init_learn(self) -> None:\n    \"\"\"\n        Overview:\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\n\n        .. note::\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\n\n        .. note::\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\n\n        .. note::\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\n        \"\"\"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()",
        "mutated": [
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n    \"\\n        Overview:\\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\\n\\n        .. note::\\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\\n\\n        .. note::\\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\\n        \"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Overview:\\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\\n\\n        .. note::\\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\\n\\n        .. note::\\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\\n        \"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Overview:\\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\\n\\n        .. note::\\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\\n\\n        .. note::\\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\\n        \"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Overview:\\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\\n\\n        .. note::\\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\\n\\n        .. note::\\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\\n        \"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Overview:\\n            Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.\\n            This method will be called in ``__init__`` method if ``learn`` field is in ``enable_field``.\\n\\n        .. note::\\n            For the member variables that need to be saved and loaded, please refer to the ``_state_dict_learn``             and ``_load_state_dict_learn`` methods.\\n\\n        .. note::\\n            For the member variables that need to be monitored, please refer to the ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            If you want to set some spacial member variables in ``_init_learn`` method, you'd better name them             with prefix ``_learn_`` to avoid conflict with other modes, such as ``self._learn_attr1``.\\n        \"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    self._optimizer = Adam(self._model.parameters(), lr=self._cfg.learn.learning_rate, eps=0.0003125)\n    self._gamma = self._cfg.discount_factor\n    self._nstep = self._cfg.nstep\n    self._entropy_tau = self._cfg.entropy_tau\n    self._m_alpha = self._cfg.m_alpha\n    self._target_model = copy.deepcopy(self._model)\n    if 'target_update_freq' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='assign', update_kwargs={'freq': self._cfg.learn.target_update_freq})\n    elif 'target_theta' in self._cfg.learn:\n        self._target_model = model_wrap(self._target_model, wrapper_name='target', update_type='momentum', update_kwargs={'theta': self._cfg.learn.target_theta})\n    else:\n        raise RuntimeError('DQN needs target network, please either indicate target_update_freq or target_theta')\n    self._learn_model = model_wrap(self._model, wrapper_name='argmax_sample')\n    self._learn_model.reset()\n    self._target_model.reset()"
        ]
    },
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\n        Arguments:\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\n        Returns:\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\n\n        .. note::\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\n\n        .. note::\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\n        \"\"\"\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}",
        "mutated": [
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\\n        Arguments:\\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\\n\\n        .. note::\\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\\n        '\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\\n        Arguments:\\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\\n\\n        .. note::\\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\\n        '\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\\n        Arguments:\\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\\n\\n        .. note::\\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\\n        '\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\\n        Arguments:\\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\\n\\n        .. note::\\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\\n        '\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}",
            "def _forward_learn(self, data: Dict[str, Any]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.\\n        Arguments:\\n            - data (:obj:`List[Dict[int, Any]]`): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the ``_forward_learn`` method, data often need to first be stacked in the batch                 dimension by some utility functions such as ``default_preprocess_learn``.                 For MDQN, each element in list is a dict containing at least the following keys: ``obs``, ``action``,                 ``reward``, ``next_obs``, ``done``. Sometimes, it also contains other keys such as ``weight``                 and ``value_gamma``.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of ``_monitor_vars_learn`` method.\\n\\n        .. note::\\n            The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.\\n\\n        .. note::\\n            For more detailed examples, please refer to our unittest for MDQNPolicy: ``ding.policy.tests.test_mdqn``.\\n        '\n    data = default_preprocess_learn(data, use_priority=self._priority, use_priority_IS_weight=self._cfg.priority_IS_weight, ignore_done=self._cfg.learn.ignore_done, use_nstep=True)\n    if self._cuda:\n        data = to_device(data, self._device)\n    self._learn_model.train()\n    self._target_model.train()\n    q_value = self._learn_model.forward(data['obs'])['logit']\n    with torch.no_grad():\n        target_q_value_current = self._target_model.forward(data['obs'])['logit']\n        target_q_value = self._target_model.forward(data['next_obs'])['logit']\n    data_m = m_q_1step_td_data(q_value, target_q_value_current, target_q_value, data['action'], data['reward'].squeeze(0), data['done'], data['weight'])\n    (loss, td_error_per_sample, action_gap, clipfrac) = m_q_1step_td_error(data_m, self._gamma, self._entropy_tau, self._m_alpha)\n    self._optimizer.zero_grad()\n    loss.backward()\n    if self._cfg.multi_gpu:\n        self.sync_gradients(self._learn_model)\n    self._optimizer.step()\n    self._target_model.update(self._learn_model.state_dict())\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss.item(), 'q_value': q_value.mean().item(), 'target_q_value': target_q_value.mean().item(), 'priority': td_error_per_sample.abs().tolist(), 'action_gap': action_gap.item(), 'clip_frac': clipfrac.mean().item()}"
        ]
    },
    {
        "func_name": "_monitor_vars_learn",
        "original": "def _monitor_vars_learn(self) -> List[str]:\n    \"\"\"\n        Overview:\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\n        Returns:\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\n        \"\"\"\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']",
        "mutated": [
            "def _monitor_vars_learn(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\\n        Returns:\\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\\n        '\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']",
            "def _monitor_vars_learn(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\\n        Returns:\\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\\n        '\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']",
            "def _monitor_vars_learn(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\\n        Returns:\\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\\n        '\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']",
            "def _monitor_vars_learn(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\\n        Returns:\\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\\n        '\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']",
            "def _monitor_vars_learn(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Return the necessary keys for logging the return dict of ``self._forward_learn``. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.\\n        Returns:\\n            - necessary_keys (:obj:`List[str]`): The list of the necessary keys to be logged.\\n        '\n    return ['cur_lr', 'total_loss', 'q_value', 'action_gap', 'clip_frac']"
        ]
    }
]