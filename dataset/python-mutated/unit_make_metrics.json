[
    {
        "func_name": "pyunit_make_metrics",
        "original": "def pyunit_make_metrics(weights_col=None):\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05",
        "mutated": [
            "def pyunit_make_metrics(weights_col=None):\n    if False:\n        i = 10\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05",
            "def pyunit_make_metrics(weights_col=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05",
            "def pyunit_make_metrics(weights_col=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05",
            "def pyunit_make_metrics(weights_col=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05",
            "def pyunit_make_metrics(weights_col=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    fr = h2o.import_file(pyunit_utils.locate('smalldata/logreg/prostate.csv'))\n    fr['CAPSULE'] = fr['CAPSULE'].asfactor()\n    fr['RACE'] = fr['RACE'].asfactor()\n    fr.describe()\n    response = 'AGE'\n    predictors = list(set(fr.names) - {'ID', response})\n    weights = None\n    if weights_col:\n        weights = h2o.assign(fr.runif(42), 'weights')\n        fr[weights_col] = weights\n    print('\\n\\n======= REGRESSION ========\\n')\n    for distr in ['gaussian', 'poisson', 'laplace', 'gamma']:\n        if weights_col is not None and distr == 'laplace':\n            continue\n        print('distribution: %s' % distr)\n        model = H2OGradientBoostingEstimator(distribution=distr, ntrees=2, max_depth=3, min_rows=1, learn_rate=0.1, nbins=20, weights_column=weights_col)\n        model.train(x=predictors, y=response, training_frame=fr)\n        predicted = h2o.assign(model.predict(fr), 'pred')\n        actual = fr[response]\n        m0 = model.model_performance(train=True)\n        m1 = h2o.make_metrics(predicted, actual, distribution=distr, weights=weights)\n        m2 = h2o.make_metrics(predicted, actual, weights=weights)\n        print('model performance:')\n        print(m0)\n        print('make_metrics (distribution=%s):' % distr)\n        print(m1)\n        print('make_metrics (distribution=None):')\n        print(m2)\n        assert abs(m0.mae() - m1.mae()) < 1e-05\n        assert abs(m0.mse() - m1.mse()) < 1e-05\n        assert abs(m0.rmse() - m1.rmse()) < 1e-05\n        assert abs(m0.mean_residual_deviance() - m1.mean_residual_deviance()) < 1e-05\n        assert abs(m0.rmsle() - m1.rmsle()) < 1e-05\n        assert abs(m2.mae() - m1.mae()) < 1e-05\n        assert abs(m2.mse() - m1.mse()) < 1e-05\n        assert abs(m2.rmse() - m1.rmse()) < 1e-05\n        assert (abs(m1.mean_residual_deviance() - m2.mean_residual_deviance()) < 1e-07) == (distr == 'gaussian')\n        assert abs(m2.rmsle() - m1.rmsle()) < 1e-05\n    print('\\n\\n======= BINOMIAL ========\\n')\n    response = 'CAPSULE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='bernoulli', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, seed=1, weights_column=weights_col)\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[2], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = ['0', '1']\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights)\n    m2 = h2o.make_metrics(predicted, actual, weights=weights)\n    print('m0:')\n    print(m0)\n    print('m1:')\n    print(m1)\n    print('m2:')\n    print(m2)\n    assert m0.accuracy()[0][1] + m0.error()[0][1] == 1\n    assert len(m0.accuracy(thresholds='all')) == len(m0.fprs)\n    assert m0.accuracy().value == m1.accuracy().value == m0.accuracy()[0][1]\n    assert m0.accuracy().value + m0.error().value == 1\n    assert isinstance(m0.accuracy(thresholds=0.4).value, float)\n    assert m0.accuracy(thresholds=0.4).value == m1.accuracy(thresholds=0.4).value == m0.accuracy(thresholds=0.4)[0][1]\n    assert m0.accuracy(thresholds=0.4).value + m0.error(thresholds=0.4).value == 1\n    assert isinstance(m0.accuracy(thresholds=[0.4]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4]).value) == 1\n    assert m0.accuracy(thresholds=[0.4]).value[0] == m0.accuracy(thresholds=0.4).value\n    assert isinstance(m0.accuracy(thresholds=[0.4, 0.5]).value, list)\n    assert len(m0.accuracy(thresholds=[0.4, 0.5]).value) == 2\n    assert m0.accuracy(thresholds=[0.4, 0.5]).value == [m0.accuracy(thresholds=0.4).value, m0.accuracy(thresholds=0.5).value]\n    base_metrics_methods_failing_on_H2OBinomialModelMetrics = ['aic', 'loglikelihood', 'mae', 'mean_per_class_error', 'mean_residual_deviance', 'rmsle']\n    for metric_method in (m for m in base_metric_methods if m not in base_metrics_methods_failing_on_H2OBinomialModelMetrics):\n        m0mm = getattr(m0, metric_method)()\n        m1mm = getattr(m1, metric_method)()\n        m2mm = getattr(m2, metric_method)()\n        assert m0mm == m1mm or abs(m0mm - m1mm) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm - m2mm) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    assert abs(m0.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    assert abs(m2.mean_per_class_error()[0][1] - m1.mean_per_class_error()[0][1]) < 1e-05\n    failures = 0\n    for metric_method in base_metrics_methods_failing_on_H2OBinomialModelMetrics:\n        for m in [m0, m1, m2]:\n            try:\n                assert isinstance(getattr(m, metric_method)(), float)\n            except:\n                failures += 1\n    assert failures == 3 * len(base_metrics_methods_failing_on_H2OBinomialModelMetrics)\n    binomial_only_metric_methods = ['accuracy', 'F0point5', 'F1', 'F2', 'mcc', 'max_per_class_error', 'mean_per_class_error', 'precision', 'recall', 'specificity', 'fallout', 'missrate', 'sensitivity', 'fpr', 'fnr', 'tpr', 'tnr']\n    for metric_method in (m for m in binomial_only_metric_methods):\n        m0mm = getattr(m0, metric_method)()[0]\n        m1mm = getattr(m1, metric_method)()[0]\n        m2mm = getattr(m2, metric_method)()[0]\n        assert m0mm == m1mm or abs(m0mm[1] - m1mm[1]) < 1e-05, '{} is different for model_performance and make_metrics on [0, 1] domain'.format(metric_method)\n        assert m1mm == m2mm or abs(m1mm[1] - m2mm[1]) < 1e-05, '{} is different for make_metrics on [0, 1] domain and make_metrics without domain'.format(metric_method)\n    cm0 = m0.confusion_matrix(metrics=max_metrics)\n    assert len(cm0) == len(max_metrics)\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    cm0t = m0.confusion_matrix(metrics=max_metrics, thresholds=[0.3, 0.6])\n    assert len(cm0t) == 2 + len(max_metrics)\n    assert 2 == sum([not any((m in header for m in max_metrics)) for header in map(lambda cm: cm.table._table_header, cm0t)]), 'missing or duplicate headers without metric (thresholds only CMs)'\n    assert all([any((m in header for header in map(lambda cm: cm.table._table_header, cm0t) for m in max_metrics))]), 'got duplicate CM headers, although all metrics are different'\n    print('\\n\\n======= MULTINOMIAL ========\\n')\n    response = 'RACE'\n    predictors = list(set(fr.names) - {'ID', response})\n    model = H2OGradientBoostingEstimator(distribution='multinomial', ntrees=2, max_depth=3, min_rows=1, learn_rate=0.01, nbins=20, weights_column=weights_col, auc_type='MACRO_OVR')\n    model.train(x=predictors, y=response, training_frame=fr)\n    predicted = h2o.assign(model.predict(fr)[1:], 'pred')\n    actual = h2o.assign(fr[response].asfactor(), 'act')\n    domain = fr[response].levels()[0]\n    m0 = model.model_performance(train=True)\n    m1 = h2o.make_metrics(predicted, actual, domain=domain, weights=weights, auc_type='MACRO_OVR')\n    m2 = h2o.make_metrics(predicted, actual, weights=weights, auc_type='MACRO_OVR')\n    assert abs(m0.mse() - m1.mse()) < 1e-05\n    assert abs(m0.rmse() - m1.rmse()) < 1e-05\n    assert abs(m0.logloss() - m1.logloss()) < 1e-05\n    assert abs(m0.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m0.auc() - m1.auc()) < 1e-05\n    assert abs(m0.aucpr() - m1.aucpr()) < 1e-05\n    assert abs(m2.mse() - m1.mse()) < 1e-05\n    assert abs(m2.rmse() - m1.rmse()) < 1e-05\n    assert abs(m2.logloss() - m1.logloss()) < 1e-05\n    assert abs(m2.mean_per_class_error() - m1.mean_per_class_error()) < 1e-05\n    assert abs(m2.auc() - m1.auc()) < 1e-05\n    assert abs(m2.aucpr() - m1.aucpr()) < 1e-05"
        ]
    },
    {
        "func_name": "pyunit_make_metrics_uplift",
        "original": "def pyunit_make_metrics_uplift():\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')",
        "mutated": [
            "def pyunit_make_metrics_uplift():\n    if False:\n        i = 10\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')",
            "def pyunit_make_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')",
            "def pyunit_make_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')",
            "def pyunit_make_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')",
            "def pyunit_make_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('======= UPLIFT BINOMIAL ========')\n    treatment_column = 'treatment'\n    response_column = 'outcome'\n    feature_cols = ['feature_' + str(x) for x in range(1, 13)]\n    train = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_train.csv'))\n    train[treatment_column] = train[treatment_column].asfactor()\n    train[response_column] = train[response_column].asfactor()\n    test = h2o.import_file(pyunit_utils.locate('smalldata/uplift/upliftml_test.csv'))\n    test[treatment_column] = test[treatment_column].asfactor()\n    test[response_column] = test[response_column].asfactor()\n    nbins = 20\n    model = H2OUpliftRandomForestEstimator(treatment_column=treatment_column, seed=42, auuc_nbins=nbins, score_each_iteration=True, ntrees=3)\n    model.train(y=response_column, x=feature_cols, training_frame=train, validation_frame=test)\n    m0 = model.model_performance(valid=True)\n    predicted = h2o.assign(model.predict(test)[0], 'pred')\n    actual = test[response_column]\n    treatment = test[treatment_column]\n    m1 = model.model_performance(test_data=test, auuc_type='AUTO')\n    m2 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=nbins)\n    m3 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=m1.thresholds())\n    m4 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', custom_auuc_thresholds=model.default_auuc_thresholds())\n    new_nbins = nbins - 10\n    m5 = h2o.make_metrics(predicted, actual, treatment=treatment, auuc_type='AUTO', auuc_nbins=new_nbins)\n    print('Model AUUC: {}'.format(model.auuc()))\n    print('thresholds: {}'.format(model.default_auuc_thresholds()))\n    print('Model performance AUUC: {}'.format(m0.auuc()))\n    print('thresholds: {}'.format(m0.thresholds()))\n    print('Model performance AUUC: {}'.format(m1.auuc()))\n    print('thresholds: {}'.format(m1.thresholds()))\n    print('Make AUUC with no custom thresholds: {}'.format(m2.auuc()))\n    print('thresholds: {}'.format(m2.thresholds()))\n    print('Make AUUC with custom thresholds from m1: {}'.format(m3.auuc()))\n    print('thresholds: {}'.format(m3.thresholds()))\n    print('Make AUUC with custom thresholds from model defaults: {}'.format(m4.auuc()))\n    print('thresholds: {}'.format(m4.thresholds()))\n    print('Make AUUC with no custom thresholds but change nbins parameter: {}'.format(m5.auuc()))\n    print('thresholds: {}'.format(m5.thresholds()))\n    tol = 1e-05\n    assert abs(model.auuc() - m0.auuc()) > tol\n    assert abs(m0.auuc() - m1.auuc()) < tol\n    assert abs(m1.auuc() - m2.auuc()) < tol\n    assert abs(m1.auuc() - m3.auuc()) < tol\n    assert abs(m3.auuc() - m5.auuc()) > tol\n    print('===========================')"
        ]
    },
    {
        "func_name": "test_model_metrics_basic",
        "original": "def test_model_metrics_basic():\n    pyunit_make_metrics()",
        "mutated": [
            "def test_model_metrics_basic():\n    if False:\n        i = 10\n    pyunit_make_metrics()",
            "def test_model_metrics_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pyunit_make_metrics()",
            "def test_model_metrics_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pyunit_make_metrics()",
            "def test_model_metrics_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pyunit_make_metrics()",
            "def test_model_metrics_basic():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pyunit_make_metrics()"
        ]
    },
    {
        "func_name": "test_model_metrics_weights",
        "original": "def test_model_metrics_weights():\n    pyunit_make_metrics(weights_col='weights')",
        "mutated": [
            "def test_model_metrics_weights():\n    if False:\n        i = 10\n    pyunit_make_metrics(weights_col='weights')",
            "def test_model_metrics_weights():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pyunit_make_metrics(weights_col='weights')",
            "def test_model_metrics_weights():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pyunit_make_metrics(weights_col='weights')",
            "def test_model_metrics_weights():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pyunit_make_metrics(weights_col='weights')",
            "def test_model_metrics_weights():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pyunit_make_metrics(weights_col='weights')"
        ]
    },
    {
        "func_name": "test_model_metrics_uplift",
        "original": "def test_model_metrics_uplift():\n    pyunit_make_metrics_uplift()",
        "mutated": [
            "def test_model_metrics_uplift():\n    if False:\n        i = 10\n    pyunit_make_metrics_uplift()",
            "def test_model_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pyunit_make_metrics_uplift()",
            "def test_model_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pyunit_make_metrics_uplift()",
            "def test_model_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pyunit_make_metrics_uplift()",
            "def test_model_metrics_uplift():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pyunit_make_metrics_uplift()"
        ]
    },
    {
        "func_name": "suite_model_metrics",
        "original": "def suite_model_metrics():\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]",
        "mutated": [
            "def suite_model_metrics():\n    if False:\n        i = 10\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]",
            "def suite_model_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]",
            "def suite_model_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]",
            "def suite_model_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]",
            "def suite_model_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def test_model_metrics_basic():\n        pyunit_make_metrics()\n\n    def test_model_metrics_weights():\n        pyunit_make_metrics(weights_col='weights')\n\n    def test_model_metrics_uplift():\n        pyunit_make_metrics_uplift()\n    return [test_model_metrics_basic, test_model_metrics_weights, test_model_metrics_uplift]"
        ]
    }
]