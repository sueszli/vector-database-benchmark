[
    {
        "func_name": "__init__",
        "original": "def __init__(self, V, D, K, activation=tf.tanh):\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation",
        "mutated": [
            "def __init__(self, V, D, K, activation=tf.tanh):\n    if False:\n        i = 10\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation",
            "def __init__(self, V, D, K, activation=tf.tanh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation",
            "def __init__(self, V, D, K, activation=tf.tanh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation",
            "def __init__(self, V, D, K, activation=tf.tanh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation",
            "def __init__(self, V, D, K, activation=tf.tanh):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.V = V\n    self.D = D\n    self.K = K\n    self.f = activation"
        ]
    },
    {
        "func_name": "dot1",
        "original": "def dot1(a, B):\n    return tf.tensordot(a, B, axes=[[0], [1]])",
        "mutated": [
            "def dot1(a, B):\n    if False:\n        i = 10\n    return tf.tensordot(a, B, axes=[[0], [1]])",
            "def dot1(a, B):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return tf.tensordot(a, B, axes=[[0], [1]])",
            "def dot1(a, B):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return tf.tensordot(a, B, axes=[[0], [1]])",
            "def dot1(a, B):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return tf.tensordot(a, B, axes=[[0], [1]])",
            "def dot1(a, B):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return tf.tensordot(a, B, axes=[[0], [1]])"
        ]
    },
    {
        "func_name": "dot2",
        "original": "def dot2(B, a):\n    return tf.tensordot(B, a, axes=[[1], [0]])",
        "mutated": [
            "def dot2(B, a):\n    if False:\n        i = 10\n    return tf.tensordot(B, a, axes=[[1], [0]])",
            "def dot2(B, a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return tf.tensordot(B, a, axes=[[1], [0]])",
            "def dot2(B, a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return tf.tensordot(B, a, axes=[[1], [0]])",
            "def dot2(B, a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return tf.tensordot(B, a, axes=[[1], [0]])",
            "def dot2(B, a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return tf.tensordot(B, a, axes=[[1], [0]])"
        ]
    },
    {
        "func_name": "recursive_net_transform",
        "original": "def recursive_net_transform(hiddens, n):\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)",
        "mutated": [
            "def recursive_net_transform(hiddens, n):\n    if False:\n        i = 10\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)",
            "def recursive_net_transform(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)",
            "def recursive_net_transform(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)",
            "def recursive_net_transform(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)",
            "def recursive_net_transform(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    h_left = hiddens.read(left_children[n])\n    h_right = hiddens.read(right_children[n])\n    return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)"
        ]
    },
    {
        "func_name": "recurrence",
        "original": "def recurrence(hiddens, n):\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)",
        "mutated": [
            "def recurrence(hiddens, n):\n    if False:\n        i = 10\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)",
            "def recurrence(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)",
            "def recurrence(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)",
            "def recurrence(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)",
            "def recurrence(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    w = words[n]\n    h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n    hiddens = hiddens.write(n, h_n)\n    n = tf.add(n, 1)\n    return (hiddens, n)"
        ]
    },
    {
        "func_name": "condition",
        "original": "def condition(hiddens, n):\n    return tf.less(n, tf.shape(input=words)[0])",
        "mutated": [
            "def condition(hiddens, n):\n    if False:\n        i = 10\n    return tf.less(n, tf.shape(input=words)[0])",
            "def condition(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return tf.less(n, tf.shape(input=words)[0])",
            "def condition(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return tf.less(n, tf.shape(input=words)[0])",
            "def condition(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return tf.less(n, tf.shape(input=words)[0])",
            "def condition(hiddens, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return tf.less(n, tf.shape(input=words)[0])"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()",
        "mutated": [
            "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    if False:\n        i = 10\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, trees, test_trees, reg=0.001, epochs=8, train_inner_nodes=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    D = self.D\n    V = self.V\n    K = self.K\n    N = len(trees)\n    We = init_weight(V, D)\n    W11 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W22 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W12 = np.random.randn(D, D, D) / np.sqrt(3 * D)\n    W1 = init_weight(D, D)\n    W2 = init_weight(D, D)\n    bh = np.zeros(D)\n    Wo = init_weight(D, K)\n    bo = np.zeros(K)\n    self.We = tf.Variable(We.astype(np.float32))\n    self.W11 = tf.Variable(W11.astype(np.float32))\n    self.W22 = tf.Variable(W22.astype(np.float32))\n    self.W12 = tf.Variable(W12.astype(np.float32))\n    self.W1 = tf.Variable(W1.astype(np.float32))\n    self.W2 = tf.Variable(W2.astype(np.float32))\n    self.bh = tf.Variable(bh.astype(np.float32))\n    self.Wo = tf.Variable(Wo.astype(np.float32))\n    self.bo = tf.Variable(bo.astype(np.float32))\n    self.weights = [self.We, self.W11, self.W22, self.W12, self.W1, self.W2, self.Wo]\n    words = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='words')\n    left_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='left_children')\n    right_children = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='right_children')\n    labels = tf.compat.v1.placeholder(tf.int32, shape=(None,), name='labels')\n    self.words = words\n    self.left = left_children\n    self.right = right_children\n    self.labels = labels\n\n    def dot1(a, B):\n        return tf.tensordot(a, B, axes=[[0], [1]])\n\n    def dot2(B, a):\n        return tf.tensordot(B, a, axes=[[1], [0]])\n\n    def recursive_net_transform(hiddens, n):\n        h_left = hiddens.read(left_children[n])\n        h_right = hiddens.read(right_children[n])\n        return self.f(dot1(h_left, dot2(self.W11, h_left)) + dot1(h_right, dot2(self.W22, h_right)) + dot1(h_left, dot2(self.W12, h_right)) + dot1(h_left, self.W1) + dot1(h_right, self.W2) + self.bh)\n\n    def recurrence(hiddens, n):\n        w = words[n]\n        h_n = tf.cond(pred=w >= 0, true_fn=lambda : tf.nn.embedding_lookup(params=self.We, ids=w), false_fn=lambda : recursive_net_transform(hiddens, n))\n        hiddens = hiddens.write(n, h_n)\n        n = tf.add(n, 1)\n        return (hiddens, n)\n\n    def condition(hiddens, n):\n        return tf.less(n, tf.shape(input=words)[0])\n    hiddens = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False)\n    (hiddens, _) = tf.while_loop(cond=condition, body=recurrence, loop_vars=[hiddens, tf.constant(0)], parallel_iterations=1)\n    h = hiddens.stack()\n    logits = tf.matmul(h, self.Wo) + self.bo\n    prediction_op = tf.argmax(input=logits, axis=1)\n    self.prediction_op = prediction_op\n    rcost = reg * sum((tf.nn.l2_loss(p) for p in self.weights))\n    if train_inner_nodes:\n        labeled_indices = tf.compat.v1.where(labels >= 0)\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.gather(logits, labeled_indices), labels=tf.gather(labels, labeled_indices))) + rcost\n    else:\n        cost_op = tf.reduce_mean(input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits[-1], labels=labels[-1])) + rcost\n    train_op = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.008).minimize(cost_op)\n    self.session = tf.compat.v1.Session()\n    init_op = tf.compat.v1.global_variables_initializer()\n    self.session.run(init_op)\n    costs = []\n    sequence_indexes = range(N)\n    for i in range(epochs):\n        t0 = datetime.now()\n        sequence_indexes = shuffle(sequence_indexes)\n        n_correct = 0\n        n_total = 0\n        cost = 0\n        it = 0\n        for j in sequence_indexes:\n            (words_, left, right, lab) = trees[j]\n            (c, p, _) = self.session.run((cost_op, prediction_op, train_op), feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            if np.isnan(c):\n                print(\"Cost is nan! Let's stop here.                         Why don't you try decreasing the learning rate?\")\n                for p in self.params:\n                    print(p.get_value().sum())\n                exit()\n            cost += c\n            n_correct += p[-1] == lab[-1]\n            n_total += 1\n            it += 1\n            if it % 10 == 0:\n                sys.stdout.write('j/N: %d/%d correct rate so far: %f, cost so far: %f\\r' % (it, N, float(n_correct) / n_total, cost))\n                sys.stdout.flush()\n        n_test_correct = 0\n        n_test_total = 0\n        for (words_, left, right, lab) in test_trees:\n            p = self.session.run(prediction_op, feed_dict={words: words_, left_children: left, right_children: right, labels: lab})\n            n_test_correct += p[-1] == lab[-1]\n            n_test_total += 1\n        print('i:', i, 'cost:', cost, 'train acc:', float(n_correct) / n_total, 'test acc:', float(n_test_correct) / n_test_total, 'time for epoch:', datetime.now() - t0)\n        costs.append(cost)\n    plt.plot(costs)\n    plt.show()"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, words, left, right, lab):\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})",
        "mutated": [
            "def predict(self, words, left, right, lab):\n    if False:\n        i = 10\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})",
            "def predict(self, words, left, right, lab):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})",
            "def predict(self, words, left, right, lab):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})",
            "def predict(self, words, left, right, lab):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})",
            "def predict(self, words, left, right, lab):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.session.run(self.prediction_op, feed_dict={self.words: words, self.left: left, self.right: right, self.labels: lab})"
        ]
    },
    {
        "func_name": "score",
        "original": "def score(self, trees):\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total",
        "mutated": [
            "def score(self, trees):\n    if False:\n        i = 10\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total",
            "def score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total",
            "def score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total",
            "def score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total",
            "def score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    n_total = len(trees)\n    n_correct = 0\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        n_correct += p[-1] == lab[-1]\n    return float(n_correct) / n_total"
        ]
    },
    {
        "func_name": "f1_score",
        "original": "def f1_score(self, trees):\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()",
        "mutated": [
            "def f1_score(self, trees):\n    if False:\n        i = 10\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()",
            "def f1_score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()",
            "def f1_score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()",
            "def f1_score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()",
            "def f1_score(self, trees):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    Y = []\n    P = []\n    for (words, left, right, lab) in trees:\n        p = self.predict(words, left, right, lab)\n        Y.append(lab[-1])\n        P.append(p[-1])\n    return f1_score(Y, P, average=None).mean()"
        ]
    },
    {
        "func_name": "add_idx_to_tree",
        "original": "def add_idx_to_tree(tree, current_idx):\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx",
        "mutated": [
            "def add_idx_to_tree(tree, current_idx):\n    if False:\n        i = 10\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx",
            "def add_idx_to_tree(tree, current_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx",
            "def add_idx_to_tree(tree, current_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx",
            "def add_idx_to_tree(tree, current_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx",
            "def add_idx_to_tree(tree, current_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tree is None:\n        return current_idx\n    current_idx = add_idx_to_tree(tree.left, current_idx)\n    current_idx = add_idx_to_tree(tree.right, current_idx)\n    tree.idx = current_idx\n    current_idx += 1\n    return current_idx"
        ]
    },
    {
        "func_name": "tree2list",
        "original": "def tree2list(tree, parent_idx, is_binary=False):\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)",
        "mutated": [
            "def tree2list(tree, parent_idx, is_binary=False):\n    if False:\n        i = 10\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)",
            "def tree2list(tree, parent_idx, is_binary=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)",
            "def tree2list(tree, parent_idx, is_binary=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)",
            "def tree2list(tree, parent_idx, is_binary=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)",
            "def tree2list(tree, parent_idx, is_binary=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tree is None:\n        return ([], [], [], [])\n    (words_left, left_child_left, right_child_left, labels_left) = tree2list(tree.left, tree.idx, is_binary)\n    (words_right, left_child_right, right_child_right, labels_right) = tree2list(tree.right, tree.idx, is_binary)\n    if tree.word is None:\n        w = -1\n        left = tree.left.idx\n        right = tree.right.idx\n    else:\n        w = tree.word\n        left = -1\n        right = -1\n    words = words_left + words_right + [w]\n    left_child = left_child_left + left_child_right + [left]\n    right_child = right_child_left + right_child_right + [right]\n    if is_binary:\n        if tree.label > 2:\n            label = 1\n        elif tree.label < 2:\n            label = 0\n        else:\n            label = -1\n    else:\n        label = tree.label\n    labels = labels_left + labels_right + [label]\n    return (words, left_child, right_child, labels)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(is_binary=True):\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))",
        "mutated": [
            "def main(is_binary=True):\n    if False:\n        i = 10\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))",
            "def main(is_binary=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))",
            "def main(is_binary=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))",
            "def main(is_binary=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))",
            "def main(is_binary=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train, test, word2idx) = get_ptb_data()\n    for t in train:\n        add_idx_to_tree(t, 0)\n    train = [tree2list(t, -1, is_binary) for t in train]\n    if is_binary:\n        train = [t for t in train if t[3][-1] >= 0]\n    for t in test:\n        add_idx_to_tree(t, 0)\n    test = [tree2list(t, -1, is_binary) for t in test]\n    if is_binary:\n        test = [t for t in test if t[3][-1] >= 0]\n    train = shuffle(train)\n    test = shuffle(test)\n    smalltest = test[:1000]\n    V = len(word2idx)\n    print('vocab size:', V)\n    D = 10\n    K = 2 if is_binary else 5\n    model = RecursiveNN(V, D, K)\n    model.fit(train, smalltest, reg=0.001, epochs=20, train_inner_nodes=True)\n    print('train accuracy:', model.score(train))\n    print('test accuracy:', model.score(test))\n    print('train f1:', model.f1_score(train))\n    print('test f1:', model.f1_score(test))"
        ]
    }
]