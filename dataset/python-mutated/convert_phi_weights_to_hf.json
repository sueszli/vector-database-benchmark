[
    {
        "func_name": "convert_weights",
        "original": "def convert_weights(original_weights, mapping, config):\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights",
        "mutated": [
            "def convert_weights(original_weights, mapping, config):\n    if False:\n        i = 10\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights",
            "def convert_weights(original_weights, mapping, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights",
            "def convert_weights(original_weights, mapping, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights",
            "def convert_weights(original_weights, mapping, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights",
            "def convert_weights(original_weights, mapping, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n    range_change = {f'layers.{k}.': f'layers.{v}.' for (k, v) in zip(range(1, config.num_hidden_layers + 1), range(0, config.num_hidden_layers))}\n    mapping.update(**range_change)\n    for original_weights_key in original_weights_keys:\n        new_key = original_weights_key\n        if 'rotary_emb' in new_key:\n            continue\n        if 'Wqkv' in new_key:\n            if 'weight' in new_key:\n                weight = original_weights[new_key]\n                weights_shape = weight.shape\n                weight = weight.view(3, config.num_attention_heads, -1, config.hidden_size).transpose(0, 1).reshape(*weights_shape)\n                original_weights[new_key] = weight\n            elif 'bias' in new_key:\n                bias = original_weights[new_key]\n                bias_shape = bias.shape\n                bias = bias.view(3, config.num_attention_heads, -1).transpose(0, 1).reshape(*bias_shape)\n                original_weights[new_key] = bias\n        for (k, v) in mapping.items():\n            if k in new_key:\n                new_key = new_key.replace(k, v)\n        converted_weights[new_key] = original_weights.pop(original_weights_key)\n    return converted_weights"
        ]
    },
    {
        "func_name": "_download",
        "original": "def _download(url: str, root: str):\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)",
        "mutated": [
            "def _download(url: str, root: str):\n    if False:\n        i = 10\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)",
            "def _download(url: str, root: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)",
            "def _download(url: str, root: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)",
            "def _download(url: str, root: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)",
            "def _download(url: str, root: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    repo_id = f\"{url.split('/')[3]}/{url.split('/')[4]}\"\n    filename = f\"{url.split('/')[-1]}\"\n    hf_hub_download(repo_id=repo_id, filename=filename, force_filename=root, local_dir_use_symlinks=False)"
        ]
    },
    {
        "func_name": "convert_phi_weights",
        "original": "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()",
        "mutated": [
            "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    if False:\n        i = 10\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()",
            "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()",
            "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()",
            "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()",
            "def convert_phi_weights(checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device = 'cuda' if torch.cuda.is_available() and use_cuda else 'cpu'\n    for (each_model_name, each_model_url) in _MODELS.items():\n        converted_checkpoint = {}\n        model_path = os.path.join(checkpoint_path, each_model_name + '_' + each_model_url.split('/')[-1])\n        if not os.path.exists(model_path):\n            print(f'\\n{each_model_name} was not found! Downloading it to {model_path}')\n            _download(url=each_model_url, root=model_path)\n        model_checkpoint = torch.load(model_path, map_location=device)\n        model_type = each_model_name.split('/')[1]\n        config = PhiConfig.from_pretrained(f'susnato/{model_type}_dev')\n        converted_checkpoint.update(**convert_weights(model_checkpoint, PHI_MAPPING, config))\n        if save_weights_directly:\n            save_weights_path = os.path.join(pytorch_dump_folder_path, each_model_name.split('/')[-1] + '_' + each_model_url.split('/')[-1])\n            torch.save(converted_checkpoint, save_weights_path)\n            print(f'Model weights saved at {save_weights_path}!')\n        else:\n            model = PhiForCausalLM(config).to(device)\n            model.load_state_dict(converted_checkpoint, strict=True)\n            save_model_path = os.path.join(pytorch_dump_folder_path, model_type)\n            model.save_pretrained(save_model_path)\n            print(f'Model saved at {save_model_path}!')\n            del config, model\n        del model_checkpoint, converted_checkpoint\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()"
        ]
    }
]