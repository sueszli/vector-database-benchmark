[
    {
        "func_name": "data_integrity",
        "original": "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    \"\"\"Suite for detecting integrity issues within a single dataset.\n\n    List of Checks:\n        .. list-table:: List of Checks\n           :widths: 50 50\n           :header-rows: 1\n\n           * - Check Example\n             - API Reference\n           * - :ref:`tabular__is_single_value`\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\n           * - :ref:`tabular__special_chars`\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\n           * - :ref:`tabular__mixed_nulls`\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\n           * - :ref:`tabular__mixed_data_types`\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\n           * - :ref:`tabular__string_mismatch`\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\n           * - :ref:`tabular__data_duplicates`\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\n           * - :ref:`tabular__string_length_out_of_bounds`\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\n           * - :ref:`tabular__conflicting_labels`\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\n           * - :ref:`tabular__outlier_sample_detection`\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\n           * - :ref:`tabular__feature_label_correlation`\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\n           * - :ref:`tabular__identifier_label_correlation`\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\n           * - :ref:`tabular__feature_feature_correlation`\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\n\n    Parameters\n    ----------\n    columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be ignored. If None, no columns will be ignored.\n    n_top_columns : int , optional\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\n    n_samples : int , default: 1_000_000\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\n    random_state : int, default: 42\n        random seed for all checks.\n    n_to_show : int , default: 5\n        number of top results to show (check dependent)\n    **kwargs : dict\n        additional arguments to pass to the checks.\n\n    Returns\n    -------\n    Suite\n        A suite for detecting integrity issues within a single dataset.\n\n    Examples\n    --------\n    >>> from deepchecks.tabular.suites import data_integrity\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\n    >>> result = suite.run()\n    >>> result.show()\n\n    See Also\n    --------\n    :ref:`quick_data_integrity`\n    \"\"\"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())",
        "mutated": [
            "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n    \"Suite for detecting integrity issues within a single dataset.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__is_single_value`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\\n           * - :ref:`tabular__special_chars`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\\n           * - :ref:`tabular__mixed_nulls`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\\n           * - :ref:`tabular__mixed_data_types`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__data_duplicates`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\\n           * - :ref:`tabular__string_length_out_of_bounds`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\\n           * - :ref:`tabular__conflicting_labels`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\\n           * - :ref:`tabular__outlier_sample_detection`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__identifier_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for detecting integrity issues within a single dataset.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import data_integrity\\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_data_integrity`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())",
            "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Suite for detecting integrity issues within a single dataset.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__is_single_value`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\\n           * - :ref:`tabular__special_chars`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\\n           * - :ref:`tabular__mixed_nulls`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\\n           * - :ref:`tabular__mixed_data_types`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__data_duplicates`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\\n           * - :ref:`tabular__string_length_out_of_bounds`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\\n           * - :ref:`tabular__conflicting_labels`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\\n           * - :ref:`tabular__outlier_sample_detection`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__identifier_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for detecting integrity issues within a single dataset.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import data_integrity\\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_data_integrity`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())",
            "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Suite for detecting integrity issues within a single dataset.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__is_single_value`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\\n           * - :ref:`tabular__special_chars`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\\n           * - :ref:`tabular__mixed_nulls`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\\n           * - :ref:`tabular__mixed_data_types`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__data_duplicates`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\\n           * - :ref:`tabular__string_length_out_of_bounds`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\\n           * - :ref:`tabular__conflicting_labels`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\\n           * - :ref:`tabular__outlier_sample_detection`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__identifier_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for detecting integrity issues within a single dataset.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import data_integrity\\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_data_integrity`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())",
            "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Suite for detecting integrity issues within a single dataset.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__is_single_value`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\\n           * - :ref:`tabular__special_chars`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\\n           * - :ref:`tabular__mixed_nulls`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\\n           * - :ref:`tabular__mixed_data_types`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__data_duplicates`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\\n           * - :ref:`tabular__string_length_out_of_bounds`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\\n           * - :ref:`tabular__conflicting_labels`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\\n           * - :ref:`tabular__outlier_sample_detection`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__identifier_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for detecting integrity issues within a single dataset.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import data_integrity\\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_data_integrity`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())",
            "def data_integrity(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Suite for detecting integrity issues within a single dataset.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__is_single_value`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IsSingleValue`\\n           * - :ref:`tabular__special_chars`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.SpecialCharacters`\\n           * - :ref:`tabular__mixed_nulls`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedNulls`\\n           * - :ref:`tabular__mixed_data_types`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.MixedDataTypes`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__data_duplicates`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.DataDuplicates`\\n           * - :ref:`tabular__string_length_out_of_bounds`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringLengthOutOfBounds`\\n           * - :ref:`tabular__conflicting_labels`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.ConflictingLabels`\\n           * - :ref:`tabular__outlier_sample_detection`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.OutlierSampleDetection`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__identifier_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.IdentifierLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for detecting integrity issues within a single dataset.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import data_integrity\\n    >>> suite = data_integrity(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_data_integrity`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Data Integrity Suite', IsSingleValue(**kwargs).add_condition_not_single_value(), SpecialCharacters(**kwargs).add_condition_ratio_of_special_characters_less_or_equal(), MixedNulls(**kwargs).add_condition_different_nulls_less_equal_to(), MixedDataTypes(**kwargs).add_condition_rare_type_ratio_not_in_range(), StringMismatch(**kwargs).add_condition_no_variants(), DataDuplicates(**kwargs).add_condition_ratio_less_or_equal(), StringLengthOutOfBounds(**kwargs).add_condition_ratio_of_outliers_less_or_equal(), ConflictingLabels(**kwargs).add_condition_ratio_of_conflicting_labels_less_or_equal(), OutlierSampleDetection(**kwargs), FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than(), FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold(), IdentifierLabelCorrelation(**kwargs).add_condition_pps_less_or_equal())"
        ]
    },
    {
        "func_name": "train_test_validation",
        "original": "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    \"\"\"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\n\n    List of Checks:\n        .. list-table:: List of Checks\n           :widths: 50 50\n           :header-rows: 1\n\n           * - Check Example\n             - API Reference\n           * - :ref:`tabular__datasets_size_comparison`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\n           * - :ref:`tabular__new_label`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\n           * - :ref:`tabular__new_category`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\n           * - :ref:`tabular__string_mismatch_comparison`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\n           * - :ref:`tabular__index_leakage`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\n           * - :ref:`tabular__train_test_samples_mix`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\n           * - :ref:`tabular__feature_label_correlation_change`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\n           * - :ref:`tabular__feature_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\n           * - :ref:`tabular__label_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\n           * - :ref:`tabular__multivariate_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\n\n    Parameters\n    ----------\n    columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be ignored. If None, no columns will be ignored.\n    n_top_columns : int , optional\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\n    n_samples : int , default: None\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\n    random_state : int, default: 42\n        random seed for all checkss.\n    n_to_show : int , default: 5\n        number of top results to show (check dependent)\n    **kwargs : dict\n        additional arguments to pass to the checks.\n\n    Returns\n    -------\n    Suite\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\n\n    Examples\n    --------\n    >>> from deepchecks.tabular.suites import train_test_validation\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\n    >>> result = suite.run()\n    >>> result.show()\n\n    See Also\n    --------\n    :ref:`quick_train_test_validation`\n    \"\"\"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())",
        "mutated": [
            "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n    \"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__datasets_size_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\\n           * - :ref:`tabular__new_label`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\\n           * - :ref:`tabular__new_category`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\\n           * - :ref:`tabular__index_leakage`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\\n           * - :ref:`tabular__train_test_samples_mix`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: None\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checkss.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import train_test_validation\\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_train_test_validation`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())",
            "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__datasets_size_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\\n           * - :ref:`tabular__new_label`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\\n           * - :ref:`tabular__new_category`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\\n           * - :ref:`tabular__index_leakage`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\\n           * - :ref:`tabular__train_test_samples_mix`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: None\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checkss.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import train_test_validation\\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_train_test_validation`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())",
            "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__datasets_size_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\\n           * - :ref:`tabular__new_label`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\\n           * - :ref:`tabular__new_category`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\\n           * - :ref:`tabular__index_leakage`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\\n           * - :ref:`tabular__train_test_samples_mix`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: None\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checkss.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import train_test_validation\\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_train_test_validation`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())",
            "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__datasets_size_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\\n           * - :ref:`tabular__new_label`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\\n           * - :ref:`tabular__new_category`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\\n           * - :ref:`tabular__index_leakage`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\\n           * - :ref:`tabular__train_test_samples_mix`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: None\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checkss.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import train_test_validation\\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_train_test_validation`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())",
            "def train_test_validation(columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Suite for validating correctness of train-test split, including distribution,     leakage and integrity checks.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__datasets_size_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DatasetsSizeComparison`\\n           * - :ref:`tabular__new_label`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.NewLabelTrainTest`\\n           * - :ref:`tabular__new_category`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.CategoryMismatchTrainTest`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__date_train_test_validation_leakage_duplicates`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageDuplicates`\\n           * - :ref:`tabular__date_train_test_validation_leakage_overlap`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.DateTrainTestLeakageOverlap`\\n           * - :ref:`tabular__index_leakage`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.IndexTrainTestLeakage`\\n           * - :ref:`tabular__train_test_samples_mix`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.TrainTestSamplesMix`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n\\n    Parameters\\n    ----------\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: None\\n        number of samples to use for checks that sample data. If none, using the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checkss.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for validating correctness of train-test split, including distribution,         leakage and integrity checks.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import train_test_validation\\n    >>> suite = train_test_validation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_train_test_validation`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Train Test Validation Suite', DatasetsSizeComparison(**kwargs).add_condition_test_train_size_ratio_greater_than(), NewLabelTrainTest(**kwargs).add_condition_new_labels_number_less_or_equal(), NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal(), StringMismatchComparison(**kwargs).add_condition_no_new_variants(), DateTrainTestLeakageDuplicates(**kwargs).add_condition_leakage_ratio_less_or_equal(), DateTrainTestLeakageOverlap(**kwargs).add_condition_leakage_ratio_less_or_equal(), IndexTrainTestLeakage(**kwargs).add_condition_ratio_less_or_equal(), TrainTestSamplesMix(**kwargs).add_condition_duplicates_ratio_less_or_equal(), FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than().add_condition_feature_pps_in_train_less_than(), FeatureDrift(**kwargs).add_condition_drift_score_less_than(), LabelDrift(**kwargs).add_condition_drift_score_less_than(), MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())"
        ]
    },
    {
        "func_name": "model_evaluation",
        "original": "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    \"\"\"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\n\n    List of Checks:\n        .. list-table:: List of Checks\n           :widths: 50 50\n           :header-rows: 1\n\n           * - Check Example\n             - API Reference\n           * - :ref:`tabular__roc_report`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\n           * - :ref:`tabular__confusion_matrix_report`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\n           * - :ref:`tabular__weak_segments_performance`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\n           * - :ref:`tabular__prediction_drift`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\n           * - :ref:`tabular__simple_model_comparison`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\n           * - :ref:`tabular__calibration_score`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\n           * - :ref:`tabular__regression_systematic_error`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\n           * - :ref:`tabular__regression_error_distribution`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\n           * - :ref:`tabular__unused_features`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\n           * - :ref:`tabular__boosting_overfit`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\n           * - :ref:`tabular__model_inference_time`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\n           * - :ref:`tabular__prediction_drift`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\n\n    Parameters\n    ----------\n    alternative_scorers : Dict[str, Callable], default: None\n        An optional dictionary of scorer name to scorer functions.\n        If none given, use default scorers\n    columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be ignored. If None, no columns will be ignored.\n    n_top_columns : int , optional\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\n    n_samples : int , default: 1_000_000\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\n    random_state : int, default: 42\n        random seed for all checks.\n    n_to_show : int , default: 5\n        number of top results to show (check dependent)\n    **kwargs : dict\n        additional arguments to pass to the checks.\n\n    Returns\n    -------\n    Suite\n        A suite for evaluating the model's performance.\n\n    Examples\n    --------\n    >>> from deepchecks.tabular.suites import model_evaluation\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\n    >>> result = suite.run()\n    >>> result.show()\n\n    See Also\n    --------\n    :ref:`quick_full_suite`\n    \"\"\"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())",
        "mutated": [
            "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n    \"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__simple_model_comparison`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\\n           * - :ref:`tabular__calibration_score`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\\n           * - :ref:`tabular__regression_systematic_error`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__unused_features`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\\n           * - :ref:`tabular__boosting_overfit`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\\n           * - :ref:`tabular__model_inference_time`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n\\n    Parameters\\n    ----------\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import model_evaluation\\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())",
            "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__simple_model_comparison`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\\n           * - :ref:`tabular__calibration_score`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\\n           * - :ref:`tabular__regression_systematic_error`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__unused_features`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\\n           * - :ref:`tabular__boosting_overfit`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\\n           * - :ref:`tabular__model_inference_time`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n\\n    Parameters\\n    ----------\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import model_evaluation\\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())",
            "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__simple_model_comparison`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\\n           * - :ref:`tabular__calibration_score`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\\n           * - :ref:`tabular__regression_systematic_error`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__unused_features`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\\n           * - :ref:`tabular__boosting_overfit`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\\n           * - :ref:`tabular__model_inference_time`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n\\n    Parameters\\n    ----------\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import model_evaluation\\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())",
            "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__simple_model_comparison`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\\n           * - :ref:`tabular__calibration_score`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\\n           * - :ref:`tabular__regression_systematic_error`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__unused_features`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\\n           * - :ref:`tabular__boosting_overfit`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\\n           * - :ref:`tabular__model_inference_time`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n\\n    Parameters\\n    ----------\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import model_evaluation\\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())",
            "def model_evaluation(alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Suite for evaluating the model's performance over different metrics, segments, error analysis, examining        overfitting, comparing to baseline, and more.\\n\\n    List of Checks:\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__simple_model_comparison`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SimpleModelComparison`\\n           * - :ref:`tabular__calibration_score`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.CalibrationScore`\\n           * - :ref:`tabular__regression_systematic_error`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionSystematicError`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__unused_features`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.UnusedFeatures`\\n           * - :ref:`tabular__boosting_overfit`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.BoostingOverfit`\\n           * - :ref:`tabular__model_inference_time`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ModelInferenceTime`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n\\n    Parameters\\n    ----------\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import model_evaluation\\n    >>> suite = model_evaluation(columns=['a', 'b', 'c'], n_samples=1_000_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    return Suite('Model Evaluation Suite', TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than(), RocReport(**kwargs).add_condition_auc_greater_than(), ConfusionMatrixReport(**kwargs), PredictionDrift(**kwargs).add_condition_drift_score_less_than(), SimpleModelComparison(**kwargs).add_condition_gain_greater_than(), WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), CalibrationScore(**kwargs), RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than().add_condition_systematic_error_ratio_to_rmse_less_than(), UnusedFeatures(**kwargs).add_condition_number_of_high_variance_unused_features_less_or_equal(), BoostingOverfit(**kwargs).add_condition_test_score_percent_decline_less_than(), ModelInferenceTime(**kwargs).add_condition_inference_time_less_than())"
        ]
    },
    {
        "func_name": "production_suite",
        "original": "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    \"\"\"Suite for testing the model in production.\n\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\n    integrity issues that may occur in production.\n\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\n        .. list-table:: List of Checks\n           :widths: 50 50\n           :header-rows: 1\n\n           * - Check Example\n             - API Reference\n           * - :ref:`tabular__roc_report`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\n           * - :ref:`tabular__confusion_matrix_report`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\n           * - :ref:`tabular__weak_segments_performance`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\n           * - :ref:`tabular__regression_error_distribution`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\n           * - :ref:`tabular__string_mismatch_comparison`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\n           * - :ref:`tabular__feature_label_correlation_change`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\n           * - :ref:`tabular__feature_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\n           * - :ref:`tabular__label_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\n           * - :ref:`tabular__multivariate_drift`\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\n           * - :ref:`tabular__prediction_drift`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\n           * - :ref:`tabular__prediction_drift`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\n           * - :ref:`tabular__string_mismatch`\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\n           * - :ref:`tabular__feature_label_correlation`\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\n           * - :ref:`tabular__feature_feature_correlation`\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\n           * - :ref:`tabular__single_dataset_performance`\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\n\n    Parameters\n    ----------\n    task_type : str, default: None\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\n        classification and regression will be added to the suite.\n    is_comparative : bool, default: True\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\n        checks inspecting the production data only.\n    alternative_scorers : Dict[str, Callable], default: None\n        An optional dictionary of scorer name to scorer functions.\n        If none given, use default scorers\n    columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\n        The columns to be ignored. If None, no columns will be ignored.\n    n_top_columns : int , optional\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\n    n_samples : int , default: 1_000_000\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\n    random_state : int, default: 42\n        random seed for all checks.\n    n_to_show : int , default: 5\n        number of top results to show (check dependent)\n    **kwargs : dict\n        additional arguments to pass to the checks.\n\n    Returns\n    -------\n    Suite\n        A suite for evaluating the model's performance.\n\n    Examples\n    --------\n    >>> from deepchecks.tabular.suites import production_suite\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\n    >>> result = suite.run()\n    >>> result.show()\n\n    See Also\n    --------\n    :ref:`quick_full_suite`\n    \"\"\"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)",
        "mutated": [
            "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n    \"Suite for testing the model in production.\\n\\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\\n    integrity issues that may occur in production.\\n\\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n           * - :ref:`tabular__single_dataset_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\\n\\n    Parameters\\n    ----------\\n    task_type : str, default: None\\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\\n        classification and regression will be added to the suite.\\n    is_comparative : bool, default: True\\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\\n        checks inspecting the production data only.\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import production_suite\\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)",
            "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Suite for testing the model in production.\\n\\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\\n    integrity issues that may occur in production.\\n\\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n           * - :ref:`tabular__single_dataset_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\\n\\n    Parameters\\n    ----------\\n    task_type : str, default: None\\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\\n        classification and regression will be added to the suite.\\n    is_comparative : bool, default: True\\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\\n        checks inspecting the production data only.\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import production_suite\\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)",
            "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Suite for testing the model in production.\\n\\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\\n    integrity issues that may occur in production.\\n\\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n           * - :ref:`tabular__single_dataset_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\\n\\n    Parameters\\n    ----------\\n    task_type : str, default: None\\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\\n        classification and regression will be added to the suite.\\n    is_comparative : bool, default: True\\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\\n        checks inspecting the production data only.\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import production_suite\\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)",
            "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Suite for testing the model in production.\\n\\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\\n    integrity issues that may occur in production.\\n\\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n           * - :ref:`tabular__single_dataset_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\\n\\n    Parameters\\n    ----------\\n    task_type : str, default: None\\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\\n        classification and regression will be added to the suite.\\n    is_comparative : bool, default: True\\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\\n        checks inspecting the production data only.\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import production_suite\\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)",
            "def production_suite(task_type: str=None, is_comparative: bool=True, alternative_scorers: Dict[str, Callable]=None, columns: Union[Hashable, List[Hashable]]=None, ignore_columns: Union[Hashable, List[Hashable]]=None, n_top_columns: int=None, n_samples: int=None, random_state: int=42, n_to_show: int=5, **kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Suite for testing the model in production.\\n\\n    The suite contains checks for evaluating the model's performance. Checks for detecting drift and checks for data\\n    integrity issues that may occur in production.\\n\\n    List of Checks (exact checks depend on the task type and the is_comparative flag):\\n        .. list-table:: List of Checks\\n           :widths: 50 50\\n           :header-rows: 1\\n\\n           * - Check Example\\n             - API Reference\\n           * - :ref:`tabular__roc_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RocReport`\\n           * - :ref:`tabular__confusion_matrix_report`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.ConfusionMatrixReport`\\n           * - :ref:`tabular__weak_segments_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.WeakSegmentPerformance`\\n           * - :ref:`tabular__regression_error_distribution`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.RegressionErrorDistribution`\\n           * - :ref:`tabular__string_mismatch_comparison`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.StringMismatchComparison`\\n           * - :ref:`tabular__feature_label_correlation_change`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureLabelCorrelationChange`\\n           * - :ref:`tabular__feature_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.FeatureDrift`\\n           * - :ref:`tabular__label_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.LabelDrift`\\n           * - :ref:`tabular__multivariate_drift`\\n             - :class:`~deepchecks.tabular.checks.train_test_validation.MultivariateDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__prediction_drift`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.PredictionDrift`\\n           * - :ref:`tabular__string_mismatch`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.StringMismatch`\\n           * - :ref:`tabular__feature_label_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureLabelCorrelation`\\n           * - :ref:`tabular__feature_feature_correlation`\\n             - :class:`~deepchecks.tabular.checks.data_integrity.FeatureFeatureCorrelation`\\n           * - :ref:`tabular__single_dataset_performance`\\n             - :class:`~deepchecks.tabular.checks.model_evaluation.SingleDatasetPerformance`\\n\\n    Parameters\\n    ----------\\n    task_type : str, default: None\\n        The type of the task. Must be one of 'binary', 'multiclass' or 'regression'. If not given, both checks for\\n        classification and regression will be added to the suite.\\n    is_comparative : bool, default: True\\n        Whether to add the checks comparing the production data to some reference data, or if False, to add the\\n        checks inspecting the production data only.\\n    alternative_scorers : Dict[str, Callable], default: None\\n        An optional dictionary of scorer name to scorer functions.\\n        If none given, use default scorers\\n    columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be checked. If None, all columns will be checked except the ones in `ignore_columns`.\\n    ignore_columns : Union[Hashable, List[Hashable]] , default: None\\n        The columns to be ignored. If None, no columns will be ignored.\\n    n_top_columns : int , optional\\n        number of columns to show ordered by feature importance (date, index, label are first) (check dependent)\\n    n_samples : int , default: 1_000_000\\n        number of samples to use for checks that sample data. If none, use the default n_samples per check.\\n    random_state : int, default: 42\\n        random seed for all checks.\\n    n_to_show : int , default: 5\\n        number of top results to show (check dependent)\\n    **kwargs : dict\\n        additional arguments to pass to the checks.\\n\\n    Returns\\n    -------\\n    Suite\\n        A suite for evaluating the model's performance.\\n\\n    Examples\\n    --------\\n    >>> from deepchecks.tabular.suites import production_suite\\n    >>> suite = production_suite(task_type='binary', n_samples=10_000)\\n    >>> result = suite.run()\\n    >>> result.show()\\n\\n    See Also\\n    --------\\n    :ref:`quick_full_suite`\\n    \"\n    args = locals()\n    args.pop('kwargs')\n    non_none_args = {k: v for (k, v) in args.items() if v is not None}\n    kwargs = {**non_none_args, **kwargs}\n    checks = [WeakSegmentsPerformance(**kwargs).add_condition_segments_relative_performance_greater_than(), PercentOfNulls(**kwargs)]\n    regression_checks = [RegressionErrorDistribution(**kwargs).add_condition_kurtosis_greater_than()]\n    classification_checks = [ConfusionMatrixReport(**kwargs), RocReport(**kwargs).add_condition_auc_greater_than()]\n    if task_type is None:\n        checks.extend(classification_checks)\n        checks.extend(regression_checks)\n    elif task_type == TaskType.REGRESSION.value:\n        checks.extend(regression_checks)\n    else:\n        checks.extend(classification_checks)\n    if is_comparative:\n        checks.append(StringMismatchComparison(**kwargs).add_condition_no_new_variants())\n        checks.append(FeatureLabelCorrelationChange(**kwargs).add_condition_feature_pps_difference_less_than())\n        checks.append(FeatureDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(MultivariateDrift(**kwargs).add_condition_overall_drift_value_less_than())\n        checks.append(LabelDrift(ignore_na=True, **kwargs).add_condition_drift_score_less_than())\n        checks.append(PredictionDrift(**kwargs).add_condition_drift_score_less_than())\n        checks.append(TrainTestPerformance(**kwargs).add_condition_train_test_relative_degradation_less_than())\n        checks.append(NewCategoryTrainTest(**kwargs).add_condition_new_category_ratio_less_or_equal())\n    else:\n        checks.append(StringMismatch(**kwargs).add_condition_no_variants())\n        checks.append(FeatureLabelCorrelation(**kwargs).add_condition_feature_pps_less_than())\n        checks.append(FeatureFeatureCorrelation(**kwargs).add_condition_max_number_of_pairs_above_threshold())\n        checks.append(SingleDatasetPerformance(**kwargs))\n    return Suite('Production Suite', *checks)"
        ]
    },
    {
        "func_name": "full_suite",
        "original": "def full_suite(**kwargs) -> Suite:\n    \"\"\"Create a suite that includes many of the implemented checks, for a quick overview of your model and data.\"\"\"\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))",
        "mutated": [
            "def full_suite(**kwargs) -> Suite:\n    if False:\n        i = 10\n    'Create a suite that includes many of the implemented checks, for a quick overview of your model and data.'\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))",
            "def full_suite(**kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create a suite that includes many of the implemented checks, for a quick overview of your model and data.'\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))",
            "def full_suite(**kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create a suite that includes many of the implemented checks, for a quick overview of your model and data.'\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))",
            "def full_suite(**kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create a suite that includes many of the implemented checks, for a quick overview of your model and data.'\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))",
            "def full_suite(**kwargs) -> Suite:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create a suite that includes many of the implemented checks, for a quick overview of your model and data.'\n    return Suite('Full Suite', model_evaluation(**kwargs), train_test_validation(**kwargs), data_integrity(**kwargs))"
        ]
    }
]