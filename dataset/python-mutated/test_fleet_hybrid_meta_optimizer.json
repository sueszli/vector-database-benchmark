[
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp",
        "original": "def test_opt_sharding_with_pp(self):\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
        "mutated": [
            "def test_opt_sharding_with_pp(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_opt_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_opt_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_opt_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])",
            "def test_opt_sharding_with_pp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = False\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    created_ring_ids = [op.desc.attr('ring_id') for op in startup_prog_ops if op.type == 'c_comm_init']\n    self.assertIn(self.dp_ring_id, created_ring_ids)\n    self.assertIn(self.pp_pair_ring_id, created_ring_ids)\n    pp_group_waiting_prots = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_0':\n            pp_group_waiting_prots = op.desc.attr('other_endpoints')\n    self.assertEqual(pp_group_waiting_prots, ['127.0.0.1:36003'])\n    dp_group_waiting_ports = None\n    for op in startup_prog_ops:\n        if op.type == 'c_gen_nccl_id' and op.desc.output_arg_names()[0] == 'comm_id_3':\n            dp_group_waiting_ports = op.desc.attr('other_endpoints')\n    self.assertEqual(dp_group_waiting_ports, ['127.0.0.1:36002'])"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_with_allreduce_fuse",
        "original": "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_with_allreduce_fuse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'tanh', 'mul', 'elementwise_add', 'softmax', 'softmax_with_cross_entropy', 'reduce_mean', 'fill_constant', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'tanh_grad', 'elementwise_add_grad', 'mul_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'fill_constant', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_amp_gclip",
        "original": "def test_opt_sharding_with_pp_amp_gclip(self):\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_amp_gclip(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'fill_constant', 'cast', 'sum', 'coalesce_tensor', 'c_reduce_sum', 'coalesce_tensor', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_amp_gclip_fuse_gm",
        "original": "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_fuse_gm(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'tanh', 'cast', 'cast', 'mul', 'cast', 'elementwise_add', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'elementwise_mul', 'momentum', 'momentum', 'momentum', 'momentum', 'momentum', 'coalesce_tensor', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast",
        "original": "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    if False:\n        i = 10\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_ckp_fuse_gm_optcast(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.pp_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'pipeline')\n    self.set_strategy(strategy, 'amp')\n    strategy.amp_configs = {'custom_black_varnames': ['fc_6.b_0']}\n    strategy.recompute = True\n    strategy.recompute_configs = {'checkpoints': ['fc_0.tmp_2', 'fc_1.tmp_2', 'fc_2.tmp_2', 'fc_3.tmp_2']}\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True, 'optimize_cast': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    strategy.fuse_grad_merge = True\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast', 'cast', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh', 'cast', 'mul', 'cast', 'elementwise_add', 'cast', 'softmax', 'cast', 'softmax_with_cross_entropy', 'reduce_mean', 'elementwise_mul', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'coalesce_tensor', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'softmax_with_cross_entropy_grad', 'cast', 'softmax_grad', 'cast', 'elementwise_add_grad', 'cast', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'cast', 'mul', 'elementwise_add', 'cast', 'tanh_grad', 'cast', 'elementwise_add_grad', 'mul_grad', 'cast', 'c_sync_calc_stream', 'send_v2', 'cast', 'sum', 'sum', 'cast', 'sum', 'c_reduce_sum', 'c_reduce_sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'cast', 'momentum', 'momentum', 'cast', 'coalesce_tensor', 'c_broadcast', 'c_broadcast', 'coalesce_tensor', 'c_broadcast'])"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    os.environ['PADDLE_TRAINER_ID'] = '3'\n    os.environ['PADDLE_TRAINER_ENDPOINTS'] = '127.0.0.1:36001,127.0.0.1:36002,127.0.0.1:36003,127.0.0.1:36004'\n    self.mp_ring_id = 0\n    self.sharding_ring_id = 1\n    self.dp_ring_id = 2\n    self.global_ring_id = 3\n    self.pp_pair_ring_id = 20\n    self._debug = False"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_amp_gclip_boundary",
        "original": "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    \"\"\"\n        test optimizer sharding without parameter\n        test loss grad scale value\n        \"\"\"\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    if False:\n        i = 10\n    '\\n        test optimizer sharding without parameter\\n        test loss grad scale value\\n        '\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        test optimizer sharding without parameter\\n        test loss grad scale value\\n        '\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        test optimizer sharding without parameter\\n        test loss grad scale value\\n        '\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        test optimizer sharding without parameter\\n        test loss grad scale value\\n        '\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        test optimizer sharding without parameter\\n        test loss grad scale value\\n        '\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    for op in main_prog_ops:\n        if is_loss_grad_op(op):\n            self.assertEqual(op.type, 'fill_constant')\n            self.assertTrue(op.has_attr('value'))\n            scale = strategy.pipeline_configs['accumulate_steps'] * strategy.sharding_configs['dp_degree']\n            loss_scale = 1.0 / scale\n            self.assertAlmostEqual(float(op.attr('value')), loss_scale)\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'fill_constant', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'c_broadcast'])"
        ]
    },
    {
        "func_name": "test_opt_sharding_with_pp_amp_gclip_boundary_card1",
        "original": "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    \"\"\"test optimizer sharding without parameter in card0\"\"\"\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])",
        "mutated": [
            "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    if False:\n        i = 10\n    'test optimizer sharding without parameter in card0'\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'test optimizer sharding without parameter in card0'\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'test optimizer sharding without parameter in card0'\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'test optimizer sharding without parameter in card0'\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])",
            "def test_opt_sharding_with_pp_amp_gclip_boundary_card1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'test optimizer sharding without parameter in card0'\n    os.environ['PADDLE_TRAINER_ID'] = '1'\n    (train_prog, startup_prog) = (static.Program(), static.Program())\n    (avg_cost, strategy) = self.boundary_net(train_prog, startup_prog)\n    self.set_strategy(strategy, 'amp')\n    self.set_strategy(strategy, 'pipeline')\n    strategy.sharding = True\n    strategy.sharding_configs = {'sharding_degree': 1, 'pp_degree': 2, 'dp_degree': 2, '_dp_as_optimizer_sharding': True}\n    strategy.fuse_all_reduce_ops = True\n    strategy.fuse_grad_size_in_MB = 32\n    clip = paddle.nn.ClipGradByGlobalNorm(1.0)\n    self.optimizer(avg_cost, strategy, train_prog, startup_prog, grad_clip=clip)\n    train_prog = train_prog._pipeline_opt['section_program']\n    startup_prog = startup_prog._pipeline_opt['startup_program']\n    self.debug_program(train_prog, startup_prog)\n    startup_prog_ops = startup_prog.global_block().ops\n    main_prog_ops = train_prog.global_block().ops\n    startup_prog_op_types = [op.type for op in startup_prog_ops]\n    main_prog_op_types = [op.type for op in main_prog_ops]\n    self.assertEqual(startup_prog_op_types, ['uniform_random', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'fill_constant', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_gen_nccl_id', 'c_comm_init', 'c_broadcast'])\n    self.assertEqual(main_prog_op_types, ['recv_v2', 'cast', 'matmul_v2', 'cast', 'reduce_mean', 'elementwise_mul', 'fill_constant', 'elementwise_mul_grad', 'reduce_mean_grad', 'cast', 'matmul_v2_grad', 'c_sync_calc_stream', 'send_v2', 'fill_constant', 'cast', 'sum', 'c_reduce_sum', 'c_sync_comm_stream', 'check_finite_and_unscale', 'cast', 'c_allreduce_max', 'c_allreduce_max', 'cast', 'update_loss_scaling', 'squared_l2_norm', 'sum', 'c_allreduce_sum', 'c_allreduce_sum', 'sqrt', 'fill_constant', 'elementwise_max', 'elementwise_div', 'elementwise_mul', 'momentum', 'c_broadcast'])"
        ]
    }
]