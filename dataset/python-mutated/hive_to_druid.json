[
    {
        "func_name": "__init__",
        "original": "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties",
        "mutated": [
            "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    if False:\n        i = 10\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties",
            "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties",
            "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties",
            "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties",
            "def __init__(self, *, sql: str, druid_datasource: str, ts_dim: str, metric_spec: list[Any] | None=None, hive_cli_conn_id: str='hive_cli_default', druid_ingest_conn_id: str='druid_ingest_default', metastore_conn_id: str='metastore_default', hadoop_dependency_coordinates: list[str] | None=None, intervals: list[Any] | None=None, num_shards: float=-1, target_partition_size: int=-1, query_granularity: str='NONE', segment_granularity: str='DAY', hive_tblproperties: dict[Any, Any] | None=None, job_properties: dict[Any, Any] | None=None, **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(**kwargs)\n    self.sql = sql\n    self.druid_datasource = druid_datasource\n    self.ts_dim = ts_dim\n    self.intervals = intervals or ['{{ ds }}/{{ tomorrow_ds }}']\n    self.num_shards = num_shards\n    self.target_partition_size = target_partition_size\n    self.query_granularity = query_granularity\n    self.segment_granularity = segment_granularity\n    self.metric_spec = metric_spec or [{'name': 'count', 'type': 'count'}]\n    self.hive_cli_conn_id = hive_cli_conn_id\n    self.hadoop_dependency_coordinates = hadoop_dependency_coordinates\n    self.druid_ingest_conn_id = druid_ingest_conn_id\n    self.metastore_conn_id = metastore_conn_id\n    self.hive_tblproperties = hive_tblproperties or {}\n    self.job_properties = job_properties"
        ]
    },
    {
        "func_name": "execute",
        "original": "def execute(self, context: Context) -> None:\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)",
        "mutated": [
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id)\n    self.log.info('Extracting data from Hive')\n    hive_table = 'druid.' + context['task_instance_key_str'].replace('.', '_')\n    sql = self.sql.strip().strip(';')\n    tblproperties = ''.join((f\", '{k}' = '{v}'\" for (k, v) in self.hive_tblproperties.items()))\n    hql = f\"        SET mapred.output.compress=false;\\n        SET hive.exec.compress.output=false;\\n        DROP TABLE IF EXISTS {hive_table};\\n        CREATE TABLE {hive_table}\\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\\n        STORED AS TEXTFILE\\n        TBLPROPERTIES ('serialization.null.format' = ''{tblproperties})\\n        AS\\n        {sql}\\n        \"\n    self.log.info('Running command:\\n %s', hql)\n    hive.run_cli(hql)\n    meta_hook = HiveMetastoreHook(self.metastore_conn_id)\n    table = meta_hook.get_table(hive_table)\n    columns = [col.name for col in table.sd.cols]\n    static_path = meta_hook.get_table(hive_table).sd.location\n    druid = DruidHook(druid_ingest_conn_id=self.druid_ingest_conn_id)\n    try:\n        index_spec = self.construct_ingest_query(static_path=static_path, columns=columns)\n        self.log.info('Inserting rows into Druid, hdfs path: %s', static_path)\n        druid.submit_indexing_job(index_spec)\n        self.log.info('Load seems to have succeeded!')\n    finally:\n        self.log.info('Cleaning up by dropping the temp Hive table %s', hive_table)\n        hql = f'DROP TABLE IF EXISTS {hive_table}'\n        hive.run_cli(hql)"
        ]
    },
    {
        "func_name": "construct_ingest_query",
        "original": "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    \"\"\"\n        Build an ingest query for an HDFS TSV load.\n\n        :param static_path: The path on hdfs where the data is\n        :param columns: List of all the columns that are available\n        \"\"\"\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict",
        "mutated": [
            "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Build an ingest query for an HDFS TSV load.\\n\\n        :param static_path: The path on hdfs where the data is\\n        :param columns: List of all the columns that are available\\n        '\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict",
            "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Build an ingest query for an HDFS TSV load.\\n\\n        :param static_path: The path on hdfs where the data is\\n        :param columns: List of all the columns that are available\\n        '\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict",
            "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Build an ingest query for an HDFS TSV load.\\n\\n        :param static_path: The path on hdfs where the data is\\n        :param columns: List of all the columns that are available\\n        '\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict",
            "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Build an ingest query for an HDFS TSV load.\\n\\n        :param static_path: The path on hdfs where the data is\\n        :param columns: List of all the columns that are available\\n        '\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict",
            "def construct_ingest_query(self, static_path: str, columns: list[str]) -> dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Build an ingest query for an HDFS TSV load.\\n\\n        :param static_path: The path on hdfs where the data is\\n        :param columns: List of all the columns that are available\\n        '\n    num_shards = self.num_shards\n    target_partition_size = self.target_partition_size\n    if self.target_partition_size == -1:\n        if self.num_shards == -1:\n            target_partition_size = DEFAULT_TARGET_PARTITION_SIZE\n    else:\n        num_shards = -1\n    metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']\n    dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]\n    ingest_query_dict: dict[str, Any] = {'type': 'index_hadoop', 'spec': {'dataSchema': {'metricsSpec': self.metric_spec, 'granularitySpec': {'queryGranularity': self.query_granularity, 'intervals': self.intervals, 'type': 'uniform', 'segmentGranularity': self.segment_granularity}, 'parser': {'type': 'string', 'parseSpec': {'columns': columns, 'dimensionsSpec': {'dimensionExclusions': [], 'dimensions': dimensions, 'spatialDimensions': []}, 'timestampSpec': {'column': self.ts_dim, 'format': 'auto'}, 'format': 'tsv'}}, 'dataSource': self.druid_datasource}, 'tuningConfig': {'type': 'hadoop', 'jobProperties': {'mapreduce.job.user.classpath.first': 'false', 'mapreduce.map.output.compress': 'false', 'mapreduce.output.fileoutputformat.compress': 'false'}, 'partitionsSpec': {'type': 'hashed', 'targetPartitionSize': target_partition_size, 'numShards': num_shards}}, 'ioConfig': {'inputSpec': {'paths': static_path, 'type': 'static'}, 'type': 'hadoop'}}}\n    if self.job_properties:\n        ingest_query_dict['spec']['tuningConfig']['jobProperties'].update(self.job_properties)\n    if self.hadoop_dependency_coordinates:\n        ingest_query_dict['hadoopDependencyCoordinates'] = self.hadoop_dependency_coordinates\n    return ingest_query_dict"
        ]
    }
]