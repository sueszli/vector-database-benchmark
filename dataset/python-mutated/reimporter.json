[
    {
        "func_name": "process_parsed_findings",
        "original": "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)",
        "mutated": [
            "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    if False:\n        i = 10\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)",
            "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)",
            "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)",
            "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)",
            "@dojo_async_task\n@app.task(ignore_result=False)\ndef process_parsed_findings(self, test, parsed_findings, scan_type, user, active=None, verified=None, minimum_severity=None, endpoints_to_add=None, push_to_jira=None, group_by=None, now=timezone.now(), service=None, scan_date=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    items = parsed_findings\n    original_items = list(test.finding_set.all())\n    new_items = []\n    mitigated_count = 0\n    finding_count = 0\n    finding_added_count = 0\n    reactivated_count = 0\n    reactivated_items = []\n    unchanged_count = 0\n    unchanged_items = []\n    logger.debug('starting reimport of %i items.', len(items) if items else 0)\n    deduplication_algorithm = test.deduplication_algorithm\n    i = 0\n    group_names_to_findings_dict = {}\n    logger.debug('STEP 1: looping over findings from the reimported report and trying to match them to existing findings')\n    deduplicationLogger.debug('Algorithm used for matching new findings to existing findings: %s', deduplication_algorithm)\n    for item in items:\n        if item.severity.lower().startswith('info') and item.severity != 'Info':\n            item.severity = 'Info'\n        item.numerical_severity = Finding.get_numerical_severity(item.severity)\n        if minimum_severity and Finding.SEVERITIES[item.severity] > Finding.SEVERITIES[minimum_severity]:\n            continue\n        component_name = item.component_name if hasattr(item, 'component_name') else None\n        component_version = item.component_version if hasattr(item, 'component_version') else None\n        if not hasattr(item, 'test'):\n            item.test = test\n        if service:\n            item.service = service\n        if item.dynamic_finding:\n            for e in item.unsaved_endpoints:\n                try:\n                    e.clean()\n                except ValidationError as err:\n                    logger.warning(\"DefectDojo is storing broken endpoint because cleaning wasn't successful: {}\".format(err))\n        item.hash_code = item.compute_hash_code()\n        deduplicationLogger.debug(\"item's hash_code: %s\", item.hash_code)\n        findings = reimporter_utils.match_new_finding_to_existing_finding(item, test, deduplication_algorithm)\n        deduplicationLogger.debug('found %i findings matching with current new finding', len(findings))\n        if findings:\n            finding = findings[0]\n            if finding.false_p or finding.out_of_scope or finding.risk_accepted:\n                logger.debug('%i: skipping existing finding (it is marked as false positive:%s and/or out of scope:%s or is a risk accepted:%s): %i:%s:%s:%s', i, finding.false_p, finding.out_of_scope, finding.risk_accepted, finding.id, finding, finding.component_name, finding.component_version)\n                if finding.false_p == item.false_p and finding.out_of_scope == item.out_of_scope and (finding.risk_accepted == item.risk_accepted):\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    continue\n            elif finding.is_mitigated:\n                if item.is_mitigated:\n                    unchanged_items.append(finding)\n                    unchanged_count += 1\n                    if item.mitigated:\n                        logger.debug('item mitigated time: ' + str(item.mitigated.timestamp()))\n                        logger.debug('finding mitigated time: ' + str(finding.mitigated.timestamp()))\n                        if item.mitigated.timestamp() == finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding have the same mitigation date, will skip as they are the same.')\n                            continue\n                        if item.mitigated.timestamp() != finding.mitigated.timestamp():\n                            logger.debug('New imported finding and already existing finding are both mitigated but have different dates, not taking action')\n                            continue\n                    else:\n                        continue\n                else:\n                    if not do_not_reactivate:\n                        logger.debug('%i: reactivating: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = None\n                        finding.is_mitigated = False\n                        finding.mitigated_by = None\n                        finding.active = True\n                        if verified is not None:\n                            finding.verified = verified\n                    if do_not_reactivate:\n                        logger.debug(\"%i: skipping reactivating by user's choice do_not_reactivate: %i:%s:%s:%s\", i, finding.id, finding, finding.component_name, finding.component_version)\n                        existing_note = finding.notes.filter(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                        if len(existing_note) == 0:\n                            note = Notes(entry='Finding has skipped reactivation from %s re-upload with user decision do_not_reactivate.' % scan_type, author=user)\n                            note.save()\n                            finding.notes.add(note)\n                            finding.save(dedupe_option=False)\n                        continue\n                finding.component_name = finding.component_name if finding.component_name else component_name\n                finding.component_version = finding.component_version if finding.component_version else component_version\n                finding.save(dedupe_option=False)\n                note = Notes(entry='Re-activated by %s re-upload.' % scan_type, author=user)\n                note.save()\n                endpoint_statuses = finding.status_finding.exclude(Q(false_positive=True) | Q(out_of_scope=True) | Q(risk_accepted=True))\n                reimporter_utils.chunk_endpoints_and_reactivate(endpoint_statuses)\n                finding.notes.add(note)\n                reactivated_items.append(finding)\n                reactivated_count += 1\n            else:\n                logger.debug('%i: updating existing finding: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                if not (finding.mitigated and finding.is_mitigated):\n                    logger.debug('Reimported item matches a finding that is currently open.')\n                    if item.is_mitigated:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.mitigated = item.mitigated\n                        finding.is_mitigated = True\n                        finding.mitigated_by = item.mitigated_by\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    elif item.risk_accepted or item.false_p or item.out_of_scope:\n                        logger.debug('Reimported mitigated item matches a finding that is currently open, closing.')\n                        logger.debug('%i: closing: %i:%s:%s:%s', i, finding.id, finding, finding.component_name, finding.component_version)\n                        finding.risk_accepted = item.risk_accepted\n                        finding.false_p = item.false_p\n                        finding.out_of_scope = item.out_of_scope\n                        finding.active = False\n                        if verified is not None:\n                            finding.verified = verified\n                    else:\n                        unchanged_items.append(finding)\n                        unchanged_count += 1\n                if component_name is not None and (not finding.component_name) or (component_version is not None and (not finding.component_version)):\n                    finding.component_name = finding.component_name if finding.component_name else component_name\n                    finding.component_version = finding.component_version if finding.component_version else component_version\n                    finding.save(dedupe_option=False)\n            if finding.dynamic_finding:\n                logger.debug('Re-import found an existing dynamic finding for this new finding. Checking the status of endpoints')\n                reimporter_utils.update_endpoint_status(finding, item, user)\n        else:\n            item.reporter = user\n            item.last_reviewed = timezone.now()\n            item.last_reviewed_by = user\n            if active is not None:\n                item.active = active\n            if verified is not None:\n                item.verified = verified\n            if scan_date:\n                item.date = scan_date.date()\n            item.save(dedupe_option=False)\n            logger.debug('%i: reimport created new finding as no existing finding match: %i:%s:%s:%s', i, item.id, item, item.component_name, item.component_version)\n            if is_finding_groups_enabled() and group_by:\n                name = finding_helper.get_group_by_group_name(item, group_by)\n                if name is not None:\n                    if name in group_names_to_findings_dict:\n                        group_names_to_findings_dict[name].append(item)\n                    else:\n                        group_names_to_findings_dict[name] = [item]\n            finding_added_count += 1\n            new_items.append(item)\n            finding = item\n            if hasattr(item, 'unsaved_req_resp'):\n                for req_resp in item.unsaved_req_resp:\n                    burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(req_resp['req'].encode('utf-8')), burpResponseBase64=base64.b64encode(req_resp['resp'].encode('utf-8')))\n                    burp_rr.clean()\n                    burp_rr.save()\n            if item.unsaved_request and item.unsaved_response:\n                burp_rr = BurpRawRequestResponse(finding=finding, burpRequestBase64=base64.b64encode(item.unsaved_request.encode()), burpResponseBase64=base64.b64encode(item.unsaved_response.encode()))\n                burp_rr.clean()\n                burp_rr.save()\n        if finding:\n            finding_count += 1\n            importer_utils.chunk_endpoints_and_disperse(finding, test, item.unsaved_endpoints)\n            if endpoints_to_add:\n                importer_utils.chunk_endpoints_and_disperse(finding, test, endpoints_to_add)\n            if item.unsaved_tags:\n                finding.tags = item.unsaved_tags\n            if item.unsaved_files:\n                for unsaved_file in item.unsaved_files:\n                    data = base64.b64decode(unsaved_file.get('data'))\n                    title = unsaved_file.get('title', '<No title>')\n                    (file_upload, file_upload_created) = FileUpload.objects.get_or_create(title=title)\n                    file_upload.file.save(title, ContentFile(data))\n                    file_upload.save()\n                    finding.files.add(file_upload)\n            if finding.unsaved_vulnerability_ids:\n                importer_utils.handle_vulnerability_ids(finding)\n            finding.component_name = finding.component_name if finding.component_name else component_name\n            finding.component_version = finding.component_version if finding.component_version else component_version\n            if is_finding_groups_enabled() and group_by:\n                finding.save()\n            else:\n                finding.save(push_to_jira=push_to_jira)\n    to_mitigate = set(original_items) - set(reactivated_items) - set(unchanged_items)\n    untouched = set(unchanged_items) - set(to_mitigate) - set(new_items)\n    for (group_name, findings) in group_names_to_findings_dict.items():\n        finding_helper.add_findings_to_auto_group(group_name, findings, group_by, create_finding_groups_for_all_findings, **kwargs)\n        if push_to_jira:\n            if findings[0].finding_group is not None:\n                jira_helper.push_to_jira(findings[0].finding_group)\n            else:\n                jira_helper.push_to_jira(findings[0])\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in reactivated_items + unchanged_items if finding.finding_group is not None and (not finding.is_mitigated)]):\n            jira_helper.push_to_jira(finding_group)\n    sync = kwargs.get('sync', False)\n    if not sync:\n        serialized_new_items = [serializers.serialize('json', [finding]) for finding in new_items]\n        serialized_reactivated_items = [serializers.serialize('json', [finding]) for finding in reactivated_items]\n        serialized_to_mitigate = [serializers.serialize('json', [finding]) for finding in to_mitigate]\n        serialized_untouched = [serializers.serialize('json', [finding]) for finding in untouched]\n        return (serialized_new_items, serialized_reactivated_items, serialized_to_mitigate, serialized_untouched)\n    return (new_items, reactivated_items, to_mitigate, untouched)"
        ]
    },
    {
        "func_name": "close_old_findings",
        "original": "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings",
        "mutated": [
            "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    if False:\n        i = 10\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings",
            "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings",
            "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings",
            "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings",
            "def close_old_findings(self, test, to_mitigate, scan_date_time, user, push_to_jira=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logger.debug('IMPORT_SCAN: Closing findings no longer present in scan report')\n    mitigated_findings = []\n    for finding in to_mitigate:\n        if not finding.mitigated or not finding.is_mitigated:\n            logger.debug('mitigating finding: %i:%s', finding.id, finding)\n            finding.mitigated = scan_date_time\n            finding.is_mitigated = True\n            finding.mitigated_by = user\n            finding.active = False\n            endpoint_status = finding.status_finding.all()\n            reimporter_utils.mitigate_endpoint_status(endpoint_status, user, kwuser=user, sync=True)\n            if is_finding_groups_enabled() and finding.finding_group:\n                finding.save(dedupe_option=False)\n            else:\n                finding.save(push_to_jira=push_to_jira, dedupe_option=False)\n            note = Notes(entry='Mitigated by %s re-upload.' % test.test_type, author=user)\n            note.save()\n            finding.notes.add(note)\n            mitigated_findings.append(finding)\n    if is_finding_groups_enabled() and push_to_jira:\n        for finding_group in set([finding.finding_group for finding in to_mitigate if finding.finding_group is not None]):\n            jira_helper.push_to_jira(finding_group)\n    return mitigated_findings"
        ]
    },
    {
        "func_name": "reimport_scan",
        "original": "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)",
        "mutated": [
            "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    if False:\n        i = 10\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)",
            "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)",
            "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)",
            "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)",
            "def reimport_scan(self, scan, scan_type, test, active=None, verified=None, tags=None, minimum_severity=None, user=None, endpoints_to_add=None, scan_date=None, version=None, branch_tag=None, build_id=None, commit_hash=None, push_to_jira=None, close_old_findings=True, group_by=None, api_scan_configuration=None, service=None, do_not_reactivate=False, create_finding_groups_for_all_findings=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logger.debug(f'REIMPORT_SCAN: parameters: {locals()}')\n    user = user or get_current_user()\n    now = timezone.now()\n    if api_scan_configuration:\n        if api_scan_configuration.product != test.engagement.product:\n            raise ValidationError('API Scan Configuration has to be from same product as the Test')\n        if test.api_scan_configuration != api_scan_configuration:\n            test.api_scan_configuration = api_scan_configuration\n            test.save()\n    parser = get_parser(scan_type)\n    if hasattr(parser, 'get_tests'):\n        logger.debug('REIMPORT_SCAN parser v2: Create parse findings')\n        try:\n            tests = parser.get_tests(scan_type, scan)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n        parsed_findings = []\n        for test_raw in tests:\n            parsed_findings.extend(test_raw.findings)\n    else:\n        logger.debug('REIMPORT_SCAN: Parse findings')\n        try:\n            parsed_findings = parser.get_findings(scan, test)\n        except ValueError as e:\n            logger.warning(e)\n            raise ValidationError(e)\n    logger.debug('REIMPORT_SCAN: Processing findings')\n    new_findings = []\n    reactivated_findings = []\n    findings_to_mitigate = []\n    untouched_findings = []\n    if settings.ASYNC_FINDING_IMPORT:\n        chunk_list = importer_utils.chunk_list(parsed_findings)\n        results_list = []\n        for findings_list in chunk_list:\n            result = self.process_parsed_findings(test, findings_list, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=False, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n            results_list += [result]\n        logger.debug('REIMPORT_SCAN: Collecting Findings')\n        for results in results_list:\n            (serial_new_findings, serial_reactivated_findings, serial_findings_to_mitigate, serial_untouched_findings) = results.get()\n            new_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_new_findings]\n            reactivated_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_reactivated_findings]\n            findings_to_mitigate += [next(serializers.deserialize('json', finding)).object for finding in serial_findings_to_mitigate]\n            untouched_findings += [next(serializers.deserialize('json', finding)).object for finding in serial_untouched_findings]\n        logger.debug('REIMPORT_SCAN: All Findings Collected')\n        test.percent_complete = 50\n        test.save()\n        importer_utils.update_test_progress(test)\n    else:\n        (new_findings, reactivated_findings, findings_to_mitigate, untouched_findings) = self.process_parsed_findings(test, parsed_findings, scan_type, user, active=active, verified=verified, minimum_severity=minimum_severity, endpoints_to_add=endpoints_to_add, push_to_jira=push_to_jira, group_by=group_by, now=now, service=service, scan_date=scan_date, sync=True, do_not_reactivate=do_not_reactivate, create_finding_groups_for_all_findings=create_finding_groups_for_all_findings)\n    closed_findings = []\n    if close_old_findings:\n        logger.debug('REIMPORT_SCAN: Closing findings no longer present in scan report')\n        closed_findings = self.close_old_findings(test, findings_to_mitigate, scan_date, user=user, push_to_jira=push_to_jira)\n    logger.debug('REIMPORT_SCAN: Updating test/engagement timestamps')\n    importer_utils.update_timestamps(test, version, branch_tag, build_id, commit_hash, now, scan_date)\n    logger.debug('REIMPORT_SCAN: Updating test tags')\n    importer_utils.update_tags(test, tags)\n    test_import = None\n    if settings.TRACK_IMPORT_HISTORY:\n        logger.debug('REIMPORT_SCAN: Updating Import History')\n        test_import = importer_utils.update_import_history(Test_Import.REIMPORT_TYPE, active, verified, tags, minimum_severity, endpoints_to_add, version, branch_tag, build_id, commit_hash, push_to_jira, close_old_findings, test, new_findings, closed_findings, reactivated_findings, untouched_findings)\n    logger.debug('REIMPORT_SCAN: Generating notifications')\n    updated_count = len(closed_findings) + len(reactivated_findings) + len(new_findings)\n    if updated_count > 0:\n        notifications_helper.notify_scan_added(test, updated_count, new_findings=new_findings, findings_mitigated=closed_findings, findings_reactivated=reactivated_findings, findings_untouched=untouched_findings)\n    logger.debug('REIMPORT_SCAN: Done')\n    return (test, updated_count, len(new_findings), len(closed_findings), len(reactivated_findings), len(untouched_findings), test_import)"
        ]
    }
]