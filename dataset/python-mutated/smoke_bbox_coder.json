[
    {
        "func_name": "__init__",
        "original": "def __init__(self, base_depth, base_dims, code_size):\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size",
        "mutated": [
            "def __init__(self, base_depth, base_dims, code_size):\n    if False:\n        i = 10\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size",
            "def __init__(self, base_depth, base_dims, code_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size",
            "def __init__(self, base_depth, base_dims, code_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size",
            "def __init__(self, base_depth, base_dims, code_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size",
            "def __init__(self, base_depth, base_dims, code_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SMOKECoder, self).__init__()\n    self.base_depth = base_depth\n    self.base_dims = base_dims\n    self.bbox_code_size = code_size"
        ]
    },
    {
        "func_name": "encode",
        "original": "def encode(self, locations, dimensions, orientations, input_metas):\n    \"\"\"Encode CameraInstance3DBoxes by locations, dimensions, orientations.\n\n        Args:\n            locations (Tensor): Center location for 3D boxes.\n                (N, 3)\n            dimensions (Tensor): Dimensions for 3D boxes.\n                shape (N, 3)\n            orientations (Tensor): Orientations for 3D boxes.\n                shape (N, 1)\n            input_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Return:\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\n                shape (N, bbox_code_size).\n        \"\"\"\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes",
        "mutated": [
            "def encode(self, locations, dimensions, orientations, input_metas):\n    if False:\n        i = 10\n    'Encode CameraInstance3DBoxes by locations, dimensions, orientations.\\n\\n        Args:\\n            locations (Tensor): Center location for 3D boxes.\\n                (N, 3)\\n            dimensions (Tensor): Dimensions for 3D boxes.\\n                shape (N, 3)\\n            orientations (Tensor): Orientations for 3D boxes.\\n                shape (N, 1)\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Return:\\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\\n                shape (N, bbox_code_size).\\n        '\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes",
            "def encode(self, locations, dimensions, orientations, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Encode CameraInstance3DBoxes by locations, dimensions, orientations.\\n\\n        Args:\\n            locations (Tensor): Center location for 3D boxes.\\n                (N, 3)\\n            dimensions (Tensor): Dimensions for 3D boxes.\\n                shape (N, 3)\\n            orientations (Tensor): Orientations for 3D boxes.\\n                shape (N, 1)\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Return:\\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\\n                shape (N, bbox_code_size).\\n        '\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes",
            "def encode(self, locations, dimensions, orientations, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Encode CameraInstance3DBoxes by locations, dimensions, orientations.\\n\\n        Args:\\n            locations (Tensor): Center location for 3D boxes.\\n                (N, 3)\\n            dimensions (Tensor): Dimensions for 3D boxes.\\n                shape (N, 3)\\n            orientations (Tensor): Orientations for 3D boxes.\\n                shape (N, 1)\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Return:\\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\\n                shape (N, bbox_code_size).\\n        '\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes",
            "def encode(self, locations, dimensions, orientations, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Encode CameraInstance3DBoxes by locations, dimensions, orientations.\\n\\n        Args:\\n            locations (Tensor): Center location for 3D boxes.\\n                (N, 3)\\n            dimensions (Tensor): Dimensions for 3D boxes.\\n                shape (N, 3)\\n            orientations (Tensor): Orientations for 3D boxes.\\n                shape (N, 1)\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Return:\\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\\n                shape (N, bbox_code_size).\\n        '\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes",
            "def encode(self, locations, dimensions, orientations, input_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Encode CameraInstance3DBoxes by locations, dimensions, orientations.\\n\\n        Args:\\n            locations (Tensor): Center location for 3D boxes.\\n                (N, 3)\\n            dimensions (Tensor): Dimensions for 3D boxes.\\n                shape (N, 3)\\n            orientations (Tensor): Orientations for 3D boxes.\\n                shape (N, 1)\\n            input_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Return:\\n            :obj:`CameraInstance3DBoxes`: 3D bboxes of batch images,\\n                shape (N, bbox_code_size).\\n        '\n    bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    assert bboxes.shape[1] == self.bbox_code_size, 'bboxes shape dose notmatch the bbox_code_size.'\n    batch_bboxes = input_metas[0]['box_type_3d'](bboxes, box_dim=self.bbox_code_size)\n    return batch_bboxes"
        ]
    },
    {
        "func_name": "decode",
        "original": "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    \"\"\"Decode regression into locations, dimensions, orientations.\n\n        Args:\n            reg (Tensor): Batch regression for each predict center2d point.\n                shape: (batch * K (max_objs), C)\n            points(Tensor): Batch projected bbox centers on image plane.\n                shape: (batch * K (max_objs) , 2)\n            labels (Tensor): Batch predict class label for each predict\n                center2d point.\n                shape: (batch, K (max_objs))\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\n            trans_mats (Tensor): transformation matrix from original image\n                to feature map.\n                shape: (batch, 3, 3)\n            locations (None | Tensor): if locations is None, this function\n                is used to decode while inference, otherwise, it's used while\n                training using the ground truth 3d bbox locations.\n                shape: (batch * K (max_objs), 3)\n\n        Return:\n            tuple(Tensor): The tuple has components below:\n                - locations (Tensor): Centers of 3D boxes.\n                    shape: (batch * K (max_objs), 3)\n                - dimensions (Tensor): Dimensions of 3D boxes.\n                    shape: (batch * K (max_objs), 3)\n                - orientations (Tensor): Orientations of 3D\n                    boxes.\n                    shape: (batch * K (max_objs), 1)\n        \"\"\"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)",
        "mutated": [
            "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    if False:\n        i = 10\n    \"Decode regression into locations, dimensions, orientations.\\n\\n        Args:\\n            reg (Tensor): Batch regression for each predict center2d point.\\n                shape: (batch * K (max_objs), C)\\n            points(Tensor): Batch projected bbox centers on image plane.\\n                shape: (batch * K (max_objs) , 2)\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (batch, K (max_objs))\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            locations (None | Tensor): if locations is None, this function\\n                is used to decode while inference, otherwise, it's used while\\n                training using the ground truth 3d bbox locations.\\n                shape: (batch * K (max_objs), 3)\\n\\n        Return:\\n            tuple(Tensor): The tuple has components below:\\n                - locations (Tensor): Centers of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - dimensions (Tensor): Dimensions of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - orientations (Tensor): Orientations of 3D\\n                    boxes.\\n                    shape: (batch * K (max_objs), 1)\\n        \"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)",
            "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Decode regression into locations, dimensions, orientations.\\n\\n        Args:\\n            reg (Tensor): Batch regression for each predict center2d point.\\n                shape: (batch * K (max_objs), C)\\n            points(Tensor): Batch projected bbox centers on image plane.\\n                shape: (batch * K (max_objs) , 2)\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (batch, K (max_objs))\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            locations (None | Tensor): if locations is None, this function\\n                is used to decode while inference, otherwise, it's used while\\n                training using the ground truth 3d bbox locations.\\n                shape: (batch * K (max_objs), 3)\\n\\n        Return:\\n            tuple(Tensor): The tuple has components below:\\n                - locations (Tensor): Centers of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - dimensions (Tensor): Dimensions of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - orientations (Tensor): Orientations of 3D\\n                    boxes.\\n                    shape: (batch * K (max_objs), 1)\\n        \"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)",
            "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Decode regression into locations, dimensions, orientations.\\n\\n        Args:\\n            reg (Tensor): Batch regression for each predict center2d point.\\n                shape: (batch * K (max_objs), C)\\n            points(Tensor): Batch projected bbox centers on image plane.\\n                shape: (batch * K (max_objs) , 2)\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (batch, K (max_objs))\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            locations (None | Tensor): if locations is None, this function\\n                is used to decode while inference, otherwise, it's used while\\n                training using the ground truth 3d bbox locations.\\n                shape: (batch * K (max_objs), 3)\\n\\n        Return:\\n            tuple(Tensor): The tuple has components below:\\n                - locations (Tensor): Centers of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - dimensions (Tensor): Dimensions of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - orientations (Tensor): Orientations of 3D\\n                    boxes.\\n                    shape: (batch * K (max_objs), 1)\\n        \"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)",
            "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Decode regression into locations, dimensions, orientations.\\n\\n        Args:\\n            reg (Tensor): Batch regression for each predict center2d point.\\n                shape: (batch * K (max_objs), C)\\n            points(Tensor): Batch projected bbox centers on image plane.\\n                shape: (batch * K (max_objs) , 2)\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (batch, K (max_objs))\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            locations (None | Tensor): if locations is None, this function\\n                is used to decode while inference, otherwise, it's used while\\n                training using the ground truth 3d bbox locations.\\n                shape: (batch * K (max_objs), 3)\\n\\n        Return:\\n            tuple(Tensor): The tuple has components below:\\n                - locations (Tensor): Centers of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - dimensions (Tensor): Dimensions of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - orientations (Tensor): Orientations of 3D\\n                    boxes.\\n                    shape: (batch * K (max_objs), 1)\\n        \"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)",
            "def decode(self, reg, points, labels, cam2imgs, trans_mats, locations=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Decode regression into locations, dimensions, orientations.\\n\\n        Args:\\n            reg (Tensor): Batch regression for each predict center2d point.\\n                shape: (batch * K (max_objs), C)\\n            points(Tensor): Batch projected bbox centers on image plane.\\n                shape: (batch * K (max_objs) , 2)\\n            labels (Tensor): Batch predict class label for each predict\\n                center2d point.\\n                shape: (batch, K (max_objs))\\n            cam2imgs (Tensor): Batch images' camera intrinsic matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            locations (None | Tensor): if locations is None, this function\\n                is used to decode while inference, otherwise, it's used while\\n                training using the ground truth 3d bbox locations.\\n                shape: (batch * K (max_objs), 3)\\n\\n        Return:\\n            tuple(Tensor): The tuple has components below:\\n                - locations (Tensor): Centers of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - dimensions (Tensor): Dimensions of 3D boxes.\\n                    shape: (batch * K (max_objs), 3)\\n                - orientations (Tensor): Orientations of 3D\\n                    boxes.\\n                    shape: (batch * K (max_objs), 1)\\n        \"\n    depth_offsets = reg[:, 0]\n    centers2d_offsets = reg[:, 1:3]\n    dimensions_offsets = reg[:, 3:6]\n    orientations = reg[:, 6:8]\n    depths = self._decode_depth(depth_offsets)\n    pred_locations = self._decode_location(points, centers2d_offsets, depths, cam2imgs, trans_mats)\n    pred_dimensions = self._decode_dimension(labels, dimensions_offsets)\n    if locations is None:\n        pred_orientations = self._decode_orientation(orientations, pred_locations)\n    else:\n        pred_orientations = self._decode_orientation(orientations, locations)\n    return (pred_locations, pred_dimensions, pred_orientations)"
        ]
    },
    {
        "func_name": "_decode_depth",
        "original": "def _decode_depth(self, depth_offsets):\n    \"\"\"Transform depth offset to depth.\"\"\"\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths",
        "mutated": [
            "def _decode_depth(self, depth_offsets):\n    if False:\n        i = 10\n    'Transform depth offset to depth.'\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths",
            "def _decode_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform depth offset to depth.'\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths",
            "def _decode_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform depth offset to depth.'\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths",
            "def _decode_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform depth offset to depth.'\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths",
            "def _decode_depth(self, depth_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform depth offset to depth.'\n    base_depth = depth_offsets.new_tensor(self.base_depth)\n    depths = depth_offsets * base_depth[1] + base_depth[0]\n    return depths"
        ]
    },
    {
        "func_name": "_decode_location",
        "original": "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    \"\"\"Retrieve objects location in camera coordinate based on projected\n        points.\n\n        Args:\n            points (Tensor): Projected points on feature map in (x, y)\n                shape: (batch * K, 2)\n            centers2d_offset (Tensor): Project points offset in\n                (delta_x, delta_y). shape: (batch * K, 2)\n            depths (Tensor): Object depth z.\n                shape: (batch * K)\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\n            trans_mats (Tensor): transformation matrix from original image\n                to feature map.\n                shape: (batch, 3, 3)\n        \"\"\"\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]",
        "mutated": [
            "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    if False:\n        i = 10\n    'Retrieve objects location in camera coordinate based on projected\\n        points.\\n\\n        Args:\\n            points (Tensor): Projected points on feature map in (x, y)\\n                shape: (batch * K, 2)\\n            centers2d_offset (Tensor): Project points offset in\\n                (delta_x, delta_y). shape: (batch * K, 2)\\n            depths (Tensor): Object depth z.\\n                shape: (batch * K)\\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n        '\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]",
            "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Retrieve objects location in camera coordinate based on projected\\n        points.\\n\\n        Args:\\n            points (Tensor): Projected points on feature map in (x, y)\\n                shape: (batch * K, 2)\\n            centers2d_offset (Tensor): Project points offset in\\n                (delta_x, delta_y). shape: (batch * K, 2)\\n            depths (Tensor): Object depth z.\\n                shape: (batch * K)\\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n        '\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]",
            "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Retrieve objects location in camera coordinate based on projected\\n        points.\\n\\n        Args:\\n            points (Tensor): Projected points on feature map in (x, y)\\n                shape: (batch * K, 2)\\n            centers2d_offset (Tensor): Project points offset in\\n                (delta_x, delta_y). shape: (batch * K, 2)\\n            depths (Tensor): Object depth z.\\n                shape: (batch * K)\\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n        '\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]",
            "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Retrieve objects location in camera coordinate based on projected\\n        points.\\n\\n        Args:\\n            points (Tensor): Projected points on feature map in (x, y)\\n                shape: (batch * K, 2)\\n            centers2d_offset (Tensor): Project points offset in\\n                (delta_x, delta_y). shape: (batch * K, 2)\\n            depths (Tensor): Object depth z.\\n                shape: (batch * K)\\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n        '\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]",
            "def _decode_location(self, points, centers2d_offsets, depths, cam2imgs, trans_mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Retrieve objects location in camera coordinate based on projected\\n        points.\\n\\n        Args:\\n            points (Tensor): Projected points on feature map in (x, y)\\n                shape: (batch * K, 2)\\n            centers2d_offset (Tensor): Project points offset in\\n                (delta_x, delta_y). shape: (batch * K, 2)\\n            depths (Tensor): Object depth z.\\n                shape: (batch * K)\\n            cam2imgs (Tensor): Batch camera intrinsics matrix.\\n                shape: kitti (batch, 4, 4)  nuscenes (batch, 3, 3)\\n            trans_mats (Tensor): transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n        '\n    N = centers2d_offsets.shape[0]\n    N_batch = cam2imgs.shape[0]\n    batch_id = torch.arange(N_batch).unsqueeze(1)\n    obj_id = batch_id.repeat(1, N // N_batch).flatten()\n    trans_mats_inv = trans_mats.inverse()[obj_id]\n    cam2imgs_inv = cam2imgs.inverse()[obj_id]\n    centers2d = points + centers2d_offsets\n    centers2d_extend = torch.cat((centers2d, centers2d.new_ones(N, 1)), dim=1)\n    centers2d_extend = centers2d_extend.unsqueeze(-1)\n    centers2d_img = torch.matmul(trans_mats_inv, centers2d_extend)\n    centers2d_img = centers2d_img * depths.view(N, -1, 1)\n    if cam2imgs.shape[1] == 4:\n        centers2d_img = torch.cat((centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)\n    locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)\n    return locations[:, :3]"
        ]
    },
    {
        "func_name": "_decode_dimension",
        "original": "def _decode_dimension(self, labels, dims_offset):\n    \"\"\"Transform dimension offsets to dimension according to its category.\n\n        Args:\n            labels (Tensor): Each points' category id.\n                shape: (N, K)\n            dims_offset (Tensor): Dimension offsets.\n                shape: (N, 3)\n        \"\"\"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions",
        "mutated": [
            "def _decode_dimension(self, labels, dims_offset):\n    if False:\n        i = 10\n    \"Transform dimension offsets to dimension according to its category.\\n\\n        Args:\\n            labels (Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (Tensor): Dimension offsets.\\n                shape: (N, 3)\\n        \"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions",
            "def _decode_dimension(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Transform dimension offsets to dimension according to its category.\\n\\n        Args:\\n            labels (Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (Tensor): Dimension offsets.\\n                shape: (N, 3)\\n        \"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions",
            "def _decode_dimension(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Transform dimension offsets to dimension according to its category.\\n\\n        Args:\\n            labels (Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (Tensor): Dimension offsets.\\n                shape: (N, 3)\\n        \"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions",
            "def _decode_dimension(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Transform dimension offsets to dimension according to its category.\\n\\n        Args:\\n            labels (Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (Tensor): Dimension offsets.\\n                shape: (N, 3)\\n        \"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions",
            "def _decode_dimension(self, labels, dims_offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Transform dimension offsets to dimension according to its category.\\n\\n        Args:\\n            labels (Tensor): Each points' category id.\\n                shape: (N, K)\\n            dims_offset (Tensor): Dimension offsets.\\n                shape: (N, 3)\\n        \"\n    labels = labels.flatten().long()\n    base_dims = dims_offset.new_tensor(self.base_dims)\n    dims_select = base_dims[labels, :]\n    dimensions = dims_offset.exp() * dims_select\n    return dimensions"
        ]
    },
    {
        "func_name": "_decode_orientation",
        "original": "def _decode_orientation(self, ori_vector, locations):\n    \"\"\"Retrieve object orientation.\n\n        Args:\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\n                shape: (N, 2)\n            locations (Tensor): Object location.\n                shape: (N, 3)\n\n        Return:\n            Tensor: yaw(Orientation). Notice that the yaw's\n                range is [-np.pi, np.pi].\n                shape\uff1a(N, 1\uff09\n        \"\"\"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws",
        "mutated": [
            "def _decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n    \"Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\\n                shape: (N, 2)\\n            locations (Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Return:\\n            Tensor: yaw(Orientation). Notice that the yaw's\\n                range is [-np.pi, np.pi].\\n                shape\uff1a(N, 1\uff09\\n        \"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws",
            "def _decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\\n                shape: (N, 2)\\n            locations (Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Return:\\n            Tensor: yaw(Orientation). Notice that the yaw's\\n                range is [-np.pi, np.pi].\\n                shape\uff1a(N, 1\uff09\\n        \"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws",
            "def _decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\\n                shape: (N, 2)\\n            locations (Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Return:\\n            Tensor: yaw(Orientation). Notice that the yaw's\\n                range is [-np.pi, np.pi].\\n                shape\uff1a(N, 1\uff09\\n        \"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws",
            "def _decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\\n                shape: (N, 2)\\n            locations (Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Return:\\n            Tensor: yaw(Orientation). Notice that the yaw's\\n                range is [-np.pi, np.pi].\\n                shape\uff1a(N, 1\uff09\\n        \"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws",
            "def _decode_orientation(self, ori_vector, locations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Retrieve object orientation.\\n\\n        Args:\\n            ori_vector (Tensor): Local orientation in [sin, cos] format.\\n                shape: (N, 2)\\n            locations (Tensor): Object location.\\n                shape: (N, 3)\\n\\n        Return:\\n            Tensor: yaw(Orientation). Notice that the yaw's\\n                range is [-np.pi, np.pi].\\n                shape\uff1a(N, 1\uff09\\n        \"\n    assert len(ori_vector) == len(locations)\n    locations = locations.view(-1, 3)\n    rays = torch.atan(locations[:, 0] / (locations[:, 2] + 1e-07))\n    alphas = torch.atan(ori_vector[:, 0] / (ori_vector[:, 1] + 1e-07))\n    cos_pos_inds = (ori_vector[:, 1] >= 0).nonzero(as_tuple=False)\n    cos_neg_inds = (ori_vector[:, 1] < 0).nonzero(as_tuple=False)\n    alphas[cos_pos_inds] -= np.pi / 2\n    alphas[cos_neg_inds] += np.pi / 2\n    yaws = alphas + rays\n    larger_inds = (yaws > np.pi).nonzero(as_tuple=False)\n    small_inds = (yaws < -np.pi).nonzero(as_tuple=False)\n    if len(larger_inds) != 0:\n        yaws[larger_inds] -= 2 * np.pi\n    if len(small_inds) != 0:\n        yaws[small_inds] += 2 * np.pi\n    yaws = yaws.unsqueeze(-1)\n    return yaws"
        ]
    }
]