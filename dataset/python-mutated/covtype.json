[
    {
        "func_name": "normalize_cov_type",
        "original": "def normalize_cov_type(cov_type):\n    \"\"\"\n    Normalize the cov_type string to a canonical version\n\n    Parameters\n    ----------\n    cov_type : str\n\n    Returns\n    -------\n    normalized_cov_type : str\n    \"\"\"\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type",
        "mutated": [
            "def normalize_cov_type(cov_type):\n    if False:\n        i = 10\n    '\\n    Normalize the cov_type string to a canonical version\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n\\n    Returns\\n    -------\\n    normalized_cov_type : str\\n    '\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type",
            "def normalize_cov_type(cov_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Normalize the cov_type string to a canonical version\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n\\n    Returns\\n    -------\\n    normalized_cov_type : str\\n    '\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type",
            "def normalize_cov_type(cov_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Normalize the cov_type string to a canonical version\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n\\n    Returns\\n    -------\\n    normalized_cov_type : str\\n    '\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type",
            "def normalize_cov_type(cov_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Normalize the cov_type string to a canonical version\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n\\n    Returns\\n    -------\\n    normalized_cov_type : str\\n    '\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type",
            "def normalize_cov_type(cov_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Normalize the cov_type string to a canonical version\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n\\n    Returns\\n    -------\\n    normalized_cov_type : str\\n    '\n    if cov_type == 'nw-panel':\n        cov_type = 'hac-panel'\n    if cov_type == 'nw-groupsum':\n        cov_type = 'hac-groupsum'\n    return cov_type"
        ]
    },
    {
        "func_name": "get_robustcov_results",
        "original": "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    \"\"\"create new results instance with robust covariance as default\n\n    Parameters\n    ----------\n    cov_type : str\n        the type of robust sandwich estimator to use. see Notes below\n    use_t : bool\n        If true, then the t distribution is used for inference.\n        If false, then the normal distribution is used.\n    kwds : depends on cov_type\n        Required or optional arguments for robust covariance calculation.\n        see Notes below\n\n    Returns\n    -------\n    results : results instance\n        This method creates a new results instance with the requested\n        robust covariance as the default covariance of the parameters.\n        Inferential statistics like p-values and hypothesis tests will be\n        based on this covariance matrix.\n\n    Notes\n    -----\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\n    future version.\n\n    The covariance keywords provide an option 'scaling_factor' to adjust the\n    scaling of the covariance matrix, that is the covariance is multiplied by\n    this factor if it is given and is not `None`. This allows the user to\n    adjust the scaling of the covariance matrix to match other statistical\n    packages.\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\n    correction so that the robust covariance matrices match those of Stata in\n    some models like GLM and discrete Models.\n\n    The following covariance types and required or optional arguments are\n    currently available:\n\n    - 'HC0', 'HC1', 'HC2', 'HC3': heteroscedasticity robust covariance\n\n      - no keyword arguments\n\n    - 'HAC': heteroskedasticity-autocorrelation robust covariance\n\n      ``maxlags`` :  integer, required\n        number of lags to use\n\n      ``kernel`` : {callable, str}, optional\n        kernels currently available kernels are ['bartlett', 'uniform'],\n        default is Bartlett\n\n      ``use_correction``: bool, optional\n        If true, use small sample correction\n\n    - 'cluster': clustered covariance estimator\n\n      ``groups`` : array_like[int], required :\n        Integer-valued index of clusters or groups.\n\n      ``use_correction``: bool, optional\n        If True the sandwich covariance is calculated with a small\n        sample correction.\n        If False the sandwich covariance is calculated without\n        small sample correction.\n\n      ``df_correction``: bool, optional\n        If True (default), then the degrees of freedom for the\n        inferential statistics and hypothesis tests, such as\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\n        based on the number of groups minus one instead of the\n        total number of observations minus the number of explanatory\n        variables. `df_resid` of the results instance is also\n        adjusted. When `use_t` is also True, then pvalues are\n        computed using the Student's t distribution using the\n        corrected values. These may differ substantially from\n        p-values based on the normal is the number of groups is\n        small.\n        If False, then `df_resid` of the results instance is not\n        adjusted.\n\n\n    - 'hac-groupsum': Driscoll and Kraay, heteroscedasticity and\n      autocorrelation robust covariance for panel data\n      # TODO: more options needed here\n\n      ``time`` : array_like, required\n        index of time periods\n      ``maxlags`` : integer, required\n        number of lags to use\n      ``kernel`` : {callable, str}, optional\n        The available kernels are ['bartlett', 'uniform']. The default is\n        Bartlett.\n      ``use_correction`` : {False, 'hac', 'cluster'}, optional\n        If False the the sandwich covariance is calculated without small\n        sample correction. If `use_correction = 'cluster'` (default),\n        then the same small sample correction as in the case of\n        `covtype='cluster'` is used.\n      ``df_correction`` : bool, optional\n        The adjustment to df_resid, see cov_type 'cluster' above\n\n    - 'hac-panel': heteroscedasticity and autocorrelation robust standard\n      errors in panel data. The data needs to be sorted in this case, the\n      time series for each panel unit or cluster need to be stacked. The\n      membership to a time series of an individual or group can be either\n      specified by group indicators or by increasing time periods. One of\n      ``groups`` or ``time`` is required. # TODO: we need more options here\n\n      ``groups`` : array_like[int]\n        indicator for groups\n      ``time`` : array_like[int]\n        index of time periods\n      ``maxlags`` : int, required\n        number of lags to use\n      ``kernel`` : {callable, str}, optional\n        Available kernels are ['bartlett', 'uniform'], default\n        is Bartlett\n      ``use_correction`` : {False, 'hac', 'cluster'}, optional\n        If False the sandwich covariance is calculated without\n        small sample correction.\n      ``df_correction`` : bool, optional\n        Adjustment to df_resid, see cov_type 'cluster' above\n\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\n    not bool, needs to be in {False, 'hac', 'cluster'}.\n\n    .. todo:: Currently there is no check for extra or misspelled keywords,\n         except in the case of cov_type `HCx`\n    \"\"\"\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res",
        "mutated": [
            "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    if False:\n        i = 10\n    'create new results instance with robust covariance as default\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n        the type of robust sandwich estimator to use. see Notes below\\n    use_t : bool\\n        If true, then the t distribution is used for inference.\\n        If false, then the normal distribution is used.\\n    kwds : depends on cov_type\\n        Required or optional arguments for robust covariance calculation.\\n        see Notes below\\n\\n    Returns\\n    -------\\n    results : results instance\\n        This method creates a new results instance with the requested\\n        robust covariance as the default covariance of the parameters.\\n        Inferential statistics like p-values and hypothesis tests will be\\n        based on this covariance matrix.\\n\\n    Notes\\n    -----\\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\\n    future version.\\n\\n    The covariance keywords provide an option \\'scaling_factor\\' to adjust the\\n    scaling of the covariance matrix, that is the covariance is multiplied by\\n    this factor if it is given and is not `None`. This allows the user to\\n    adjust the scaling of the covariance matrix to match other statistical\\n    packages.\\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\\n    correction so that the robust covariance matrices match those of Stata in\\n    some models like GLM and discrete Models.\\n\\n    The following covariance types and required or optional arguments are\\n    currently available:\\n\\n    - \\'HC0\\', \\'HC1\\', \\'HC2\\', \\'HC3\\': heteroscedasticity robust covariance\\n\\n      - no keyword arguments\\n\\n    - \\'HAC\\': heteroskedasticity-autocorrelation robust covariance\\n\\n      ``maxlags`` :  integer, required\\n        number of lags to use\\n\\n      ``kernel`` : {callable, str}, optional\\n        kernels currently available kernels are [\\'bartlett\\', \\'uniform\\'],\\n        default is Bartlett\\n\\n      ``use_correction``: bool, optional\\n        If true, use small sample correction\\n\\n    - \\'cluster\\': clustered covariance estimator\\n\\n      ``groups`` : array_like[int], required :\\n        Integer-valued index of clusters or groups.\\n\\n      ``use_correction``: bool, optional\\n        If True the sandwich covariance is calculated with a small\\n        sample correction.\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n\\n      ``df_correction``: bool, optional\\n        If True (default), then the degrees of freedom for the\\n        inferential statistics and hypothesis tests, such as\\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\\n        based on the number of groups minus one instead of the\\n        total number of observations minus the number of explanatory\\n        variables. `df_resid` of the results instance is also\\n        adjusted. When `use_t` is also True, then pvalues are\\n        computed using the Student\\'s t distribution using the\\n        corrected values. These may differ substantially from\\n        p-values based on the normal is the number of groups is\\n        small.\\n        If False, then `df_resid` of the results instance is not\\n        adjusted.\\n\\n\\n    - \\'hac-groupsum\\': Driscoll and Kraay, heteroscedasticity and\\n      autocorrelation robust covariance for panel data\\n      # TODO: more options needed here\\n\\n      ``time`` : array_like, required\\n        index of time periods\\n      ``maxlags`` : integer, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        The available kernels are [\\'bartlett\\', \\'uniform\\']. The default is\\n        Bartlett.\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the the sandwich covariance is calculated without small\\n        sample correction. If `use_correction = \\'cluster\\'` (default),\\n        then the same small sample correction as in the case of\\n        `covtype=\\'cluster\\'` is used.\\n      ``df_correction`` : bool, optional\\n        The adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    - \\'hac-panel\\': heteroscedasticity and autocorrelation robust standard\\n      errors in panel data. The data needs to be sorted in this case, the\\n      time series for each panel unit or cluster need to be stacked. The\\n      membership to a time series of an individual or group can be either\\n      specified by group indicators or by increasing time periods. One of\\n      ``groups`` or ``time`` is required. # TODO: we need more options here\\n\\n      ``groups`` : array_like[int]\\n        indicator for groups\\n      ``time`` : array_like[int]\\n        index of time periods\\n      ``maxlags`` : int, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        Available kernels are [\\'bartlett\\', \\'uniform\\'], default\\n        is Bartlett\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n      ``df_correction`` : bool, optional\\n        Adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\\n    not bool, needs to be in {False, \\'hac\\', \\'cluster\\'}.\\n\\n    .. todo:: Currently there is no check for extra or misspelled keywords,\\n         except in the case of cov_type `HCx`\\n    '\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res",
            "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'create new results instance with robust covariance as default\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n        the type of robust sandwich estimator to use. see Notes below\\n    use_t : bool\\n        If true, then the t distribution is used for inference.\\n        If false, then the normal distribution is used.\\n    kwds : depends on cov_type\\n        Required or optional arguments for robust covariance calculation.\\n        see Notes below\\n\\n    Returns\\n    -------\\n    results : results instance\\n        This method creates a new results instance with the requested\\n        robust covariance as the default covariance of the parameters.\\n        Inferential statistics like p-values and hypothesis tests will be\\n        based on this covariance matrix.\\n\\n    Notes\\n    -----\\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\\n    future version.\\n\\n    The covariance keywords provide an option \\'scaling_factor\\' to adjust the\\n    scaling of the covariance matrix, that is the covariance is multiplied by\\n    this factor if it is given and is not `None`. This allows the user to\\n    adjust the scaling of the covariance matrix to match other statistical\\n    packages.\\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\\n    correction so that the robust covariance matrices match those of Stata in\\n    some models like GLM and discrete Models.\\n\\n    The following covariance types and required or optional arguments are\\n    currently available:\\n\\n    - \\'HC0\\', \\'HC1\\', \\'HC2\\', \\'HC3\\': heteroscedasticity robust covariance\\n\\n      - no keyword arguments\\n\\n    - \\'HAC\\': heteroskedasticity-autocorrelation robust covariance\\n\\n      ``maxlags`` :  integer, required\\n        number of lags to use\\n\\n      ``kernel`` : {callable, str}, optional\\n        kernels currently available kernels are [\\'bartlett\\', \\'uniform\\'],\\n        default is Bartlett\\n\\n      ``use_correction``: bool, optional\\n        If true, use small sample correction\\n\\n    - \\'cluster\\': clustered covariance estimator\\n\\n      ``groups`` : array_like[int], required :\\n        Integer-valued index of clusters or groups.\\n\\n      ``use_correction``: bool, optional\\n        If True the sandwich covariance is calculated with a small\\n        sample correction.\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n\\n      ``df_correction``: bool, optional\\n        If True (default), then the degrees of freedom for the\\n        inferential statistics and hypothesis tests, such as\\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\\n        based on the number of groups minus one instead of the\\n        total number of observations minus the number of explanatory\\n        variables. `df_resid` of the results instance is also\\n        adjusted. When `use_t` is also True, then pvalues are\\n        computed using the Student\\'s t distribution using the\\n        corrected values. These may differ substantially from\\n        p-values based on the normal is the number of groups is\\n        small.\\n        If False, then `df_resid` of the results instance is not\\n        adjusted.\\n\\n\\n    - \\'hac-groupsum\\': Driscoll and Kraay, heteroscedasticity and\\n      autocorrelation robust covariance for panel data\\n      # TODO: more options needed here\\n\\n      ``time`` : array_like, required\\n        index of time periods\\n      ``maxlags`` : integer, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        The available kernels are [\\'bartlett\\', \\'uniform\\']. The default is\\n        Bartlett.\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the the sandwich covariance is calculated without small\\n        sample correction. If `use_correction = \\'cluster\\'` (default),\\n        then the same small sample correction as in the case of\\n        `covtype=\\'cluster\\'` is used.\\n      ``df_correction`` : bool, optional\\n        The adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    - \\'hac-panel\\': heteroscedasticity and autocorrelation robust standard\\n      errors in panel data. The data needs to be sorted in this case, the\\n      time series for each panel unit or cluster need to be stacked. The\\n      membership to a time series of an individual or group can be either\\n      specified by group indicators or by increasing time periods. One of\\n      ``groups`` or ``time`` is required. # TODO: we need more options here\\n\\n      ``groups`` : array_like[int]\\n        indicator for groups\\n      ``time`` : array_like[int]\\n        index of time periods\\n      ``maxlags`` : int, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        Available kernels are [\\'bartlett\\', \\'uniform\\'], default\\n        is Bartlett\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n      ``df_correction`` : bool, optional\\n        Adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\\n    not bool, needs to be in {False, \\'hac\\', \\'cluster\\'}.\\n\\n    .. todo:: Currently there is no check for extra or misspelled keywords,\\n         except in the case of cov_type `HCx`\\n    '\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res",
            "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'create new results instance with robust covariance as default\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n        the type of robust sandwich estimator to use. see Notes below\\n    use_t : bool\\n        If true, then the t distribution is used for inference.\\n        If false, then the normal distribution is used.\\n    kwds : depends on cov_type\\n        Required or optional arguments for robust covariance calculation.\\n        see Notes below\\n\\n    Returns\\n    -------\\n    results : results instance\\n        This method creates a new results instance with the requested\\n        robust covariance as the default covariance of the parameters.\\n        Inferential statistics like p-values and hypothesis tests will be\\n        based on this covariance matrix.\\n\\n    Notes\\n    -----\\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\\n    future version.\\n\\n    The covariance keywords provide an option \\'scaling_factor\\' to adjust the\\n    scaling of the covariance matrix, that is the covariance is multiplied by\\n    this factor if it is given and is not `None`. This allows the user to\\n    adjust the scaling of the covariance matrix to match other statistical\\n    packages.\\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\\n    correction so that the robust covariance matrices match those of Stata in\\n    some models like GLM and discrete Models.\\n\\n    The following covariance types and required or optional arguments are\\n    currently available:\\n\\n    - \\'HC0\\', \\'HC1\\', \\'HC2\\', \\'HC3\\': heteroscedasticity robust covariance\\n\\n      - no keyword arguments\\n\\n    - \\'HAC\\': heteroskedasticity-autocorrelation robust covariance\\n\\n      ``maxlags`` :  integer, required\\n        number of lags to use\\n\\n      ``kernel`` : {callable, str}, optional\\n        kernels currently available kernels are [\\'bartlett\\', \\'uniform\\'],\\n        default is Bartlett\\n\\n      ``use_correction``: bool, optional\\n        If true, use small sample correction\\n\\n    - \\'cluster\\': clustered covariance estimator\\n\\n      ``groups`` : array_like[int], required :\\n        Integer-valued index of clusters or groups.\\n\\n      ``use_correction``: bool, optional\\n        If True the sandwich covariance is calculated with a small\\n        sample correction.\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n\\n      ``df_correction``: bool, optional\\n        If True (default), then the degrees of freedom for the\\n        inferential statistics and hypothesis tests, such as\\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\\n        based on the number of groups minus one instead of the\\n        total number of observations minus the number of explanatory\\n        variables. `df_resid` of the results instance is also\\n        adjusted. When `use_t` is also True, then pvalues are\\n        computed using the Student\\'s t distribution using the\\n        corrected values. These may differ substantially from\\n        p-values based on the normal is the number of groups is\\n        small.\\n        If False, then `df_resid` of the results instance is not\\n        adjusted.\\n\\n\\n    - \\'hac-groupsum\\': Driscoll and Kraay, heteroscedasticity and\\n      autocorrelation robust covariance for panel data\\n      # TODO: more options needed here\\n\\n      ``time`` : array_like, required\\n        index of time periods\\n      ``maxlags`` : integer, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        The available kernels are [\\'bartlett\\', \\'uniform\\']. The default is\\n        Bartlett.\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the the sandwich covariance is calculated without small\\n        sample correction. If `use_correction = \\'cluster\\'` (default),\\n        then the same small sample correction as in the case of\\n        `covtype=\\'cluster\\'` is used.\\n      ``df_correction`` : bool, optional\\n        The adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    - \\'hac-panel\\': heteroscedasticity and autocorrelation robust standard\\n      errors in panel data. The data needs to be sorted in this case, the\\n      time series for each panel unit or cluster need to be stacked. The\\n      membership to a time series of an individual or group can be either\\n      specified by group indicators or by increasing time periods. One of\\n      ``groups`` or ``time`` is required. # TODO: we need more options here\\n\\n      ``groups`` : array_like[int]\\n        indicator for groups\\n      ``time`` : array_like[int]\\n        index of time periods\\n      ``maxlags`` : int, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        Available kernels are [\\'bartlett\\', \\'uniform\\'], default\\n        is Bartlett\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n      ``df_correction`` : bool, optional\\n        Adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\\n    not bool, needs to be in {False, \\'hac\\', \\'cluster\\'}.\\n\\n    .. todo:: Currently there is no check for extra or misspelled keywords,\\n         except in the case of cov_type `HCx`\\n    '\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res",
            "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'create new results instance with robust covariance as default\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n        the type of robust sandwich estimator to use. see Notes below\\n    use_t : bool\\n        If true, then the t distribution is used for inference.\\n        If false, then the normal distribution is used.\\n    kwds : depends on cov_type\\n        Required or optional arguments for robust covariance calculation.\\n        see Notes below\\n\\n    Returns\\n    -------\\n    results : results instance\\n        This method creates a new results instance with the requested\\n        robust covariance as the default covariance of the parameters.\\n        Inferential statistics like p-values and hypothesis tests will be\\n        based on this covariance matrix.\\n\\n    Notes\\n    -----\\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\\n    future version.\\n\\n    The covariance keywords provide an option \\'scaling_factor\\' to adjust the\\n    scaling of the covariance matrix, that is the covariance is multiplied by\\n    this factor if it is given and is not `None`. This allows the user to\\n    adjust the scaling of the covariance matrix to match other statistical\\n    packages.\\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\\n    correction so that the robust covariance matrices match those of Stata in\\n    some models like GLM and discrete Models.\\n\\n    The following covariance types and required or optional arguments are\\n    currently available:\\n\\n    - \\'HC0\\', \\'HC1\\', \\'HC2\\', \\'HC3\\': heteroscedasticity robust covariance\\n\\n      - no keyword arguments\\n\\n    - \\'HAC\\': heteroskedasticity-autocorrelation robust covariance\\n\\n      ``maxlags`` :  integer, required\\n        number of lags to use\\n\\n      ``kernel`` : {callable, str}, optional\\n        kernels currently available kernels are [\\'bartlett\\', \\'uniform\\'],\\n        default is Bartlett\\n\\n      ``use_correction``: bool, optional\\n        If true, use small sample correction\\n\\n    - \\'cluster\\': clustered covariance estimator\\n\\n      ``groups`` : array_like[int], required :\\n        Integer-valued index of clusters or groups.\\n\\n      ``use_correction``: bool, optional\\n        If True the sandwich covariance is calculated with a small\\n        sample correction.\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n\\n      ``df_correction``: bool, optional\\n        If True (default), then the degrees of freedom for the\\n        inferential statistics and hypothesis tests, such as\\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\\n        based on the number of groups minus one instead of the\\n        total number of observations minus the number of explanatory\\n        variables. `df_resid` of the results instance is also\\n        adjusted. When `use_t` is also True, then pvalues are\\n        computed using the Student\\'s t distribution using the\\n        corrected values. These may differ substantially from\\n        p-values based on the normal is the number of groups is\\n        small.\\n        If False, then `df_resid` of the results instance is not\\n        adjusted.\\n\\n\\n    - \\'hac-groupsum\\': Driscoll and Kraay, heteroscedasticity and\\n      autocorrelation robust covariance for panel data\\n      # TODO: more options needed here\\n\\n      ``time`` : array_like, required\\n        index of time periods\\n      ``maxlags`` : integer, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        The available kernels are [\\'bartlett\\', \\'uniform\\']. The default is\\n        Bartlett.\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the the sandwich covariance is calculated without small\\n        sample correction. If `use_correction = \\'cluster\\'` (default),\\n        then the same small sample correction as in the case of\\n        `covtype=\\'cluster\\'` is used.\\n      ``df_correction`` : bool, optional\\n        The adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    - \\'hac-panel\\': heteroscedasticity and autocorrelation robust standard\\n      errors in panel data. The data needs to be sorted in this case, the\\n      time series for each panel unit or cluster need to be stacked. The\\n      membership to a time series of an individual or group can be either\\n      specified by group indicators or by increasing time periods. One of\\n      ``groups`` or ``time`` is required. # TODO: we need more options here\\n\\n      ``groups`` : array_like[int]\\n        indicator for groups\\n      ``time`` : array_like[int]\\n        index of time periods\\n      ``maxlags`` : int, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        Available kernels are [\\'bartlett\\', \\'uniform\\'], default\\n        is Bartlett\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n      ``df_correction`` : bool, optional\\n        Adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\\n    not bool, needs to be in {False, \\'hac\\', \\'cluster\\'}.\\n\\n    .. todo:: Currently there is no check for extra or misspelled keywords,\\n         except in the case of cov_type `HCx`\\n    '\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res",
            "def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'create new results instance with robust covariance as default\\n\\n    Parameters\\n    ----------\\n    cov_type : str\\n        the type of robust sandwich estimator to use. see Notes below\\n    use_t : bool\\n        If true, then the t distribution is used for inference.\\n        If false, then the normal distribution is used.\\n    kwds : depends on cov_type\\n        Required or optional arguments for robust covariance calculation.\\n        see Notes below\\n\\n    Returns\\n    -------\\n    results : results instance\\n        This method creates a new results instance with the requested\\n        robust covariance as the default covariance of the parameters.\\n        Inferential statistics like p-values and hypothesis tests will be\\n        based on this covariance matrix.\\n\\n    Notes\\n    -----\\n    Warning: Some of the options and defaults in cov_kwds may be changed in a\\n    future version.\\n\\n    The covariance keywords provide an option \\'scaling_factor\\' to adjust the\\n    scaling of the covariance matrix, that is the covariance is multiplied by\\n    this factor if it is given and is not `None`. This allows the user to\\n    adjust the scaling of the covariance matrix to match other statistical\\n    packages.\\n    For example, `scaling_factor=(nobs - 1.) / (nobs - k_params)` provides a\\n    correction so that the robust covariance matrices match those of Stata in\\n    some models like GLM and discrete Models.\\n\\n    The following covariance types and required or optional arguments are\\n    currently available:\\n\\n    - \\'HC0\\', \\'HC1\\', \\'HC2\\', \\'HC3\\': heteroscedasticity robust covariance\\n\\n      - no keyword arguments\\n\\n    - \\'HAC\\': heteroskedasticity-autocorrelation robust covariance\\n\\n      ``maxlags`` :  integer, required\\n        number of lags to use\\n\\n      ``kernel`` : {callable, str}, optional\\n        kernels currently available kernels are [\\'bartlett\\', \\'uniform\\'],\\n        default is Bartlett\\n\\n      ``use_correction``: bool, optional\\n        If true, use small sample correction\\n\\n    - \\'cluster\\': clustered covariance estimator\\n\\n      ``groups`` : array_like[int], required :\\n        Integer-valued index of clusters or groups.\\n\\n      ``use_correction``: bool, optional\\n        If True the sandwich covariance is calculated with a small\\n        sample correction.\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n\\n      ``df_correction``: bool, optional\\n        If True (default), then the degrees of freedom for the\\n        inferential statistics and hypothesis tests, such as\\n        pvalues, f_pvalue, conf_int, and t_test and f_test, are\\n        based on the number of groups minus one instead of the\\n        total number of observations minus the number of explanatory\\n        variables. `df_resid` of the results instance is also\\n        adjusted. When `use_t` is also True, then pvalues are\\n        computed using the Student\\'s t distribution using the\\n        corrected values. These may differ substantially from\\n        p-values based on the normal is the number of groups is\\n        small.\\n        If False, then `df_resid` of the results instance is not\\n        adjusted.\\n\\n\\n    - \\'hac-groupsum\\': Driscoll and Kraay, heteroscedasticity and\\n      autocorrelation robust covariance for panel data\\n      # TODO: more options needed here\\n\\n      ``time`` : array_like, required\\n        index of time periods\\n      ``maxlags`` : integer, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        The available kernels are [\\'bartlett\\', \\'uniform\\']. The default is\\n        Bartlett.\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the the sandwich covariance is calculated without small\\n        sample correction. If `use_correction = \\'cluster\\'` (default),\\n        then the same small sample correction as in the case of\\n        `covtype=\\'cluster\\'` is used.\\n      ``df_correction`` : bool, optional\\n        The adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    - \\'hac-panel\\': heteroscedasticity and autocorrelation robust standard\\n      errors in panel data. The data needs to be sorted in this case, the\\n      time series for each panel unit or cluster need to be stacked. The\\n      membership to a time series of an individual or group can be either\\n      specified by group indicators or by increasing time periods. One of\\n      ``groups`` or ``time`` is required. # TODO: we need more options here\\n\\n      ``groups`` : array_like[int]\\n        indicator for groups\\n      ``time`` : array_like[int]\\n        index of time periods\\n      ``maxlags`` : int, required\\n        number of lags to use\\n      ``kernel`` : {callable, str}, optional\\n        Available kernels are [\\'bartlett\\', \\'uniform\\'], default\\n        is Bartlett\\n      ``use_correction`` : {False, \\'hac\\', \\'cluster\\'}, optional\\n        If False the sandwich covariance is calculated without\\n        small sample correction.\\n      ``df_correction`` : bool, optional\\n        Adjustment to df_resid, see cov_type \\'cluster\\' above\\n\\n    **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\\n    not bool, needs to be in {False, \\'hac\\', \\'cluster\\'}.\\n\\n    .. todo:: Currently there is no check for extra or misspelled keywords,\\n         except in the case of cov_type `HCx`\\n    '\n    import statsmodels.stats.sandwich_covariance as sw\n    cov_type = normalize_cov_type(cov_type)\n    if 'kernel' in kwds:\n        kwds['weights_func'] = kwds.pop('kernel')\n    if 'weights_func' in kwds and (not callable(kwds['weights_func'])):\n        kwds['weights_func'] = sw.kernel_dict[kwds['weights_func']]\n    sc_factor = kwds.pop('scaling_factor', None)\n    use_self = kwds.pop('use_self', False)\n    if use_self:\n        res = self\n    else:\n        res = self.__class__(self.model, self.params, normalized_cov_params=self.normalized_cov_params, scale=self.scale)\n    res.cov_type = cov_type\n    if use_t is None:\n        use_t = self.use_t\n    res.cov_kwds = {'use_t': use_t}\n    res.use_t = use_t\n    adjust_df = False\n    if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n        df_correction = kwds.get('df_correction', None)\n        if df_correction is not False:\n            adjust_df = True\n    res.cov_kwds['adjust_df'] = adjust_df\n    if cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n        if kwds:\n            raise ValueError('heteroscedasticity robust covariance does not use keywords')\n        res.cov_kwds['description'] = descriptions[cov_type.upper()]\n        res.cov_params_default = getattr(self, 'cov_' + cov_type.upper(), None)\n        if res.cov_params_default is None:\n            res.cov_params_default = sw.cov_white_simple(self, use_correction=False)\n    elif cov_type.lower() == 'hac':\n        maxlags = kwds['maxlags']\n        res.cov_kwds['maxlags'] = maxlags\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        use_correction = kwds.get('use_correction', False)\n        res.cov_kwds['use_correction'] = use_correction\n        res.cov_kwds['description'] = descriptions['HAC'].format(maxlags=maxlags, correction=['without', 'with'][use_correction])\n        res.cov_params_default = sw.cov_hac_simple(self, nlags=maxlags, weights_func=weights_func, use_correction=use_correction)\n    elif cov_type.lower() == 'cluster':\n        groups = kwds['groups']\n        if not hasattr(groups, 'shape'):\n            groups = np.asarray(groups).T\n        if groups.ndim >= 2:\n            groups = groups.squeeze()\n        res.cov_kwds['groups'] = groups\n        use_correction = kwds.get('use_correction', True)\n        res.cov_kwds['use_correction'] = use_correction\n        if groups.ndim == 1:\n            if adjust_df:\n                self.n_groups = n_groups = len(np.unique(groups))\n            res.cov_params_default = sw.cov_cluster(self, groups, use_correction=use_correction)\n        elif groups.ndim == 2:\n            if hasattr(groups, 'values'):\n                groups = groups.values\n            if adjust_df:\n                n_groups0 = len(np.unique(groups[:, 0]))\n                n_groups1 = len(np.unique(groups[:, 1]))\n                self.n_groups = (n_groups0, n_groups1)\n                n_groups = min(n_groups0, n_groups1)\n            res.cov_params_default = sw.cov_cluster_2groups(self, groups, use_correction=use_correction)[0]\n        else:\n            raise ValueError('only two groups are supported')\n        res.cov_kwds['description'] = descriptions['cluster']\n    elif cov_type.lower() == 'hac-panel':\n        res.cov_kwds['time'] = time = kwds.get('time', None)\n        res.cov_kwds['groups'] = groups = kwds.get('groups', None)\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'hac')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if groups is not None:\n            groups = np.asarray(groups)\n            tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n            nobs_ = len(groups)\n        elif time is not None:\n            time = np.asarray(time)\n            tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n            nobs_ = len(time)\n        else:\n            raise ValueError('either time or groups needs to be given')\n        groupidx = lzip([0] + tt, tt + [nobs_])\n        self.n_groups = n_groups = len(groupidx)\n        res.cov_params_default = sw.cov_nw_panel(self, maxlags, groupidx, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Panel']\n    elif cov_type.lower() == 'hac-groupsum':\n        res.cov_kwds['time'] = time = kwds['time']\n        res.cov_kwds['maxlags'] = maxlags = kwds['maxlags']\n        use_correction = kwds.get('use_correction', 'cluster')\n        res.cov_kwds['use_correction'] = use_correction\n        weights_func = kwds.get('weights_func', sw.weights_bartlett)\n        res.cov_kwds['weights_func'] = weights_func\n        if adjust_df:\n            tt = np.nonzero(time[1:] < time[:-1])[0] + 1\n            self.n_groups = n_groups = len(tt) + 1\n        res.cov_params_default = sw.cov_nw_groupsum(self, maxlags, time, weights_func=weights_func, use_correction=use_correction)\n        res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n    else:\n        raise ValueError('cov_type not recognized. See docstring for ' + 'available options and spelling')\n    res.cov_kwds['scaling_factor'] = sc_factor\n    if sc_factor is not None:\n        res.cov_params_default *= sc_factor\n    if adjust_df:\n        res.df_resid_inference = n_groups - 1\n    return res"
        ]
    }
]