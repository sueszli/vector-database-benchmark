[
    {
        "func_name": "__init__",
        "original": "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class",
        "mutated": [
            "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class",
            "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class",
            "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class",
            "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class",
            "def __init__(self, estimator, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(ROCAUC, self).__init__(estimator, ax=ax, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    self.binary = binary\n    if self.binary:\n        self.micro = False\n        self.macro = False\n        self.per_class = False\n    else:\n        self.micro = micro\n        self.macro = macro\n        self.per_class = per_class"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, X, y=None):\n    \"\"\"\n        Fit the classification model.\n        \"\"\"\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)",
        "mutated": [
            "def fit(self, X, y=None):\n    if False:\n        i = 10\n    '\\n        Fit the classification model.\\n        '\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)",
            "def fit(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fit the classification model.\\n        '\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)",
            "def fit(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fit the classification model.\\n        '\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)",
            "def fit(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fit the classification model.\\n        '\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)",
            "def fit(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fit the classification model.\\n        '\n    ttype = type_of_target(y)\n    if ttype.startswith(MULTICLASS):\n        self.target_type_ = MULTICLASS\n    elif ttype.startswith(BINARY):\n        self.target_type_ = BINARY\n    else:\n        raise YellowbrickValueError(\"{} does not support target type '{}', please provide a binary or multiclass single-output target\".format(self.__class__.__name__, ttype))\n    return super(ROCAUC, self).fit(X, y)"
        ]
    },
    {
        "func_name": "score",
        "original": "def score(self, X, y=None):\n    \"\"\"\n        Generates the predicted target values using the Scikit-Learn\n        estimator.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features\n\n        y : ndarray or Series of length n\n            An array or series of target or class values\n\n        Returns\n        -------\n        score_ : float\n            Global accuracy unless micro or macro scores are requested.\n        \"\"\"\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_",
        "mutated": [
            "def score(self, X, y=None):\n    if False:\n        i = 10\n    '\\n        Generates the predicted target values using the Scikit-Learn\\n        estimator.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values\\n\\n        Returns\\n        -------\\n        score_ : float\\n            Global accuracy unless micro or macro scores are requested.\\n        '\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_",
            "def score(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Generates the predicted target values using the Scikit-Learn\\n        estimator.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values\\n\\n        Returns\\n        -------\\n        score_ : float\\n            Global accuracy unless micro or macro scores are requested.\\n        '\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_",
            "def score(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Generates the predicted target values using the Scikit-Learn\\n        estimator.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values\\n\\n        Returns\\n        -------\\n        score_ : float\\n            Global accuracy unless micro or macro scores are requested.\\n        '\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_",
            "def score(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Generates the predicted target values using the Scikit-Learn\\n        estimator.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values\\n\\n        Returns\\n        -------\\n        score_ : float\\n            Global accuracy unless micro or macro scores are requested.\\n        '\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_",
            "def score(self, X, y=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Generates the predicted target values using the Scikit-Learn\\n        estimator.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features\\n\\n        y : ndarray or Series of length n\\n            An array or series of target or class values\\n\\n        Returns\\n        -------\\n        score_ : float\\n            Global accuracy unless micro or macro scores are requested.\\n        '\n    super(ROCAUC, self).score(X, y)\n    y_pred = self._get_y_scores(X)\n    if self.target_type_ == BINARY:\n        if (self.micro or self.macro) and (not self.per_class):\n            raise ModelError('no curves will be drawn; ', 'set per_class=True or micro=False and macro=False.')\n        if (self.micro or self.macro) and len(y_pred.shape) == 1:\n            raise ModelError('no curves will be drawn; set binary=True.')\n    if self.target_type_ == MULTICLASS:\n        if not self.micro and (not self.macro) and (not self.per_class):\n            raise YellowbrickValueError('no curves will be drawn; specify micro, macro, or per_class')\n    classes = np.unique(y)\n    n_classes = len(classes)\n    self.fpr = dict()\n    self.tpr = dict()\n    self.roc_auc = dict()\n    if self.target_type_ is BINARY and (not self.per_class):\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[BINARY], self.tpr[BINARY], _) = roc_curve(y, y_pred)\n        self.roc_auc[BINARY] = auc(self.fpr[BINARY], self.tpr[BINARY])\n    elif self.target_type_ is BINARY and self.per_class:\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred[:, 1])\n        else:\n            (self.fpr[1], self.tpr[1], _) = roc_curve(y, y_pred)\n        self.roc_auc[1] = auc(self.fpr[1], self.tpr[1])\n        if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, y_pred[:, 0])\n        else:\n            (self.fpr[0], self.tpr[0], _) = roc_curve(1 - y, -y_pred)\n        self.roc_auc[0] = auc(self.fpr[0], self.tpr[0])\n    else:\n        for (i, c) in enumerate(classes):\n            (self.fpr[i], self.tpr[i], _) = roc_curve(y, y_pred[:, i], pos_label=c)\n            self.roc_auc[i] = auc(self.fpr[i], self.tpr[i])\n    if self.micro:\n        self._score_micro_average(y, y_pred, classes, n_classes)\n    if self.macro:\n        self._score_macro_average(n_classes)\n    self.draw()\n    if self.micro:\n        self.score_ = self.roc_auc[MICRO]\n    if self.macro:\n        self.score_ = self.roc_auc[MACRO]\n    return self.score_"
        ]
    },
    {
        "func_name": "draw",
        "original": "def draw(self):\n    \"\"\"\n        Renders ROC-AUC plot.\n        Called internally by score, possibly more than once\n\n        Returns\n        -------\n        ax : the axis with the plotted figure\n        \"\"\"\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax",
        "mutated": [
            "def draw(self):\n    if False:\n        i = 10\n    '\\n        Renders ROC-AUC plot.\\n        Called internally by score, possibly more than once\\n\\n        Returns\\n        -------\\n        ax : the axis with the plotted figure\\n        '\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Renders ROC-AUC plot.\\n        Called internally by score, possibly more than once\\n\\n        Returns\\n        -------\\n        ax : the axis with the plotted figure\\n        '\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Renders ROC-AUC plot.\\n        Called internally by score, possibly more than once\\n\\n        Returns\\n        -------\\n        ax : the axis with the plotted figure\\n        '\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Renders ROC-AUC plot.\\n        Called internally by score, possibly more than once\\n\\n        Returns\\n        -------\\n        ax : the axis with the plotted figure\\n        '\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax",
            "def draw(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Renders ROC-AUC plot.\\n        Called internally by score, possibly more than once\\n\\n        Returns\\n        -------\\n        ax : the axis with the plotted figure\\n        '\n    colors = self.class_colors_[0:len(self.classes_)]\n    n_classes = len(colors)\n    if self.target_type_ == BINARY and (not self.per_class):\n        self.ax.plot(self.fpr[BINARY], self.tpr[BINARY], label='ROC for binary decision, AUC = {:0.2f}'.format(self.roc_auc[BINARY]))\n    if self.per_class:\n        for (i, color) in zip(range(n_classes), colors):\n            self.ax.plot(self.fpr[i], self.tpr[i], color=color, label='ROC of class {}, AUC = {:0.2f}'.format(self.classes_[i], self.roc_auc[i]))\n    if self.micro:\n        self.ax.plot(self.fpr[MICRO], self.tpr[MICRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='micro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['micro']))\n    if self.macro:\n        self.ax.plot(self.fpr[MACRO], self.tpr[MACRO], linestyle='--', color=self.class_colors_[len(self.classes_) - 1], label='macro-average ROC curve, AUC = {:0.2f}'.format(self.roc_auc['macro']))\n    self.ax.plot([0, 1], [0, 1], linestyle=':', c=LINE_COLOR)\n    return self.ax"
        ]
    },
    {
        "func_name": "finalize",
        "original": "def finalize(self, **kwargs):\n    \"\"\"\n        Sets a title and axis labels of the figures and ensures the axis limits\n        are scaled between the valid ROCAUC score values.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        Notes\n        -----\n        Generally this method is called from show and not directly by the user.\n        \"\"\"\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')",
        "mutated": [
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n    '\\n        Sets a title and axis labels of the figures and ensures the axis limits\\n        are scaled between the valid ROCAUC score values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Sets a title and axis labels of the figures and ensures the axis limits\\n        are scaled between the valid ROCAUC score values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Sets a title and axis labels of the figures and ensures the axis limits\\n        are scaled between the valid ROCAUC score values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Sets a title and axis labels of the figures and ensures the axis limits\\n        are scaled between the valid ROCAUC score values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')",
            "def finalize(self, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Sets a title and axis labels of the figures and ensures the axis limits\\n        are scaled between the valid ROCAUC score values.\\n\\n        Parameters\\n        ----------\\n        kwargs: generic keyword arguments.\\n\\n        Notes\\n        -----\\n        Generally this method is called from show and not directly by the user.\\n        '\n    self.set_title('ROC Curves for {}'.format(self.name))\n    self.ax.legend(loc='lower right', frameon=True)\n    self.ax.set_xlim([0.0, 1.0])\n    self.ax.set_ylim([0.0, 1.0])\n    self.ax.set_ylabel('True Positive Rate')\n    self.ax.set_xlabel('False Positive Rate')"
        ]
    },
    {
        "func_name": "_get_y_scores",
        "original": "def _get_y_scores(self, X):\n    \"\"\"\n        The ``roc_curve`` metric requires target scores that can either be the\n        probability estimates of the positive class, confidence values or non-\n        thresholded measure of decisions (as returned by \"decision_function\").\n\n        This method computes the scores by resolving the estimator methods\n        that retreive these values.\n\n        .. todo:: implement confidence values metric.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features -- generally the test data\n            that is associated with y_true values.\n        \"\"\"\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')",
        "mutated": [
            "def _get_y_scores(self, X):\n    if False:\n        i = 10\n    '\\n        The ``roc_curve`` metric requires target scores that can either be the\\n        probability estimates of the positive class, confidence values or non-\\n        thresholded measure of decisions (as returned by \"decision_function\").\\n\\n        This method computes the scores by resolving the estimator methods\\n        that retreive these values.\\n\\n        .. todo:: implement confidence values metric.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features -- generally the test data\\n            that is associated with y_true values.\\n        '\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')",
            "def _get_y_scores(self, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        The ``roc_curve`` metric requires target scores that can either be the\\n        probability estimates of the positive class, confidence values or non-\\n        thresholded measure of decisions (as returned by \"decision_function\").\\n\\n        This method computes the scores by resolving the estimator methods\\n        that retreive these values.\\n\\n        .. todo:: implement confidence values metric.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features -- generally the test data\\n            that is associated with y_true values.\\n        '\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')",
            "def _get_y_scores(self, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        The ``roc_curve`` metric requires target scores that can either be the\\n        probability estimates of the positive class, confidence values or non-\\n        thresholded measure of decisions (as returned by \"decision_function\").\\n\\n        This method computes the scores by resolving the estimator methods\\n        that retreive these values.\\n\\n        .. todo:: implement confidence values metric.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features -- generally the test data\\n            that is associated with y_true values.\\n        '\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')",
            "def _get_y_scores(self, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        The ``roc_curve`` metric requires target scores that can either be the\\n        probability estimates of the positive class, confidence values or non-\\n        thresholded measure of decisions (as returned by \"decision_function\").\\n\\n        This method computes the scores by resolving the estimator methods\\n        that retreive these values.\\n\\n        .. todo:: implement confidence values metric.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features -- generally the test data\\n            that is associated with y_true values.\\n        '\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')",
            "def _get_y_scores(self, X):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        The ``roc_curve`` metric requires target scores that can either be the\\n        probability estimates of the positive class, confidence values or non-\\n        thresholded measure of decisions (as returned by \"decision_function\").\\n\\n        This method computes the scores by resolving the estimator methods\\n        that retreive these values.\\n\\n        .. todo:: implement confidence values metric.\\n\\n        Parameters\\n        ----------\\n        X : ndarray or DataFrame of shape n x m\\n            A matrix of n instances with m features -- generally the test data\\n            that is associated with y_true values.\\n        '\n    attrs = ('predict_proba', 'decision_function')\n    for attr in attrs:\n        try:\n            method = getattr(self.estimator, attr, None)\n            if method:\n                return method(X)\n        except AttributeError:\n            continue\n    raise ModelError('ROCAUC requires estimators with predict_proba or decision_function methods.')"
        ]
    },
    {
        "func_name": "_score_micro_average",
        "original": "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    \"\"\"\n        Compute the micro average scores for the ROCAUC curves.\n        \"\"\"\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])",
        "mutated": [
            "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    if False:\n        i = 10\n    '\\n        Compute the micro average scores for the ROCAUC curves.\\n        '\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])",
            "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the micro average scores for the ROCAUC curves.\\n        '\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])",
            "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the micro average scores for the ROCAUC curves.\\n        '\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])",
            "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the micro average scores for the ROCAUC curves.\\n        '\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])",
            "def _score_micro_average(self, y, y_pred, classes, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the micro average scores for the ROCAUC curves.\\n        '\n    y = label_binarize(y, classes=classes)\n    if n_classes == 2:\n        y = np.hstack((1 - y, y))\n    (self.fpr[MICRO], self.tpr[MICRO], _) = roc_curve(y.ravel(), y_pred.ravel())\n    self.roc_auc[MICRO] = auc(self.fpr[MICRO], self.tpr[MICRO])"
        ]
    },
    {
        "func_name": "_score_macro_average",
        "original": "def _score_macro_average(self, n_classes):\n    \"\"\"\n        Compute the macro average scores for the ROCAUC curves.\n        \"\"\"\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])",
        "mutated": [
            "def _score_macro_average(self, n_classes):\n    if False:\n        i = 10\n    '\\n        Compute the macro average scores for the ROCAUC curves.\\n        '\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])",
            "def _score_macro_average(self, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the macro average scores for the ROCAUC curves.\\n        '\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])",
            "def _score_macro_average(self, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the macro average scores for the ROCAUC curves.\\n        '\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])",
            "def _score_macro_average(self, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the macro average scores for the ROCAUC curves.\\n        '\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])",
            "def _score_macro_average(self, n_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the macro average scores for the ROCAUC curves.\\n        '\n    all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n    avg_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        avg_tpr += np.interp(all_fpr, self.fpr[i], self.tpr[i])\n    avg_tpr /= n_classes\n    self.fpr[MACRO] = all_fpr\n    self.tpr[MACRO] = avg_tpr\n    self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])"
        ]
    },
    {
        "func_name": "roc_auc",
        "original": "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    \"\"\"ROCAUC\n\n    Receiver Operating Characteristic (ROC) curves are a measure of a\n    classifier's predictive quality that compares and visualizes the tradeoff\n    between the models' sensitivity and specificity. The ROC curve displays\n    the true positive rate on the Y axis and the false positive rate on the\n    X axis on both a global average and per-class basis. The ideal point is\n    therefore the top-left corner of the plot: false positives are zero and\n    true positives are one.\n\n    This leads to another metric, area under the curve  (AUC), a computation\n    of the relationship between false positives and true positives. The higher\n    the AUC, the better the model generally is. However, it is also important\n    to inspect the \"steepness\" of the curve, as this describes the\n    maximization of the true positive rate while minimizing the false positive\n    rate. Generalizing \"steepness\" usually leads to discussions about\n    convexity, which we do not get into here.\n\n    Parameters\n    ----------\n    estimator : estimator\n        A scikit-learn estimator that should be a classifier. If the model is\n        not a classifier, an exception is raised. If the internal model is not\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\n        by ``is_fitted``.\n\n    X_train : array-like, 2D\n        The table of instance data or independent variables that describe the outcome of\n        the dependent variable, y. Used to fit the visualizer and also to score the\n        visualizer if test splits are not specified.\n\n    y_train : array-like, 2D\n        The vector of target data or the dependent variable predicted by X. Used to fit\n        the visualizer and also to score the visualizer if test splits not specified.\n\n    X_test: array-like, 2D, default: None\n        The table of instance data or independent variables that describe the outcome of\n        the dependent variable, y. Used to score the visualizer if specified.\n\n    y_test: array-like, 1D, default: None\n        The vector of target data or the dependent variable predicted by X.\n        Used to score the visualizer if specified.\n\n    ax : matplotlib Axes, default: None\n        The axes to plot the figure on. If not specified the current axes will be\n        used (or generated if required).\n\n    test_size : float, default=0.2\n        The percentage of the data to reserve as test data.\n\n    random_state : int or None, default=None\n        The value to seed the random number generator for shuffling data.\n\n    micro : bool, default: True\n        Plot the micro-averages ROC curve, computed from the sum of all true\n        positives and false positives across all classes. Micro is not defined\n        for binary classification problems with estimators with only a\n        decision_function method.\n\n    macro : bool, default: True\n        Plot the macro-averages ROC curve, which simply takes the average of\n        curves across all classes. Macro is not defined for binary\n        classification problems with estimators with only a decision_function\n        method.\n\n    per_class : bool, default: True\n        Plot the ROC curves for each individual class. This should be set\n        to false if only the macro or micro average curves are required. For true\n        binary classifiers, setting per_class=False will plot the positive class\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\n        the negative class if only a decision_function method exists on the estimator.\n\n    binary : bool, default: False\n        This argument quickly resets the visualizer for true binary classification\n        by updating the micro, macro, and per_class arguments to False (do not use\n        in conjunction with those other arguments). Note that this is not a true\n        hyperparameter to the visualizer, it just collects other parameters into\n        a single, simpler argument.\n\n    classes : list of str, defult: None\n        The class labels to use for the legend ordered by the index of the sorted\n        classes discovered in the ``fit()`` method. Specifying classes in this\n        manner is used to change the class names to a more specific format or\n        to label encoded integer classes. Some visualizers may also use this\n        field to filter the visualization for specific classes. For more advanced\n        usage specify an encoder rather than class labels.\n\n    encoder : dict or LabelEncoder, default: None\n        A mapping of classes to human readable labels. Often there is a mismatch\n        between desired class labels and those contained in the target variable\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\n        ensuring that classes are labeled correctly in the visualization.\n\n    is_fitted : bool or str, default=\"auto\"\n        Specify if the wrapped estimator is already fitted. If False, the estimator\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\n        modified. If \"auto\" (default), a helper method will check if the estimator\n        is fitted before fitting it again.\n\n    force_model : bool, default: False\n        Do not check to ensure that the underlying estimator is a classifier. This\n        will prevent an exception when the visualizer is initialized but may result\n        in unexpected or unintended behavior.\n\n    show: bool, default: True\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\n        calls ``finalize()``\n\n    kwargs : dict\n        Keyword arguments passed to the visualizer base classes.\n\n    Notes\n    -----\n    ROC curves are typically used in binary classification, and in fact the\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\n    binary classifiers. As a result it is necessary to binarize the output or\n    to use one-vs-rest or one-vs-all strategies of classification. The\n    visualizer does its best to handle multiple situations, but exceptions can\n    arise from unexpected models or outputs.\n\n    Another important point is the relationship of class labels specified on\n    initialization to those drawn on the curves. The classes are not used to\n    constrain ordering or filter curves; the ROC computation happens on the\n    unique values specified in the target vector to the ``score`` method. To\n    ensure the best quality visualization, do not use a LabelEncoder for this\n    and do not pass in class labels.\n\n    .. seealso:: https://bit.ly/2IORWO2\n    .. todo:: Allow the class list to filter the curves on the visualization.\n\n    Examples\n    --------\n    >>> from yellowbrick.classifier import ROCAUC\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> data = load_data(\"occupancy\")\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\n    >>> X = data[features].values\n    >>> y = data.occupancy.values\n    >>> roc_auc(LogisticRegression(), X, y)\n\n    Returns\n    -------\n    viz : ROCAUC\n        Returns the fitted, finalized visualizer object\n    \"\"\"\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
        "mutated": [
            "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n    'ROCAUC\\n\\n    Receiver Operating Characteristic (ROC) curves are a measure of a\\n    classifier\\'s predictive quality that compares and visualizes the tradeoff\\n    between the models\\' sensitivity and specificity. The ROC curve displays\\n    the true positive rate on the Y axis and the false positive rate on the\\n    X axis on both a global average and per-class basis. The ideal point is\\n    therefore the top-left corner of the plot: false positives are zero and\\n    true positives are one.\\n\\n    This leads to another metric, area under the curve  (AUC), a computation\\n    of the relationship between false positives and true positives. The higher\\n    the AUC, the better the model generally is. However, it is also important\\n    to inspect the \"steepness\" of the curve, as this describes the\\n    maximization of the true positive rate while minimizing the false positive\\n    rate. Generalizing \"steepness\" usually leads to discussions about\\n    convexity, which we do not get into here.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X_train : array-like, 2D\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to fit the visualizer and also to score the\\n        visualizer if test splits are not specified.\\n\\n    y_train : array-like, 2D\\n        The vector of target data or the dependent variable predicted by X. Used to fit\\n        the visualizer and also to score the visualizer if test splits not specified.\\n\\n    X_test: array-like, 2D, default: None\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to score the visualizer if specified.\\n\\n    y_test: array-like, 1D, default: None\\n        The vector of target data or the dependent variable predicted by X.\\n        Used to score the visualizer if specified.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    test_size : float, default=0.2\\n        The percentage of the data to reserve as test data.\\n\\n    random_state : int or None, default=None\\n        The value to seed the random number generator for shuffling data.\\n\\n    micro : bool, default: True\\n        Plot the micro-averages ROC curve, computed from the sum of all true\\n        positives and false positives across all classes. Micro is not defined\\n        for binary classification problems with estimators with only a\\n        decision_function method.\\n\\n    macro : bool, default: True\\n        Plot the macro-averages ROC curve, which simply takes the average of\\n        curves across all classes. Macro is not defined for binary\\n        classification problems with estimators with only a decision_function\\n        method.\\n\\n    per_class : bool, default: True\\n        Plot the ROC curves for each individual class. This should be set\\n        to false if only the macro or micro average curves are required. For true\\n        binary classifiers, setting per_class=False will plot the positive class\\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\\n        the negative class if only a decision_function method exists on the estimator.\\n\\n    binary : bool, default: False\\n        This argument quickly resets the visualizer for true binary classification\\n        by updating the micro, macro, and per_class arguments to False (do not use\\n        in conjunction with those other arguments). Note that this is not a true\\n        hyperparameter to the visualizer, it just collects other parameters into\\n        a single, simpler argument.\\n\\n    classes : list of str, defult: None\\n        The class labels to use for the legend ordered by the index of the sorted\\n        classes discovered in the ``fit()`` method. Specifying classes in this\\n        manner is used to change the class names to a more specific format or\\n        to label encoded integer classes. Some visualizers may also use this\\n        field to filter the visualization for specific classes. For more advanced\\n        usage specify an encoder rather than class labels.\\n\\n    encoder : dict or LabelEncoder, default: None\\n        A mapping of classes to human readable labels. Often there is a mismatch\\n        between desired class labels and those contained in the target variable\\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\\n        ensuring that classes are labeled correctly in the visualization.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    ROC curves are typically used in binary classification, and in fact the\\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\\n    binary classifiers. As a result it is necessary to binarize the output or\\n    to use one-vs-rest or one-vs-all strategies of classification. The\\n    visualizer does its best to handle multiple situations, but exceptions can\\n    arise from unexpected models or outputs.\\n\\n    Another important point is the relationship of class labels specified on\\n    initialization to those drawn on the curves. The classes are not used to\\n    constrain ordering or filter curves; the ROC computation happens on the\\n    unique values specified in the target vector to the ``score`` method. To\\n    ensure the best quality visualization, do not use a LabelEncoder for this\\n    and do not pass in class labels.\\n\\n    .. seealso:: https://bit.ly/2IORWO2\\n    .. todo:: Allow the class list to filter the curves on the visualization.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier import ROCAUC\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> data = load_data(\"occupancy\")\\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\\n    >>> X = data[features].values\\n    >>> y = data.occupancy.values\\n    >>> roc_auc(LogisticRegression(), X, y)\\n\\n    Returns\\n    -------\\n    viz : ROCAUC\\n        Returns the fitted, finalized visualizer object\\n    '\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'ROCAUC\\n\\n    Receiver Operating Characteristic (ROC) curves are a measure of a\\n    classifier\\'s predictive quality that compares and visualizes the tradeoff\\n    between the models\\' sensitivity and specificity. The ROC curve displays\\n    the true positive rate on the Y axis and the false positive rate on the\\n    X axis on both a global average and per-class basis. The ideal point is\\n    therefore the top-left corner of the plot: false positives are zero and\\n    true positives are one.\\n\\n    This leads to another metric, area under the curve  (AUC), a computation\\n    of the relationship between false positives and true positives. The higher\\n    the AUC, the better the model generally is. However, it is also important\\n    to inspect the \"steepness\" of the curve, as this describes the\\n    maximization of the true positive rate while minimizing the false positive\\n    rate. Generalizing \"steepness\" usually leads to discussions about\\n    convexity, which we do not get into here.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X_train : array-like, 2D\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to fit the visualizer and also to score the\\n        visualizer if test splits are not specified.\\n\\n    y_train : array-like, 2D\\n        The vector of target data or the dependent variable predicted by X. Used to fit\\n        the visualizer and also to score the visualizer if test splits not specified.\\n\\n    X_test: array-like, 2D, default: None\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to score the visualizer if specified.\\n\\n    y_test: array-like, 1D, default: None\\n        The vector of target data or the dependent variable predicted by X.\\n        Used to score the visualizer if specified.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    test_size : float, default=0.2\\n        The percentage of the data to reserve as test data.\\n\\n    random_state : int or None, default=None\\n        The value to seed the random number generator for shuffling data.\\n\\n    micro : bool, default: True\\n        Plot the micro-averages ROC curve, computed from the sum of all true\\n        positives and false positives across all classes. Micro is not defined\\n        for binary classification problems with estimators with only a\\n        decision_function method.\\n\\n    macro : bool, default: True\\n        Plot the macro-averages ROC curve, which simply takes the average of\\n        curves across all classes. Macro is not defined for binary\\n        classification problems with estimators with only a decision_function\\n        method.\\n\\n    per_class : bool, default: True\\n        Plot the ROC curves for each individual class. This should be set\\n        to false if only the macro or micro average curves are required. For true\\n        binary classifiers, setting per_class=False will plot the positive class\\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\\n        the negative class if only a decision_function method exists on the estimator.\\n\\n    binary : bool, default: False\\n        This argument quickly resets the visualizer for true binary classification\\n        by updating the micro, macro, and per_class arguments to False (do not use\\n        in conjunction with those other arguments). Note that this is not a true\\n        hyperparameter to the visualizer, it just collects other parameters into\\n        a single, simpler argument.\\n\\n    classes : list of str, defult: None\\n        The class labels to use for the legend ordered by the index of the sorted\\n        classes discovered in the ``fit()`` method. Specifying classes in this\\n        manner is used to change the class names to a more specific format or\\n        to label encoded integer classes. Some visualizers may also use this\\n        field to filter the visualization for specific classes. For more advanced\\n        usage specify an encoder rather than class labels.\\n\\n    encoder : dict or LabelEncoder, default: None\\n        A mapping of classes to human readable labels. Often there is a mismatch\\n        between desired class labels and those contained in the target variable\\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\\n        ensuring that classes are labeled correctly in the visualization.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    ROC curves are typically used in binary classification, and in fact the\\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\\n    binary classifiers. As a result it is necessary to binarize the output or\\n    to use one-vs-rest or one-vs-all strategies of classification. The\\n    visualizer does its best to handle multiple situations, but exceptions can\\n    arise from unexpected models or outputs.\\n\\n    Another important point is the relationship of class labels specified on\\n    initialization to those drawn on the curves. The classes are not used to\\n    constrain ordering or filter curves; the ROC computation happens on the\\n    unique values specified in the target vector to the ``score`` method. To\\n    ensure the best quality visualization, do not use a LabelEncoder for this\\n    and do not pass in class labels.\\n\\n    .. seealso:: https://bit.ly/2IORWO2\\n    .. todo:: Allow the class list to filter the curves on the visualization.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier import ROCAUC\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> data = load_data(\"occupancy\")\\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\\n    >>> X = data[features].values\\n    >>> y = data.occupancy.values\\n    >>> roc_auc(LogisticRegression(), X, y)\\n\\n    Returns\\n    -------\\n    viz : ROCAUC\\n        Returns the fitted, finalized visualizer object\\n    '\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'ROCAUC\\n\\n    Receiver Operating Characteristic (ROC) curves are a measure of a\\n    classifier\\'s predictive quality that compares and visualizes the tradeoff\\n    between the models\\' sensitivity and specificity. The ROC curve displays\\n    the true positive rate on the Y axis and the false positive rate on the\\n    X axis on both a global average and per-class basis. The ideal point is\\n    therefore the top-left corner of the plot: false positives are zero and\\n    true positives are one.\\n\\n    This leads to another metric, area under the curve  (AUC), a computation\\n    of the relationship between false positives and true positives. The higher\\n    the AUC, the better the model generally is. However, it is also important\\n    to inspect the \"steepness\" of the curve, as this describes the\\n    maximization of the true positive rate while minimizing the false positive\\n    rate. Generalizing \"steepness\" usually leads to discussions about\\n    convexity, which we do not get into here.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X_train : array-like, 2D\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to fit the visualizer and also to score the\\n        visualizer if test splits are not specified.\\n\\n    y_train : array-like, 2D\\n        The vector of target data or the dependent variable predicted by X. Used to fit\\n        the visualizer and also to score the visualizer if test splits not specified.\\n\\n    X_test: array-like, 2D, default: None\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to score the visualizer if specified.\\n\\n    y_test: array-like, 1D, default: None\\n        The vector of target data or the dependent variable predicted by X.\\n        Used to score the visualizer if specified.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    test_size : float, default=0.2\\n        The percentage of the data to reserve as test data.\\n\\n    random_state : int or None, default=None\\n        The value to seed the random number generator for shuffling data.\\n\\n    micro : bool, default: True\\n        Plot the micro-averages ROC curve, computed from the sum of all true\\n        positives and false positives across all classes. Micro is not defined\\n        for binary classification problems with estimators with only a\\n        decision_function method.\\n\\n    macro : bool, default: True\\n        Plot the macro-averages ROC curve, which simply takes the average of\\n        curves across all classes. Macro is not defined for binary\\n        classification problems with estimators with only a decision_function\\n        method.\\n\\n    per_class : bool, default: True\\n        Plot the ROC curves for each individual class. This should be set\\n        to false if only the macro or micro average curves are required. For true\\n        binary classifiers, setting per_class=False will plot the positive class\\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\\n        the negative class if only a decision_function method exists on the estimator.\\n\\n    binary : bool, default: False\\n        This argument quickly resets the visualizer for true binary classification\\n        by updating the micro, macro, and per_class arguments to False (do not use\\n        in conjunction with those other arguments). Note that this is not a true\\n        hyperparameter to the visualizer, it just collects other parameters into\\n        a single, simpler argument.\\n\\n    classes : list of str, defult: None\\n        The class labels to use for the legend ordered by the index of the sorted\\n        classes discovered in the ``fit()`` method. Specifying classes in this\\n        manner is used to change the class names to a more specific format or\\n        to label encoded integer classes. Some visualizers may also use this\\n        field to filter the visualization for specific classes. For more advanced\\n        usage specify an encoder rather than class labels.\\n\\n    encoder : dict or LabelEncoder, default: None\\n        A mapping of classes to human readable labels. Often there is a mismatch\\n        between desired class labels and those contained in the target variable\\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\\n        ensuring that classes are labeled correctly in the visualization.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    ROC curves are typically used in binary classification, and in fact the\\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\\n    binary classifiers. As a result it is necessary to binarize the output or\\n    to use one-vs-rest or one-vs-all strategies of classification. The\\n    visualizer does its best to handle multiple situations, but exceptions can\\n    arise from unexpected models or outputs.\\n\\n    Another important point is the relationship of class labels specified on\\n    initialization to those drawn on the curves. The classes are not used to\\n    constrain ordering or filter curves; the ROC computation happens on the\\n    unique values specified in the target vector to the ``score`` method. To\\n    ensure the best quality visualization, do not use a LabelEncoder for this\\n    and do not pass in class labels.\\n\\n    .. seealso:: https://bit.ly/2IORWO2\\n    .. todo:: Allow the class list to filter the curves on the visualization.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier import ROCAUC\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> data = load_data(\"occupancy\")\\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\\n    >>> X = data[features].values\\n    >>> y = data.occupancy.values\\n    >>> roc_auc(LogisticRegression(), X, y)\\n\\n    Returns\\n    -------\\n    viz : ROCAUC\\n        Returns the fitted, finalized visualizer object\\n    '\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'ROCAUC\\n\\n    Receiver Operating Characteristic (ROC) curves are a measure of a\\n    classifier\\'s predictive quality that compares and visualizes the tradeoff\\n    between the models\\' sensitivity and specificity. The ROC curve displays\\n    the true positive rate on the Y axis and the false positive rate on the\\n    X axis on both a global average and per-class basis. The ideal point is\\n    therefore the top-left corner of the plot: false positives are zero and\\n    true positives are one.\\n\\n    This leads to another metric, area under the curve  (AUC), a computation\\n    of the relationship between false positives and true positives. The higher\\n    the AUC, the better the model generally is. However, it is also important\\n    to inspect the \"steepness\" of the curve, as this describes the\\n    maximization of the true positive rate while minimizing the false positive\\n    rate. Generalizing \"steepness\" usually leads to discussions about\\n    convexity, which we do not get into here.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X_train : array-like, 2D\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to fit the visualizer and also to score the\\n        visualizer if test splits are not specified.\\n\\n    y_train : array-like, 2D\\n        The vector of target data or the dependent variable predicted by X. Used to fit\\n        the visualizer and also to score the visualizer if test splits not specified.\\n\\n    X_test: array-like, 2D, default: None\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to score the visualizer if specified.\\n\\n    y_test: array-like, 1D, default: None\\n        The vector of target data or the dependent variable predicted by X.\\n        Used to score the visualizer if specified.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    test_size : float, default=0.2\\n        The percentage of the data to reserve as test data.\\n\\n    random_state : int or None, default=None\\n        The value to seed the random number generator for shuffling data.\\n\\n    micro : bool, default: True\\n        Plot the micro-averages ROC curve, computed from the sum of all true\\n        positives and false positives across all classes. Micro is not defined\\n        for binary classification problems with estimators with only a\\n        decision_function method.\\n\\n    macro : bool, default: True\\n        Plot the macro-averages ROC curve, which simply takes the average of\\n        curves across all classes. Macro is not defined for binary\\n        classification problems with estimators with only a decision_function\\n        method.\\n\\n    per_class : bool, default: True\\n        Plot the ROC curves for each individual class. This should be set\\n        to false if only the macro or micro average curves are required. For true\\n        binary classifiers, setting per_class=False will plot the positive class\\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\\n        the negative class if only a decision_function method exists on the estimator.\\n\\n    binary : bool, default: False\\n        This argument quickly resets the visualizer for true binary classification\\n        by updating the micro, macro, and per_class arguments to False (do not use\\n        in conjunction with those other arguments). Note that this is not a true\\n        hyperparameter to the visualizer, it just collects other parameters into\\n        a single, simpler argument.\\n\\n    classes : list of str, defult: None\\n        The class labels to use for the legend ordered by the index of the sorted\\n        classes discovered in the ``fit()`` method. Specifying classes in this\\n        manner is used to change the class names to a more specific format or\\n        to label encoded integer classes. Some visualizers may also use this\\n        field to filter the visualization for specific classes. For more advanced\\n        usage specify an encoder rather than class labels.\\n\\n    encoder : dict or LabelEncoder, default: None\\n        A mapping of classes to human readable labels. Often there is a mismatch\\n        between desired class labels and those contained in the target variable\\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\\n        ensuring that classes are labeled correctly in the visualization.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    ROC curves are typically used in binary classification, and in fact the\\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\\n    binary classifiers. As a result it is necessary to binarize the output or\\n    to use one-vs-rest or one-vs-all strategies of classification. The\\n    visualizer does its best to handle multiple situations, but exceptions can\\n    arise from unexpected models or outputs.\\n\\n    Another important point is the relationship of class labels specified on\\n    initialization to those drawn on the curves. The classes are not used to\\n    constrain ordering or filter curves; the ROC computation happens on the\\n    unique values specified in the target vector to the ``score`` method. To\\n    ensure the best quality visualization, do not use a LabelEncoder for this\\n    and do not pass in class labels.\\n\\n    .. seealso:: https://bit.ly/2IORWO2\\n    .. todo:: Allow the class list to filter the curves on the visualization.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier import ROCAUC\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> data = load_data(\"occupancy\")\\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\\n    >>> X = data[features].values\\n    >>> y = data.occupancy.values\\n    >>> roc_auc(LogisticRegression(), X, y)\\n\\n    Returns\\n    -------\\n    viz : ROCAUC\\n        Returns the fitted, finalized visualizer object\\n    '\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer",
            "def roc_auc(estimator, X_train, y_train, X_test=None, y_test=None, ax=None, micro=True, macro=True, per_class=True, binary=False, classes=None, encoder=None, is_fitted='auto', force_model=False, show=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'ROCAUC\\n\\n    Receiver Operating Characteristic (ROC) curves are a measure of a\\n    classifier\\'s predictive quality that compares and visualizes the tradeoff\\n    between the models\\' sensitivity and specificity. The ROC curve displays\\n    the true positive rate on the Y axis and the false positive rate on the\\n    X axis on both a global average and per-class basis. The ideal point is\\n    therefore the top-left corner of the plot: false positives are zero and\\n    true positives are one.\\n\\n    This leads to another metric, area under the curve  (AUC), a computation\\n    of the relationship between false positives and true positives. The higher\\n    the AUC, the better the model generally is. However, it is also important\\n    to inspect the \"steepness\" of the curve, as this describes the\\n    maximization of the true positive rate while minimizing the false positive\\n    rate. Generalizing \"steepness\" usually leads to discussions about\\n    convexity, which we do not get into here.\\n\\n    Parameters\\n    ----------\\n    estimator : estimator\\n        A scikit-learn estimator that should be a classifier. If the model is\\n        not a classifier, an exception is raised. If the internal model is not\\n        fitted, it is fit when the visualizer is fitted, unless otherwise specified\\n        by ``is_fitted``.\\n\\n    X_train : array-like, 2D\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to fit the visualizer and also to score the\\n        visualizer if test splits are not specified.\\n\\n    y_train : array-like, 2D\\n        The vector of target data or the dependent variable predicted by X. Used to fit\\n        the visualizer and also to score the visualizer if test splits not specified.\\n\\n    X_test: array-like, 2D, default: None\\n        The table of instance data or independent variables that describe the outcome of\\n        the dependent variable, y. Used to score the visualizer if specified.\\n\\n    y_test: array-like, 1D, default: None\\n        The vector of target data or the dependent variable predicted by X.\\n        Used to score the visualizer if specified.\\n\\n    ax : matplotlib Axes, default: None\\n        The axes to plot the figure on. If not specified the current axes will be\\n        used (or generated if required).\\n\\n    test_size : float, default=0.2\\n        The percentage of the data to reserve as test data.\\n\\n    random_state : int or None, default=None\\n        The value to seed the random number generator for shuffling data.\\n\\n    micro : bool, default: True\\n        Plot the micro-averages ROC curve, computed from the sum of all true\\n        positives and false positives across all classes. Micro is not defined\\n        for binary classification problems with estimators with only a\\n        decision_function method.\\n\\n    macro : bool, default: True\\n        Plot the macro-averages ROC curve, which simply takes the average of\\n        curves across all classes. Macro is not defined for binary\\n        classification problems with estimators with only a decision_function\\n        method.\\n\\n    per_class : bool, default: True\\n        Plot the ROC curves for each individual class. This should be set\\n        to false if only the macro or micro average curves are required. For true\\n        binary classifiers, setting per_class=False will plot the positive class\\n        ROC curve, and per_class=True will use ``1-P(1)`` to compute the curve of\\n        the negative class if only a decision_function method exists on the estimator.\\n\\n    binary : bool, default: False\\n        This argument quickly resets the visualizer for true binary classification\\n        by updating the micro, macro, and per_class arguments to False (do not use\\n        in conjunction with those other arguments). Note that this is not a true\\n        hyperparameter to the visualizer, it just collects other parameters into\\n        a single, simpler argument.\\n\\n    classes : list of str, defult: None\\n        The class labels to use for the legend ordered by the index of the sorted\\n        classes discovered in the ``fit()`` method. Specifying classes in this\\n        manner is used to change the class names to a more specific format or\\n        to label encoded integer classes. Some visualizers may also use this\\n        field to filter the visualization for specific classes. For more advanced\\n        usage specify an encoder rather than class labels.\\n\\n    encoder : dict or LabelEncoder, default: None\\n        A mapping of classes to human readable labels. Often there is a mismatch\\n        between desired class labels and those contained in the target variable\\n        passed to ``fit()`` or ``score()``. The encoder disambiguates this mismatch\\n        ensuring that classes are labeled correctly in the visualization.\\n\\n    is_fitted : bool or str, default=\"auto\"\\n        Specify if the wrapped estimator is already fitted. If False, the estimator\\n        will be fit when the visualizer is fit, otherwise, the estimator will not be\\n        modified. If \"auto\" (default), a helper method will check if the estimator\\n        is fitted before fitting it again.\\n\\n    force_model : bool, default: False\\n        Do not check to ensure that the underlying estimator is a classifier. This\\n        will prevent an exception when the visualizer is initialized but may result\\n        in unexpected or unintended behavior.\\n\\n    show: bool, default: True\\n        If True, calls ``show()``, which in turn calls ``plt.show()`` however you cannot\\n        call ``plt.savefig`` from this signature, nor ``clear_figure``. If False, simply\\n        calls ``finalize()``\\n\\n    kwargs : dict\\n        Keyword arguments passed to the visualizer base classes.\\n\\n    Notes\\n    -----\\n    ROC curves are typically used in binary classification, and in fact the\\n    Scikit-Learn ``roc_curve`` metric is only able to perform metrics for\\n    binary classifiers. As a result it is necessary to binarize the output or\\n    to use one-vs-rest or one-vs-all strategies of classification. The\\n    visualizer does its best to handle multiple situations, but exceptions can\\n    arise from unexpected models or outputs.\\n\\n    Another important point is the relationship of class labels specified on\\n    initialization to those drawn on the curves. The classes are not used to\\n    constrain ordering or filter curves; the ROC computation happens on the\\n    unique values specified in the target vector to the ``score`` method. To\\n    ensure the best quality visualization, do not use a LabelEncoder for this\\n    and do not pass in class labels.\\n\\n    .. seealso:: https://bit.ly/2IORWO2\\n    .. todo:: Allow the class list to filter the curves on the visualization.\\n\\n    Examples\\n    --------\\n    >>> from yellowbrick.classifier import ROCAUC\\n    >>> from sklearn.linear_model import LogisticRegression\\n    >>> data = load_data(\"occupancy\")\\n    >>> features = [\"temp\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]\\n    >>> X = data[features].values\\n    >>> y = data.occupancy.values\\n    >>> roc_auc(LogisticRegression(), X, y)\\n\\n    Returns\\n    -------\\n    viz : ROCAUC\\n        Returns the fitted, finalized visualizer object\\n    '\n    visualizer = ROCAUC(estimator=estimator, ax=ax, micro=micro, macro=macro, per_class=per_class, binary=binary, classes=classes, encoder=encoder, is_fitted=is_fitted, force_model=force_model, **kwargs)\n    visualizer.fit(X_train, y_train, **kwargs)\n    if X_test is not None and y_test is not None:\n        visualizer.score(X_test, y_test)\n    else:\n        visualizer.score(X_train, y_train)\n    if show:\n        visualizer.show()\n    else:\n        visualizer.finalize()\n    return visualizer"
        ]
    }
]