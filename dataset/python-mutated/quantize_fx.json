[
    {
        "func_name": "attach_preserved_attrs_to_model",
        "original": "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    \"\"\" Store preserved attributes to the model.meta so that it can be preserved during deepcopy\n    \"\"\"\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)",
        "mutated": [
            "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n    ' Store preserved attributes to the model.meta so that it can be preserved during deepcopy\\n    '\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)",
            "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Store preserved attributes to the model.meta so that it can be preserved during deepcopy\\n    '\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)",
            "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Store preserved attributes to the model.meta so that it can be preserved during deepcopy\\n    '\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)",
            "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Store preserved attributes to the model.meta so that it can be preserved during deepcopy\\n    '\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)",
            "def attach_preserved_attrs_to_model(model: Union[GraphModule, torch.nn.Module], preserved_attrs: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Store preserved attributes to the model.meta so that it can be preserved during deepcopy\\n    '\n    model.meta[_USER_PRESERVED_ATTRIBUTES_KEY] = copy.copy(preserved_attrs)\n    for (attr_name, attr) in model.meta[_USER_PRESERVED_ATTRIBUTES_KEY].items():\n        setattr(model, attr_name, attr)"
        ]
    },
    {
        "func_name": "_check_is_graph_module",
        "original": "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')",
        "mutated": [
            "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')",
            "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')",
            "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')",
            "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')",
            "def _check_is_graph_module(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(model, GraphModule):\n        raise ValueError('input model must be a GraphModule, ' + 'Got type:' + str(type(model)) + ' Please make ' + 'sure to follow the tutorials.')"
        ]
    },
    {
        "func_name": "_attach_meta_to_node_if_not_exist",
        "original": "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    \"\"\" Attach meta field to all nodes of the graph if it does not exist,\n    meta field is a field stores some meta information about the node, such\n    as dtype and shape information for output of the node, this only exists\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\n    so we add it here to avoid checking this all over the places\n    \"\"\"\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}",
        "mutated": [
            "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    if False:\n        i = 10\n    ' Attach meta field to all nodes of the graph if it does not exist,\\n    meta field is a field stores some meta information about the node, such\\n    as dtype and shape information for output of the node, this only exists\\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\\n    so we add it here to avoid checking this all over the places\\n    '\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}",
            "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Attach meta field to all nodes of the graph if it does not exist,\\n    meta field is a field stores some meta information about the node, such\\n    as dtype and shape information for output of the node, this only exists\\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\\n    so we add it here to avoid checking this all over the places\\n    '\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}",
            "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Attach meta field to all nodes of the graph if it does not exist,\\n    meta field is a field stores some meta information about the node, such\\n    as dtype and shape information for output of the node, this only exists\\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\\n    so we add it here to avoid checking this all over the places\\n    '\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}",
            "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Attach meta field to all nodes of the graph if it does not exist,\\n    meta field is a field stores some meta information about the node, such\\n    as dtype and shape information for output of the node, this only exists\\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\\n    so we add it here to avoid checking this all over the places\\n    '\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}",
            "def _attach_meta_to_node_if_not_exist(model: GraphModule) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Attach meta field to all nodes of the graph if it does not exist,\\n    meta field is a field stores some meta information about the node, such\\n    as dtype and shape information for output of the node, this only exists\\n    if the program is captured by make_fx (used in quantize_pt2e flow), if\\n    the program is captured by torch.fx symbolic tracing, this field may not exist,\\n    so we add it here to avoid checking this all over the places\\n    '\n    for node in model.graph.nodes:\n        if not hasattr(node, 'meta'):\n            node.meta = {}"
        ]
    },
    {
        "func_name": "_swap_ff_with_fxff",
        "original": "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    \"\"\" Swap FloatFunctional with FXFloatFunctional\n    \"\"\"\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()",
        "mutated": [
            "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n    ' Swap FloatFunctional with FXFloatFunctional\\n    '\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()",
            "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Swap FloatFunctional with FXFloatFunctional\\n    '\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()",
            "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Swap FloatFunctional with FXFloatFunctional\\n    '\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()",
            "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Swap FloatFunctional with FXFloatFunctional\\n    '\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()",
            "def _swap_ff_with_fxff(model: torch.nn.Module) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Swap FloatFunctional with FXFloatFunctional\\n    '\n    modules_to_swap = []\n    for (name, module) in model.named_children():\n        if isinstance(module, torch.ao.nn.quantized.FloatFunctional):\n            modules_to_swap.append(name)\n        else:\n            _swap_ff_with_fxff(module)\n    for name in modules_to_swap:\n        del model._modules[name]\n        model._modules[name] = torch.ao.nn.quantized.FXFloatFunctional()"
        ]
    },
    {
        "func_name": "_fuse_fx",
        "original": "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Internal helper function to fuse modules in preparation for quantization\n\n    Args:\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\n    \"\"\"\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)",
        "mutated": [
            "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Internal helper function to fuse modules in preparation for quantization\\n\\n    Args:\\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\\n    '\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)",
            "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Internal helper function to fuse modules in preparation for quantization\\n\\n    Args:\\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\\n    '\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)",
            "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Internal helper function to fuse modules in preparation for quantization\\n\\n    Args:\\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\\n    '\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)",
            "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Internal helper function to fuse modules in preparation for quantization\\n\\n    Args:\\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\\n    '\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)",
            "def _fuse_fx(model: GraphModule, is_qat: bool, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Internal helper function to fuse modules in preparation for quantization\\n\\n    Args:\\n        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)\\n    '\n    _check_is_graph_module(model)\n    return fuse(model, is_qat, fuse_custom_config, backend_config)"
        ]
    },
    {
        "func_name": "_prepare_fx",
        "original": "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    \"\"\" Internal helper function for prepare_fx\n    Args:\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\n      `is_standalone_module`: a boolean flag indicates whether we are\n      quantizing a standalone module or not, a standalone module\n      is a submodule of the parent module that is not inlined in the\nforward graph of the parent module,\n      the way we quantize standalone module is described in:\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\n    \"\"\"\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared",
        "mutated": [
            "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    if False:\n        i = 10\n    ' Internal helper function for prepare_fx\\n    Args:\\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\\n      `is_standalone_module`: a boolean flag indicates whether we are\\n      quantizing a standalone module or not, a standalone module\\n      is a submodule of the parent module that is not inlined in the\\nforward graph of the parent module,\\n      the way we quantize standalone module is described in:\\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\\n    '\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared",
            "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Internal helper function for prepare_fx\\n    Args:\\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\\n      `is_standalone_module`: a boolean flag indicates whether we are\\n      quantizing a standalone module or not, a standalone module\\n      is a submodule of the parent module that is not inlined in the\\nforward graph of the parent module,\\n      the way we quantize standalone module is described in:\\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\\n    '\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared",
            "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Internal helper function for prepare_fx\\n    Args:\\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\\n      `is_standalone_module`: a boolean flag indicates whether we are\\n      quantizing a standalone module or not, a standalone module\\n      is a submodule of the parent module that is not inlined in the\\nforward graph of the parent module,\\n      the way we quantize standalone module is described in:\\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\\n    '\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared",
            "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Internal helper function for prepare_fx\\n    Args:\\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\\n      `is_standalone_module`: a boolean flag indicates whether we are\\n      quantizing a standalone module or not, a standalone module\\n      is a submodule of the parent module that is not inlined in the\\nforward graph of the parent module,\\n      the way we quantize standalone module is described in:\\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\\n    '\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared",
            "def _prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Internal helper function for prepare_fx\\n    Args:\\n      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:\\n      see docs for :func:`~torch.ao.quantization.prepare_fx`\\n      `is_standalone_module`: a boolean flag indicates whether we are\\n      quantizing a standalone module or not, a standalone module\\n      is a submodule of the parent module that is not inlined in the\\nforward graph of the parent module,\\n      the way we quantize standalone module is described in:\\n      :func:`~torch.ao.quantization._prepare_standalone_module_fx`\\n    '\n    if prepare_custom_config is None:\n        prepare_custom_config = PrepareCustomConfig()\n    if _equalization_config is None:\n        _equalization_config = QConfigMapping()\n    if isinstance(prepare_custom_config, Dict):\n        warnings.warn('Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported in a future version. Please pass in a PrepareCustomConfig instead.')\n        prepare_custom_config = PrepareCustomConfig.from_dict(prepare_custom_config)\n    _swap_ff_with_fxff(model)\n    (skipped_module_names, skipped_module_classes) = get_skipped_module_name_and_classes(prepare_custom_config, is_standalone_module)\n    preserved_attr_names = prepare_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    tracer = QuantizationTracer(skipped_module_names, skipped_module_classes)\n    graph_module = GraphModule(model, tracer.trace(model))\n    _attach_meta_to_node_if_not_exist(graph_module)\n    fuse_custom_config = FuseCustomConfig().set_preserved_attributes(prepare_custom_config.preserved_attributes)\n    graph_module = _fuse_fx(graph_module, is_qat, fuse_custom_config, backend_config)\n    prepared = prepare(graph_module, qconfig_mapping, is_qat, tracer.node_name_to_scope, example_inputs=example_inputs, prepare_custom_config=prepare_custom_config, _equalization_config=_equalization_config, backend_config=backend_config, is_standalone_module=is_standalone_module)\n    attach_preserved_attrs_to_model(prepared, preserved_attrs)\n    return prepared"
        ]
    },
    {
        "func_name": "_prepare_standalone_module_fx",
        "original": "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\n    parent module.\n    standalone_module means it a submodule that is not inlined in parent module,\n    and will be quantized separately as one unit.\n\n    How the standalone module is observed is specified by `input_quantized_idxs` and\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\n\n    Returns:\n\n        * model(GraphModule): prepared standalone module. It has these attributes in\n          model.meta:\n\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\n              indexes for the graph input that is expected to be quantized,\n              same as input_quantized_idxs configuration provided\n              for the standalone module\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\n              indexs for the graph output that is quantized\n              same as input_quantized_idxs configuration provided\n              for the standalone module\n\n    \"\"\"\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)",
        "mutated": [
            "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\\n    parent module.\\n    standalone_module means it a submodule that is not inlined in parent module,\\n    and will be quantized separately as one unit.\\n\\n    How the standalone module is observed is specified by `input_quantized_idxs` and\\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\\n\\n    Returns:\\n\\n        * model(GraphModule): prepared standalone module. It has these attributes in\\n          model.meta:\\n\\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\\n              indexes for the graph input that is expected to be quantized,\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\\n              indexs for the graph output that is quantized\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n\\n    '\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)",
            "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\\n    parent module.\\n    standalone_module means it a submodule that is not inlined in parent module,\\n    and will be quantized separately as one unit.\\n\\n    How the standalone module is observed is specified by `input_quantized_idxs` and\\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\\n\\n    Returns:\\n\\n        * model(GraphModule): prepared standalone module. It has these attributes in\\n          model.meta:\\n\\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\\n              indexes for the graph input that is expected to be quantized,\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\\n              indexs for the graph output that is quantized\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n\\n    '\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)",
            "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\\n    parent module.\\n    standalone_module means it a submodule that is not inlined in parent module,\\n    and will be quantized separately as one unit.\\n\\n    How the standalone module is observed is specified by `input_quantized_idxs` and\\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\\n\\n    Returns:\\n\\n        * model(GraphModule): prepared standalone module. It has these attributes in\\n          model.meta:\\n\\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\\n              indexes for the graph input that is expected to be quantized,\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\\n              indexs for the graph output that is quantized\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n\\n    '\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)",
            "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\\n    parent module.\\n    standalone_module means it a submodule that is not inlined in parent module,\\n    and will be quantized separately as one unit.\\n\\n    How the standalone module is observed is specified by `input_quantized_idxs` and\\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\\n\\n    Returns:\\n\\n        * model(GraphModule): prepared standalone module. It has these attributes in\\n          model.meta:\\n\\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\\n              indexes for the graph input that is expected to be quantized,\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\\n              indexs for the graph output that is quantized\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n\\n    '\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)",
            "def _prepare_standalone_module_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], is_qat: bool, example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' [Internal use only] Prepare a standalone module, so that it can be used when quantizing the\\n    parent module.\\n    standalone_module means it a submodule that is not inlined in parent module,\\n    and will be quantized separately as one unit.\\n\\n    How the standalone module is observed is specified by `input_quantized_idxs` and\\n    `output_quantized_idxs` in the prepare_custom_config for the standalone module\\n\\n    Returns:\\n\\n        * model(GraphModule): prepared standalone module. It has these attributes in\\n          model.meta:\\n\\n            * `standalone_module_input_quantized_idxs(List[Int])`: a list of\\n              indexes for the graph input that is expected to be quantized,\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n            * `standalone_module_output_quantized_idxs(List[Int])`: a list of\\n              indexs for the graph output that is quantized\\n              same as input_quantized_idxs configuration provided\\n              for the standalone module\\n\\n    '\n    return _prepare_fx(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, backend_config=backend_config, is_standalone_module=True)"
        ]
    },
    {
        "func_name": "fuse_fx",
        "original": "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\n\n    Args:\n\n        * `model` (torch.nn.Module): a torch.nn.Module model\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\n    Example::\n\n        from torch.ao.quantization import fuse_fx\n        m = Model().eval()\n        m = fuse_fx(m)\n\n    \"\"\"\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module",
        "mutated": [
            "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\\n\\n    Args:\\n\\n        * `model` (torch.nn.Module): a torch.nn.Module model\\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\\n    Example::\\n\\n        from torch.ao.quantization import fuse_fx\\n        m = Model().eval()\\n        m = fuse_fx(m)\\n\\n    '\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module",
            "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\\n\\n    Args:\\n\\n        * `model` (torch.nn.Module): a torch.nn.Module model\\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\\n    Example::\\n\\n        from torch.ao.quantization import fuse_fx\\n        m = Model().eval()\\n        m = fuse_fx(m)\\n\\n    '\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module",
            "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\\n\\n    Args:\\n\\n        * `model` (torch.nn.Module): a torch.nn.Module model\\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\\n    Example::\\n\\n        from torch.ao.quantization import fuse_fx\\n        m = Model().eval()\\n        m = fuse_fx(m)\\n\\n    '\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module",
            "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\\n\\n    Args:\\n\\n        * `model` (torch.nn.Module): a torch.nn.Module model\\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\\n    Example::\\n\\n        from torch.ao.quantization import fuse_fx\\n        m = Model().eval()\\n        m = fuse_fx(m)\\n\\n    '\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module",
            "def fuse_fx(model: torch.nn.Module, fuse_custom_config: Union[FuseCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\\n    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py\\n\\n    Args:\\n\\n        * `model` (torch.nn.Module): a torch.nn.Module model\\n        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.\\n            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details\\n    Example::\\n\\n        from torch.ao.quantization import fuse_fx\\n        m = Model().eval()\\n        m = fuse_fx(m)\\n\\n    '\n    if fuse_custom_config is None:\n        fuse_custom_config = FuseCustomConfig()\n    if isinstance(fuse_custom_config, Dict):\n        warnings.warn('Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported in a future version. Please pass in a FuseCustomConfig instead.')\n        fuse_custom_config = FuseCustomConfig.from_dict(fuse_custom_config)\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.fuse_fx')\n    preserved_attr_names = fuse_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(model, attr) for attr in preserved_attr_names if hasattr(model, attr)}\n    graph_module = torch.fx.symbolic_trace(model)\n    _attach_meta_to_node_if_not_exist(graph_module)\n    graph_module = _fuse_fx(graph_module, False, fuse_custom_config, backend_config)\n    attach_preserved_attrs_to_model(graph_module, preserved_attrs)\n    return graph_module"
        ]
    },
    {
        "func_name": "prepare_fx",
        "original": "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Prepare a model for post training quantization\n\n    Args:\n      * `model` (torch.nn.Module): torch.nn.Module model\n\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\n         for more details\n\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\n         Tuple of positional args (keyword args can be passed as positional args as well)\n\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\n\n      * `_equalization_config`: config for specifying how to perform equalization on the model\n\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\n         in a backend, this includes how the operators are observed,\n         supported fusion patterns, how quantize/dequantize ops are\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\n\n    Return:\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\n\n    Example::\n\n        import torch\n        from torch.ao.quantization import get_default_qconfig_mapping\n        from torch.ao.quantization.quantize_fx import prepare_fx\n\n        class Submodule(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n            def forward(self, x):\n                x = self.linear(x)\n                return x\n\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n                self.sub = Submodule()\n\n            def forward(self, x):\n                x = self.linear(x)\n                x = self.sub(x) + x\n                return x\n\n        # initialize a floating point model\n        float_model = M().eval()\n\n        # define calibration function\n        def calibrate(model, data_loader):\n            model.eval()\n            with torch.no_grad():\n                for image, target in data_loader:\n                    model(image)\n\n        # qconfig is the configuration for how we insert observers for a particular\n        # operator\n        # qconfig = get_default_qconfig(\"fbgemm\")\n        # Example of customizing qconfig:\n        # qconfig = torch.ao.quantization.QConfig(\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\n        # `activation` and `weight` are constructors of observer module\n\n        # qconfig_mapping is a collection of quantization configurations, user can\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\n        # in the model through qconfig_mapping\n        # the following call will get the qconfig_mapping that works best for models\n        # that target \"fbgemm\" backend\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n\n        # We can customize qconfig_mapping in different ways.\n        # e.g. set the global qconfig, which means we will use the same qconfig for\n        # all operators in the model, this can be overwritten by other settings\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\n        # e.g. quantize the linear submodule with a specific qconfig\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\n        # e.g. quantize all nn.Linear modules with a specific qconfig\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\n        # argument\n\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\n        # outputs in the model\n        # currently it's not used, but please make sure model(*example_inputs) runs\n        example_inputs = (torch.randn(1, 3, 224, 224),)\n\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\n        # backend_config. If the configuration for an operator in qconfig_mapping\n        # is supported in the backend_config (meaning it's supported by the target\n        # hardware), we'll insert observer modules according to the qconfig_mapping\n        # otherwise the configuration in qconfig_mapping will be ignored\n        #\n        # Example:\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\n        # activation and qint8 for weight:\n        # qconfig = torch.ao.quantization.QConfig(\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\n        # Note: current qconfig api does not support setting output observer, but\n        # we may extend this to support these more fine grained control in the\n        # future\n        #\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\n        # in backend config, linear module also supports in this configuration:\n        # weighted_int8_dtype_config = DTypeConfig(\n        #   input_dtype=torch.quint8,\n        #   output_dtype=torch.quint8,\n        #   weight_dtype=torch.qint8,\n        #   bias_type=torch.float)\n\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\n        #    ...\n\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\n        # is supported by the backend_config and insert observers and fake quant modules\n        # in the model\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\n        # Run calibration\n        calibrate(prepared_model, sample_inference_data)\n    \"\"\"\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)",
        "mutated": [
            "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Prepare a model for post training quantization\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n\\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\\n         for more details\\n\\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\\n         Tuple of positional args (keyword args can be passed as positional args as well)\\n\\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\\n\\n      * `_equalization_config`: config for specifying how to perform equalization on the model\\n\\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\\n         in a backend, this includes how the operators are observed,\\n         supported fusion patterns, how quantize/dequantize ops are\\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().eval()\\n\\n        # define calibration function\\n        def calibrate(model, data_loader):\\n            model.eval()\\n            with torch.no_grad():\\n                for image, target in data_loader:\\n                    model(image)\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways.\\n        # e.g. set the global qconfig, which means we will use the same qconfig for\\n        # all operators in the model, this can be overwritten by other settings\\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\\n        # e.g. quantize the linear submodule with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\\n        # e.g. quantize all nn.Linear modules with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\\n        # argument\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config. If the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert observer modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        #\\n        # Example:\\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\\n        # activation and qint8 for weight:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\\n        # Note: current qconfig api does not support setting output observer, but\\n        # we may extend this to support these more fine grained control in the\\n        # future\\n        #\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # in backend config, linear module also supports in this configuration:\\n        # weighted_int8_dtype_config = DTypeConfig(\\n        #   input_dtype=torch.quint8,\\n        #   output_dtype=torch.quint8,\\n        #   weight_dtype=torch.qint8,\\n        #   bias_type=torch.float)\\n\\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\\n        #    ...\\n\\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\\n        # is supported by the backend_config and insert observers and fake quant modules\\n        # in the model\\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run calibration\\n        calibrate(prepared_model, sample_inference_data)\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)",
            "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Prepare a model for post training quantization\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n\\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\\n         for more details\\n\\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\\n         Tuple of positional args (keyword args can be passed as positional args as well)\\n\\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\\n\\n      * `_equalization_config`: config for specifying how to perform equalization on the model\\n\\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\\n         in a backend, this includes how the operators are observed,\\n         supported fusion patterns, how quantize/dequantize ops are\\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().eval()\\n\\n        # define calibration function\\n        def calibrate(model, data_loader):\\n            model.eval()\\n            with torch.no_grad():\\n                for image, target in data_loader:\\n                    model(image)\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways.\\n        # e.g. set the global qconfig, which means we will use the same qconfig for\\n        # all operators in the model, this can be overwritten by other settings\\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\\n        # e.g. quantize the linear submodule with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\\n        # e.g. quantize all nn.Linear modules with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\\n        # argument\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config. If the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert observer modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        #\\n        # Example:\\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\\n        # activation and qint8 for weight:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\\n        # Note: current qconfig api does not support setting output observer, but\\n        # we may extend this to support these more fine grained control in the\\n        # future\\n        #\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # in backend config, linear module also supports in this configuration:\\n        # weighted_int8_dtype_config = DTypeConfig(\\n        #   input_dtype=torch.quint8,\\n        #   output_dtype=torch.quint8,\\n        #   weight_dtype=torch.qint8,\\n        #   bias_type=torch.float)\\n\\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\\n        #    ...\\n\\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\\n        # is supported by the backend_config and insert observers and fake quant modules\\n        # in the model\\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run calibration\\n        calibrate(prepared_model, sample_inference_data)\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)",
            "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Prepare a model for post training quantization\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n\\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\\n         for more details\\n\\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\\n         Tuple of positional args (keyword args can be passed as positional args as well)\\n\\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\\n\\n      * `_equalization_config`: config for specifying how to perform equalization on the model\\n\\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\\n         in a backend, this includes how the operators are observed,\\n         supported fusion patterns, how quantize/dequantize ops are\\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().eval()\\n\\n        # define calibration function\\n        def calibrate(model, data_loader):\\n            model.eval()\\n            with torch.no_grad():\\n                for image, target in data_loader:\\n                    model(image)\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways.\\n        # e.g. set the global qconfig, which means we will use the same qconfig for\\n        # all operators in the model, this can be overwritten by other settings\\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\\n        # e.g. quantize the linear submodule with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\\n        # e.g. quantize all nn.Linear modules with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\\n        # argument\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config. If the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert observer modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        #\\n        # Example:\\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\\n        # activation and qint8 for weight:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\\n        # Note: current qconfig api does not support setting output observer, but\\n        # we may extend this to support these more fine grained control in the\\n        # future\\n        #\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # in backend config, linear module also supports in this configuration:\\n        # weighted_int8_dtype_config = DTypeConfig(\\n        #   input_dtype=torch.quint8,\\n        #   output_dtype=torch.quint8,\\n        #   weight_dtype=torch.qint8,\\n        #   bias_type=torch.float)\\n\\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\\n        #    ...\\n\\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\\n        # is supported by the backend_config and insert observers and fake quant modules\\n        # in the model\\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run calibration\\n        calibrate(prepared_model, sample_inference_data)\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)",
            "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Prepare a model for post training quantization\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n\\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\\n         for more details\\n\\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\\n         Tuple of positional args (keyword args can be passed as positional args as well)\\n\\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\\n\\n      * `_equalization_config`: config for specifying how to perform equalization on the model\\n\\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\\n         in a backend, this includes how the operators are observed,\\n         supported fusion patterns, how quantize/dequantize ops are\\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().eval()\\n\\n        # define calibration function\\n        def calibrate(model, data_loader):\\n            model.eval()\\n            with torch.no_grad():\\n                for image, target in data_loader:\\n                    model(image)\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways.\\n        # e.g. set the global qconfig, which means we will use the same qconfig for\\n        # all operators in the model, this can be overwritten by other settings\\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\\n        # e.g. quantize the linear submodule with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\\n        # e.g. quantize all nn.Linear modules with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\\n        # argument\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config. If the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert observer modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        #\\n        # Example:\\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\\n        # activation and qint8 for weight:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\\n        # Note: current qconfig api does not support setting output observer, but\\n        # we may extend this to support these more fine grained control in the\\n        # future\\n        #\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # in backend config, linear module also supports in this configuration:\\n        # weighted_int8_dtype_config = DTypeConfig(\\n        #   input_dtype=torch.quint8,\\n        #   output_dtype=torch.quint8,\\n        #   weight_dtype=torch.qint8,\\n        #   bias_type=torch.float)\\n\\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\\n        #    ...\\n\\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\\n        # is supported by the backend_config and insert observers and fake quant modules\\n        # in the model\\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run calibration\\n        calibrate(prepared_model, sample_inference_data)\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)",
            "def prepare_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, _equalization_config: Optional[Union[QConfigMapping, Dict[str, Any]]]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Prepare a model for post training quantization\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n\\n      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is\\n         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`\\n         for more details\\n\\n      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,\\n         Tuple of positional args (keyword args can be passed as positional args as well)\\n\\n      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.\\n          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details\\n\\n      * `_equalization_config`: config for specifying how to perform equalization on the model\\n\\n      * `backend_config` (BackendConfig): config that specifies how operators are quantized\\n         in a backend, this includes how the operators are observed,\\n         supported fusion patterns, how quantize/dequantize ops are\\n         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n      A GraphModule with observer (configured by qconfig_mapping), ready for calibration\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().eval()\\n\\n        # define calibration function\\n        def calibrate(model, data_loader):\\n            model.eval()\\n            with torch.no_grad():\\n                for image, target in data_loader:\\n                    model(image)\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),\\n        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways.\\n        # e.g. set the global qconfig, which means we will use the same qconfig for\\n        # all operators in the model, this can be overwritten by other settings\\n        # qconfig_mapping = QConfigMapping().set_global(qconfig)\\n        # e.g. quantize the linear submodule with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\\n        # e.g. quantize all nn.Linear modules with a specific qconfig\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\\n        # argument\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config. If the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert observer modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        #\\n        # Example:\\n        # in qconfig_mapping, user sets linear module to be quantized with quint8 for\\n        # activation and qint8 for weight:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),\\n        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))\\n        # Note: current qconfig api does not support setting output observer, but\\n        # we may extend this to support these more fine grained control in the\\n        # future\\n        #\\n        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\\n        # in backend config, linear module also supports in this configuration:\\n        # weighted_int8_dtype_config = DTypeConfig(\\n        #   input_dtype=torch.quint8,\\n        #   output_dtype=torch.quint8,\\n        #   weight_dtype=torch.qint8,\\n        #   bias_type=torch.float)\\n\\n        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\\\\n        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\\\\n        #    .add_dtype_config(weighted_int8_dtype_config) \\\\\\n        #    ...\\n\\n        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\\n        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping\\n        # is supported by the backend_config and insert observers and fake quant modules\\n        # in the model\\n        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run calibration\\n        calibrate(prepared_model, sample_inference_data)\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_fx')\n    return _prepare_fx(model, qconfig_mapping, False, example_inputs, prepare_custom_config, _equalization_config, backend_config)"
        ]
    },
    {
        "func_name": "prepare_qat_fx",
        "original": "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Prepare a model for quantization aware training\n\n    Args:\n      * `model` (torch.nn.Module): torch.nn.Module model\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\n\n    Return:\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\n      quantization aware training\n\n    Example::\n\n        import torch\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\n\n        class Submodule(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n            def forward(self, x):\n                x = self.linear(x)\n                return x\n\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n                self.sub = Submodule()\n\n            def forward(self, x):\n                x = self.linear(x)\n                x = self.sub(x) + x\n                return x\n\n        # initialize a floating point model\n        float_model = M().train()\n        # (optional, but preferred) load the weights from pretrained model\n        # float_model.load_weights(...)\n\n        # define the training loop for quantization aware training\n        def train_loop(model, train_data):\n            model.train()\n            for image, target in data_loader:\n                ...\n\n        # qconfig is the configuration for how we insert observers for a particular\n        # operator\n        # qconfig = get_default_qconfig(\"fbgemm\")\n        # Example of customizing qconfig:\n        # qconfig = torch.ao.quantization.QConfig(\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\n        # `activation` and `weight` are constructors of observer module\n\n        # qconfig_mapping is a collection of quantization configurations, user can\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\n        # in the model through qconfig_mapping\n        # the following call will get the qconfig_mapping that works best for models\n        # that target \"fbgemm\" backend\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\n\n        # We can customize qconfig_mapping in different ways, please take a look at\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\n        # to configure this\n\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\n        # outputs in the model\n        # currently it's not used, but please make sure model(*example_inputs) runs\n        example_inputs = (torch.randn(1, 3, 224, 224),)\n\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\n        # backend_config, if the configuration for an operator in qconfig_mapping\n        # is supported in the backend_config (meaning it's supported by the target\n        # hardware), we'll insert fake_quantize modules according to the qconfig_mapping\n        # otherwise the configuration in qconfig_mapping will be ignored\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\n        # how qconfig_mapping interacts with backend_config\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\n        # Run training\n        train_loop(prepared_model, train_loop)\n\n    \"\"\"\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)",
        "mutated": [
            "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Prepare a model for quantization aware training\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n\\n    Return:\\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\\n      quantization aware training\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().train()\\n        # (optional, but preferred) load the weights from pretrained model\\n        # float_model.load_weights(...)\\n\\n        # define the training loop for quantization aware training\\n        def train_loop(model, train_data):\\n            model.train()\\n            for image, target in data_loader:\\n                ...\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways, please take a look at\\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\\n        # to configure this\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config, if the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert fake_quantize modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\\n        # how qconfig_mapping interacts with backend_config\\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run training\\n        train_loop(prepared_model, train_loop)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)",
            "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Prepare a model for quantization aware training\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n\\n    Return:\\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\\n      quantization aware training\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().train()\\n        # (optional, but preferred) load the weights from pretrained model\\n        # float_model.load_weights(...)\\n\\n        # define the training loop for quantization aware training\\n        def train_loop(model, train_data):\\n            model.train()\\n            for image, target in data_loader:\\n                ...\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways, please take a look at\\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\\n        # to configure this\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config, if the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert fake_quantize modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\\n        # how qconfig_mapping interacts with backend_config\\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run training\\n        train_loop(prepared_model, train_loop)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)",
            "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Prepare a model for quantization aware training\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n\\n    Return:\\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\\n      quantization aware training\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().train()\\n        # (optional, but preferred) load the weights from pretrained model\\n        # float_model.load_weights(...)\\n\\n        # define the training loop for quantization aware training\\n        def train_loop(model, train_data):\\n            model.train()\\n            for image, target in data_loader:\\n                ...\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways, please take a look at\\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\\n        # to configure this\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config, if the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert fake_quantize modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\\n        # how qconfig_mapping interacts with backend_config\\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run training\\n        train_loop(prepared_model, train_loop)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)",
            "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Prepare a model for quantization aware training\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n\\n    Return:\\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\\n      quantization aware training\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().train()\\n        # (optional, but preferred) load the weights from pretrained model\\n        # float_model.load_weights(...)\\n\\n        # define the training loop for quantization aware training\\n        def train_loop(model, train_data):\\n            model.train()\\n            for image, target in data_loader:\\n                ...\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways, please take a look at\\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\\n        # to configure this\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config, if the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert fake_quantize modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\\n        # how qconfig_mapping interacts with backend_config\\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run training\\n        train_loop(prepared_model, train_loop)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)",
            "def prepare_qat_fx(model: torch.nn.Module, qconfig_mapping: Union[QConfigMapping, Dict[str, Any]], example_inputs: Tuple[Any, ...], prepare_custom_config: Union[PrepareCustomConfig, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Prepare a model for quantization aware training\\n\\n    Args:\\n      * `model` (torch.nn.Module): torch.nn.Module model\\n      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`\\n\\n    Return:\\n      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for\\n      quantization aware training\\n\\n    Example::\\n\\n        import torch\\n        from torch.ao.quantization import get_default_qat_qconfig_mapping\\n        from torch.ao.quantization.quantize_fx import prepare_qat_fx\\n\\n        class Submodule(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n            def forward(self, x):\\n                x = self.linear(x)\\n                return x\\n\\n        class M(torch.nn.Module):\\n            def __init__(self):\\n                super().__init__()\\n                self.linear = torch.nn.Linear(5, 5)\\n                self.sub = Submodule()\\n\\n            def forward(self, x):\\n                x = self.linear(x)\\n                x = self.sub(x) + x\\n                return x\\n\\n        # initialize a floating point model\\n        float_model = M().train()\\n        # (optional, but preferred) load the weights from pretrained model\\n        # float_model.load_weights(...)\\n\\n        # define the training loop for quantization aware training\\n        def train_loop(model, train_data):\\n            model.train()\\n            for image, target in data_loader:\\n                ...\\n\\n        # qconfig is the configuration for how we insert observers for a particular\\n        # operator\\n        # qconfig = get_default_qconfig(\"fbgemm\")\\n        # Example of customizing qconfig:\\n        # qconfig = torch.ao.quantization.QConfig(\\n        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\\n        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\\n        # `activation` and `weight` are constructors of observer module\\n\\n        # qconfig_mapping is a collection of quantization configurations, user can\\n        # set the qconfig for each operator (torch op calls, functional calls, module calls)\\n        # in the model through qconfig_mapping\\n        # the following call will get the qconfig_mapping that works best for models\\n        # that target \"fbgemm\" backend\\n        qconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\\n\\n        # We can customize qconfig_mapping in different ways, please take a look at\\n        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\\n        # to configure this\\n\\n        # example_inputs is a tuple of inputs, that is used to infer the type of the\\n        # outputs in the model\\n        # currently it\\'s not used, but please make sure model(*example_inputs) runs\\n        example_inputs = (torch.randn(1, 3, 224, 224),)\\n\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\\n        # backend_config, if the configuration for an operator in qconfig_mapping\\n        # is supported in the backend_config (meaning it\\'s supported by the target\\n        # hardware), we\\'ll insert fake_quantize modules according to the qconfig_mapping\\n        # otherwise the configuration in qconfig_mapping will be ignored\\n        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\\n        # how qconfig_mapping interacts with backend_config\\n        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\\n        # Run training\\n        train_loop(prepared_model, train_loop)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.prepare_qat_fx')\n    return _prepare_fx(model, qconfig_mapping, True, example_inputs, prepare_custom_config, backend_config=backend_config)"
        ]
    },
    {
        "func_name": "_convert_fx",
        "original": "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    \"\"\" `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\n    \"\"\"\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized",
        "mutated": [
            "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    if False:\n        i = 10\n    ' `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    '\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized",
            "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    '\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized",
            "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    '\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized",
            "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    '\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized",
            "def _convert_fx(graph_module: GraphModule, is_reference: bool, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, is_standalone_module: bool=False, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None, is_decomposed: bool=False) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    '\n    if convert_custom_config is None:\n        convert_custom_config = ConvertCustomConfig()\n    if isinstance(convert_custom_config, Dict):\n        warnings.warn('Passing a convert_custom_config_dict to convert is deprecated and will not be supported in a future version. Please pass in a ConvertCustomConfig instead.')\n        convert_custom_config = ConvertCustomConfig.from_dict(convert_custom_config)\n    _check_is_graph_module(graph_module)\n    preserved_attr_names = convert_custom_config.preserved_attributes\n    preserved_attrs = {attr: getattr(graph_module, attr) for attr in preserved_attr_names if hasattr(graph_module, attr)}\n    quantized = convert(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=is_decomposed)\n    attach_preserved_attrs_to_model(quantized, preserved_attrs)\n    return quantized"
        ]
    },
    {
        "func_name": "convert_fx",
        "original": "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Convert a calibrated or trained model to a quantized model\n\n    Args:\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\n\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\n\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\n\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\n\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\n\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\n\n            qconfig_mapping = QConfigMapping\n                .set_global(qconfig_from_prepare)\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\n\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\n            operators should be quantized in the backend, this includes quantization\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\n            observer placement for each operators and fused operators.\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\n\n    Return:\n        A quantized model (torch.nn.Module)\n\n    Example::\n\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\n        # convert_fx converts a calibrated/trained model to a quantized model for the\n        # target hardware, this includes converting the model first to a reference\n        # quantized model, and then lower the reference quantized model to a backend\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\n        # they share the same set of quantized operators, so we are using the same\n        # lowering procedure\n        #\n        # backend_config defines the corresponding reference quantized module for\n        # the weighted modules in the model, e.g. nn.Linear\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\n        quantized_model = convert_fx(prepared_model)\n\n    \"\"\"\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
        "mutated": [
            "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Convert a calibrated or trained model to a quantized model\\n\\n    Args:\\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n\\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\\n\\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\\n\\n            qconfig_mapping = QConfigMapping\\n                .set_global(qconfig_from_prepare)\\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend, this includes quantization\\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\\n            observer placement for each operators and fused operators.\\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n        A quantized model (torch.nn.Module)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # convert_fx converts a calibrated/trained model to a quantized model for the\\n        # target hardware, this includes converting the model first to a reference\\n        # quantized model, and then lower the reference quantized model to a backend\\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\\n        # they share the same set of quantized operators, so we are using the same\\n        # lowering procedure\\n        #\\n        # backend_config defines the corresponding reference quantized module for\\n        # the weighted modules in the model, e.g. nn.Linear\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        quantized_model = convert_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Convert a calibrated or trained model to a quantized model\\n\\n    Args:\\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n\\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\\n\\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\\n\\n            qconfig_mapping = QConfigMapping\\n                .set_global(qconfig_from_prepare)\\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend, this includes quantization\\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\\n            observer placement for each operators and fused operators.\\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n        A quantized model (torch.nn.Module)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # convert_fx converts a calibrated/trained model to a quantized model for the\\n        # target hardware, this includes converting the model first to a reference\\n        # quantized model, and then lower the reference quantized model to a backend\\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\\n        # they share the same set of quantized operators, so we are using the same\\n        # lowering procedure\\n        #\\n        # backend_config defines the corresponding reference quantized module for\\n        # the weighted modules in the model, e.g. nn.Linear\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        quantized_model = convert_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Convert a calibrated or trained model to a quantized model\\n\\n    Args:\\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n\\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\\n\\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\\n\\n            qconfig_mapping = QConfigMapping\\n                .set_global(qconfig_from_prepare)\\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend, this includes quantization\\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\\n            observer placement for each operators and fused operators.\\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n        A quantized model (torch.nn.Module)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # convert_fx converts a calibrated/trained model to a quantized model for the\\n        # target hardware, this includes converting the model first to a reference\\n        # quantized model, and then lower the reference quantized model to a backend\\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\\n        # they share the same set of quantized operators, so we are using the same\\n        # lowering procedure\\n        #\\n        # backend_config defines the corresponding reference quantized module for\\n        # the weighted modules in the model, e.g. nn.Linear\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        quantized_model = convert_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Convert a calibrated or trained model to a quantized model\\n\\n    Args:\\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n\\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\\n\\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\\n\\n            qconfig_mapping = QConfigMapping\\n                .set_global(qconfig_from_prepare)\\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend, this includes quantization\\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\\n            observer placement for each operators and fused operators.\\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n        A quantized model (torch.nn.Module)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # convert_fx converts a calibrated/trained model to a quantized model for the\\n        # target hardware, this includes converting the model first to a reference\\n        # quantized model, and then lower the reference quantized model to a backend\\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\\n        # they share the same set of quantized operators, so we are using the same\\n        # lowering procedure\\n        #\\n        # backend_config defines the corresponding reference quantized module for\\n        # the weighted modules in the model, e.g. nn.Linear\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        quantized_model = convert_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Convert a calibrated or trained model to a quantized model\\n\\n    Args:\\n        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n\\n           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,\\n           with the same values or `None`. Additional keys can be specified with values set to `None`.\\n\\n          For each entry whose value is set to None, we skip quantizing that entry in the model::\\n\\n            qconfig_mapping = QConfigMapping\\n                .set_global(qconfig_from_prepare)\\n                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\\n                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\\n                .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend, this includes quantization\\n            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),\\n            observer placement for each operators and fused operators.\\n            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details\\n\\n    Return:\\n        A quantized model (torch.nn.Module)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # convert_fx converts a calibrated/trained model to a quantized model for the\\n        # target hardware, this includes converting the model first to a reference\\n        # quantized model, and then lower the reference quantized model to a backend\\n        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\\n        # they share the same set of quantized operators, so we are using the same\\n        # lowering procedure\\n        #\\n        # backend_config defines the corresponding reference quantized module for\\n        # the weighted modules in the model, e.g. nn.Linear\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        quantized_model = convert_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_fx')\n    return _convert_fx(graph_module, is_reference=False, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)"
        ]
    },
    {
        "func_name": "convert_to_reference_fx",
        "original": "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Convert a calibrated or trained model to a reference quantized model,\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\n    reference quantized model is a standard representation of a quantized model provided\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\n    hardware, like accelerators\n\n    Args:\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\n\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\n\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\n            operators should be quantized in the backend. See\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n    Return:\n        A reference quantized model (GraphModule)\n\n    Example::\n\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\n\n    \"\"\"\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
        "mutated": [
            "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Convert a calibrated or trained model to a reference quantized model,\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Convert a calibrated or trained model to a reference quantized model,\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Convert a calibrated or trained model to a reference quantized model,\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Convert a calibrated or trained model to a reference quantized model,\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)",
            "def convert_to_reference_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, _remove_qconfig: bool=True, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Convert a calibrated or trained model to a reference quantized model,\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule)\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = convert_to_reference_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx.convert_to_reference_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=_remove_qconfig, qconfig_mapping=qconfig_mapping, backend_config=backend_config)"
        ]
    },
    {
        "func_name": "_convert_to_reference_decomposed_fx",
        "original": "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" Convert a calibrated or trained model to a reference quantized model, with\n    decomposed representation for quantized Tensor\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\n    reference quantized model is a standard representation of a quantized model provided\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\n    hardware, like accelerators\n\n    Note: this is not public API\n\n    Args:\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\n\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\n\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\n            operators should be quantized in the backend. See\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\n\n    Return:\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\n\n    Example::\n\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\n\n    \"\"\"\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)",
        "mutated": [
            "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' Convert a calibrated or trained model to a reference quantized model, with\\n    decomposed representation for quantized Tensor\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Note: this is not public API\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)",
            "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' Convert a calibrated or trained model to a reference quantized model, with\\n    decomposed representation for quantized Tensor\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Note: this is not public API\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)",
            "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' Convert a calibrated or trained model to a reference quantized model, with\\n    decomposed representation for quantized Tensor\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Note: this is not public API\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)",
            "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' Convert a calibrated or trained model to a reference quantized model, with\\n    decomposed representation for quantized Tensor\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Note: this is not public API\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)",
            "def _convert_to_reference_decomposed_fx(graph_module: GraphModule, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None, qconfig_mapping: Union[QConfigMapping, Dict[str, Any], None]=None, backend_config: Union[BackendConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' Convert a calibrated or trained model to a reference quantized model, with\\n    decomposed representation for quantized Tensor\\n    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,\\n    reference quantized model is a standard representation of a quantized model provided\\n    by FX Graph Mode Quantization, it can be further lowered to run on the target\\n    hardware, like accelerators\\n\\n    Note: this is not public API\\n\\n    Args:\\n        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)\\n\\n        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.\\n\\n        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.\\n            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n         * `backend_config` (BackendConfig): A configuration for the backend which describes how\\n            operators should be quantized in the backend. See\\n            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.\\n\\n    Return:\\n        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor\\n\\n    Example::\\n\\n        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\\n        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\\n        # e.g. backend_config = get_default_backend_config(\"fbgemm\")\\n        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)\\n\\n    '\n    torch._C._log_api_usage_once('quantization_api.quantize_fx._convert_to_reference_decomposed_fx')\n    return _convert_fx(graph_module, is_reference=True, convert_custom_config=convert_custom_config, _remove_qconfig=False, qconfig_mapping=qconfig_mapping, backend_config=backend_config, is_decomposed=True)"
        ]
    },
    {
        "func_name": "_convert_standalone_module_fx",
        "original": "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    \"\"\" [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\n    and convert it to a quantized model\n\n    Returns a quantized standalone module, whether input/output is quantized is\n    specified by prepare_custom_config, with\n    input_quantized_idxs, output_quantized_idxs, please\n    see docs for prepare_fx for details\n    \"\"\"\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)",
        "mutated": [
            "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n    ' [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    and convert it to a quantized model\\n\\n    Returns a quantized standalone module, whether input/output is quantized is\\n    specified by prepare_custom_config, with\\n    input_quantized_idxs, output_quantized_idxs, please\\n    see docs for prepare_fx for details\\n    '\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)",
            "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ' [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    and convert it to a quantized model\\n\\n    Returns a quantized standalone module, whether input/output is quantized is\\n    specified by prepare_custom_config, with\\n    input_quantized_idxs, output_quantized_idxs, please\\n    see docs for prepare_fx for details\\n    '\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)",
            "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ' [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    and convert it to a quantized model\\n\\n    Returns a quantized standalone module, whether input/output is quantized is\\n    specified by prepare_custom_config, with\\n    input_quantized_idxs, output_quantized_idxs, please\\n    see docs for prepare_fx for details\\n    '\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)",
            "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ' [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    and convert it to a quantized model\\n\\n    Returns a quantized standalone module, whether input/output is quantized is\\n    specified by prepare_custom_config, with\\n    input_quantized_idxs, output_quantized_idxs, please\\n    see docs for prepare_fx for details\\n    '\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)",
            "def _convert_standalone_module_fx(graph_module: GraphModule, is_reference: bool=False, convert_custom_config: Union[ConvertCustomConfig, Dict[str, Any], None]=None) -> GraphModule:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ' [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`\\n    and convert it to a quantized model\\n\\n    Returns a quantized standalone module, whether input/output is quantized is\\n    specified by prepare_custom_config, with\\n    input_quantized_idxs, output_quantized_idxs, please\\n    see docs for prepare_fx for details\\n    '\n    return _convert_fx(graph_module, is_reference, convert_custom_config, is_standalone_module=True)"
        ]
    }
]