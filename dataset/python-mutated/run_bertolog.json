[
    {
        "func_name": "entropy",
        "original": "def entropy(p):\n    \"\"\"Compute the entropy of a probability distribution\"\"\"\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
        "mutated": [
            "def entropy(p):\n    if False:\n        i = 10\n    'Compute the entropy of a probability distribution'\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the entropy of a probability distribution'\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the entropy of a probability distribution'\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the entropy of a probability distribution'\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the entropy of a probability distribution'\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)"
        ]
    },
    {
        "func_name": "print_2d_tensor",
        "original": "def print_2d_tensor(tensor):\n    \"\"\"Print a 2D tensor\"\"\"\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
        "mutated": [
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))"
        ]
    },
    {
        "func_name": "compute_heads_importance",
        "original": "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    \"\"\"This method shows how to compute:\n    - head attention entropy\n    - head importance scores according to http://arxiv.org/abs/1905.10650\n    \"\"\"\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)",
        "mutated": [
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    preds = None\n    labels = None\n    tot_tokens = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        for (k, v) in inputs.items():\n            inputs[k] = v.to(args.device)\n        outputs = model(**inputs, head_mask=head_mask)\n        (loss, logits, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach()) * inputs['attention_mask'].float().unsqueeze(1)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            labels = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            labels = np.append(labels, inputs['labels'].detach().cpu().numpy(), axis=0)\n        tot_tokens += inputs['attention_mask'].float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    np.save(os.path.join(args.output_dir, 'attn_entropy.npy'), attn_entropy.detach().cpu().numpy())\n    np.save(os.path.join(args.output_dir, 'head_importance.npy'), head_importance.detach().cpu().numpy())\n    logger.info('Attention entropies')\n    print_2d_tensor(attn_entropy)\n    logger.info('Head importance scores')\n    print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, preds, labels)"
        ]
    },
    {
        "func_name": "mask_heads",
        "original": "def mask_heads(args, model, eval_dataloader):\n    \"\"\"This method shows how to mask head (set some heads to zero), to test the effect on the network,\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n    \"\"\"\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
        "mutated": [
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask"
        ]
    },
    {
        "func_name": "prune_heads",
        "original": "def prune_heads(args, model, eval_dataloader, head_mask):\n    \"\"\"This method shows how to prune head (remove heads weights) based on\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n    \"\"\"\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)",
        "mutated": [
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, preds, labels) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    preds = np.argmax(preds, axis=1) if args.output_mode == 'classification' else np.squeeze(preds)\n    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (new timing / original timing): %f percents', original_time / new_time * 100)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--task_name', default=None, type=str, required=True, help='The name of the task to train selected in the list: ' + ', '.join(glue_processors.keys()))\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from huggingface.co')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    set_seed(args.seed)\n    args.task_name = args.task_name.lower()\n    if args.task_name not in glue_processors:\n        raise ValueError('Task not found: %s' % args.task_name)\n    processor = glue_processors[args.task_name]()\n    args.output_mode = glue_output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name, output_attentions=True, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, cache_dir=args.cache_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode='dev')\n    if args.data_subset > 0:\n        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)"
        ]
    }
]