[
    {
        "func_name": "_high_leverage",
        "original": "def _high_leverage(results):\n    return 2.0 * (results.df_model + 1) / results.nobs",
        "mutated": [
            "def _high_leverage(results):\n    if False:\n        i = 10\n    return 2.0 * (results.df_model + 1) / results.nobs",
            "def _high_leverage(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 2.0 * (results.df_model + 1) / results.nobs",
            "def _high_leverage(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 2.0 * (results.df_model + 1) / results.nobs",
            "def _high_leverage(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 2.0 * (results.df_model + 1) / results.nobs",
            "def _high_leverage(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 2.0 * (results.df_model + 1) / results.nobs"
        ]
    },
    {
        "func_name": "add_lowess",
        "original": "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    \"\"\"\n    Add Lowess line to a plot.\n\n    Parameters\n    ----------\n    ax : AxesSubplot\n        The Axes to which to add the plot\n    lines_idx : int\n        This is the line on the existing plot to which you want to add\n        a smoothed lowess line.\n    frac : float\n        The fraction of the points to use when doing the lowess fit.\n    lowess_kwargs\n        Additional keyword arguments are passes to lowess.\n\n    Returns\n    -------\n    Figure\n        The figure that holds the instance.\n    \"\"\"\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure",
        "mutated": [
            "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    if False:\n        i = 10\n    '\\n    Add Lowess line to a plot.\\n\\n    Parameters\\n    ----------\\n    ax : AxesSubplot\\n        The Axes to which to add the plot\\n    lines_idx : int\\n        This is the line on the existing plot to which you want to add\\n        a smoothed lowess line.\\n    frac : float\\n        The fraction of the points to use when doing the lowess fit.\\n    lowess_kwargs\\n        Additional keyword arguments are passes to lowess.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure that holds the instance.\\n    '\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure",
            "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Add Lowess line to a plot.\\n\\n    Parameters\\n    ----------\\n    ax : AxesSubplot\\n        The Axes to which to add the plot\\n    lines_idx : int\\n        This is the line on the existing plot to which you want to add\\n        a smoothed lowess line.\\n    frac : float\\n        The fraction of the points to use when doing the lowess fit.\\n    lowess_kwargs\\n        Additional keyword arguments are passes to lowess.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure that holds the instance.\\n    '\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure",
            "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Add Lowess line to a plot.\\n\\n    Parameters\\n    ----------\\n    ax : AxesSubplot\\n        The Axes to which to add the plot\\n    lines_idx : int\\n        This is the line on the existing plot to which you want to add\\n        a smoothed lowess line.\\n    frac : float\\n        The fraction of the points to use when doing the lowess fit.\\n    lowess_kwargs\\n        Additional keyword arguments are passes to lowess.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure that holds the instance.\\n    '\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure",
            "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Add Lowess line to a plot.\\n\\n    Parameters\\n    ----------\\n    ax : AxesSubplot\\n        The Axes to which to add the plot\\n    lines_idx : int\\n        This is the line on the existing plot to which you want to add\\n        a smoothed lowess line.\\n    frac : float\\n        The fraction of the points to use when doing the lowess fit.\\n    lowess_kwargs\\n        Additional keyword arguments are passes to lowess.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure that holds the instance.\\n    '\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure",
            "def add_lowess(ax, lines_idx=0, frac=0.2, **lowess_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Add Lowess line to a plot.\\n\\n    Parameters\\n    ----------\\n    ax : AxesSubplot\\n        The Axes to which to add the plot\\n    lines_idx : int\\n        This is the line on the existing plot to which you want to add\\n        a smoothed lowess line.\\n    frac : float\\n        The fraction of the points to use when doing the lowess fit.\\n    lowess_kwargs\\n        Additional keyword arguments are passes to lowess.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure that holds the instance.\\n    '\n    y0 = ax.get_lines()[lines_idx]._y\n    x0 = ax.get_lines()[lines_idx]._x\n    lres = lowess(y0, x0, frac=frac, **lowess_kwargs)\n    ax.plot(lres[:, 0], lres[:, 1], 'r', lw=1.5)\n    return ax.figure"
        ]
    },
    {
        "func_name": "plot_fit",
        "original": "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    \"\"\"\n    Plot fit against one regressor.\n\n    This creates one graph with the scatterplot of observed values\n    compared to fitted values.\n\n    Parameters\n    ----------\n    results : Results\n        A result instance with resid, model.endog and model.exog as\n        attributes.\n    exog_idx : {int, str}\n        Name or index of regressor in exog matrix.\n    y_true : array_like. optional\n        If this is not None, then the array is added to the plot.\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n    vlines : bool, optional\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\n        plotted.\n    **kwargs\n        The keyword arguments are passed to the plot command for the fitted\n        values points.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    Examples\n    --------\n    Load the Statewide Crime data set and perform linear regression with\n    `poverty` and `hs_grad` as variables and `murder` as the response\n\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n\n    >>> data = sm.datasets.statecrime.load_pandas().data\n    >>> murder = data['murder']\n    >>> X = data[['poverty', 'hs_grad']]\n\n    >>> X[\"constant\"] = 1\n    >>> y = murder\n    >>> model = sm.OLS(y, X)\n    >>> results = model.fit()\n\n    Create a plot just for the variable 'Poverty.'\n    Note that vertical bars representing uncertainty are plotted since vlines is true\n\n    >>> fig, ax = plt.subplots()\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\n    >>> ax.set_ylabel(\"Murder Rate\")\n    >>> ax.set_xlabel(\"Poverty Level\")\n    >>> ax.set_title(\"Linear Regression\")\n\n    >>> plt.show()\n\n    .. plot:: plots/graphics_plot_fit_ex.py\n    \"\"\"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig",
        "mutated": [
            "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    if False:\n        i = 10\n    '\\n    Plot fit against one regressor.\\n\\n    This creates one graph with the scatterplot of observed values\\n    compared to fitted values.\\n\\n    Parameters\\n    ----------\\n    results : Results\\n        A result instance with resid, model.endog and model.exog as\\n        attributes.\\n    exog_idx : {int, str}\\n        Name or index of regressor in exog matrix.\\n    y_true : array_like. optional\\n        If this is not None, then the array is added to the plot.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    vlines : bool, optional\\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\\n        plotted.\\n    **kwargs\\n        The keyword arguments are passed to the plot command for the fitted\\n        values points.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and perform linear regression with\\n    `poverty` and `hs_grad` as variables and `murder` as the response\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> data = sm.datasets.statecrime.load_pandas().data\\n    >>> murder = data[\\'murder\\']\\n    >>> X = data[[\\'poverty\\', \\'hs_grad\\']]\\n\\n    >>> X[\"constant\"] = 1\\n    >>> y = murder\\n    >>> model = sm.OLS(y, X)\\n    >>> results = model.fit()\\n\\n    Create a plot just for the variable \\'Poverty.\\'\\n    Note that vertical bars representing uncertainty are plotted since vlines is true\\n\\n    >>> fig, ax = plt.subplots()\\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\\n    >>> ax.set_ylabel(\"Murder Rate\")\\n    >>> ax.set_xlabel(\"Poverty Level\")\\n    >>> ax.set_title(\"Linear Regression\")\\n\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_plot_fit_ex.py\\n    '\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig",
            "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Plot fit against one regressor.\\n\\n    This creates one graph with the scatterplot of observed values\\n    compared to fitted values.\\n\\n    Parameters\\n    ----------\\n    results : Results\\n        A result instance with resid, model.endog and model.exog as\\n        attributes.\\n    exog_idx : {int, str}\\n        Name or index of regressor in exog matrix.\\n    y_true : array_like. optional\\n        If this is not None, then the array is added to the plot.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    vlines : bool, optional\\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\\n        plotted.\\n    **kwargs\\n        The keyword arguments are passed to the plot command for the fitted\\n        values points.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and perform linear regression with\\n    `poverty` and `hs_grad` as variables and `murder` as the response\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> data = sm.datasets.statecrime.load_pandas().data\\n    >>> murder = data[\\'murder\\']\\n    >>> X = data[[\\'poverty\\', \\'hs_grad\\']]\\n\\n    >>> X[\"constant\"] = 1\\n    >>> y = murder\\n    >>> model = sm.OLS(y, X)\\n    >>> results = model.fit()\\n\\n    Create a plot just for the variable \\'Poverty.\\'\\n    Note that vertical bars representing uncertainty are plotted since vlines is true\\n\\n    >>> fig, ax = plt.subplots()\\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\\n    >>> ax.set_ylabel(\"Murder Rate\")\\n    >>> ax.set_xlabel(\"Poverty Level\")\\n    >>> ax.set_title(\"Linear Regression\")\\n\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_plot_fit_ex.py\\n    '\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig",
            "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Plot fit against one regressor.\\n\\n    This creates one graph with the scatterplot of observed values\\n    compared to fitted values.\\n\\n    Parameters\\n    ----------\\n    results : Results\\n        A result instance with resid, model.endog and model.exog as\\n        attributes.\\n    exog_idx : {int, str}\\n        Name or index of regressor in exog matrix.\\n    y_true : array_like. optional\\n        If this is not None, then the array is added to the plot.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    vlines : bool, optional\\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\\n        plotted.\\n    **kwargs\\n        The keyword arguments are passed to the plot command for the fitted\\n        values points.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and perform linear regression with\\n    `poverty` and `hs_grad` as variables and `murder` as the response\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> data = sm.datasets.statecrime.load_pandas().data\\n    >>> murder = data[\\'murder\\']\\n    >>> X = data[[\\'poverty\\', \\'hs_grad\\']]\\n\\n    >>> X[\"constant\"] = 1\\n    >>> y = murder\\n    >>> model = sm.OLS(y, X)\\n    >>> results = model.fit()\\n\\n    Create a plot just for the variable \\'Poverty.\\'\\n    Note that vertical bars representing uncertainty are plotted since vlines is true\\n\\n    >>> fig, ax = plt.subplots()\\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\\n    >>> ax.set_ylabel(\"Murder Rate\")\\n    >>> ax.set_xlabel(\"Poverty Level\")\\n    >>> ax.set_title(\"Linear Regression\")\\n\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_plot_fit_ex.py\\n    '\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig",
            "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Plot fit against one regressor.\\n\\n    This creates one graph with the scatterplot of observed values\\n    compared to fitted values.\\n\\n    Parameters\\n    ----------\\n    results : Results\\n        A result instance with resid, model.endog and model.exog as\\n        attributes.\\n    exog_idx : {int, str}\\n        Name or index of regressor in exog matrix.\\n    y_true : array_like. optional\\n        If this is not None, then the array is added to the plot.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    vlines : bool, optional\\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\\n        plotted.\\n    **kwargs\\n        The keyword arguments are passed to the plot command for the fitted\\n        values points.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and perform linear regression with\\n    `poverty` and `hs_grad` as variables and `murder` as the response\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> data = sm.datasets.statecrime.load_pandas().data\\n    >>> murder = data[\\'murder\\']\\n    >>> X = data[[\\'poverty\\', \\'hs_grad\\']]\\n\\n    >>> X[\"constant\"] = 1\\n    >>> y = murder\\n    >>> model = sm.OLS(y, X)\\n    >>> results = model.fit()\\n\\n    Create a plot just for the variable \\'Poverty.\\'\\n    Note that vertical bars representing uncertainty are plotted since vlines is true\\n\\n    >>> fig, ax = plt.subplots()\\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\\n    >>> ax.set_ylabel(\"Murder Rate\")\\n    >>> ax.set_xlabel(\"Poverty Level\")\\n    >>> ax.set_title(\"Linear Regression\")\\n\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_plot_fit_ex.py\\n    '\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig",
            "def plot_fit(results, exog_idx, y_true=None, ax=None, vlines=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Plot fit against one regressor.\\n\\n    This creates one graph with the scatterplot of observed values\\n    compared to fitted values.\\n\\n    Parameters\\n    ----------\\n    results : Results\\n        A result instance with resid, model.endog and model.exog as\\n        attributes.\\n    exog_idx : {int, str}\\n        Name or index of regressor in exog matrix.\\n    y_true : array_like. optional\\n        If this is not None, then the array is added to the plot.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    vlines : bool, optional\\n        If this not True, then the uncertainty (pointwise prediction intervals) of the fit is not\\n        plotted.\\n    **kwargs\\n        The keyword arguments are passed to the plot command for the fitted\\n        values points.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and perform linear regression with\\n    `poverty` and `hs_grad` as variables and `murder` as the response\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> data = sm.datasets.statecrime.load_pandas().data\\n    >>> murder = data[\\'murder\\']\\n    >>> X = data[[\\'poverty\\', \\'hs_grad\\']]\\n\\n    >>> X[\"constant\"] = 1\\n    >>> y = murder\\n    >>> model = sm.OLS(y, X)\\n    >>> results = model.fit()\\n\\n    Create a plot just for the variable \\'Poverty.\\'\\n    Note that vertical bars representing uncertainty are plotted since vlines is true\\n\\n    >>> fig, ax = plt.subplots()\\n    >>> fig = sm.graphics.plot_fit(results, 0, ax=ax)\\n    >>> ax.set_ylabel(\"Murder Rate\")\\n    >>> ax.set_xlabel(\"Poverty Level\")\\n    >>> ax.set_title(\"Linear Regression\")\\n\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_plot_fit_ex.py\\n    '\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y = results.model.endog\n    x1 = results.model.exog[:, exog_idx]\n    x1_argsort = np.argsort(x1)\n    y = y[x1_argsort]\n    x1 = x1[x1_argsort]\n    ax.plot(x1, y, 'bo', label=results.model.endog_names)\n    if y_true is not None:\n        ax.plot(x1, y_true[x1_argsort], 'b-', label='True values')\n    title = 'Fitted values versus %s' % exog_name\n    ax.plot(x1, results.fittedvalues[x1_argsort], 'D', color='r', label='fitted', **kwargs)\n    if vlines is True:\n        (_, iv_l, iv_u) = wls_prediction_std(results)\n        ax.vlines(x1, iv_l[x1_argsort], iv_u[x1_argsort], linewidth=1, color='k', alpha=0.7)\n    ax.set_title(title)\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(results.model.endog_names)\n    ax.legend(loc='best', numpoints=1)\n    return fig"
        ]
    },
    {
        "func_name": "plot_regress_exog",
        "original": "def plot_regress_exog(results, exog_idx, fig=None):\n    \"\"\"Plot regression results against one regressor.\n\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\n    'residuals versus exog', 'fitted versus exog' and\n    'fitted plus residual versus exog'\n\n    Parameters\n    ----------\n    results : result instance\n        A result instance with resid, model.endog and model.exog as attributes.\n    exog_idx : int or str\n        Name or index of regressor in exog matrix.\n    fig : Figure, optional\n        If given, this figure is simply returned.  Otherwise a new figure is\n        created.\n\n    Returns\n    -------\n    Figure\n        The value of `fig` if provided. Otherwise a new instance.\n\n    Examples\n    --------\n    Load the Statewide Crime data set and build a model with regressors\n    including the rate of high school graduation (hs_grad), population in urban\n    areas (urban), households below poverty line (poverty), and single person\n    households (single).  Outcome variable is the murder rate (murder).\n\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\n    and CCPR plot for poverty rate.\n\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.formula.api as smf\n\n    >>> fig = plt.figure(figsize=(8, 6))\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\n    ...                   data=crime_data.data).fit()\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_regress_exog.py\n    \"\"\"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig",
        "mutated": [
            "def plot_regress_exog(results, exog_idx, fig=None):\n    if False:\n        i = 10\n    \"Plot regression results against one regressor.\\n\\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\\n    'residuals versus exog', 'fitted versus exog' and\\n    'fitted plus residual versus exog'\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A result instance with resid, model.endog and model.exog as attributes.\\n    exog_idx : int or str\\n        Name or index of regressor in exog matrix.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        The value of `fig` if provided. Otherwise a new instance.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and build a model with regressors\\n    including the rate of high school graduation (hs_grad), population in urban\\n    areas (urban), households below poverty line (poverty), and single person\\n    households (single).  Outcome variable is the murder rate (murder).\\n\\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\\n    and CCPR plot for poverty rate.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_regress_exog.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig",
            "def plot_regress_exog(results, exog_idx, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Plot regression results against one regressor.\\n\\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\\n    'residuals versus exog', 'fitted versus exog' and\\n    'fitted plus residual versus exog'\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A result instance with resid, model.endog and model.exog as attributes.\\n    exog_idx : int or str\\n        Name or index of regressor in exog matrix.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        The value of `fig` if provided. Otherwise a new instance.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and build a model with regressors\\n    including the rate of high school graduation (hs_grad), population in urban\\n    areas (urban), households below poverty line (poverty), and single person\\n    households (single).  Outcome variable is the murder rate (murder).\\n\\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\\n    and CCPR plot for poverty rate.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_regress_exog.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig",
            "def plot_regress_exog(results, exog_idx, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Plot regression results against one regressor.\\n\\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\\n    'residuals versus exog', 'fitted versus exog' and\\n    'fitted plus residual versus exog'\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A result instance with resid, model.endog and model.exog as attributes.\\n    exog_idx : int or str\\n        Name or index of regressor in exog matrix.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        The value of `fig` if provided. Otherwise a new instance.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and build a model with regressors\\n    including the rate of high school graduation (hs_grad), population in urban\\n    areas (urban), households below poverty line (poverty), and single person\\n    households (single).  Outcome variable is the murder rate (murder).\\n\\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\\n    and CCPR plot for poverty rate.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_regress_exog.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig",
            "def plot_regress_exog(results, exog_idx, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Plot regression results against one regressor.\\n\\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\\n    'residuals versus exog', 'fitted versus exog' and\\n    'fitted plus residual versus exog'\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A result instance with resid, model.endog and model.exog as attributes.\\n    exog_idx : int or str\\n        Name or index of regressor in exog matrix.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        The value of `fig` if provided. Otherwise a new instance.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and build a model with regressors\\n    including the rate of high school graduation (hs_grad), population in urban\\n    areas (urban), households below poverty line (poverty), and single person\\n    households (single).  Outcome variable is the murder rate (murder).\\n\\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\\n    and CCPR plot for poverty rate.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_regress_exog.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig",
            "def plot_regress_exog(results, exog_idx, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Plot regression results against one regressor.\\n\\n    This plots four graphs in a 2 by 2 figure: 'endog versus exog',\\n    'residuals versus exog', 'fitted versus exog' and\\n    'fitted plus residual versus exog'\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A result instance with resid, model.endog and model.exog as attributes.\\n    exog_idx : int or str\\n        Name or index of regressor in exog matrix.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        The value of `fig` if provided. Otherwise a new instance.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and build a model with regressors\\n    including the rate of high school graduation (hs_grad), population in urban\\n    areas (urban), households below poverty line (poverty), and single person\\n    households (single).  Outcome variable is the murder rate (murder).\\n\\n    Build a 2 by 2 figure based on poverty showing fitted versus actual murder\\n    rate, residuals versus the poverty rate, partial regression plot of poverty,\\n    and CCPR plot for poverty rate.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_regress_exog.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    y_name = results.model.endog_names\n    x1 = results.model.exog[:, exog_idx]\n    (prstd, iv_l, iv_u) = wls_prediction_std(results)\n    ax = fig.add_subplot(2, 2, 1)\n    ax.plot(x1, results.model.endog, 'o', color='b', alpha=0.9, label=y_name)\n    ax.plot(x1, results.fittedvalues, 'D', color='r', label='fitted', alpha=0.5)\n    ax.vlines(x1, iv_l, iv_u, linewidth=1, color='k', alpha=0.7)\n    ax.set_title('Y and Fitted vs. X', fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel(y_name)\n    ax.legend(loc='best')\n    ax = fig.add_subplot(2, 2, 2)\n    ax.plot(x1, results.resid, 'o')\n    ax.axhline(y=0, color='black')\n    ax.set_title('Residuals versus %s' % exog_name, fontsize='large')\n    ax.set_xlabel(exog_name)\n    ax.set_ylabel('resid')\n    ax = fig.add_subplot(2, 2, 3)\n    exog_noti = np.ones(results.model.exog.shape[1], bool)\n    exog_noti[exog_idx] = False\n    exog_others = results.model.exog[:, exog_noti]\n    from pandas import Series\n    fig = plot_partregress(results.model.data.orig_endog, Series(x1, name=exog_name, index=results.model.data.row_labels), exog_others, obs_labels=False, ax=ax)\n    ax.set_title('Partial regression plot', fontsize='large')\n    ax = fig.add_subplot(2, 2, 4)\n    fig = plot_ccpr(results, exog_idx, ax=ax)\n    ax.set_title('CCPR Plot', fontsize='large')\n    fig.suptitle('Regression Plots for %s' % exog_name, fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    return fig"
        ]
    },
    {
        "func_name": "_partial_regression",
        "original": "def _partial_regression(endog, exog_i, exog_others):\n    \"\"\"Partial regression.\n\n    regress endog on exog_i conditional on exog_others\n\n    uses OLS\n\n    Parameters\n    ----------\n    endog : array_like\n    exog : array_like\n    exog_others : array_like\n\n    Returns\n    -------\n    res1c : OLS results instance\n\n    (res1a, res1b) : tuple of OLS results instances\n         results from regression of endog on exog_others and of exog_i on\n         exog_others\n    \"\"\"\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))",
        "mutated": [
            "def _partial_regression(endog, exog_i, exog_others):\n    if False:\n        i = 10\n    'Partial regression.\\n\\n    regress endog on exog_i conditional on exog_others\\n\\n    uses OLS\\n\\n    Parameters\\n    ----------\\n    endog : array_like\\n    exog : array_like\\n    exog_others : array_like\\n\\n    Returns\\n    -------\\n    res1c : OLS results instance\\n\\n    (res1a, res1b) : tuple of OLS results instances\\n         results from regression of endog on exog_others and of exog_i on\\n         exog_others\\n    '\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))",
            "def _partial_regression(endog, exog_i, exog_others):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Partial regression.\\n\\n    regress endog on exog_i conditional on exog_others\\n\\n    uses OLS\\n\\n    Parameters\\n    ----------\\n    endog : array_like\\n    exog : array_like\\n    exog_others : array_like\\n\\n    Returns\\n    -------\\n    res1c : OLS results instance\\n\\n    (res1a, res1b) : tuple of OLS results instances\\n         results from regression of endog on exog_others and of exog_i on\\n         exog_others\\n    '\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))",
            "def _partial_regression(endog, exog_i, exog_others):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Partial regression.\\n\\n    regress endog on exog_i conditional on exog_others\\n\\n    uses OLS\\n\\n    Parameters\\n    ----------\\n    endog : array_like\\n    exog : array_like\\n    exog_others : array_like\\n\\n    Returns\\n    -------\\n    res1c : OLS results instance\\n\\n    (res1a, res1b) : tuple of OLS results instances\\n         results from regression of endog on exog_others and of exog_i on\\n         exog_others\\n    '\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))",
            "def _partial_regression(endog, exog_i, exog_others):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Partial regression.\\n\\n    regress endog on exog_i conditional on exog_others\\n\\n    uses OLS\\n\\n    Parameters\\n    ----------\\n    endog : array_like\\n    exog : array_like\\n    exog_others : array_like\\n\\n    Returns\\n    -------\\n    res1c : OLS results instance\\n\\n    (res1a, res1b) : tuple of OLS results instances\\n         results from regression of endog on exog_others and of exog_i on\\n         exog_others\\n    '\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))",
            "def _partial_regression(endog, exog_i, exog_others):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Partial regression.\\n\\n    regress endog on exog_i conditional on exog_others\\n\\n    uses OLS\\n\\n    Parameters\\n    ----------\\n    endog : array_like\\n    exog : array_like\\n    exog_others : array_like\\n\\n    Returns\\n    -------\\n    res1c : OLS results instance\\n\\n    (res1a, res1b) : tuple of OLS results instances\\n         results from regression of endog on exog_others and of exog_i on\\n         exog_others\\n    '\n    res1a = OLS(endog, exog_others).fit()\n    res1b = OLS(exog_i, exog_others).fit()\n    res1c = OLS(res1a.resid, res1b.resid).fit()\n    return (res1c, (res1a, res1b))"
        ]
    },
    {
        "func_name": "plot_partregress",
        "original": "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    \"\"\"Plot partial regression for a single regressor.\n\n    Parameters\n    ----------\n    endog : {ndarray, str}\n       The endogenous or response variable. If string is given, you can use a\n       arbitrary translations as with a formula.\n    exog_i : {ndarray, str}\n        The exogenous, explanatory variable. If string is given, you can use a\n        arbitrary translations as with a formula.\n    exog_others : {ndarray, list[str]}\n        Any other exogenous, explanatory variables. If a list of strings is\n        given, each item is a term in formula. You can use a arbitrary\n        translations as with a formula. The effect of these variables will be\n        removed by OLS regression.\n    data : {DataFrame, dict}\n        Some kind of data structure with names if the other variables are\n        given as strings.\n    title_kwargs : dict\n        Keyword arguments to pass on for the title. The key to control the\n        fonts is fontdict.\n    obs_labels : {bool, array_like}\n        Whether or not to annotate the plot points with their observation\n        labels. If obs_labels is a boolean, the point labels will try to do\n        the right thing. First it will try to use the index of data, then\n        fall back to the index of exog_i. Alternatively, you may give an\n        array-like object corresponding to the observation numbers.\n    label_kwargs : dict\n        Keyword arguments that control annotate for the observation labels.\n    ax : AxesSubplot, optional\n        If given, this subplot is used to plot in instead of a new figure being\n        created.\n    ret_coords : bool\n        If True will return the coordinates of the points in the plot. You\n        can use this to add your own annotations.\n    eval_env : int\n        Patsy eval environment if user functions and formulas are used in\n        defining endog or exog.\n    **kwargs\n        The keyword arguments passed to plot for the points.\n\n    Returns\n    -------\n    fig : Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n    coords : list, optional\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\n\n    See Also\n    --------\n    plot_partregress_grid : Plot partial regression for a set of regressors.\n\n    Notes\n    -----\n    The slope of the fitted line is the that of `exog_i` in the full\n    multiple regression. The individual points can be used to assess the\n    influence of points on the estimated coefficient.\n\n    Examples\n    --------\n    Load the Statewide Crime data set and plot partial regression of the rate\n    of high school graduation (hs_grad) on the murder rate(murder).\n\n    The effects of the percent of the population living in urban areas (urban),\n    below the poverty line (poverty) , and in a single person household (single)\n    are removed by OLS regression.\n\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\n    ...                              exog_others=['urban', 'poverty', 'single'],\n    ...                              data=crime_data.data, obs_labels=False)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_partregress.py\n\n    More detailed examples can be found in the Regression Plots notebook\n    on the examples page.\n    \"\"\"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig",
        "mutated": [
            "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    if False:\n        i = 10\n    \"Plot partial regression for a single regressor.\\n\\n    Parameters\\n    ----------\\n    endog : {ndarray, str}\\n       The endogenous or response variable. If string is given, you can use a\\n       arbitrary translations as with a formula.\\n    exog_i : {ndarray, str}\\n        The exogenous, explanatory variable. If string is given, you can use a\\n        arbitrary translations as with a formula.\\n    exog_others : {ndarray, list[str]}\\n        Any other exogenous, explanatory variables. If a list of strings is\\n        given, each item is a term in formula. You can use a arbitrary\\n        translations as with a formula. The effect of these variables will be\\n        removed by OLS regression.\\n    data : {DataFrame, dict}\\n        Some kind of data structure with names if the other variables are\\n        given as strings.\\n    title_kwargs : dict\\n        Keyword arguments to pass on for the title. The key to control the\\n        fonts is fontdict.\\n    obs_labels : {bool, array_like}\\n        Whether or not to annotate the plot points with their observation\\n        labels. If obs_labels is a boolean, the point labels will try to do\\n        the right thing. First it will try to use the index of data, then\\n        fall back to the index of exog_i. Alternatively, you may give an\\n        array-like object corresponding to the observation numbers.\\n    label_kwargs : dict\\n        Keyword arguments that control annotate for the observation labels.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    ret_coords : bool\\n        If True will return the coordinates of the points in the plot. You\\n        can use this to add your own annotations.\\n    eval_env : int\\n        Patsy eval environment if user functions and formulas are used in\\n        defining endog or exog.\\n    **kwargs\\n        The keyword arguments passed to plot for the points.\\n\\n    Returns\\n    -------\\n    fig : Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n    coords : list, optional\\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\\n\\n    See Also\\n    --------\\n    plot_partregress_grid : Plot partial regression for a set of regressors.\\n\\n    Notes\\n    -----\\n    The slope of the fitted line is the that of `exog_i` in the full\\n    multiple regression. The individual points can be used to assess the\\n    influence of points on the estimated coefficient.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and plot partial regression of the rate\\n    of high school graduation (hs_grad) on the murder rate(murder).\\n\\n    The effects of the percent of the population living in urban areas (urban),\\n    below the poverty line (poverty) , and in a single person household (single)\\n    are removed by OLS regression.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\\n    ...                              exog_others=['urban', 'poverty', 'single'],\\n    ...                              data=crime_data.data, obs_labels=False)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress.py\\n\\n    More detailed examples can be found in the Regression Plots notebook\\n    on the examples page.\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig",
            "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Plot partial regression for a single regressor.\\n\\n    Parameters\\n    ----------\\n    endog : {ndarray, str}\\n       The endogenous or response variable. If string is given, you can use a\\n       arbitrary translations as with a formula.\\n    exog_i : {ndarray, str}\\n        The exogenous, explanatory variable. If string is given, you can use a\\n        arbitrary translations as with a formula.\\n    exog_others : {ndarray, list[str]}\\n        Any other exogenous, explanatory variables. If a list of strings is\\n        given, each item is a term in formula. You can use a arbitrary\\n        translations as with a formula. The effect of these variables will be\\n        removed by OLS regression.\\n    data : {DataFrame, dict}\\n        Some kind of data structure with names if the other variables are\\n        given as strings.\\n    title_kwargs : dict\\n        Keyword arguments to pass on for the title. The key to control the\\n        fonts is fontdict.\\n    obs_labels : {bool, array_like}\\n        Whether or not to annotate the plot points with their observation\\n        labels. If obs_labels is a boolean, the point labels will try to do\\n        the right thing. First it will try to use the index of data, then\\n        fall back to the index of exog_i. Alternatively, you may give an\\n        array-like object corresponding to the observation numbers.\\n    label_kwargs : dict\\n        Keyword arguments that control annotate for the observation labels.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    ret_coords : bool\\n        If True will return the coordinates of the points in the plot. You\\n        can use this to add your own annotations.\\n    eval_env : int\\n        Patsy eval environment if user functions and formulas are used in\\n        defining endog or exog.\\n    **kwargs\\n        The keyword arguments passed to plot for the points.\\n\\n    Returns\\n    -------\\n    fig : Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n    coords : list, optional\\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\\n\\n    See Also\\n    --------\\n    plot_partregress_grid : Plot partial regression for a set of regressors.\\n\\n    Notes\\n    -----\\n    The slope of the fitted line is the that of `exog_i` in the full\\n    multiple regression. The individual points can be used to assess the\\n    influence of points on the estimated coefficient.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and plot partial regression of the rate\\n    of high school graduation (hs_grad) on the murder rate(murder).\\n\\n    The effects of the percent of the population living in urban areas (urban),\\n    below the poverty line (poverty) , and in a single person household (single)\\n    are removed by OLS regression.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\\n    ...                              exog_others=['urban', 'poverty', 'single'],\\n    ...                              data=crime_data.data, obs_labels=False)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress.py\\n\\n    More detailed examples can be found in the Regression Plots notebook\\n    on the examples page.\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig",
            "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Plot partial regression for a single regressor.\\n\\n    Parameters\\n    ----------\\n    endog : {ndarray, str}\\n       The endogenous or response variable. If string is given, you can use a\\n       arbitrary translations as with a formula.\\n    exog_i : {ndarray, str}\\n        The exogenous, explanatory variable. If string is given, you can use a\\n        arbitrary translations as with a formula.\\n    exog_others : {ndarray, list[str]}\\n        Any other exogenous, explanatory variables. If a list of strings is\\n        given, each item is a term in formula. You can use a arbitrary\\n        translations as with a formula. The effect of these variables will be\\n        removed by OLS regression.\\n    data : {DataFrame, dict}\\n        Some kind of data structure with names if the other variables are\\n        given as strings.\\n    title_kwargs : dict\\n        Keyword arguments to pass on for the title. The key to control the\\n        fonts is fontdict.\\n    obs_labels : {bool, array_like}\\n        Whether or not to annotate the plot points with their observation\\n        labels. If obs_labels is a boolean, the point labels will try to do\\n        the right thing. First it will try to use the index of data, then\\n        fall back to the index of exog_i. Alternatively, you may give an\\n        array-like object corresponding to the observation numbers.\\n    label_kwargs : dict\\n        Keyword arguments that control annotate for the observation labels.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    ret_coords : bool\\n        If True will return the coordinates of the points in the plot. You\\n        can use this to add your own annotations.\\n    eval_env : int\\n        Patsy eval environment if user functions and formulas are used in\\n        defining endog or exog.\\n    **kwargs\\n        The keyword arguments passed to plot for the points.\\n\\n    Returns\\n    -------\\n    fig : Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n    coords : list, optional\\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\\n\\n    See Also\\n    --------\\n    plot_partregress_grid : Plot partial regression for a set of regressors.\\n\\n    Notes\\n    -----\\n    The slope of the fitted line is the that of `exog_i` in the full\\n    multiple regression. The individual points can be used to assess the\\n    influence of points on the estimated coefficient.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and plot partial regression of the rate\\n    of high school graduation (hs_grad) on the murder rate(murder).\\n\\n    The effects of the percent of the population living in urban areas (urban),\\n    below the poverty line (poverty) , and in a single person household (single)\\n    are removed by OLS regression.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\\n    ...                              exog_others=['urban', 'poverty', 'single'],\\n    ...                              data=crime_data.data, obs_labels=False)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress.py\\n\\n    More detailed examples can be found in the Regression Plots notebook\\n    on the examples page.\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig",
            "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Plot partial regression for a single regressor.\\n\\n    Parameters\\n    ----------\\n    endog : {ndarray, str}\\n       The endogenous or response variable. If string is given, you can use a\\n       arbitrary translations as with a formula.\\n    exog_i : {ndarray, str}\\n        The exogenous, explanatory variable. If string is given, you can use a\\n        arbitrary translations as with a formula.\\n    exog_others : {ndarray, list[str]}\\n        Any other exogenous, explanatory variables. If a list of strings is\\n        given, each item is a term in formula. You can use a arbitrary\\n        translations as with a formula. The effect of these variables will be\\n        removed by OLS regression.\\n    data : {DataFrame, dict}\\n        Some kind of data structure with names if the other variables are\\n        given as strings.\\n    title_kwargs : dict\\n        Keyword arguments to pass on for the title. The key to control the\\n        fonts is fontdict.\\n    obs_labels : {bool, array_like}\\n        Whether or not to annotate the plot points with their observation\\n        labels. If obs_labels is a boolean, the point labels will try to do\\n        the right thing. First it will try to use the index of data, then\\n        fall back to the index of exog_i. Alternatively, you may give an\\n        array-like object corresponding to the observation numbers.\\n    label_kwargs : dict\\n        Keyword arguments that control annotate for the observation labels.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    ret_coords : bool\\n        If True will return the coordinates of the points in the plot. You\\n        can use this to add your own annotations.\\n    eval_env : int\\n        Patsy eval environment if user functions and formulas are used in\\n        defining endog or exog.\\n    **kwargs\\n        The keyword arguments passed to plot for the points.\\n\\n    Returns\\n    -------\\n    fig : Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n    coords : list, optional\\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\\n\\n    See Also\\n    --------\\n    plot_partregress_grid : Plot partial regression for a set of regressors.\\n\\n    Notes\\n    -----\\n    The slope of the fitted line is the that of `exog_i` in the full\\n    multiple regression. The individual points can be used to assess the\\n    influence of points on the estimated coefficient.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and plot partial regression of the rate\\n    of high school graduation (hs_grad) on the murder rate(murder).\\n\\n    The effects of the percent of the population living in urban areas (urban),\\n    below the poverty line (poverty) , and in a single person household (single)\\n    are removed by OLS regression.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\\n    ...                              exog_others=['urban', 'poverty', 'single'],\\n    ...                              data=crime_data.data, obs_labels=False)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress.py\\n\\n    More detailed examples can be found in the Regression Plots notebook\\n    on the examples page.\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig",
            "def plot_partregress(endog, exog_i, exog_others, data=None, title_kwargs={}, obs_labels=True, label_kwargs={}, ax=None, ret_coords=False, eval_env=1, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Plot partial regression for a single regressor.\\n\\n    Parameters\\n    ----------\\n    endog : {ndarray, str}\\n       The endogenous or response variable. If string is given, you can use a\\n       arbitrary translations as with a formula.\\n    exog_i : {ndarray, str}\\n        The exogenous, explanatory variable. If string is given, you can use a\\n        arbitrary translations as with a formula.\\n    exog_others : {ndarray, list[str]}\\n        Any other exogenous, explanatory variables. If a list of strings is\\n        given, each item is a term in formula. You can use a arbitrary\\n        translations as with a formula. The effect of these variables will be\\n        removed by OLS regression.\\n    data : {DataFrame, dict}\\n        Some kind of data structure with names if the other variables are\\n        given as strings.\\n    title_kwargs : dict\\n        Keyword arguments to pass on for the title. The key to control the\\n        fonts is fontdict.\\n    obs_labels : {bool, array_like}\\n        Whether or not to annotate the plot points with their observation\\n        labels. If obs_labels is a boolean, the point labels will try to do\\n        the right thing. First it will try to use the index of data, then\\n        fall back to the index of exog_i. Alternatively, you may give an\\n        array-like object corresponding to the observation numbers.\\n    label_kwargs : dict\\n        Keyword arguments that control annotate for the observation labels.\\n    ax : AxesSubplot, optional\\n        If given, this subplot is used to plot in instead of a new figure being\\n        created.\\n    ret_coords : bool\\n        If True will return the coordinates of the points in the plot. You\\n        can use this to add your own annotations.\\n    eval_env : int\\n        Patsy eval environment if user functions and formulas are used in\\n        defining endog or exog.\\n    **kwargs\\n        The keyword arguments passed to plot for the points.\\n\\n    Returns\\n    -------\\n    fig : Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n    coords : list, optional\\n        If ret_coords is True, return a tuple of arrays (x_coords, y_coords).\\n\\n    See Also\\n    --------\\n    plot_partregress_grid : Plot partial regression for a set of regressors.\\n\\n    Notes\\n    -----\\n    The slope of the fitted line is the that of `exog_i` in the full\\n    multiple regression. The individual points can be used to assess the\\n    influence of points on the estimated coefficient.\\n\\n    Examples\\n    --------\\n    Load the Statewide Crime data set and plot partial regression of the rate\\n    of high school graduation (hs_grad) on the murder rate(murder).\\n\\n    The effects of the percent of the population living in urban areas (urban),\\n    below the poverty line (poverty) , and in a single person household (single)\\n    are removed by OLS regression.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> sm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\\n    ...                              exog_others=['urban', 'poverty', 'single'],\\n    ...                              data=crime_data.data, obs_labels=False)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress.py\\n\\n    More detailed examples can be found in the Regression Plots notebook\\n    on the examples page.\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if isinstance(endog, str):\n        endog = dmatrix(endog + '-1', data, eval_env=eval_env)\n    if isinstance(exog_others, str):\n        RHS = dmatrix(exog_others, data, eval_env=eval_env)\n    elif isinstance(exog_others, list):\n        RHS = '+'.join(exog_others)\n        RHS = dmatrix(RHS, data, eval_env=eval_env)\n    else:\n        RHS = exog_others\n    RHS_isemtpy = False\n    if isinstance(RHS, np.ndarray) and RHS.size == 0:\n        RHS_isemtpy = True\n    elif isinstance(RHS, pd.DataFrame) and RHS.empty:\n        RHS_isemtpy = True\n    if isinstance(exog_i, str):\n        exog_i = dmatrix(exog_i + '-1', data, eval_env=eval_env)\n    if RHS_isemtpy:\n        endog = np.asarray(endog)\n        exog_i = np.asarray(exog_i)\n        ax.plot(endog, exog_i, 'o', **kwargs)\n        fitted_line = OLS(endog, exog_i).fit()\n        x_axis_endog_name = 'x' if isinstance(exog_i, np.ndarray) else exog_i.name\n        y_axis_endog_name = 'y' if isinstance(endog, np.ndarray) else endog.design_info.column_names[0]\n    else:\n        res_yaxis = OLS(endog, RHS).fit()\n        res_xaxis = OLS(exog_i, RHS).fit()\n        xaxis_resid = res_xaxis.resid\n        yaxis_resid = res_yaxis.resid\n        x_axis_endog_name = res_xaxis.model.endog_names\n        y_axis_endog_name = res_yaxis.model.endog_names\n        ax.plot(xaxis_resid, yaxis_resid, 'o', **kwargs)\n        fitted_line = OLS(yaxis_resid, xaxis_resid).fit()\n    fig = abline_plot(0, np.asarray(fitted_line.params)[0], color='k', ax=ax)\n    if x_axis_endog_name == 'y':\n        x_axis_endog_name = 'x'\n    ax.set_xlabel('e(%s | X)' % x_axis_endog_name)\n    ax.set_ylabel('e(%s | X)' % y_axis_endog_name)\n    ax.set_title('Partial Regression Plot', **title_kwargs)\n    if obs_labels is True:\n        if data is not None:\n            obs_labels = data.index\n        elif hasattr(exog_i, 'index'):\n            obs_labels = exog_i.index\n        else:\n            obs_labels = res_xaxis.model.data.row_labels\n        if obs_labels is None:\n            obs_labels = lrange(len(exog_i))\n    if obs_labels is not False:\n        if len(obs_labels) != len(exog_i):\n            raise ValueError('obs_labels does not match length of exog_i')\n        label_kwargs.update(dict(ha='center', va='bottom'))\n        ax = utils.annotate_axes(lrange(len(obs_labels)), obs_labels, lzip(res_xaxis.resid, res_yaxis.resid), [(0, 5)] * len(obs_labels), 'x-large', ax=ax, **label_kwargs)\n    if ret_coords:\n        return (fig, (res_xaxis.resid, res_yaxis.resid))\n    else:\n        return fig"
        ]
    },
    {
        "func_name": "plot_partregress_grid",
        "original": "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    \"\"\"\n    Plot partial regression for a set of regressors.\n\n    Parameters\n    ----------\n    results : Results instance\n        A regression model results instance.\n    exog_idx : {None, list[int], list[str]}\n        The indices  or column names of the exog used in the plot, default is\n        all.\n    grid : {None, tuple[int]}\n        If grid is given, then it is used for the arrangement of the subplots.\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\n        one, if there are only 2 subplots, and the number of columns is two\n        otherwise.\n    fig : Figure, optional\n        If given, this figure is simply returned.  Otherwise a new figure is\n        created.\n\n    Returns\n    -------\n    Figure\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\n\n    See Also\n    --------\n    plot_partregress : Plot partial regression for a single regressor.\n    plot_ccpr : Plot CCPR against one regressor\n\n    Notes\n    -----\n    A subplot is created for each explanatory variable given by exog_idx.\n    The partial regression plot shows the relationship between the response\n    and the given explanatory variable after removing the effect of all other\n    explanatory variables in exog.\n\n    References\n    ----------\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\n\n    Examples\n    --------\n    Using the state crime dataset separately plot the effect of the each\n    variable on the on the outcome, murder rate while accounting for the effect\n    of all other variables in the model visualized with a grid of partial\n    regression plots.\n\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.formula.api as smf\n\n    >>> fig = plt.figure(figsize=(8, 6))\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\n    ...                   data=crime_data.data).fit()\n    >>> plot_partregress_grid(results, fig=fig)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_partregress_grid.py\n    \"\"\"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
        "mutated": [
            "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n    \"\\n    Plot partial regression for a set of regressors.\\n\\n    Parameters\\n    ----------\\n    results : Results instance\\n        A regression model results instance.\\n    exog_idx : {None, list[int], list[str]}\\n        The indices  or column names of the exog used in the plot, default is\\n        all.\\n    grid : {None, tuple[int]}\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\\n        one, if there are only 2 subplots, and the number of columns is two\\n        otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\\n\\n    See Also\\n    --------\\n    plot_partregress : Plot partial regression for a single regressor.\\n    plot_ccpr : Plot CCPR against one regressor\\n\\n    Notes\\n    -----\\n    A subplot is created for each explanatory variable given by exog_idx.\\n    The partial regression plot shows the relationship between the response\\n    and the given explanatory variable after removing the effect of all other\\n    explanatory variables in exog.\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model visualized with a grid of partial\\n    regression plots.\\n\\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> plot_partregress_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress_grid.py\\n    \"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Plot partial regression for a set of regressors.\\n\\n    Parameters\\n    ----------\\n    results : Results instance\\n        A regression model results instance.\\n    exog_idx : {None, list[int], list[str]}\\n        The indices  or column names of the exog used in the plot, default is\\n        all.\\n    grid : {None, tuple[int]}\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\\n        one, if there are only 2 subplots, and the number of columns is two\\n        otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\\n\\n    See Also\\n    --------\\n    plot_partregress : Plot partial regression for a single regressor.\\n    plot_ccpr : Plot CCPR against one regressor\\n\\n    Notes\\n    -----\\n    A subplot is created for each explanatory variable given by exog_idx.\\n    The partial regression plot shows the relationship between the response\\n    and the given explanatory variable after removing the effect of all other\\n    explanatory variables in exog.\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model visualized with a grid of partial\\n    regression plots.\\n\\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> plot_partregress_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress_grid.py\\n    \"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Plot partial regression for a set of regressors.\\n\\n    Parameters\\n    ----------\\n    results : Results instance\\n        A regression model results instance.\\n    exog_idx : {None, list[int], list[str]}\\n        The indices  or column names of the exog used in the plot, default is\\n        all.\\n    grid : {None, tuple[int]}\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\\n        one, if there are only 2 subplots, and the number of columns is two\\n        otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\\n\\n    See Also\\n    --------\\n    plot_partregress : Plot partial regression for a single regressor.\\n    plot_ccpr : Plot CCPR against one regressor\\n\\n    Notes\\n    -----\\n    A subplot is created for each explanatory variable given by exog_idx.\\n    The partial regression plot shows the relationship between the response\\n    and the given explanatory variable after removing the effect of all other\\n    explanatory variables in exog.\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model visualized with a grid of partial\\n    regression plots.\\n\\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> plot_partregress_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress_grid.py\\n    \"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Plot partial regression for a set of regressors.\\n\\n    Parameters\\n    ----------\\n    results : Results instance\\n        A regression model results instance.\\n    exog_idx : {None, list[int], list[str]}\\n        The indices  or column names of the exog used in the plot, default is\\n        all.\\n    grid : {None, tuple[int]}\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\\n        one, if there are only 2 subplots, and the number of columns is two\\n        otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\\n\\n    See Also\\n    --------\\n    plot_partregress : Plot partial regression for a single regressor.\\n    plot_ccpr : Plot CCPR against one regressor\\n\\n    Notes\\n    -----\\n    A subplot is created for each explanatory variable given by exog_idx.\\n    The partial regression plot shows the relationship between the response\\n    and the given explanatory variable after removing the effect of all other\\n    explanatory variables in exog.\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model visualized with a grid of partial\\n    regression plots.\\n\\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> plot_partregress_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress_grid.py\\n    \"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_partregress_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Plot partial regression for a set of regressors.\\n\\n    Parameters\\n    ----------\\n    results : Results instance\\n        A regression model results instance.\\n    exog_idx : {None, list[int], list[str]}\\n        The indices  or column names of the exog used in the plot, default is\\n        all.\\n    grid : {None, tuple[int]}\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        The format of grid is  (nrows, ncols). If grid is None, then ncol is\\n        one, if there are only 2 subplots, and the number of columns is two\\n        otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `fig` is None, the created figure.  Otherwise `fig` itself.\\n\\n    See Also\\n    --------\\n    plot_partregress : Plot partial regression for a single regressor.\\n    plot_ccpr : Plot CCPR against one regressor\\n\\n    Notes\\n    -----\\n    A subplot is created for each explanatory variable given by exog_idx.\\n    The partial regression plot shows the relationship between the response\\n    and the given explanatory variable after removing the effect of all other\\n    explanatory variables in exog.\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/partregr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model visualized with a grid of partial\\n    regression plots.\\n\\n    >>> from statsmodels.graphics.regressionplots import plot_partregress_grid\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 6))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> plot_partregress_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_partregress_grid.py\\n    \"\n    import pandas\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    y = pandas.Series(results.model.endog, name=results.model.endog_names)\n    exog = results.model.exog\n    k_vars = exog.shape[1]\n    nrows = (len(exog_idx) + 1) // 2\n    ncols = 1 if nrows == len(exog_idx) else 2\n    if grid is not None:\n        (nrows, ncols) = grid\n    if ncols > 1:\n        title_kwargs = {'fontdict': {'fontsize': 'small'}}\n    other_names = np.array(results.model.exog_names)\n    for (i, idx) in enumerate(exog_idx):\n        others = lrange(k_vars)\n        others.pop(idx)\n        exog_others = pandas.DataFrame(exog[:, others], columns=other_names[others])\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        plot_partregress(y, pandas.Series(exog[:, idx], name=other_names[idx]), exog_others, ax=ax, title_kwargs=title_kwargs, obs_labels=False)\n        ax.set_title('')\n    fig.suptitle('Partial Regression Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig"
        ]
    },
    {
        "func_name": "plot_ccpr",
        "original": "def plot_ccpr(results, exog_idx, ax=None):\n    \"\"\"\n    Plot CCPR against one regressor.\n\n    Generates a component and component-plus-residual (CCPR) plot.\n\n    Parameters\n    ----------\n    results : result instance\n        A regression results instance.\n    exog_idx : {int, str}\n        Exogenous, explanatory variable. If string is given, it should\n        be the variable name that you want to use, and you can use arbitrary\n        translations as with a formula.\n    ax : AxesSubplot, optional\n        If given, it is used to plot in instead of a new figure being\n        created.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    See Also\n    --------\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\n\n    Notes\n    -----\n    The CCPR plot provides a way to judge the effect of one regressor on the\n    response variable by taking into account the effects of the other\n    independent variables. The partial residuals plot is defined as\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\n    X_i to show where the fitted line would lie. Care should be taken if X_i\n    is highly correlated with any of the other independent variables. If this\n    is the case, the variance evident in the plot will be an underestimate of\n    the true variance.\n\n    References\n    ----------\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\n\n    Examples\n    --------\n    Using the state crime dataset plot the effect of the rate of single\n    households ('single') on the murder rate while accounting for high school\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\n    of poverty ('poverty').\n\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.formula.api as smf\n\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\n    ...                   data=crime_data.data).fit()\n    >>> sm.graphics.plot_ccpr(results, 'single')\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_ccpr.py\n    \"\"\"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig",
        "mutated": [
            "def plot_ccpr(results, exog_idx, ax=None):\n    if False:\n        i = 10\n    \"\\n    Plot CCPR against one regressor.\\n\\n    Generates a component and component-plus-residual (CCPR) plot.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A regression results instance.\\n    exog_idx : {int, str}\\n        Exogenous, explanatory variable. If string is given, it should\\n        be the variable name that you want to use, and you can use arbitrary\\n        translations as with a formula.\\n    ax : AxesSubplot, optional\\n        If given, it is used to plot in instead of a new figure being\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\\n\\n    Notes\\n    -----\\n    The CCPR plot provides a way to judge the effect of one regressor on the\\n    response variable by taking into account the effects of the other\\n    independent variables. The partial residuals plot is defined as\\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\\n    X_i to show where the fitted line would lie. Care should be taken if X_i\\n    is highly correlated with any of the other independent variables. If this\\n    is the case, the variance evident in the plot will be an underestimate of\\n    the true variance.\\n\\n    References\\n    ----------\\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset plot the effect of the rate of single\\n    households ('single') on the murder rate while accounting for high school\\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\\n    of poverty ('poverty').\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr(results, 'single')\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr.py\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig",
            "def plot_ccpr(results, exog_idx, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Plot CCPR against one regressor.\\n\\n    Generates a component and component-plus-residual (CCPR) plot.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A regression results instance.\\n    exog_idx : {int, str}\\n        Exogenous, explanatory variable. If string is given, it should\\n        be the variable name that you want to use, and you can use arbitrary\\n        translations as with a formula.\\n    ax : AxesSubplot, optional\\n        If given, it is used to plot in instead of a new figure being\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\\n\\n    Notes\\n    -----\\n    The CCPR plot provides a way to judge the effect of one regressor on the\\n    response variable by taking into account the effects of the other\\n    independent variables. The partial residuals plot is defined as\\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\\n    X_i to show where the fitted line would lie. Care should be taken if X_i\\n    is highly correlated with any of the other independent variables. If this\\n    is the case, the variance evident in the plot will be an underestimate of\\n    the true variance.\\n\\n    References\\n    ----------\\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset plot the effect of the rate of single\\n    households ('single') on the murder rate while accounting for high school\\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\\n    of poverty ('poverty').\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr(results, 'single')\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr.py\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig",
            "def plot_ccpr(results, exog_idx, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Plot CCPR against one regressor.\\n\\n    Generates a component and component-plus-residual (CCPR) plot.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A regression results instance.\\n    exog_idx : {int, str}\\n        Exogenous, explanatory variable. If string is given, it should\\n        be the variable name that you want to use, and you can use arbitrary\\n        translations as with a formula.\\n    ax : AxesSubplot, optional\\n        If given, it is used to plot in instead of a new figure being\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\\n\\n    Notes\\n    -----\\n    The CCPR plot provides a way to judge the effect of one regressor on the\\n    response variable by taking into account the effects of the other\\n    independent variables. The partial residuals plot is defined as\\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\\n    X_i to show where the fitted line would lie. Care should be taken if X_i\\n    is highly correlated with any of the other independent variables. If this\\n    is the case, the variance evident in the plot will be an underestimate of\\n    the true variance.\\n\\n    References\\n    ----------\\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset plot the effect of the rate of single\\n    households ('single') on the murder rate while accounting for high school\\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\\n    of poverty ('poverty').\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr(results, 'single')\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr.py\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig",
            "def plot_ccpr(results, exog_idx, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Plot CCPR against one regressor.\\n\\n    Generates a component and component-plus-residual (CCPR) plot.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A regression results instance.\\n    exog_idx : {int, str}\\n        Exogenous, explanatory variable. If string is given, it should\\n        be the variable name that you want to use, and you can use arbitrary\\n        translations as with a formula.\\n    ax : AxesSubplot, optional\\n        If given, it is used to plot in instead of a new figure being\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\\n\\n    Notes\\n    -----\\n    The CCPR plot provides a way to judge the effect of one regressor on the\\n    response variable by taking into account the effects of the other\\n    independent variables. The partial residuals plot is defined as\\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\\n    X_i to show where the fitted line would lie. Care should be taken if X_i\\n    is highly correlated with any of the other independent variables. If this\\n    is the case, the variance evident in the plot will be an underestimate of\\n    the true variance.\\n\\n    References\\n    ----------\\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset plot the effect of the rate of single\\n    households ('single') on the murder rate while accounting for high school\\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\\n    of poverty ('poverty').\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr(results, 'single')\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr.py\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig",
            "def plot_ccpr(results, exog_idx, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Plot CCPR against one regressor.\\n\\n    Generates a component and component-plus-residual (CCPR) plot.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A regression results instance.\\n    exog_idx : {int, str}\\n        Exogenous, explanatory variable. If string is given, it should\\n        be the variable name that you want to use, and you can use arbitrary\\n        translations as with a formula.\\n    ax : AxesSubplot, optional\\n        If given, it is used to plot in instead of a new figure being\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr_grid : Creates CCPR plot for multiple regressors in a plot grid.\\n\\n    Notes\\n    -----\\n    The CCPR plot provides a way to judge the effect of one regressor on the\\n    response variable by taking into account the effects of the other\\n    independent variables. The partial residuals plot is defined as\\n    Residuals + B_i*X_i versus X_i. The component adds the B_i*X_i versus\\n    X_i to show where the fitted line would lie. Care should be taken if X_i\\n    is highly correlated with any of the other independent variables. If this\\n    is the case, the variance evident in the plot will be an underestimate of\\n    the true variance.\\n\\n    References\\n    ----------\\n    http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset plot the effect of the rate of single\\n    households ('single') on the murder rate while accounting for high school\\n    graduation rate ('hs_grad'), percentage of people in an urban area, and rate\\n    of poverty ('poverty').\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr(results, 'single')\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr.py\\n    \"\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    results = maybe_unwrap_results(results)\n    x1 = results.model.exog[:, exog_idx]\n    x1beta = x1 * results.params[exog_idx]\n    ax.plot(x1, x1beta + results.resid, 'o')\n    from statsmodels.tools.tools import add_constant\n    mod = OLS(x1beta, add_constant(x1)).fit()\n    params = mod.params\n    fig = abline_plot(*params, **dict(ax=ax))\n    ax.set_title('Component and component plus residual plot')\n    ax.set_ylabel('Residual + %s*beta_%d' % (exog_name, exog_idx))\n    ax.set_xlabel('%s' % exog_name)\n    return fig"
        ]
    },
    {
        "func_name": "plot_ccpr_grid",
        "original": "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    \"\"\"\n    Generate CCPR plots against a set of regressors, plot in a grid.\n\n    Generates a grid of component and component-plus-residual (CCPR) plots.\n\n    Parameters\n    ----------\n    results : result instance\n        A results instance with exog and params.\n    exog_idx : None or list of int\n        The indices or column names of the exog used in the plot.\n    grid : None or tuple of int (nrows, ncols)\n        If grid is given, then it is used for the arrangement of the subplots.\n        If grid is None, then ncol is one, if there are only 2 subplots, and\n        the number of columns is two otherwise.\n    fig : Figure, optional\n        If given, this figure is simply returned.  Otherwise a new figure is\n        created.\n\n    Returns\n    -------\n    Figure\n        If `ax` is None, the created figure.  Otherwise the figure to which\n        `ax` is connected.\n\n    See Also\n    --------\n    plot_ccpr : Creates CCPR plot for a single regressor.\n\n    Notes\n    -----\n    Partial residual plots are formed as::\n\n        Res + Betahat(i)*Xi versus Xi\n\n    and CCPR adds::\n\n        Betahat(i)*Xi versus Xi\n\n    References\n    ----------\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\n\n    Examples\n    --------\n    Using the state crime dataset separately plot the effect of the each\n    variable on the on the outcome, murder rate while accounting for the effect\n    of all other variables in the model.\n\n    >>> import statsmodels.api as sm\n    >>> import matplotlib.pyplot as plt\n    >>> import statsmodels.formula.api as smf\n\n    >>> fig = plt.figure(figsize=(8, 8))\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\n    ...                   data=crime_data.data).fit()\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_ccpr_grid.py\n    \"\"\"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
        "mutated": [
            "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n    \"\\n    Generate CCPR plots against a set of regressors, plot in a grid.\\n\\n    Generates a grid of component and component-plus-residual (CCPR) plots.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A results instance with exog and params.\\n    exog_idx : None or list of int\\n        The indices or column names of the exog used in the plot.\\n    grid : None or tuple of int (nrows, ncols)\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        If grid is None, then ncol is one, if there are only 2 subplots, and\\n        the number of columns is two otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr : Creates CCPR plot for a single regressor.\\n\\n    Notes\\n    -----\\n    Partial residual plots are formed as::\\n\\n        Res + Betahat(i)*Xi versus Xi\\n\\n    and CCPR adds::\\n\\n        Betahat(i)*Xi versus Xi\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 8))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr_grid.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Generate CCPR plots against a set of regressors, plot in a grid.\\n\\n    Generates a grid of component and component-plus-residual (CCPR) plots.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A results instance with exog and params.\\n    exog_idx : None or list of int\\n        The indices or column names of the exog used in the plot.\\n    grid : None or tuple of int (nrows, ncols)\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        If grid is None, then ncol is one, if there are only 2 subplots, and\\n        the number of columns is two otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr : Creates CCPR plot for a single regressor.\\n\\n    Notes\\n    -----\\n    Partial residual plots are formed as::\\n\\n        Res + Betahat(i)*Xi versus Xi\\n\\n    and CCPR adds::\\n\\n        Betahat(i)*Xi versus Xi\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 8))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr_grid.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Generate CCPR plots against a set of regressors, plot in a grid.\\n\\n    Generates a grid of component and component-plus-residual (CCPR) plots.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A results instance with exog and params.\\n    exog_idx : None or list of int\\n        The indices or column names of the exog used in the plot.\\n    grid : None or tuple of int (nrows, ncols)\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        If grid is None, then ncol is one, if there are only 2 subplots, and\\n        the number of columns is two otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr : Creates CCPR plot for a single regressor.\\n\\n    Notes\\n    -----\\n    Partial residual plots are formed as::\\n\\n        Res + Betahat(i)*Xi versus Xi\\n\\n    and CCPR adds::\\n\\n        Betahat(i)*Xi versus Xi\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 8))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr_grid.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Generate CCPR plots against a set of regressors, plot in a grid.\\n\\n    Generates a grid of component and component-plus-residual (CCPR) plots.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A results instance with exog and params.\\n    exog_idx : None or list of int\\n        The indices or column names of the exog used in the plot.\\n    grid : None or tuple of int (nrows, ncols)\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        If grid is None, then ncol is one, if there are only 2 subplots, and\\n        the number of columns is two otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr : Creates CCPR plot for a single regressor.\\n\\n    Notes\\n    -----\\n    Partial residual plots are formed as::\\n\\n        Res + Betahat(i)*Xi versus Xi\\n\\n    and CCPR adds::\\n\\n        Betahat(i)*Xi versus Xi\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 8))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr_grid.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig",
            "def plot_ccpr_grid(results, exog_idx=None, grid=None, fig=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Generate CCPR plots against a set of regressors, plot in a grid.\\n\\n    Generates a grid of component and component-plus-residual (CCPR) plots.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n        A results instance with exog and params.\\n    exog_idx : None or list of int\\n        The indices or column names of the exog used in the plot.\\n    grid : None or tuple of int (nrows, ncols)\\n        If grid is given, then it is used for the arrangement of the subplots.\\n        If grid is None, then ncol is one, if there are only 2 subplots, and\\n        the number of columns is two otherwise.\\n    fig : Figure, optional\\n        If given, this figure is simply returned.  Otherwise a new figure is\\n        created.\\n\\n    Returns\\n    -------\\n    Figure\\n        If `ax` is None, the created figure.  Otherwise the figure to which\\n        `ax` is connected.\\n\\n    See Also\\n    --------\\n    plot_ccpr : Creates CCPR plot for a single regressor.\\n\\n    Notes\\n    -----\\n    Partial residual plots are formed as::\\n\\n        Res + Betahat(i)*Xi versus Xi\\n\\n    and CCPR adds::\\n\\n        Betahat(i)*Xi versus Xi\\n\\n    References\\n    ----------\\n    See http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ccpr.htm\\n\\n    Examples\\n    --------\\n    Using the state crime dataset separately plot the effect of the each\\n    variable on the on the outcome, murder rate while accounting for the effect\\n    of all other variables in the model.\\n\\n    >>> import statsmodels.api as sm\\n    >>> import matplotlib.pyplot as plt\\n    >>> import statsmodels.formula.api as smf\\n\\n    >>> fig = plt.figure(figsize=(8, 8))\\n    >>> crime_data = sm.datasets.statecrime.load_pandas()\\n    >>> results = smf.ols('murder ~ hs_grad + urban + poverty + single',\\n    ...                   data=crime_data.data).fit()\\n    >>> sm.graphics.plot_ccpr_grid(results, fig=fig)\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_ccpr_grid.py\\n    \"\n    fig = utils.create_mpl_fig(fig)\n    (exog_name, exog_idx) = utils.maybe_name_or_idx(exog_idx, results.model)\n    if grid is not None:\n        (nrows, ncols) = grid\n    elif len(exog_idx) > 2:\n        nrows = int(np.ceil(len(exog_idx) / 2.0))\n        ncols = 2\n    else:\n        nrows = len(exog_idx)\n        ncols = 1\n    seen_constant = 0\n    for (i, idx) in enumerate(exog_idx):\n        if results.model.exog[:, idx].var() == 0:\n            seen_constant = 1\n            continue\n        ax = fig.add_subplot(nrows, ncols, i + 1 - seen_constant)\n        fig = plot_ccpr(results, exog_idx=idx, ax=ax)\n        ax.set_title('')\n    fig.suptitle('Component-Component Plus Residual Plot', fontsize='large')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.95)\n    return fig"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, *args, **kwargs):\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None",
        "mutated": [
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(ABLine2D, self).__init__(*args, **kwargs)\n    self.id_xlim_callback = None\n    self.id_ylim_callback = None"
        ]
    },
    {
        "func_name": "remove",
        "original": "def remove(self):\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()",
        "mutated": [
            "def remove(self):\n    if False:\n        i = 10\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()",
            "def remove(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()",
            "def remove(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()",
            "def remove(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()",
            "def remove(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ax = self.axes\n    if self.id_xlim_callback:\n        ax.callbacks.disconnect(self.id_xlim_callback)\n    if self.id_ylim_callback:\n        ax.callbacks.disconnect(self.id_ylim_callback)\n    super(ABLine2D, self).remove()"
        ]
    },
    {
        "func_name": "update_datalim",
        "original": "def update_datalim(self, ax):\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()",
        "mutated": [
            "def update_datalim(self, ax):\n    if False:\n        i = 10\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()",
            "def update_datalim(self, ax):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()",
            "def update_datalim(self, ax):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()",
            "def update_datalim(self, ax):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()",
            "def update_datalim(self, ax):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ax.set_autoscale_on(False)\n    children = ax.get_children()\n    ablines = [child for child in children if child is self]\n    abline = ablines[0]\n    x = ax.get_xlim()\n    y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    abline.set_data(x, y)\n    ax.figure.canvas.draw()"
        ]
    },
    {
        "func_name": "abline_plot",
        "original": "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    \"\"\"\n    Plot a line given an intercept and slope.\n\n    Parameters\n    ----------\n    intercept : float\n        The intercept of the line.\n    slope : float\n        The slope of the line.\n    horiz : float or array_like\n        Data for horizontal lines on the y-axis.\n    vert : array_like\n        Data for verterical lines on the x-axis.\n    model_results : statsmodels results instance\n        Any object that has a two-value `params` attribute. Assumed that it\n        is (intercept, slope).\n    ax : axes, optional\n        Matplotlib axes instance.\n    **kwargs\n        Options passed to matplotlib.pyplot.plt.\n\n    Returns\n    -------\n    Figure\n        The figure given by `ax.figure` or a new instance.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import statsmodels.api as sm\n\n    >>> np.random.seed(12345)\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\n    >>> mod = sm.OLS(y,X).fit()\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\n    >>> ax = fig.axes[0]\n    >>> ax.scatter(X[:,1], y)\n    >>> ax.margins(.1)\n    >>> import matplotlib.pyplot as plt\n    >>> plt.show()\n\n    .. plot:: plots/graphics_regression_abline.py\n    \"\"\"\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig",
        "mutated": [
            "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    if False:\n        i = 10\n    '\\n    Plot a line given an intercept and slope.\\n\\n    Parameters\\n    ----------\\n    intercept : float\\n        The intercept of the line.\\n    slope : float\\n        The slope of the line.\\n    horiz : float or array_like\\n        Data for horizontal lines on the y-axis.\\n    vert : array_like\\n        Data for verterical lines on the x-axis.\\n    model_results : statsmodels results instance\\n        Any object that has a two-value `params` attribute. Assumed that it\\n        is (intercept, slope).\\n    ax : axes, optional\\n        Matplotlib axes instance.\\n    **kwargs\\n        Options passed to matplotlib.pyplot.plt.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure given by `ax.figure` or a new instance.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n\\n    >>> np.random.seed(12345)\\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\\n    >>> mod = sm.OLS(y,X).fit()\\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\\n    >>> ax = fig.axes[0]\\n    >>> ax.scatter(X[:,1], y)\\n    >>> ax.margins(.1)\\n    >>> import matplotlib.pyplot as plt\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_abline.py\\n    '\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig",
            "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Plot a line given an intercept and slope.\\n\\n    Parameters\\n    ----------\\n    intercept : float\\n        The intercept of the line.\\n    slope : float\\n        The slope of the line.\\n    horiz : float or array_like\\n        Data for horizontal lines on the y-axis.\\n    vert : array_like\\n        Data for verterical lines on the x-axis.\\n    model_results : statsmodels results instance\\n        Any object that has a two-value `params` attribute. Assumed that it\\n        is (intercept, slope).\\n    ax : axes, optional\\n        Matplotlib axes instance.\\n    **kwargs\\n        Options passed to matplotlib.pyplot.plt.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure given by `ax.figure` or a new instance.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n\\n    >>> np.random.seed(12345)\\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\\n    >>> mod = sm.OLS(y,X).fit()\\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\\n    >>> ax = fig.axes[0]\\n    >>> ax.scatter(X[:,1], y)\\n    >>> ax.margins(.1)\\n    >>> import matplotlib.pyplot as plt\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_abline.py\\n    '\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig",
            "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Plot a line given an intercept and slope.\\n\\n    Parameters\\n    ----------\\n    intercept : float\\n        The intercept of the line.\\n    slope : float\\n        The slope of the line.\\n    horiz : float or array_like\\n        Data for horizontal lines on the y-axis.\\n    vert : array_like\\n        Data for verterical lines on the x-axis.\\n    model_results : statsmodels results instance\\n        Any object that has a two-value `params` attribute. Assumed that it\\n        is (intercept, slope).\\n    ax : axes, optional\\n        Matplotlib axes instance.\\n    **kwargs\\n        Options passed to matplotlib.pyplot.plt.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure given by `ax.figure` or a new instance.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n\\n    >>> np.random.seed(12345)\\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\\n    >>> mod = sm.OLS(y,X).fit()\\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\\n    >>> ax = fig.axes[0]\\n    >>> ax.scatter(X[:,1], y)\\n    >>> ax.margins(.1)\\n    >>> import matplotlib.pyplot as plt\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_abline.py\\n    '\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig",
            "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Plot a line given an intercept and slope.\\n\\n    Parameters\\n    ----------\\n    intercept : float\\n        The intercept of the line.\\n    slope : float\\n        The slope of the line.\\n    horiz : float or array_like\\n        Data for horizontal lines on the y-axis.\\n    vert : array_like\\n        Data for verterical lines on the x-axis.\\n    model_results : statsmodels results instance\\n        Any object that has a two-value `params` attribute. Assumed that it\\n        is (intercept, slope).\\n    ax : axes, optional\\n        Matplotlib axes instance.\\n    **kwargs\\n        Options passed to matplotlib.pyplot.plt.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure given by `ax.figure` or a new instance.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n\\n    >>> np.random.seed(12345)\\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\\n    >>> mod = sm.OLS(y,X).fit()\\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\\n    >>> ax = fig.axes[0]\\n    >>> ax.scatter(X[:,1], y)\\n    >>> ax.margins(.1)\\n    >>> import matplotlib.pyplot as plt\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_abline.py\\n    '\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig",
            "def abline_plot(intercept=None, slope=None, horiz=None, vert=None, model_results=None, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Plot a line given an intercept and slope.\\n\\n    Parameters\\n    ----------\\n    intercept : float\\n        The intercept of the line.\\n    slope : float\\n        The slope of the line.\\n    horiz : float or array_like\\n        Data for horizontal lines on the y-axis.\\n    vert : array_like\\n        Data for verterical lines on the x-axis.\\n    model_results : statsmodels results instance\\n        Any object that has a two-value `params` attribute. Assumed that it\\n        is (intercept, slope).\\n    ax : axes, optional\\n        Matplotlib axes instance.\\n    **kwargs\\n        Options passed to matplotlib.pyplot.plt.\\n\\n    Returns\\n    -------\\n    Figure\\n        The figure given by `ax.figure` or a new instance.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> import statsmodels.api as sm\\n\\n    >>> np.random.seed(12345)\\n    >>> X = sm.add_constant(np.random.normal(0, 20, size=30))\\n    >>> y = np.dot(X, [25, 3.5]) + np.random.normal(0, 30, size=30)\\n    >>> mod = sm.OLS(y,X).fit()\\n    >>> fig = sm.graphics.abline_plot(model_results=mod)\\n    >>> ax = fig.axes[0]\\n    >>> ax.scatter(X[:,1], y)\\n    >>> ax.margins(.1)\\n    >>> import matplotlib.pyplot as plt\\n    >>> plt.show()\\n\\n    .. plot:: plots/graphics_regression_abline.py\\n    '\n    if ax is not None:\n        x = ax.get_xlim()\n    else:\n        x = None\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if model_results:\n        (intercept, slope) = model_results.params\n        if x is None:\n            x = [model_results.model.exog[:, 1].min(), model_results.model.exog[:, 1].max()]\n    else:\n        if not (intercept is not None and slope is not None):\n            raise ValueError('specify slope and intercepty or model_results')\n        if x is None:\n            x = ax.get_xlim()\n    data_y = [x[0] * slope + intercept, x[1] * slope + intercept]\n    ax.set_xlim(x)\n    from matplotlib.lines import Line2D\n\n    class ABLine2D(Line2D):\n\n        def __init__(self, *args, **kwargs):\n            super(ABLine2D, self).__init__(*args, **kwargs)\n            self.id_xlim_callback = None\n            self.id_ylim_callback = None\n\n        def remove(self):\n            ax = self.axes\n            if self.id_xlim_callback:\n                ax.callbacks.disconnect(self.id_xlim_callback)\n            if self.id_ylim_callback:\n                ax.callbacks.disconnect(self.id_ylim_callback)\n            super(ABLine2D, self).remove()\n\n        def update_datalim(self, ax):\n            ax.set_autoscale_on(False)\n            children = ax.get_children()\n            ablines = [child for child in children if child is self]\n            abline = ablines[0]\n            x = ax.get_xlim()\n            y = [x[0] * slope + intercept, x[1] * slope + intercept]\n            abline.set_data(x, y)\n            ax.figure.canvas.draw()\n    line = ABLine2D(x, data_y, **kwargs)\n    ax.add_line(line)\n    line.id_xlim_callback = ax.callbacks.connect('xlim_changed', line.update_datalim)\n    line.id_ylim_callback = ax.callbacks.connect('ylim_changed', line.update_datalim)\n    if horiz:\n        ax.hline(horiz)\n    if vert:\n        ax.vline(vert)\n    return fig"
        ]
    },
    {
        "func_name": "_influence_plot",
        "original": "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig",
        "mutated": [
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    if False:\n        i = 10\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results: object\\n        Results for a fitted regression model.\\n    influence: instance\\n        The instance of Influence for model.'}))\ndef _influence_plot(results, influence, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, leverage=None, resid=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    infl = influence\n    (fig, ax) = utils.create_mpl_ax(ax)\n    if criterion.lower().startswith('coo'):\n        psize = infl.cooks_distance[0]\n    elif criterion.lower().startswith('dff'):\n        psize = np.abs(infl.dffits[0])\n    else:\n        raise ValueError('Criterion %s not understood' % criterion)\n    old_range = np.ptp(psize)\n    new_range = size ** 2 - 8 ** 2\n    psize = (psize - psize.min()) * new_range / old_range + 8 ** 2\n    if leverage is None:\n        leverage = infl.hat_matrix_diag\n    if resid is None:\n        ylabel = 'Studentized Residuals'\n        if external:\n            resid = infl.resid_studentized_external\n        else:\n            resid = infl.resid_studentized\n    else:\n        resid = np.asarray(resid)\n        ylabel = 'Residuals'\n    from scipy import stats\n    cutoff = stats.t.ppf(1.0 - alpha / 2, results.df_resid)\n    large_resid = np.abs(resid) > cutoff\n    large_leverage = leverage > _high_leverage(results)\n    large_points = np.logical_or(large_resid, large_leverage)\n    ax.scatter(leverage, resid, s=psize, alpha=plot_alpha)\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(len(resid))\n    ax = utils.annotate_axes(np.where(large_points)[0], labels, lzip(leverage, resid), lzip(-(psize / 2) ** 0.5, (psize / 2) ** 0.5), 'x-large', ax)\n    font = {'fontsize': 16, 'color': 'black'}\n    ax.set_ylabel(ylabel, **font)\n    ax.set_xlabel('Leverage', **font)\n    ax.set_title('Influence Plot', **font)\n    return fig"
        ]
    },
    {
        "func_name": "influence_plot",
        "original": "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
        "mutated": [
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res",
            "@Appender(_plot_influence_doc.format(**{'extra_params_doc': 'results : Results\\n        Results for a fitted regression model.'}))\ndef influence_plot(results, external=True, alpha=0.05, criterion='cooks', size=48, plot_alpha=0.75, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    infl = results.get_influence()\n    res = _influence_plot(results, infl, external=external, alpha=alpha, criterion=criterion, size=size, plot_alpha=plot_alpha, ax=ax, **kwargs)\n    return res"
        ]
    },
    {
        "func_name": "_plot_leverage_resid2",
        "original": "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig",
        "mutated": [
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results: object\\n    Results for a fitted regression model\\ninfluence: instance\\n    instance of Influence for model'}))\ndef _plot_leverage_resid2(results, influence, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from scipy.stats import norm, zscore\n    (fig, ax) = utils.create_mpl_ax(ax)\n    infl = influence\n    leverage = infl.hat_matrix_diag\n    resid = zscore(infl.resid)\n    ax.plot(resid ** 2, leverage, 'o', **kwargs)\n    ax.set_xlabel('Normalized residuals**2')\n    ax.set_ylabel('Leverage')\n    ax.set_title('Leverage vs. Normalized residuals squared')\n    large_leverage = leverage > _high_leverage(results)\n    cutoff = norm.ppf(1.0 - alpha / 2)\n    large_resid = np.abs(resid) > cutoff\n    labels = results.model.data.row_labels\n    if labels is None:\n        labels = lrange(int(results.nobs))\n    index = np.where(np.logical_or(large_leverage, large_resid))[0]\n    ax = utils.annotate_axes(index, labels, lzip(resid ** 2, leverage), [(0, 5)] * int(results.nobs), 'large', ax=ax, ha='center', va='bottom')\n    ax.margins(0.075, 0.075)\n    return fig"
        ]
    },
    {
        "func_name": "plot_leverage_resid2",
        "original": "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)",
        "mutated": [
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)",
            "@Appender(_plot_leverage_resid2_doc.format({'extra_params_doc': 'results : object\\n    Results for a fitted regression model'}))\ndef plot_leverage_resid2(results, alpha=0.05, ax=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    infl = results.get_influence()\n    return _plot_leverage_resid2(results, infl, alpha=alpha, ax=ax, **kwargs)"
        ]
    },
    {
        "func_name": "plot_added_variable",
        "original": "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig",
        "mutated": [
            "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    if False:\n        i = 10\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig",
            "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig",
            "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig",
            "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig",
            "@Appender(_plot_added_variable_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_added_variable(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model = results.model\n    (fig, ax) = utils.create_mpl_ax(ax)\n    (endog_resid, focus_exog_resid) = added_variable_resids(results, focus_exog, resid_type=resid_type, use_glm_weights=use_glm_weights, fit_kwargs=fit_kwargs)\n    ax.plot(focus_exog_resid, endog_resid, 'o', alpha=0.6)\n    ax.set_title('Added variable plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel(model.endog_names + ' residuals', size=15)\n    return fig"
        ]
    },
    {
        "func_name": "plot_partial_residuals",
        "original": "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
        "mutated": [
            "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    if False:\n        i = 10\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_partial_residuals_doc % {'extra_params_doc': 'results : object\\n    Results for a fitted regression model'})\ndef plot_partial_residuals(results, focus_exog, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    pr = partial_resids(results, focus_exog)\n    focus_exog_vals = results.model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, pr, 'o', alpha=0.6)\n    ax.set_title('Partial residuals plot', fontsize='large')\n    if isinstance(focus_exog, str):\n        xname = focus_exog\n    else:\n        xname = model.exog_names[focus_exog]\n    ax.set_xlabel(xname, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig"
        ]
    },
    {
        "func_name": "plot_ceres_residuals",
        "original": "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
        "mutated": [
            "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    if False:\n        i = 10\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig",
            "@Appender(_plot_ceres_residuals_doc % {'extra_params_doc': 'results : Results\\n        Results instance of a fitted regression model.'})\ndef plot_ceres_residuals(results, focus_exog, frac=0.66, cond_means=None, ax=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model = results.model\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    presid = ceres_resids(results, focus_exog, frac=frac, cond_means=cond_means)\n    focus_exog_vals = model.exog[:, focus_col]\n    (fig, ax) = utils.create_mpl_ax(ax)\n    ax.plot(focus_exog_vals, presid, 'o', alpha=0.6)\n    ax.set_title('CERES residuals plot', fontsize='large')\n    ax.set_xlabel(focus_exog, size=15)\n    ax.set_ylabel('Component plus residual', size=15)\n    return fig"
        ]
    },
    {
        "func_name": "ceres_resids",
        "original": "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    \"\"\"\n    Calculate the CERES residuals (Conditional Expectation Partial\n    Residuals) for a fitted model.\n\n    Parameters\n    ----------\n    results : model results instance\n        The fitted model for which the CERES residuals are calculated.\n    focus_exog : int\n        The column of results.model.exog used as the 'focus variable'.\n    frac : float, optional\n        Lowess smoothing parameter for estimating the conditional\n        means.  Not used if `cond_means` is provided.\n    cond_means : array_like, optional\n        If provided, the columns of this array are the conditional\n        means E[exog | focus exog], where exog ranges over some\n        or all of the columns of exog other than focus exog.  If\n        this is an empty nx0 array, the conditional means are\n        treated as being zero.  If None, the conditional means are\n        estimated.\n\n    Returns\n    -------\n    An array containing the CERES residuals.\n\n    Notes\n    -----\n    If `cond_means` is not provided, it is obtained by smoothing each\n    column of exog (except the focus column) against the focus column.\n\n    Currently only supports GLM, GEE, and OLS models.\n    \"\"\"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid",
        "mutated": [
            "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    if False:\n        i = 10\n    \"\\n    Calculate the CERES residuals (Conditional Expectation Partial\\n    Residuals) for a fitted model.\\n\\n    Parameters\\n    ----------\\n    results : model results instance\\n        The fitted model for which the CERES residuals are calculated.\\n    focus_exog : int\\n        The column of results.model.exog used as the 'focus variable'.\\n    frac : float, optional\\n        Lowess smoothing parameter for estimating the conditional\\n        means.  Not used if `cond_means` is provided.\\n    cond_means : array_like, optional\\n        If provided, the columns of this array are the conditional\\n        means E[exog | focus exog], where exog ranges over some\\n        or all of the columns of exog other than focus exog.  If\\n        this is an empty nx0 array, the conditional means are\\n        treated as being zero.  If None, the conditional means are\\n        estimated.\\n\\n    Returns\\n    -------\\n    An array containing the CERES residuals.\\n\\n    Notes\\n    -----\\n    If `cond_means` is not provided, it is obtained by smoothing each\\n    column of exog (except the focus column) against the focus column.\\n\\n    Currently only supports GLM, GEE, and OLS models.\\n    \"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid",
            "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Calculate the CERES residuals (Conditional Expectation Partial\\n    Residuals) for a fitted model.\\n\\n    Parameters\\n    ----------\\n    results : model results instance\\n        The fitted model for which the CERES residuals are calculated.\\n    focus_exog : int\\n        The column of results.model.exog used as the 'focus variable'.\\n    frac : float, optional\\n        Lowess smoothing parameter for estimating the conditional\\n        means.  Not used if `cond_means` is provided.\\n    cond_means : array_like, optional\\n        If provided, the columns of this array are the conditional\\n        means E[exog | focus exog], where exog ranges over some\\n        or all of the columns of exog other than focus exog.  If\\n        this is an empty nx0 array, the conditional means are\\n        treated as being zero.  If None, the conditional means are\\n        estimated.\\n\\n    Returns\\n    -------\\n    An array containing the CERES residuals.\\n\\n    Notes\\n    -----\\n    If `cond_means` is not provided, it is obtained by smoothing each\\n    column of exog (except the focus column) against the focus column.\\n\\n    Currently only supports GLM, GEE, and OLS models.\\n    \"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid",
            "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Calculate the CERES residuals (Conditional Expectation Partial\\n    Residuals) for a fitted model.\\n\\n    Parameters\\n    ----------\\n    results : model results instance\\n        The fitted model for which the CERES residuals are calculated.\\n    focus_exog : int\\n        The column of results.model.exog used as the 'focus variable'.\\n    frac : float, optional\\n        Lowess smoothing parameter for estimating the conditional\\n        means.  Not used if `cond_means` is provided.\\n    cond_means : array_like, optional\\n        If provided, the columns of this array are the conditional\\n        means E[exog | focus exog], where exog ranges over some\\n        or all of the columns of exog other than focus exog.  If\\n        this is an empty nx0 array, the conditional means are\\n        treated as being zero.  If None, the conditional means are\\n        estimated.\\n\\n    Returns\\n    -------\\n    An array containing the CERES residuals.\\n\\n    Notes\\n    -----\\n    If `cond_means` is not provided, it is obtained by smoothing each\\n    column of exog (except the focus column) against the focus column.\\n\\n    Currently only supports GLM, GEE, and OLS models.\\n    \"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid",
            "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Calculate the CERES residuals (Conditional Expectation Partial\\n    Residuals) for a fitted model.\\n\\n    Parameters\\n    ----------\\n    results : model results instance\\n        The fitted model for which the CERES residuals are calculated.\\n    focus_exog : int\\n        The column of results.model.exog used as the 'focus variable'.\\n    frac : float, optional\\n        Lowess smoothing parameter for estimating the conditional\\n        means.  Not used if `cond_means` is provided.\\n    cond_means : array_like, optional\\n        If provided, the columns of this array are the conditional\\n        means E[exog | focus exog], where exog ranges over some\\n        or all of the columns of exog other than focus exog.  If\\n        this is an empty nx0 array, the conditional means are\\n        treated as being zero.  If None, the conditional means are\\n        estimated.\\n\\n    Returns\\n    -------\\n    An array containing the CERES residuals.\\n\\n    Notes\\n    -----\\n    If `cond_means` is not provided, it is obtained by smoothing each\\n    column of exog (except the focus column) against the focus column.\\n\\n    Currently only supports GLM, GEE, and OLS models.\\n    \"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid",
            "def ceres_resids(results, focus_exog, frac=0.66, cond_means=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Calculate the CERES residuals (Conditional Expectation Partial\\n    Residuals) for a fitted model.\\n\\n    Parameters\\n    ----------\\n    results : model results instance\\n        The fitted model for which the CERES residuals are calculated.\\n    focus_exog : int\\n        The column of results.model.exog used as the 'focus variable'.\\n    frac : float, optional\\n        Lowess smoothing parameter for estimating the conditional\\n        means.  Not used if `cond_means` is provided.\\n    cond_means : array_like, optional\\n        If provided, the columns of this array are the conditional\\n        means E[exog | focus exog], where exog ranges over some\\n        or all of the columns of exog other than focus exog.  If\\n        this is an empty nx0 array, the conditional means are\\n        treated as being zero.  If None, the conditional means are\\n        estimated.\\n\\n    Returns\\n    -------\\n    An array containing the CERES residuals.\\n\\n    Notes\\n    -----\\n    If `cond_means` is not provided, it is obtained by smoothing each\\n    column of exog (except the focus column) against the focus column.\\n\\n    Currently only supports GLM, GEE, and OLS models.\\n    \"\n    model = results.model\n    if not isinstance(model, (GLM, GEE, OLS)):\n        raise ValueError('ceres residuals not available for %s' % model.__class__.__name__)\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    ix_nf = range(len(results.params))\n    ix_nf = list(ix_nf)\n    ix_nf.pop(focus_col)\n    nnf = len(ix_nf)\n    if cond_means is None:\n        pexog = model.exog[:, ix_nf]\n        pexog -= pexog.mean(0)\n        (u, s, vt) = np.linalg.svd(pexog, 0)\n        ii = np.flatnonzero(s > 1e-06)\n        pexog = u[:, ii]\n        fcol = model.exog[:, focus_col]\n        cond_means = np.empty((len(fcol), pexog.shape[1]))\n        for j in range(pexog.shape[1]):\n            y0 = pexog[:, j]\n            cf = lowess(y0, fcol, frac=frac, return_sorted=False)\n            cond_means[:, j] = cf\n    new_exog = np.concatenate((model.exog[:, ix_nf], cond_means), axis=1)\n    klass = model.__class__\n    init_kwargs = model._get_init_kwds()\n    new_model = klass(model.endog, new_exog, **init_kwargs)\n    new_result = new_model.fit()\n    presid = model.endog - new_result.fittedvalues\n    if isinstance(model, (GLM, GEE)):\n        presid *= model.family.link.deriv(new_result.fittedvalues)\n    if new_exog.shape[1] > nnf:\n        presid += np.dot(new_exog[:, nnf:], new_result.params[nnf:])\n    return presid"
        ]
    },
    {
        "func_name": "partial_resids",
        "original": "def partial_resids(results, focus_exog):\n    \"\"\"\n    Returns partial residuals for a fitted model with respect to a\n    'focus predictor'.\n\n    Parameters\n    ----------\n    results : results instance\n        A fitted regression model.\n    focus col : int\n        The column index of model.exog with respect to which the\n        partial residuals are calculated.\n\n    Returns\n    -------\n    An array of partial residuals.\n\n    References\n    ----------\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\n    generalized linear models.  Journal of the American Statistical\n    Association, 93:442.\n    \"\"\"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid",
        "mutated": [
            "def partial_resids(results, focus_exog):\n    if False:\n        i = 10\n    \"\\n    Returns partial residuals for a fitted model with respect to a\\n    'focus predictor'.\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        A fitted regression model.\\n    focus col : int\\n        The column index of model.exog with respect to which the\\n        partial residuals are calculated.\\n\\n    Returns\\n    -------\\n    An array of partial residuals.\\n\\n    References\\n    ----------\\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\\n    generalized linear models.  Journal of the American Statistical\\n    Association, 93:442.\\n    \"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid",
            "def partial_resids(results, focus_exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Returns partial residuals for a fitted model with respect to a\\n    'focus predictor'.\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        A fitted regression model.\\n    focus col : int\\n        The column index of model.exog with respect to which the\\n        partial residuals are calculated.\\n\\n    Returns\\n    -------\\n    An array of partial residuals.\\n\\n    References\\n    ----------\\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\\n    generalized linear models.  Journal of the American Statistical\\n    Association, 93:442.\\n    \"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid",
            "def partial_resids(results, focus_exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Returns partial residuals for a fitted model with respect to a\\n    'focus predictor'.\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        A fitted regression model.\\n    focus col : int\\n        The column index of model.exog with respect to which the\\n        partial residuals are calculated.\\n\\n    Returns\\n    -------\\n    An array of partial residuals.\\n\\n    References\\n    ----------\\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\\n    generalized linear models.  Journal of the American Statistical\\n    Association, 93:442.\\n    \"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid",
            "def partial_resids(results, focus_exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Returns partial residuals for a fitted model with respect to a\\n    'focus predictor'.\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        A fitted regression model.\\n    focus col : int\\n        The column index of model.exog with respect to which the\\n        partial residuals are calculated.\\n\\n    Returns\\n    -------\\n    An array of partial residuals.\\n\\n    References\\n    ----------\\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\\n    generalized linear models.  Journal of the American Statistical\\n    Association, 93:442.\\n    \"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid",
            "def partial_resids(results, focus_exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Returns partial residuals for a fitted model with respect to a\\n    'focus predictor'.\\n\\n    Parameters\\n    ----------\\n    results : results instance\\n        A fitted regression model.\\n    focus col : int\\n        The column index of model.exog with respect to which the\\n        partial residuals are calculated.\\n\\n    Returns\\n    -------\\n    An array of partial residuals.\\n\\n    References\\n    ----------\\n    RD Cook and R Croos-Dabrera (1998).  Partial residual plots in\\n    generalized linear models.  Journal of the American Statistical\\n    Association, 93:442.\\n    \"\n    model = results.model\n    resid = model.endog - results.predict()\n    if isinstance(model, (GLM, GEE)):\n        resid *= model.family.link.deriv(results.fittedvalues)\n    elif isinstance(model, (OLS, GLS, WLS)):\n        pass\n    else:\n        raise ValueError(\"Partial residuals for '%s' not implemented.\" % type(model))\n    if type(focus_exog) is str:\n        focus_col = model.exog_names.index(focus_exog)\n    else:\n        focus_col = focus_exog\n    focus_val = results.params[focus_col] * model.exog[:, focus_col]\n    return focus_val + resid"
        ]
    },
    {
        "func_name": "added_variable_resids",
        "original": "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    \"\"\"\n    Residualize the endog variable and a 'focus' exog variable in a\n    regression model with respect to the other exog variables.\n\n    Parameters\n    ----------\n    results : regression results instance\n        A fitted model including the focus exog and all other\n        predictors of interest.\n    focus_exog : {int, str}\n        The column of results.model.exog or a variable name that is\n        to be residualized against the other predictors.\n    resid_type : str\n        The type of residuals to use for the dependent variable.  If\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\n    use_glm_weights : bool\n        Only used if the model is a GLM or GEE.  If True, the\n        residuals for the focus predictor are computed using WLS, with\n        the weights obtained from the IRLS calculations for fitting\n        the GLM.  If False, unweighted regression is used.\n    fit_kwargs : dict, optional\n        Keyword arguments to be passed to fit when refitting the\n        model.\n\n    Returns\n    -------\n    endog_resid : array_like\n        The residuals for the original exog\n    focus_exog_resid : array_like\n        The residuals for the focus predictor\n\n    Notes\n    -----\n    The 'focus variable' residuals are always obtained using linear\n    regression.\n\n    Currently only GLM, GEE, and OLS models are supported.\n    \"\"\"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)",
        "mutated": [
            "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    if False:\n        i = 10\n    \"\\n    Residualize the endog variable and a 'focus' exog variable in a\\n    regression model with respect to the other exog variables.\\n\\n    Parameters\\n    ----------\\n    results : regression results instance\\n        A fitted model including the focus exog and all other\\n        predictors of interest.\\n    focus_exog : {int, str}\\n        The column of results.model.exog or a variable name that is\\n        to be residualized against the other predictors.\\n    resid_type : str\\n        The type of residuals to use for the dependent variable.  If\\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\\n    use_glm_weights : bool\\n        Only used if the model is a GLM or GEE.  If True, the\\n        residuals for the focus predictor are computed using WLS, with\\n        the weights obtained from the IRLS calculations for fitting\\n        the GLM.  If False, unweighted regression is used.\\n    fit_kwargs : dict, optional\\n        Keyword arguments to be passed to fit when refitting the\\n        model.\\n\\n    Returns\\n    -------\\n    endog_resid : array_like\\n        The residuals for the original exog\\n    focus_exog_resid : array_like\\n        The residuals for the focus predictor\\n\\n    Notes\\n    -----\\n    The 'focus variable' residuals are always obtained using linear\\n    regression.\\n\\n    Currently only GLM, GEE, and OLS models are supported.\\n    \"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)",
            "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Residualize the endog variable and a 'focus' exog variable in a\\n    regression model with respect to the other exog variables.\\n\\n    Parameters\\n    ----------\\n    results : regression results instance\\n        A fitted model including the focus exog and all other\\n        predictors of interest.\\n    focus_exog : {int, str}\\n        The column of results.model.exog or a variable name that is\\n        to be residualized against the other predictors.\\n    resid_type : str\\n        The type of residuals to use for the dependent variable.  If\\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\\n    use_glm_weights : bool\\n        Only used if the model is a GLM or GEE.  If True, the\\n        residuals for the focus predictor are computed using WLS, with\\n        the weights obtained from the IRLS calculations for fitting\\n        the GLM.  If False, unweighted regression is used.\\n    fit_kwargs : dict, optional\\n        Keyword arguments to be passed to fit when refitting the\\n        model.\\n\\n    Returns\\n    -------\\n    endog_resid : array_like\\n        The residuals for the original exog\\n    focus_exog_resid : array_like\\n        The residuals for the focus predictor\\n\\n    Notes\\n    -----\\n    The 'focus variable' residuals are always obtained using linear\\n    regression.\\n\\n    Currently only GLM, GEE, and OLS models are supported.\\n    \"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)",
            "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Residualize the endog variable and a 'focus' exog variable in a\\n    regression model with respect to the other exog variables.\\n\\n    Parameters\\n    ----------\\n    results : regression results instance\\n        A fitted model including the focus exog and all other\\n        predictors of interest.\\n    focus_exog : {int, str}\\n        The column of results.model.exog or a variable name that is\\n        to be residualized against the other predictors.\\n    resid_type : str\\n        The type of residuals to use for the dependent variable.  If\\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\\n    use_glm_weights : bool\\n        Only used if the model is a GLM or GEE.  If True, the\\n        residuals for the focus predictor are computed using WLS, with\\n        the weights obtained from the IRLS calculations for fitting\\n        the GLM.  If False, unweighted regression is used.\\n    fit_kwargs : dict, optional\\n        Keyword arguments to be passed to fit when refitting the\\n        model.\\n\\n    Returns\\n    -------\\n    endog_resid : array_like\\n        The residuals for the original exog\\n    focus_exog_resid : array_like\\n        The residuals for the focus predictor\\n\\n    Notes\\n    -----\\n    The 'focus variable' residuals are always obtained using linear\\n    regression.\\n\\n    Currently only GLM, GEE, and OLS models are supported.\\n    \"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)",
            "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Residualize the endog variable and a 'focus' exog variable in a\\n    regression model with respect to the other exog variables.\\n\\n    Parameters\\n    ----------\\n    results : regression results instance\\n        A fitted model including the focus exog and all other\\n        predictors of interest.\\n    focus_exog : {int, str}\\n        The column of results.model.exog or a variable name that is\\n        to be residualized against the other predictors.\\n    resid_type : str\\n        The type of residuals to use for the dependent variable.  If\\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\\n    use_glm_weights : bool\\n        Only used if the model is a GLM or GEE.  If True, the\\n        residuals for the focus predictor are computed using WLS, with\\n        the weights obtained from the IRLS calculations for fitting\\n        the GLM.  If False, unweighted regression is used.\\n    fit_kwargs : dict, optional\\n        Keyword arguments to be passed to fit when refitting the\\n        model.\\n\\n    Returns\\n    -------\\n    endog_resid : array_like\\n        The residuals for the original exog\\n    focus_exog_resid : array_like\\n        The residuals for the focus predictor\\n\\n    Notes\\n    -----\\n    The 'focus variable' residuals are always obtained using linear\\n    regression.\\n\\n    Currently only GLM, GEE, and OLS models are supported.\\n    \"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)",
            "def added_variable_resids(results, focus_exog, resid_type=None, use_glm_weights=True, fit_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Residualize the endog variable and a 'focus' exog variable in a\\n    regression model with respect to the other exog variables.\\n\\n    Parameters\\n    ----------\\n    results : regression results instance\\n        A fitted model including the focus exog and all other\\n        predictors of interest.\\n    focus_exog : {int, str}\\n        The column of results.model.exog or a variable name that is\\n        to be residualized against the other predictors.\\n    resid_type : str\\n        The type of residuals to use for the dependent variable.  If\\n        None, uses `resid_deviance` for GLM/GEE and `resid` otherwise.\\n    use_glm_weights : bool\\n        Only used if the model is a GLM or GEE.  If True, the\\n        residuals for the focus predictor are computed using WLS, with\\n        the weights obtained from the IRLS calculations for fitting\\n        the GLM.  If False, unweighted regression is used.\\n    fit_kwargs : dict, optional\\n        Keyword arguments to be passed to fit when refitting the\\n        model.\\n\\n    Returns\\n    -------\\n    endog_resid : array_like\\n        The residuals for the original exog\\n    focus_exog_resid : array_like\\n        The residuals for the focus predictor\\n\\n    Notes\\n    -----\\n    The 'focus variable' residuals are always obtained using linear\\n    regression.\\n\\n    Currently only GLM, GEE, and OLS models are supported.\\n    \"\n    model = results.model\n    if not isinstance(model, (GEE, GLM, OLS)):\n        raise ValueError('model type %s not supported for added variable residuals' % model.__class__.__name__)\n    exog = model.exog\n    endog = model.endog\n    (focus_exog, focus_col) = utils.maybe_name_or_idx(focus_exog, model)\n    focus_exog_vals = exog[:, focus_col]\n    if resid_type is None:\n        if isinstance(model, (GEE, GLM)):\n            resid_type = 'resid_deviance'\n        else:\n            resid_type = 'resid'\n    ii = range(exog.shape[1])\n    ii = list(ii)\n    ii.pop(focus_col)\n    reduced_exog = exog[:, ii]\n    start_params = results.params[ii]\n    klass = model.__class__\n    kwargs = model._get_init_kwds()\n    new_model = klass(endog, reduced_exog, **kwargs)\n    args = {'start_params': start_params}\n    if fit_kwargs is not None:\n        args.update(fit_kwargs)\n    new_result = new_model.fit(**args)\n    if not getattr(new_result, 'converged', True):\n        raise ValueError('fit did not converge when calculating added variable residuals')\n    try:\n        endog_resid = getattr(new_result, resid_type)\n    except AttributeError:\n        raise ValueError(\"'%s' residual type not available\" % resid_type)\n    import statsmodels.regression.linear_model as lm\n    if isinstance(model, (GLM, GEE)) and use_glm_weights:\n        weights = model.family.weights(results.fittedvalues)\n        if hasattr(model, 'data_weights'):\n            weights = weights * model.data_weights\n        lm_results = lm.WLS(focus_exog_vals, reduced_exog, weights).fit()\n    else:\n        lm_results = lm.OLS(focus_exog_vals, reduced_exog).fit()\n    focus_exog_resid = lm_results.resid\n    return (endog_resid, focus_exog_resid)"
        ]
    }
]