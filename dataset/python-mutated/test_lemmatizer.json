[
    {
        "func_name": "test_identity_lemmatizer",
        "original": "def test_identity_lemmatizer():\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)",
        "mutated": [
            "def test_identity_lemmatizer():\n    if False:\n        i = 10\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_identity_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_identity_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_identity_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_identity_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en', 'lemma_use_identity': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_IDENTITY_GOLD == '\\n'.join(word_lemma_pairs)"
        ]
    },
    {
        "func_name": "test_full_lemmatizer",
        "original": "def test_full_lemmatizer():\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)",
        "mutated": [
            "def test_full_lemmatizer():\n    if False:\n        i = 10\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_full_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_full_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_full_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)",
            "def test_full_lemmatizer():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f'{w.text} {w.lemma}']\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == '\\n'.join(word_lemma_pairs)"
        ]
    },
    {
        "func_name": "find_unknown_word",
        "original": "def find_unknown_word(lemmatizer, base):\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')",
        "mutated": [
            "def find_unknown_word(lemmatizer, base):\n    if False:\n        i = 10\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')",
            "def find_unknown_word(lemmatizer, base):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')",
            "def find_unknown_word(lemmatizer, base):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')",
            "def find_unknown_word(lemmatizer, base):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')",
            "def find_unknown_word(lemmatizer, base):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for i in range(10):\n        base = base + 'z'\n        if base not in lemmatizer.word_dict and all((x[0] != base for x in lemmatizer.composite_dict.keys())):\n            return base\n    raise RuntimeError('wtf?')"
        ]
    },
    {
        "func_name": "test_store_results",
        "original": "def test_store_results():\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict",
        "mutated": [
            "def test_store_results():\n    if False:\n        i = 10\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict",
            "def test_store_results():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict",
            "def test_store_results():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict",
            "def test_store_results():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict",
            "def test_store_results():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nlp = stanza.Pipeline(**{'processors': 'tokenize,pos,lemma', 'dir': TEST_MODELS_DIR, 'lang': 'en'}, lemma_store_results=True)\n    lemmatizer = nlp.processors['lemma']._trainer\n    az = find_unknown_word(lemmatizer, 'a')\n    bz = find_unknown_word(lemmatizer, 'b')\n    cz = find_unknown_word(lemmatizer, 'c')\n    doc = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == az\n    assert stuff[6][0] == bz\n    assert stuff[11][0] == cz\n    assert lemmatizer.composite_dict[az, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[bz, stuff[6][1]] == stuff[6][2]\n    assert lemmatizer.composite_dict[cz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('I found an ' + az + ' in my ' + bz + '.  It was a ' + cz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    dz = find_unknown_word(lemmatizer, 'd')\n    ez = find_unknown_word(lemmatizer, 'e')\n    fz = find_unknown_word(lemmatizer, 'f')\n    doc = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff = doc.get([TEXT, UPOS, LEMMA])\n    assert len(stuff) == 12\n    assert stuff[3][0] == dz\n    assert stuff[8][0] == ez\n    assert stuff[11][0] == fz\n    assert lemmatizer.composite_dict[dz, stuff[3][1]] == stuff[3][2]\n    assert lemmatizer.composite_dict[ez, stuff[8][1]] == stuff[8][2]\n    assert lemmatizer.composite_dict[fz, stuff[11][1]] == stuff[11][2]\n    doc2 = nlp('It was a ' + dz + '.  I found an ' + ez + ' in my ' + fz)\n    stuff2 = doc2.get([TEXT, UPOS, LEMMA])\n    assert stuff == stuff2\n    assert az not in lemmatizer.word_dict"
        ]
    }
]