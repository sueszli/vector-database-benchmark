[
    {
        "func_name": "testToyRegression",
        "original": "def testToyRegression(self):\n    \"\"\"Tests a toy regression end to end.\n\n        The test code carries a simple toy regression in the form\n            y = 2.0 x1 + 1.5 x2 + 0.5\n        by randomly generating gaussian inputs and calculating the ground\n        truth outputs in the net as well. It uses a standard SGD to then\n        train the parameters.\n        \"\"\"\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()",
        "mutated": [
            "def testToyRegression(self):\n    if False:\n        i = 10\n    'Tests a toy regression end to end.\\n\\n        The test code carries a simple toy regression in the form\\n            y = 2.0 x1 + 1.5 x2 + 0.5\\n        by randomly generating gaussian inputs and calculating the ground\\n        truth outputs in the net as well. It uses a standard SGD to then\\n        train the parameters.\\n        '\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()",
            "def testToyRegression(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Tests a toy regression end to end.\\n\\n        The test code carries a simple toy regression in the form\\n            y = 2.0 x1 + 1.5 x2 + 0.5\\n        by randomly generating gaussian inputs and calculating the ground\\n        truth outputs in the net as well. It uses a standard SGD to then\\n        train the parameters.\\n        '\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()",
            "def testToyRegression(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Tests a toy regression end to end.\\n\\n        The test code carries a simple toy regression in the form\\n            y = 2.0 x1 + 1.5 x2 + 0.5\\n        by randomly generating gaussian inputs and calculating the ground\\n        truth outputs in the net as well. It uses a standard SGD to then\\n        train the parameters.\\n        '\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()",
            "def testToyRegression(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Tests a toy regression end to end.\\n\\n        The test code carries a simple toy regression in the form\\n            y = 2.0 x1 + 1.5 x2 + 0.5\\n        by randomly generating gaussian inputs and calculating the ground\\n        truth outputs in the net as well. It uses a standard SGD to then\\n        train the parameters.\\n        '\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()",
            "def testToyRegression(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Tests a toy regression end to end.\\n\\n        The test code carries a simple toy regression in the form\\n            y = 2.0 x1 + 1.5 x2 + 0.5\\n        by randomly generating gaussian inputs and calculating the ground\\n        truth outputs in the net as well. It uses a standard SGD to then\\n        train the parameters.\\n        '\n    workspace.ResetWorkspace()\n    init_net = core.Net('init')\n    W = init_net.UniformFill([], 'W', shape=[1, 2], min=-1.0, max=1.0)\n    B = init_net.ConstantFill([], 'B', shape=[1], value=0.0)\n    W_gt = init_net.GivenTensorFill([], 'W_gt', shape=[1, 2], values=[2.0, 1.5])\n    B_gt = init_net.GivenTensorFill([], 'B_gt', shape=[1], values=[0.5])\n    LR = init_net.ConstantFill([], 'LR', shape=[1], value=-0.1)\n    ONE = init_net.ConstantFill([], 'ONE', shape=[1], value=1.0)\n    ITER = init_net.ConstantFill([], 'ITER', shape=[1], value=0, dtype=core.DataType.INT64)\n    train_net = core.Net('train')\n    X = train_net.GaussianFill([], 'X', shape=[64, 2], mean=0.0, std=1.0)\n    Y_gt = X.FC([W_gt, B_gt], 'Y_gt')\n    Y_pred = X.FC([W, B], 'Y_pred')\n    dist = train_net.SquaredL2Distance([Y_gt, Y_pred], 'dist')\n    loss = dist.AveragedLoss([], ['loss'])\n    input_to_grad = train_net.AddGradientOperators([loss], skip=2)\n    train_net.Iter(ITER, ITER)\n    train_net.LearningRate(ITER, 'LR', base_lr=-0.1, policy='step', stepsize=20, gamma=0.9)\n    train_net.WeightedSum([W, ONE, input_to_grad[str(W)], LR], W)\n    train_net.WeightedSum([B, ONE, input_to_grad[str(B)], LR], B)\n    for blob in [loss, W, B]:\n        train_net.Print(blob, [])\n    plan = core.Plan('toy_regression')\n    plan.AddStep(core.ExecutionStep('init', init_net))\n    plan.AddStep(core.ExecutionStep('train', train_net, 200))\n    workspace.RunPlan(plan)\n    W_result = workspace.FetchBlob('W')\n    B_result = workspace.FetchBlob('B')\n    np.testing.assert_array_almost_equal(W_result, [[2.0, 1.5]], decimal=2)\n    np.testing.assert_array_almost_equal(B_result, [0.5], decimal=2)\n    workspace.ResetWorkspace()"
        ]
    }
]