[
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog):\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)",
        "mutated": [
            "def __init__(self, endog, exog):\n    if False:\n        i = 10\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)",
            "def __init__(self, endog, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)",
            "def __init__(self, endog, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)",
            "def __init__(self, endog, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)",
            "def __init__(self, endog, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.endog = np.asarray(endog)\n    self.exog = np.asarray(exog)"
        ]
    },
    {
        "func_name": "calc_factors",
        "original": "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    \"\"\"get factor decomposition of exogenous variables\n\n        This uses principal component analysis to obtain the factors. The number\n        of factors kept is the maximum that will be considered in the regression.\n        \"\"\"\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs",
        "mutated": [
            "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    if False:\n        i = 10\n    'get factor decomposition of exogenous variables\\n\\n        This uses principal component analysis to obtain the factors. The number\\n        of factors kept is the maximum that will be considered in the regression.\\n        '\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs",
            "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'get factor decomposition of exogenous variables\\n\\n        This uses principal component analysis to obtain the factors. The number\\n        of factors kept is the maximum that will be considered in the regression.\\n        '\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs",
            "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'get factor decomposition of exogenous variables\\n\\n        This uses principal component analysis to obtain the factors. The number\\n        of factors kept is the maximum that will be considered in the regression.\\n        '\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs",
            "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'get factor decomposition of exogenous variables\\n\\n        This uses principal component analysis to obtain the factors. The number\\n        of factors kept is the maximum that will be considered in the regression.\\n        '\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs",
            "def calc_factors(self, x=None, keepdim=0, addconst=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'get factor decomposition of exogenous variables\\n\\n        This uses principal component analysis to obtain the factors. The number\\n        of factors kept is the maximum that will be considered in the regression.\\n        '\n    if x is None:\n        x = self.exog\n    else:\n        x = np.asarray(x)\n    (xred, fact, evals, evecs) = pca(x, keepdim=keepdim, normalize=1)\n    self.exog_reduced = xred\n    if addconst:\n        self.factors = sm.add_constant(fact, prepend=True)\n        self.hasconst = 1\n    else:\n        self.factors = fact\n        self.hasconst = 0\n    self.evals = evals\n    self.evecs = evecs"
        ]
    },
    {
        "func_name": "fit_fixed_nfact",
        "original": "def fit_fixed_nfact(self, nfact):\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()",
        "mutated": [
            "def fit_fixed_nfact(self, nfact):\n    if False:\n        i = 10\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()",
            "def fit_fixed_nfact(self, nfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()",
            "def fit_fixed_nfact(self, nfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()",
            "def fit_fixed_nfact(self, nfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()",
            "def fit_fixed_nfact(self, nfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not hasattr(self, 'factors_wconst'):\n        self.calc_factors()\n    return sm.OLS(self.endog, self.factors[:, :nfact + 1]).fit()"
        ]
    },
    {
        "func_name": "fit_find_nfact",
        "original": "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    \"\"\"estimate the model and selection criteria for up to maxfact factors\n\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\n        is false. Cross-validation is not used by default because it can be\n        time consuming to calculate.\n\n        By default the cross-validation method is Leave-one-out on the full dataset.\n        A different cross-validation sample can be specified as an argument to\n        cv_iter.\n\n        Results are attached in `results_find_nfact`\n\n\n\n        \"\"\"\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]",
        "mutated": [
            "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    if False:\n        i = 10\n    'estimate the model and selection criteria for up to maxfact factors\\n\\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\\n        is false. Cross-validation is not used by default because it can be\\n        time consuming to calculate.\\n\\n        By default the cross-validation method is Leave-one-out on the full dataset.\\n        A different cross-validation sample can be specified as an argument to\\n        cv_iter.\\n\\n        Results are attached in `results_find_nfact`\\n\\n\\n\\n        '\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]",
            "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'estimate the model and selection criteria for up to maxfact factors\\n\\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\\n        is false. Cross-validation is not used by default because it can be\\n        time consuming to calculate.\\n\\n        By default the cross-validation method is Leave-one-out on the full dataset.\\n        A different cross-validation sample can be specified as an argument to\\n        cv_iter.\\n\\n        Results are attached in `results_find_nfact`\\n\\n\\n\\n        '\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]",
            "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'estimate the model and selection criteria for up to maxfact factors\\n\\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\\n        is false. Cross-validation is not used by default because it can be\\n        time consuming to calculate.\\n\\n        By default the cross-validation method is Leave-one-out on the full dataset.\\n        A different cross-validation sample can be specified as an argument to\\n        cv_iter.\\n\\n        Results are attached in `results_find_nfact`\\n\\n\\n\\n        '\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]",
            "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'estimate the model and selection criteria for up to maxfact factors\\n\\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\\n        is false. Cross-validation is not used by default because it can be\\n        time consuming to calculate.\\n\\n        By default the cross-validation method is Leave-one-out on the full dataset.\\n        A different cross-validation sample can be specified as an argument to\\n        cv_iter.\\n\\n        Results are attached in `results_find_nfact`\\n\\n\\n\\n        '\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]",
            "def fit_find_nfact(self, maxfact=None, skip_crossval=True, cv_iter=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'estimate the model and selection criteria for up to maxfact factors\\n\\n        The selection criteria that are calculated are AIC, BIC, and R2_adj. and\\n        additionally cross-validation prediction error sum of squares if `skip_crossval`\\n        is false. Cross-validation is not used by default because it can be\\n        time consuming to calculate.\\n\\n        By default the cross-validation method is Leave-one-out on the full dataset.\\n        A different cross-validation sample can be specified as an argument to\\n        cv_iter.\\n\\n        Results are attached in `results_find_nfact`\\n\\n\\n\\n        '\n    if not hasattr(self, 'factors'):\n        self.calc_factors()\n    hasconst = self.hasconst\n    if maxfact is None:\n        maxfact = self.factors.shape[1] - hasconst\n    if maxfact + hasconst < 1:\n        raise ValueError('nothing to do, number of factors (incl. constant) should ' + 'be at least 1')\n    maxfact = min(maxfact, 10)\n    y0 = self.endog\n    results = []\n    for k in range(1, maxfact + hasconst):\n        fact = self.factors[:, :k]\n        res = sm.OLS(y0, fact).fit()\n        if not skip_crossval:\n            if cv_iter is None:\n                cv_iter = LeaveOneOut(len(y0))\n            prederr2 = 0.0\n            for (inidx, outidx) in cv_iter:\n                res_l1o = sm.OLS(y0[inidx], fact[inidx, :]).fit()\n                prederr2 += (y0[outidx] - res_l1o.model.predict(res_l1o.params, fact[outidx, :])) ** 2.0\n        else:\n            prederr2 = np.nan\n        results.append([k, res.aic, res.bic, res.rsquared_adj, prederr2])\n    self.results_find_nfact = results = np.array(results)\n    self.best_nfact = np.r_[np.argmin(results[:, 1:3], 0), np.argmax(results[:, 3], 0), np.argmin(results[:, -1], 0)]"
        ]
    },
    {
        "func_name": "summary_find_nfact",
        "original": "def summary_find_nfact(self):\n    \"\"\"provides a summary for the selection of the number of factors\n\n        Returns\n        -------\n        sumstr : str\n            summary of the results for selecting the number of factors\n\n        \"\"\"\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr",
        "mutated": [
            "def summary_find_nfact(self):\n    if False:\n        i = 10\n    'provides a summary for the selection of the number of factors\\n\\n        Returns\\n        -------\\n        sumstr : str\\n            summary of the results for selecting the number of factors\\n\\n        '\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr",
            "def summary_find_nfact(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'provides a summary for the selection of the number of factors\\n\\n        Returns\\n        -------\\n        sumstr : str\\n            summary of the results for selecting the number of factors\\n\\n        '\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr",
            "def summary_find_nfact(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'provides a summary for the selection of the number of factors\\n\\n        Returns\\n        -------\\n        sumstr : str\\n            summary of the results for selecting the number of factors\\n\\n        '\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr",
            "def summary_find_nfact(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'provides a summary for the selection of the number of factors\\n\\n        Returns\\n        -------\\n        sumstr : str\\n            summary of the results for selecting the number of factors\\n\\n        '\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr",
            "def summary_find_nfact(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'provides a summary for the selection of the number of factors\\n\\n        Returns\\n        -------\\n        sumstr : str\\n            summary of the results for selecting the number of factors\\n\\n        '\n    if not hasattr(self, 'results_find_nfact'):\n        self.fit_find_nfact()\n    results = self.results_find_nfact\n    sumstr = ''\n    sumstr += '\\n' + 'Best result for k, by AIC, BIC, R2_adj, L1O'\n    sumstr += '\\n' + ' ' * 19 + '%5d %4d %6d %5d' % tuple(self.best_nfact)\n    from statsmodels.iolib.table import SimpleTable\n    headers = 'k, AIC, BIC, R2_adj, L1O'.split(', ')\n    numformat = ['%6d'] + ['%10.3f'] * 4\n    txt_fmt1 = dict(data_fmts=numformat)\n    tabl = SimpleTable(results, headers, None, txt_fmt=txt_fmt1)\n    sumstr += '\\n' + 'PCA regression on simulated data,'\n    sumstr += '\\n' + 'DGP: 2 factors and 4 explanatory variables'\n    sumstr += '\\n' + tabl.__str__()\n    sumstr += '\\n' + 'Notes: k is number of components of PCA,'\n    sumstr += '\\n' + '       constant is added additionally'\n    sumstr += '\\n' + '       k=0 means regression on constant only'\n    sumstr += '\\n' + '       L1O: sum of squared prediction errors for leave-one-out'\n    return sumstr"
        ]
    }
]