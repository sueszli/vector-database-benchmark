[
    {
        "func_name": "__init__",
        "original": "def __init__(self, **kwds):\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}",
        "mutated": [
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.__dict__.update(kwds)\n    self._ini_keys = list(kwds.keys())\n    self.df_resid = self.k - 1\n    self.sd_eff_w_fe_hksj = np.sqrt(self.var_hksj_fe)\n    self.sd_eff_w_re_hksj = np.sqrt(self.var_hksj_re)\n    self.h2 = self.q / (self.k - 1)\n    self.i2 = 1 - 1 / self.h2\n    self.cache_ci = {}"
        ]
    },
    {
        "func_name": "conf_int_samples",
        "original": "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    \"\"\"confidence intervals for the effect size estimate of samples\n\n        Additional information needs to be provided for confidence intervals\n        that are not based on normal distribution using available variance.\n        This is likely to change in future.\n\n        Parameters\n        ----------\n        alpha : float in (0, 1)\n            Significance level for confidence interval. Nominal coverage is\n            ``1 - alpha``.\n        use_t : None or bool\n            If use_t is None, then the attribute `use_t` determines whether\n            normal or t-distribution is used for confidence intervals.\n            Specifying use_t overrides the attribute.\n            If use_t is false, then confidence intervals are based on the\n            normal distribution. If it is true, then the t-distribution is\n            used.\n        nobs : None or float\n            Number of observations used for degrees of freedom computation.\n            Only used if use_t is true.\n        ci_func : None or callable\n            User provided function to compute confidence intervals.\n            This is not used yet and will allow using non-standard confidence\n            intervals.\n\n        Returns\n        -------\n        ci_eff : tuple of ndarrays\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\n            sample.\n\n        Notes\n        -----\n        CombineResults currently only has information from the combine_effects\n        function, which does not provide details about individual samples.\n        \"\"\"\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff",
        "mutated": [
            "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    if False:\n        i = 10\n    'confidence intervals for the effect size estimate of samples\\n\\n        Additional information needs to be provided for confidence intervals\\n        that are not based on normal distribution using available variance.\\n        This is likely to change in future.\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        nobs : None or float\\n            Number of observations used for degrees of freedom computation.\\n            Only used if use_t is true.\\n        ci_func : None or callable\\n            User provided function to compute confidence intervals.\\n            This is not used yet and will allow using non-standard confidence\\n            intervals.\\n\\n        Returns\\n        -------\\n        ci_eff : tuple of ndarrays\\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\\n            sample.\\n\\n        Notes\\n        -----\\n        CombineResults currently only has information from the combine_effects\\n        function, which does not provide details about individual samples.\\n        '\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff",
            "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'confidence intervals for the effect size estimate of samples\\n\\n        Additional information needs to be provided for confidence intervals\\n        that are not based on normal distribution using available variance.\\n        This is likely to change in future.\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        nobs : None or float\\n            Number of observations used for degrees of freedom computation.\\n            Only used if use_t is true.\\n        ci_func : None or callable\\n            User provided function to compute confidence intervals.\\n            This is not used yet and will allow using non-standard confidence\\n            intervals.\\n\\n        Returns\\n        -------\\n        ci_eff : tuple of ndarrays\\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\\n            sample.\\n\\n        Notes\\n        -----\\n        CombineResults currently only has information from the combine_effects\\n        function, which does not provide details about individual samples.\\n        '\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff",
            "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'confidence intervals for the effect size estimate of samples\\n\\n        Additional information needs to be provided for confidence intervals\\n        that are not based on normal distribution using available variance.\\n        This is likely to change in future.\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        nobs : None or float\\n            Number of observations used for degrees of freedom computation.\\n            Only used if use_t is true.\\n        ci_func : None or callable\\n            User provided function to compute confidence intervals.\\n            This is not used yet and will allow using non-standard confidence\\n            intervals.\\n\\n        Returns\\n        -------\\n        ci_eff : tuple of ndarrays\\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\\n            sample.\\n\\n        Notes\\n        -----\\n        CombineResults currently only has information from the combine_effects\\n        function, which does not provide details about individual samples.\\n        '\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff",
            "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'confidence intervals for the effect size estimate of samples\\n\\n        Additional information needs to be provided for confidence intervals\\n        that are not based on normal distribution using available variance.\\n        This is likely to change in future.\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        nobs : None or float\\n            Number of observations used for degrees of freedom computation.\\n            Only used if use_t is true.\\n        ci_func : None or callable\\n            User provided function to compute confidence intervals.\\n            This is not used yet and will allow using non-standard confidence\\n            intervals.\\n\\n        Returns\\n        -------\\n        ci_eff : tuple of ndarrays\\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\\n            sample.\\n\\n        Notes\\n        -----\\n        CombineResults currently only has information from the combine_effects\\n        function, which does not provide details about individual samples.\\n        '\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff",
            "def conf_int_samples(self, alpha=0.05, use_t=None, nobs=None, ci_func=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'confidence intervals for the effect size estimate of samples\\n\\n        Additional information needs to be provided for confidence intervals\\n        that are not based on normal distribution using available variance.\\n        This is likely to change in future.\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        nobs : None or float\\n            Number of observations used for degrees of freedom computation.\\n            Only used if use_t is true.\\n        ci_func : None or callable\\n            User provided function to compute confidence intervals.\\n            This is not used yet and will allow using non-standard confidence\\n            intervals.\\n\\n        Returns\\n        -------\\n        ci_eff : tuple of ndarrays\\n            Tuple (ci_low, ci_upp) with confidence interval computed for each\\n            sample.\\n\\n        Notes\\n        -----\\n        CombineResults currently only has information from the combine_effects\\n        function, which does not provide details about individual samples.\\n        '\n    if (alpha, use_t) in self.cache_ci:\n        return self.cache_ci[alpha, use_t]\n    if use_t is None:\n        use_t = self.use_t\n    if ci_func is not None:\n        kwds = {'use_t': use_t} if use_t is not None else {}\n        ci_eff = ci_func(alpha=alpha, **kwds)\n        self.ci_sample_distr = 'ci_func'\n    else:\n        if use_t is False:\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        elif nobs is not None:\n            df_resid = nobs - 1\n            crit = stats.t.isf(alpha / 2, df_resid)\n            self.ci_sample_distr = 't'\n        else:\n            msg = '`use_t=True` requires `nobs` for each sample or `ci_func`. Using normal distribution for confidence interval of individual samples.'\n            import warnings\n            warnings.warn(msg)\n            crit = stats.norm.isf(alpha / 2)\n            self.ci_sample_distr = 'normal'\n        ci_low = self.eff - crit * self.sd_eff\n        ci_upp = self.eff + crit * self.sd_eff\n        ci_eff = (ci_low, ci_upp)\n    self.cache_ci[alpha, use_t] = ci_eff\n    return ci_eff"
        ]
    },
    {
        "func_name": "conf_int",
        "original": "def conf_int(self, alpha=0.05, use_t=None):\n    \"\"\"confidence interval for the overall mean estimate\n\n        Parameters\n        ----------\n        alpha : float in (0, 1)\n            Significance level for confidence interval. Nominal coverage is\n            ``1 - alpha``.\n        use_t : None or bool\n            If use_t is None, then the attribute `use_t` determines whether\n            normal or t-distribution is used for confidence intervals.\n            Specifying use_t overrides the attribute.\n            If use_t is false, then confidence intervals are based on the\n            normal distribution. If it is true, then the t-distribution is\n            used.\n\n        Returns\n        -------\n        ci_eff_fe : tuple of floats\n            Confidence interval for mean effects size based on fixed effects\n            model with scale=1.\n        ci_eff_re : tuple of floats\n            Confidence interval for mean effects size based on random effects\n            model with scale=1\n        ci_eff_fe_wls : tuple of floats\n            Confidence interval for mean effects size based on fixed effects\n            model with estimated scale corresponding to WLS, ie. HKSJ.\n        ci_eff_re_wls : tuple of floats\n            Confidence interval for mean effects size based on random effects\n            model with estimated scale corresponding to WLS, ie. HKSJ.\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\n            the estimated scale is 1.\n\n        \"\"\"\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)",
        "mutated": [
            "def conf_int(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n    'confidence interval for the overall mean estimate\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        ci_eff_fe : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with scale=1.\\n        ci_eff_re : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with scale=1\\n        ci_eff_fe_wls : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n        ci_eff_re_wls : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\\n            the estimated scale is 1.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)",
            "def conf_int(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'confidence interval for the overall mean estimate\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        ci_eff_fe : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with scale=1.\\n        ci_eff_re : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with scale=1\\n        ci_eff_fe_wls : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n        ci_eff_re_wls : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\\n            the estimated scale is 1.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)",
            "def conf_int(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'confidence interval for the overall mean estimate\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        ci_eff_fe : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with scale=1.\\n        ci_eff_re : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with scale=1\\n        ci_eff_fe_wls : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n        ci_eff_re_wls : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\\n            the estimated scale is 1.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)",
            "def conf_int(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'confidence interval for the overall mean estimate\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        ci_eff_fe : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with scale=1.\\n        ci_eff_re : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with scale=1\\n        ci_eff_fe_wls : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n        ci_eff_re_wls : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\\n            the estimated scale is 1.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)",
            "def conf_int(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'confidence interval for the overall mean estimate\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        ci_eff_fe : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with scale=1.\\n        ci_eff_re : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with scale=1\\n        ci_eff_fe_wls : tuple of floats\\n            Confidence interval for mean effects size based on fixed effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n        ci_eff_re_wls : tuple of floats\\n            Confidence interval for mean effects size based on random effects\\n            model with estimated scale corresponding to WLS, ie. HKSJ.\\n            If random effects method is fully iterated, i.e. Paule-Mandel, then\\n            the estimated scale is 1.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    if use_t is False:\n        crit = stats.norm.isf(alpha / 2)\n    else:\n        crit = stats.t.isf(alpha / 2, self.df_resid)\n    sgn = np.asarray([-1, 1])\n    m_fe = self.mean_effect_fe\n    m_re = self.mean_effect_re\n    ci_eff_fe = m_fe + sgn * crit * self.sd_eff_w_fe\n    ci_eff_re = m_re + sgn * crit * self.sd_eff_w_re\n    ci_eff_fe_wls = m_fe + sgn * crit * np.sqrt(self.var_hksj_fe)\n    ci_eff_re_wls = m_re + sgn * crit * np.sqrt(self.var_hksj_re)\n    return (ci_eff_fe, ci_eff_re, ci_eff_fe_wls, ci_eff_re_wls)"
        ]
    },
    {
        "func_name": "test_homogeneity",
        "original": "def test_homogeneity(self):\n    \"\"\"Test whether the means of all samples are the same\n\n        currently no options, test uses chisquare distribution\n        default might change depending on `use_t`\n\n        Returns\n        -------\n        res : HolderTuple instance\n            The results include the following attributes:\n\n            - statistic : float\n                Test statistic, ``q`` in meta-analysis, this is the\n                pearson_chi2 statistic for the fixed effects model.\n            - pvalue : float\n                P-value based on chisquare distribution.\n            - df : float\n                Degrees of freedom, equal to number of studies or samples\n                minus 1.\n        \"\"\"\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res",
        "mutated": [
            "def test_homogeneity(self):\n    if False:\n        i = 10\n    'Test whether the means of all samples are the same\\n\\n        currently no options, test uses chisquare distribution\\n        default might change depending on `use_t`\\n\\n        Returns\\n        -------\\n        res : HolderTuple instance\\n            The results include the following attributes:\\n\\n            - statistic : float\\n                Test statistic, ``q`` in meta-analysis, this is the\\n                pearson_chi2 statistic for the fixed effects model.\\n            - pvalue : float\\n                P-value based on chisquare distribution.\\n            - df : float\\n                Degrees of freedom, equal to number of studies or samples\\n                minus 1.\\n        '\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res",
            "def test_homogeneity(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test whether the means of all samples are the same\\n\\n        currently no options, test uses chisquare distribution\\n        default might change depending on `use_t`\\n\\n        Returns\\n        -------\\n        res : HolderTuple instance\\n            The results include the following attributes:\\n\\n            - statistic : float\\n                Test statistic, ``q`` in meta-analysis, this is the\\n                pearson_chi2 statistic for the fixed effects model.\\n            - pvalue : float\\n                P-value based on chisquare distribution.\\n            - df : float\\n                Degrees of freedom, equal to number of studies or samples\\n                minus 1.\\n        '\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res",
            "def test_homogeneity(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test whether the means of all samples are the same\\n\\n        currently no options, test uses chisquare distribution\\n        default might change depending on `use_t`\\n\\n        Returns\\n        -------\\n        res : HolderTuple instance\\n            The results include the following attributes:\\n\\n            - statistic : float\\n                Test statistic, ``q`` in meta-analysis, this is the\\n                pearson_chi2 statistic for the fixed effects model.\\n            - pvalue : float\\n                P-value based on chisquare distribution.\\n            - df : float\\n                Degrees of freedom, equal to number of studies or samples\\n                minus 1.\\n        '\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res",
            "def test_homogeneity(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test whether the means of all samples are the same\\n\\n        currently no options, test uses chisquare distribution\\n        default might change depending on `use_t`\\n\\n        Returns\\n        -------\\n        res : HolderTuple instance\\n            The results include the following attributes:\\n\\n            - statistic : float\\n                Test statistic, ``q`` in meta-analysis, this is the\\n                pearson_chi2 statistic for the fixed effects model.\\n            - pvalue : float\\n                P-value based on chisquare distribution.\\n            - df : float\\n                Degrees of freedom, equal to number of studies or samples\\n                minus 1.\\n        '\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res",
            "def test_homogeneity(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test whether the means of all samples are the same\\n\\n        currently no options, test uses chisquare distribution\\n        default might change depending on `use_t`\\n\\n        Returns\\n        -------\\n        res : HolderTuple instance\\n            The results include the following attributes:\\n\\n            - statistic : float\\n                Test statistic, ``q`` in meta-analysis, this is the\\n                pearson_chi2 statistic for the fixed effects model.\\n            - pvalue : float\\n                P-value based on chisquare distribution.\\n            - df : float\\n                Degrees of freedom, equal to number of studies or samples\\n                minus 1.\\n        '\n    pvalue = stats.chi2.sf(self.q, self.k - 1)\n    res = HolderTuple(statistic=self.q, pvalue=pvalue, df=self.k - 1, distr='chi2')\n    return res"
        ]
    },
    {
        "func_name": "summary_array",
        "original": "def summary_array(self, alpha=0.05, use_t=None):\n    \"\"\"Create array with sample statistics and mean estimates\n\n        Parameters\n        ----------\n        alpha : float in (0, 1)\n            Significance level for confidence interval. Nominal coverage is\n            ``1 - alpha``.\n        use_t : None or bool\n            If use_t is None, then the attribute `use_t` determines whether\n            normal or t-distribution is used for confidence intervals.\n            Specifying use_t overrides the attribute.\n            If use_t is false, then confidence intervals are based on the\n            normal distribution. If it is true, then the t-distribution is\n            used.\n\n        Returns\n        -------\n        res : ndarray\n            Array with columns\n            ['eff', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\n            Rows include statistics for samples and estimates of overall mean.\n        column_names : list of str\n            The names for the columns, used when creating summary DataFrame.\n        \"\"\"\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)",
        "mutated": [
            "def summary_array(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n    'Create array with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : ndarray\\n            Array with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n        column_names : list of str\\n            The names for the columns, used when creating summary DataFrame.\\n        '\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)",
            "def summary_array(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create array with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : ndarray\\n            Array with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n        column_names : list of str\\n            The names for the columns, used when creating summary DataFrame.\\n        '\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)",
            "def summary_array(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create array with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : ndarray\\n            Array with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n        column_names : list of str\\n            The names for the columns, used when creating summary DataFrame.\\n        '\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)",
            "def summary_array(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create array with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : ndarray\\n            Array with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n        column_names : list of str\\n            The names for the columns, used when creating summary DataFrame.\\n        '\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)",
            "def summary_array(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create array with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : ndarray\\n            Array with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n        column_names : list of str\\n            The names for the columns, used when creating summary DataFrame.\\n        '\n    (ci_low, ci_upp) = self.conf_int_samples(alpha=alpha, use_t=use_t)\n    res = np.column_stack([self.eff, self.sd_eff, ci_low, ci_upp, self.weights_rel_fe, self.weights_rel_re])\n    ci = self.conf_int(alpha=alpha, use_t=use_t)\n    res_fe = [[self.mean_effect_fe, self.sd_eff_w_fe, ci[0][0], ci[0][1], 1, np.nan]]\n    res_re = [[self.mean_effect_re, self.sd_eff_w_re, ci[1][0], ci[1][1], np.nan, 1]]\n    res_fe_wls = [[self.mean_effect_fe, self.sd_eff_w_fe_hksj, ci[2][0], ci[2][1], 1, np.nan]]\n    res_re_wls = [[self.mean_effect_re, self.sd_eff_w_re_hksj, ci[3][0], ci[3][1], np.nan, 1]]\n    res = np.concatenate([res, res_fe, res_re, res_fe_wls, res_re_wls], axis=0)\n    column_names = ['eff', 'sd_eff', 'ci_low', 'ci_upp', 'w_fe', 'w_re']\n    return (res, column_names)"
        ]
    },
    {
        "func_name": "summary_frame",
        "original": "def summary_frame(self, alpha=0.05, use_t=None):\n    \"\"\"Create DataFrame with sample statistics and mean estimates\n\n        Parameters\n        ----------\n        alpha : float in (0, 1)\n            Significance level for confidence interval. Nominal coverage is\n            ``1 - alpha``.\n        use_t : None or bool\n            If use_t is None, then the attribute `use_t` determines whether\n            normal or t-distribution is used for confidence intervals.\n            Specifying use_t overrides the attribute.\n            If use_t is false, then confidence intervals are based on the\n            normal distribution. If it is true, then the t-distribution is\n            used.\n\n        Returns\n        -------\n        res : DataFrame\n            pandas DataFrame instance with columns\n            ['eff', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\n            Rows include statistics for samples and estimates of overall mean.\n\n        \"\"\"\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results",
        "mutated": [
            "def summary_frame(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n    'Create DataFrame with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : DataFrame\\n            pandas DataFrame instance with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results",
            "def summary_frame(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create DataFrame with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : DataFrame\\n            pandas DataFrame instance with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results",
            "def summary_frame(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create DataFrame with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : DataFrame\\n            pandas DataFrame instance with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results",
            "def summary_frame(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create DataFrame with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : DataFrame\\n            pandas DataFrame instance with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results",
            "def summary_frame(self, alpha=0.05, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create DataFrame with sample statistics and mean estimates\\n\\n        Parameters\\n        ----------\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n\\n        Returns\\n        -------\\n        res : DataFrame\\n            pandas DataFrame instance with columns\\n            [\\'eff\\', \"sd_eff\", \"ci_low\", \"ci_upp\", \"w_fe\",\"w_re\"].\\n            Rows include statistics for samples and estimates of overall mean.\\n\\n        '\n    if use_t is None:\n        use_t = self.use_t\n    labels = list(self.row_names) + ['fixed effect', 'random effect', 'fixed effect wls', 'random effect wls']\n    (res, col_names) = self.summary_array(alpha=alpha, use_t=use_t)\n    results = pd.DataFrame(res, index=labels, columns=col_names)\n    return results"
        ]
    },
    {
        "func_name": "plot_forest",
        "original": "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    \"\"\"Forest plot with means and confidence intervals\n\n        Parameters\n        ----------\n        ax : None or matplotlib axis instance\n            If ax is provided, then the plot will be added to it.\n        alpha : float in (0, 1)\n            Significance level for confidence interval. Nominal coverage is\n            ``1 - alpha``.\n        use_t : None or bool\n            If use_t is None, then the attribute `use_t` determines whether\n            normal or t-distribution is used for confidence intervals.\n            Specifying use_t overrides the attribute.\n            If use_t is false, then confidence intervals are based on the\n            normal distribution. If it is true, then the t-distribution is\n            used.\n        use_exp : bool\n            If `use_exp` is True, then the effect size and confidence limits\n            will be exponentiated. This transform log-odds-ration into\n            odds-ratio, and similarly for risk-ratio.\n        ax : AxesSubplot, optional\n            If given, this axes is used to plot in instead of a new figure\n            being created.\n        kwds : optional keyword arguments\n            Keywords are forwarded to the dot_plot function that creates the\n            plot.\n\n        Returns\n        -------\n        fig : Matplotlib figure instance\n\n        See Also\n        --------\n        dot_plot\n\n        \"\"\"\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig",
        "mutated": [
            "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    if False:\n        i = 10\n    'Forest plot with means and confidence intervals\\n\\n        Parameters\\n        ----------\\n        ax : None or matplotlib axis instance\\n            If ax is provided, then the plot will be added to it.\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        use_exp : bool\\n            If `use_exp` is True, then the effect size and confidence limits\\n            will be exponentiated. This transform log-odds-ration into\\n            odds-ratio, and similarly for risk-ratio.\\n        ax : AxesSubplot, optional\\n            If given, this axes is used to plot in instead of a new figure\\n            being created.\\n        kwds : optional keyword arguments\\n            Keywords are forwarded to the dot_plot function that creates the\\n            plot.\\n\\n        Returns\\n        -------\\n        fig : Matplotlib figure instance\\n\\n        See Also\\n        --------\\n        dot_plot\\n\\n        '\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig",
            "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forest plot with means and confidence intervals\\n\\n        Parameters\\n        ----------\\n        ax : None or matplotlib axis instance\\n            If ax is provided, then the plot will be added to it.\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        use_exp : bool\\n            If `use_exp` is True, then the effect size and confidence limits\\n            will be exponentiated. This transform log-odds-ration into\\n            odds-ratio, and similarly for risk-ratio.\\n        ax : AxesSubplot, optional\\n            If given, this axes is used to plot in instead of a new figure\\n            being created.\\n        kwds : optional keyword arguments\\n            Keywords are forwarded to the dot_plot function that creates the\\n            plot.\\n\\n        Returns\\n        -------\\n        fig : Matplotlib figure instance\\n\\n        See Also\\n        --------\\n        dot_plot\\n\\n        '\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig",
            "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forest plot with means and confidence intervals\\n\\n        Parameters\\n        ----------\\n        ax : None or matplotlib axis instance\\n            If ax is provided, then the plot will be added to it.\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        use_exp : bool\\n            If `use_exp` is True, then the effect size and confidence limits\\n            will be exponentiated. This transform log-odds-ration into\\n            odds-ratio, and similarly for risk-ratio.\\n        ax : AxesSubplot, optional\\n            If given, this axes is used to plot in instead of a new figure\\n            being created.\\n        kwds : optional keyword arguments\\n            Keywords are forwarded to the dot_plot function that creates the\\n            plot.\\n\\n        Returns\\n        -------\\n        fig : Matplotlib figure instance\\n\\n        See Also\\n        --------\\n        dot_plot\\n\\n        '\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig",
            "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forest plot with means and confidence intervals\\n\\n        Parameters\\n        ----------\\n        ax : None or matplotlib axis instance\\n            If ax is provided, then the plot will be added to it.\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        use_exp : bool\\n            If `use_exp` is True, then the effect size and confidence limits\\n            will be exponentiated. This transform log-odds-ration into\\n            odds-ratio, and similarly for risk-ratio.\\n        ax : AxesSubplot, optional\\n            If given, this axes is used to plot in instead of a new figure\\n            being created.\\n        kwds : optional keyword arguments\\n            Keywords are forwarded to the dot_plot function that creates the\\n            plot.\\n\\n        Returns\\n        -------\\n        fig : Matplotlib figure instance\\n\\n        See Also\\n        --------\\n        dot_plot\\n\\n        '\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig",
            "def plot_forest(self, alpha=0.05, use_t=None, use_exp=False, ax=None, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forest plot with means and confidence intervals\\n\\n        Parameters\\n        ----------\\n        ax : None or matplotlib axis instance\\n            If ax is provided, then the plot will be added to it.\\n        alpha : float in (0, 1)\\n            Significance level for confidence interval. Nominal coverage is\\n            ``1 - alpha``.\\n        use_t : None or bool\\n            If use_t is None, then the attribute `use_t` determines whether\\n            normal or t-distribution is used for confidence intervals.\\n            Specifying use_t overrides the attribute.\\n            If use_t is false, then confidence intervals are based on the\\n            normal distribution. If it is true, then the t-distribution is\\n            used.\\n        use_exp : bool\\n            If `use_exp` is True, then the effect size and confidence limits\\n            will be exponentiated. This transform log-odds-ration into\\n            odds-ratio, and similarly for risk-ratio.\\n        ax : AxesSubplot, optional\\n            If given, this axes is used to plot in instead of a new figure\\n            being created.\\n        kwds : optional keyword arguments\\n            Keywords are forwarded to the dot_plot function that creates the\\n            plot.\\n\\n        Returns\\n        -------\\n        fig : Matplotlib figure instance\\n\\n        See Also\\n        --------\\n        dot_plot\\n\\n        '\n    from statsmodels.graphics.dotplots import dot_plot\n    res_df = self.summary_frame(alpha=alpha, use_t=use_t)\n    if use_exp:\n        res_df = np.exp(res_df[['eff', 'ci_low', 'ci_upp']])\n    hw = np.abs(res_df[['ci_low', 'ci_upp']] - res_df[['eff']].values)\n    fig = dot_plot(points=res_df['eff'], intervals=hw, lines=res_df.index, line_order=res_df.index, **kwds)\n    return fig"
        ]
    },
    {
        "func_name": "effectsize_smd",
        "original": "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    \"\"\"effect sizes for mean difference for use in meta-analysis\n\n    mean1, sd1, nobs1 are for treatment\n    mean2, sd2, nobs2 are for control\n\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\n    standardized by an estimate of the within variance.\n\n    This does not have option yet.\n    It uses standardized mean difference with bias correction as effect size.\n\n    This currently does not use np.asarray, all computations are possible in\n    pandas.\n\n    Parameters\n    ----------\n    mean1 : array\n        mean of second sample, treatment groups\n    sd1 : array\n        standard deviation of residuals in treatment groups, within\n    nobs1 : array\n        number of observations in treatment groups\n    mean2, sd2, nobs2 : arrays\n        mean, standard deviation and number of observations of control groups\n\n    Returns\n    -------\n    smd_bc : array\n        bias corrected estimate of standardized mean difference\n    var_smdbc : array\n        estimate of variance of smd_bc\n\n    Notes\n    -----\n    Status: API will still change. This is currently intended for support of\n    meta-analysis.\n\n    References\n    ----------\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\n        Chichester: Wiley.\n\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\n        Chapman & Hall/CRC Biostatistics Series.\n        Boca Raton: CRC Press/Taylor & Francis Group.\n\n    \"\"\"\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)",
        "mutated": [
            "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    if False:\n        i = 10\n    'effect sizes for mean difference for use in meta-analysis\\n\\n    mean1, sd1, nobs1 are for treatment\\n    mean2, sd2, nobs2 are for control\\n\\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\\n    standardized by an estimate of the within variance.\\n\\n    This does not have option yet.\\n    It uses standardized mean difference with bias correction as effect size.\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    mean1 : array\\n        mean of second sample, treatment groups\\n    sd1 : array\\n        standard deviation of residuals in treatment groups, within\\n    nobs1 : array\\n        number of observations in treatment groups\\n    mean2, sd2, nobs2 : arrays\\n        mean, standard deviation and number of observations of control groups\\n\\n    Returns\\n    -------\\n    smd_bc : array\\n        bias corrected estimate of standardized mean difference\\n    var_smdbc : array\\n        estimate of variance of smd_bc\\n\\n    Notes\\n    -----\\n    Status: API will still change. This is currently intended for support of\\n    meta-analysis.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)",
            "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'effect sizes for mean difference for use in meta-analysis\\n\\n    mean1, sd1, nobs1 are for treatment\\n    mean2, sd2, nobs2 are for control\\n\\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\\n    standardized by an estimate of the within variance.\\n\\n    This does not have option yet.\\n    It uses standardized mean difference with bias correction as effect size.\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    mean1 : array\\n        mean of second sample, treatment groups\\n    sd1 : array\\n        standard deviation of residuals in treatment groups, within\\n    nobs1 : array\\n        number of observations in treatment groups\\n    mean2, sd2, nobs2 : arrays\\n        mean, standard deviation and number of observations of control groups\\n\\n    Returns\\n    -------\\n    smd_bc : array\\n        bias corrected estimate of standardized mean difference\\n    var_smdbc : array\\n        estimate of variance of smd_bc\\n\\n    Notes\\n    -----\\n    Status: API will still change. This is currently intended for support of\\n    meta-analysis.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)",
            "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'effect sizes for mean difference for use in meta-analysis\\n\\n    mean1, sd1, nobs1 are for treatment\\n    mean2, sd2, nobs2 are for control\\n\\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\\n    standardized by an estimate of the within variance.\\n\\n    This does not have option yet.\\n    It uses standardized mean difference with bias correction as effect size.\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    mean1 : array\\n        mean of second sample, treatment groups\\n    sd1 : array\\n        standard deviation of residuals in treatment groups, within\\n    nobs1 : array\\n        number of observations in treatment groups\\n    mean2, sd2, nobs2 : arrays\\n        mean, standard deviation and number of observations of control groups\\n\\n    Returns\\n    -------\\n    smd_bc : array\\n        bias corrected estimate of standardized mean difference\\n    var_smdbc : array\\n        estimate of variance of smd_bc\\n\\n    Notes\\n    -----\\n    Status: API will still change. This is currently intended for support of\\n    meta-analysis.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)",
            "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'effect sizes for mean difference for use in meta-analysis\\n\\n    mean1, sd1, nobs1 are for treatment\\n    mean2, sd2, nobs2 are for control\\n\\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\\n    standardized by an estimate of the within variance.\\n\\n    This does not have option yet.\\n    It uses standardized mean difference with bias correction as effect size.\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    mean1 : array\\n        mean of second sample, treatment groups\\n    sd1 : array\\n        standard deviation of residuals in treatment groups, within\\n    nobs1 : array\\n        number of observations in treatment groups\\n    mean2, sd2, nobs2 : arrays\\n        mean, standard deviation and number of observations of control groups\\n\\n    Returns\\n    -------\\n    smd_bc : array\\n        bias corrected estimate of standardized mean difference\\n    var_smdbc : array\\n        estimate of variance of smd_bc\\n\\n    Notes\\n    -----\\n    Status: API will still change. This is currently intended for support of\\n    meta-analysis.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)",
            "def effectsize_smd(mean1, sd1, nobs1, mean2, sd2, nobs2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'effect sizes for mean difference for use in meta-analysis\\n\\n    mean1, sd1, nobs1 are for treatment\\n    mean2, sd2, nobs2 are for control\\n\\n    Effect sizes are computed for the mean difference ``mean1 - mean2``\\n    standardized by an estimate of the within variance.\\n\\n    This does not have option yet.\\n    It uses standardized mean difference with bias correction as effect size.\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    mean1 : array\\n        mean of second sample, treatment groups\\n    sd1 : array\\n        standard deviation of residuals in treatment groups, within\\n    nobs1 : array\\n        number of observations in treatment groups\\n    mean2, sd2, nobs2 : arrays\\n        mean, standard deviation and number of observations of control groups\\n\\n    Returns\\n    -------\\n    smd_bc : array\\n        bias corrected estimate of standardized mean difference\\n    var_smdbc : array\\n        estimate of variance of smd_bc\\n\\n    Notes\\n    -----\\n    Status: API will still change. This is currently intended for support of\\n    meta-analysis.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    var_diff = (sd1 ** 2 * (nobs1 - 1) + sd2 ** 2 * (nobs2 - 1)) / (nobs1 + nobs2 - 2)\n    sd_diff = np.sqrt(var_diff)\n    nobs = nobs1 + nobs2\n    bias_correction = 1 - 3 / (4 * nobs - 9)\n    smd = (mean1 - mean2) / sd_diff\n    smd_bc = bias_correction * smd\n    var_smdbc = nobs / nobs1 / nobs2 + smd_bc ** 2 / 2 / (nobs - 3.94)\n    return (smd_bc, var_smdbc)"
        ]
    },
    {
        "func_name": "effectsize_2proportions",
        "original": "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    \"\"\"Effects sizes for two sample binomial proportions\n\n    Parameters\n    ----------\n    count1, nobs1, count2, nobs2 : array_like\n        data for two samples\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\n        statistic for the comparison of two proportions\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\n    zero_correction : {None, float, \"tac\", \"clip\"}\n        Some statistics are not finite when zero counts are in the data.\n        The options to remove zeros are:\n\n        * float : if zero_correction is a single float, then it will be added\n          to all count (cells) if the sample has any zeros.\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\n          section 3.2\n        * \"clip\" : clip proportions without adding a value to all cells\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\n\n    zero_kwds : dict\n        additional options to handle zero counts\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\n        other options not yet implemented\n\n    Returns\n    -------\n    effect size : array\n        Effect size for each sample.\n    var_es : array\n        Estimate of variance of the effect size\n\n    Notes\n    -----\n    Status: API is experimental, Options for zero handling is incomplete.\n\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\n    and \"as\".\n\n    The statistics are defined as:\n\n     - risk difference = p1 - p2\n     - log risk ratio = log(p1 / p2)\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\n\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\n    sample 2 (control).\n\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\n    `rr` using `exp` function.\n\n    See Also\n    --------\n    statsmodels.stats.contingency_tables\n    \"\"\"\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)",
        "mutated": [
            "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    if False:\n        i = 10\n    'Effects sizes for two sample binomial proportions\\n\\n    Parameters\\n    ----------\\n    count1, nobs1, count2, nobs2 : array_like\\n        data for two samples\\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\\n        statistic for the comparison of two proportions\\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\\n    zero_correction : {None, float, \"tac\", \"clip\"}\\n        Some statistics are not finite when zero counts are in the data.\\n        The options to remove zeros are:\\n\\n        * float : if zero_correction is a single float, then it will be added\\n          to all count (cells) if the sample has any zeros.\\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\\n          section 3.2\\n        * \"clip\" : clip proportions without adding a value to all cells\\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\\n\\n    zero_kwds : dict\\n        additional options to handle zero counts\\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\\n        other options not yet implemented\\n\\n    Returns\\n    -------\\n    effect size : array\\n        Effect size for each sample.\\n    var_es : array\\n        Estimate of variance of the effect size\\n\\n    Notes\\n    -----\\n    Status: API is experimental, Options for zero handling is incomplete.\\n\\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\\n    and \"as\".\\n\\n    The statistics are defined as:\\n\\n     - risk difference = p1 - p2\\n     - log risk ratio = log(p1 / p2)\\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\\n\\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\\n    sample 2 (control).\\n\\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\\n    `rr` using `exp` function.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.contingency_tables\\n    '\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)",
            "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Effects sizes for two sample binomial proportions\\n\\n    Parameters\\n    ----------\\n    count1, nobs1, count2, nobs2 : array_like\\n        data for two samples\\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\\n        statistic for the comparison of two proportions\\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\\n    zero_correction : {None, float, \"tac\", \"clip\"}\\n        Some statistics are not finite when zero counts are in the data.\\n        The options to remove zeros are:\\n\\n        * float : if zero_correction is a single float, then it will be added\\n          to all count (cells) if the sample has any zeros.\\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\\n          section 3.2\\n        * \"clip\" : clip proportions without adding a value to all cells\\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\\n\\n    zero_kwds : dict\\n        additional options to handle zero counts\\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\\n        other options not yet implemented\\n\\n    Returns\\n    -------\\n    effect size : array\\n        Effect size for each sample.\\n    var_es : array\\n        Estimate of variance of the effect size\\n\\n    Notes\\n    -----\\n    Status: API is experimental, Options for zero handling is incomplete.\\n\\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\\n    and \"as\".\\n\\n    The statistics are defined as:\\n\\n     - risk difference = p1 - p2\\n     - log risk ratio = log(p1 / p2)\\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\\n\\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\\n    sample 2 (control).\\n\\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\\n    `rr` using `exp` function.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.contingency_tables\\n    '\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)",
            "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Effects sizes for two sample binomial proportions\\n\\n    Parameters\\n    ----------\\n    count1, nobs1, count2, nobs2 : array_like\\n        data for two samples\\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\\n        statistic for the comparison of two proportions\\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\\n    zero_correction : {None, float, \"tac\", \"clip\"}\\n        Some statistics are not finite when zero counts are in the data.\\n        The options to remove zeros are:\\n\\n        * float : if zero_correction is a single float, then it will be added\\n          to all count (cells) if the sample has any zeros.\\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\\n          section 3.2\\n        * \"clip\" : clip proportions without adding a value to all cells\\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\\n\\n    zero_kwds : dict\\n        additional options to handle zero counts\\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\\n        other options not yet implemented\\n\\n    Returns\\n    -------\\n    effect size : array\\n        Effect size for each sample.\\n    var_es : array\\n        Estimate of variance of the effect size\\n\\n    Notes\\n    -----\\n    Status: API is experimental, Options for zero handling is incomplete.\\n\\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\\n    and \"as\".\\n\\n    The statistics are defined as:\\n\\n     - risk difference = p1 - p2\\n     - log risk ratio = log(p1 / p2)\\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\\n\\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\\n    sample 2 (control).\\n\\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\\n    `rr` using `exp` function.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.contingency_tables\\n    '\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)",
            "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Effects sizes for two sample binomial proportions\\n\\n    Parameters\\n    ----------\\n    count1, nobs1, count2, nobs2 : array_like\\n        data for two samples\\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\\n        statistic for the comparison of two proportions\\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\\n    zero_correction : {None, float, \"tac\", \"clip\"}\\n        Some statistics are not finite when zero counts are in the data.\\n        The options to remove zeros are:\\n\\n        * float : if zero_correction is a single float, then it will be added\\n          to all count (cells) if the sample has any zeros.\\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\\n          section 3.2\\n        * \"clip\" : clip proportions without adding a value to all cells\\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\\n\\n    zero_kwds : dict\\n        additional options to handle zero counts\\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\\n        other options not yet implemented\\n\\n    Returns\\n    -------\\n    effect size : array\\n        Effect size for each sample.\\n    var_es : array\\n        Estimate of variance of the effect size\\n\\n    Notes\\n    -----\\n    Status: API is experimental, Options for zero handling is incomplete.\\n\\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\\n    and \"as\".\\n\\n    The statistics are defined as:\\n\\n     - risk difference = p1 - p2\\n     - log risk ratio = log(p1 / p2)\\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\\n\\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\\n    sample 2 (control).\\n\\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\\n    `rr` using `exp` function.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.contingency_tables\\n    '\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)",
            "def effectsize_2proportions(count1, nobs1, count2, nobs2, statistic='diff', zero_correction=None, zero_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Effects sizes for two sample binomial proportions\\n\\n    Parameters\\n    ----------\\n    count1, nobs1, count2, nobs2 : array_like\\n        data for two samples\\n    statistic : {\"diff\", \"odds-ratio\", \"risk-ratio\", \"arcsine\"}\\n        statistic for the comparison of two proportions\\n        Effect sizes for \"odds-ratio\" and \"risk-ratio\" are in logarithm.\\n    zero_correction : {None, float, \"tac\", \"clip\"}\\n        Some statistics are not finite when zero counts are in the data.\\n        The options to remove zeros are:\\n\\n        * float : if zero_correction is a single float, then it will be added\\n          to all count (cells) if the sample has any zeros.\\n        * \"tac\" : treatment arm continuity correction see Ruecker et al 2009,\\n          section 3.2\\n        * \"clip\" : clip proportions without adding a value to all cells\\n          The clip bounds can be set with zero_kwds[\"clip_bounds\"]\\n\\n    zero_kwds : dict\\n        additional options to handle zero counts\\n        \"clip_bounds\" tuple, default (1e-6, 1 - 1e-6) if zero_correction=\"clip\"\\n        other options not yet implemented\\n\\n    Returns\\n    -------\\n    effect size : array\\n        Effect size for each sample.\\n    var_es : array\\n        Estimate of variance of the effect size\\n\\n    Notes\\n    -----\\n    Status: API is experimental, Options for zero handling is incomplete.\\n\\n    The names for ``statistics`` keyword can be shortened to \"rd\", \"rr\", \"or\"\\n    and \"as\".\\n\\n    The statistics are defined as:\\n\\n     - risk difference = p1 - p2\\n     - log risk ratio = log(p1 / p2)\\n     - log odds_ratio = log(p1 / (1 - p1) * (1 - p2) / p2)\\n     - arcsine-sqrt = arcsin(sqrt(p1)) - arcsin(sqrt(p2))\\n\\n    where p1 and p2 are the estimated proportions in sample 1 (treatment) and\\n    sample 2 (control).\\n\\n    log-odds-ratio and log-risk-ratio can be transformed back to ``or`` and\\n    `rr` using `exp` function.\\n\\n    See Also\\n    --------\\n    statsmodels.stats.contingency_tables\\n    '\n    if zero_correction is None:\n        cc1 = cc2 = 0\n    elif zero_correction == 'tac':\n        nobs_t = nobs1 + nobs2\n        cc1 = nobs2 / nobs_t\n        cc2 = nobs1 / nobs_t\n    elif zero_correction == 'clip':\n        clip_bounds = zero_kwds.get('clip_bounds', (1e-06, 1 - 1e-06))\n        cc1 = cc2 = 0\n    elif zero_correction:\n        cc1 = cc2 = zero_correction\n    else:\n        msg = 'zero_correction not recognized or supported'\n        raise NotImplementedError(msg)\n    zero_mask1 = (count1 == 0) | (count1 == nobs1)\n    zero_mask2 = (count2 == 0) | (count2 == nobs2)\n    zmask = np.logical_or(zero_mask1, zero_mask2)\n    n1 = nobs1 + (cc1 + cc2) * zmask\n    n2 = nobs2 + (cc1 + cc2) * zmask\n    p1 = (count1 + cc1) / n1\n    p2 = (count2 + cc2) / n2\n    if zero_correction == 'clip':\n        p1 = np.clip(p1, *clip_bounds)\n        p2 = np.clip(p2, *clip_bounds)\n    if statistic in ['diff', 'rd']:\n        rd = p1 - p2\n        rd_var = p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2\n        eff = rd\n        var_eff = rd_var\n    elif statistic in ['risk-ratio', 'rr']:\n        log_rr = np.log(p1) - np.log(p2)\n        log_rr_var = (1 - p1) / p1 / n1 + (1 - p2) / p2 / n2\n        eff = log_rr\n        var_eff = log_rr_var\n    elif statistic in ['odds-ratio', 'or']:\n        log_or = np.log(p1) - np.log(1 - p1) - np.log(p2) + np.log(1 - p2)\n        log_or_var = 1 / (p1 * (1 - p1) * n1) + 1 / (p2 * (1 - p2) * n2)\n        eff = log_or\n        var_eff = log_or_var\n    elif statistic in ['arcsine', 'arcsin', 'as']:\n        as_ = np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2))\n        as_var = (1 / n1 + 1 / n2) / 4\n        eff = as_\n        var_eff = as_var\n    else:\n        msg = 'statistic not recognized, use one of \"rd\", \"rr\", \"or\", \"as\"'\n        raise NotImplementedError(msg)\n    return (eff, var_eff)"
        ]
    },
    {
        "func_name": "combine_effects",
        "original": "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    \"\"\"combining effect sizes for effect sizes using meta-analysis\n\n    This currently does not use np.asarray, all computations are possible in\n    pandas.\n\n    Parameters\n    ----------\n    effect : array\n        mean of effect size measure for all samples\n    variance : array\n        variance of mean or effect size measure for all samples\n    method_re : {\"iterated\", \"chi2\"}\n        method that is use to compute the between random effects variance\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\n        estimate the random effects variance. Options for the iteration can\n        be provided in the ``kwds``\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\n    row_names : list of strings (optional)\n        names for samples or studies, will be included in results summary and\n        table.\n    alpha : float in (0, 1)\n        significance level, default is 0.05, for the confidence intervals\n\n    Returns\n    -------\n    results : CombineResults\n        Contains estimation results and intermediate statistics, and includes\n        a method to return a summary table.\n        Statistics from intermediate calculations might be removed at a later\n        time.\n\n    Notes\n    -----\n    Status: Basic functionality is verified, mainly compared to R metafor\n    package. However, API might still change.\n\n    This computes both fixed effects and random effects estimates. The\n    random effects results depend on the method to estimate the RE variance.\n\n    Scale estimate\n    In fixed effects models and in random effects models without fully\n    iterated random effects variance, the model will in general not account\n    for all residual variance. Traditional meta-analysis uses a fixed\n    scale equal to 1, that might not produce test statistics and\n    confidence intervals with the correct size. Estimating the scale to account\n    for residual variance often improves the small sample properties of\n    inference and confidence intervals.\n    This adjustment to the standard errors is often referred to as HKSJ\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\n    However, this is equivalent to estimating the scale in WLS.\n    The results instance includes both, fixed scale and estimated scale\n    versions of standard errors and confidence intervals.\n\n    References\n    ----------\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\n        Chichester: Wiley.\n\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\n        Chapman & Hall/CRC Biostatistics Series.\n        Boca Raton: CRC Press/Taylor & Francis Group.\n\n    \"\"\"\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res",
        "mutated": [
            "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    if False:\n        i = 10\n    'combining effect sizes for effect sizes using meta-analysis\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    effect : array\\n        mean of effect size measure for all samples\\n    variance : array\\n        variance of mean or effect size measure for all samples\\n    method_re : {\"iterated\", \"chi2\"}\\n        method that is use to compute the between random effects variance\\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\\n        estimate the random effects variance. Options for the iteration can\\n        be provided in the ``kwds``\\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\\n    row_names : list of strings (optional)\\n        names for samples or studies, will be included in results summary and\\n        table.\\n    alpha : float in (0, 1)\\n        significance level, default is 0.05, for the confidence intervals\\n\\n    Returns\\n    -------\\n    results : CombineResults\\n        Contains estimation results and intermediate statistics, and includes\\n        a method to return a summary table.\\n        Statistics from intermediate calculations might be removed at a later\\n        time.\\n\\n    Notes\\n    -----\\n    Status: Basic functionality is verified, mainly compared to R metafor\\n    package. However, API might still change.\\n\\n    This computes both fixed effects and random effects estimates. The\\n    random effects results depend on the method to estimate the RE variance.\\n\\n    Scale estimate\\n    In fixed effects models and in random effects models without fully\\n    iterated random effects variance, the model will in general not account\\n    for all residual variance. Traditional meta-analysis uses a fixed\\n    scale equal to 1, that might not produce test statistics and\\n    confidence intervals with the correct size. Estimating the scale to account\\n    for residual variance often improves the small sample properties of\\n    inference and confidence intervals.\\n    This adjustment to the standard errors is often referred to as HKSJ\\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\\n    However, this is equivalent to estimating the scale in WLS.\\n    The results instance includes both, fixed scale and estimated scale\\n    versions of standard errors and confidence intervals.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res",
            "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'combining effect sizes for effect sizes using meta-analysis\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    effect : array\\n        mean of effect size measure for all samples\\n    variance : array\\n        variance of mean or effect size measure for all samples\\n    method_re : {\"iterated\", \"chi2\"}\\n        method that is use to compute the between random effects variance\\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\\n        estimate the random effects variance. Options for the iteration can\\n        be provided in the ``kwds``\\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\\n    row_names : list of strings (optional)\\n        names for samples or studies, will be included in results summary and\\n        table.\\n    alpha : float in (0, 1)\\n        significance level, default is 0.05, for the confidence intervals\\n\\n    Returns\\n    -------\\n    results : CombineResults\\n        Contains estimation results and intermediate statistics, and includes\\n        a method to return a summary table.\\n        Statistics from intermediate calculations might be removed at a later\\n        time.\\n\\n    Notes\\n    -----\\n    Status: Basic functionality is verified, mainly compared to R metafor\\n    package. However, API might still change.\\n\\n    This computes both fixed effects and random effects estimates. The\\n    random effects results depend on the method to estimate the RE variance.\\n\\n    Scale estimate\\n    In fixed effects models and in random effects models without fully\\n    iterated random effects variance, the model will in general not account\\n    for all residual variance. Traditional meta-analysis uses a fixed\\n    scale equal to 1, that might not produce test statistics and\\n    confidence intervals with the correct size. Estimating the scale to account\\n    for residual variance often improves the small sample properties of\\n    inference and confidence intervals.\\n    This adjustment to the standard errors is often referred to as HKSJ\\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\\n    However, this is equivalent to estimating the scale in WLS.\\n    The results instance includes both, fixed scale and estimated scale\\n    versions of standard errors and confidence intervals.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res",
            "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'combining effect sizes for effect sizes using meta-analysis\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    effect : array\\n        mean of effect size measure for all samples\\n    variance : array\\n        variance of mean or effect size measure for all samples\\n    method_re : {\"iterated\", \"chi2\"}\\n        method that is use to compute the between random effects variance\\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\\n        estimate the random effects variance. Options for the iteration can\\n        be provided in the ``kwds``\\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\\n    row_names : list of strings (optional)\\n        names for samples or studies, will be included in results summary and\\n        table.\\n    alpha : float in (0, 1)\\n        significance level, default is 0.05, for the confidence intervals\\n\\n    Returns\\n    -------\\n    results : CombineResults\\n        Contains estimation results and intermediate statistics, and includes\\n        a method to return a summary table.\\n        Statistics from intermediate calculations might be removed at a later\\n        time.\\n\\n    Notes\\n    -----\\n    Status: Basic functionality is verified, mainly compared to R metafor\\n    package. However, API might still change.\\n\\n    This computes both fixed effects and random effects estimates. The\\n    random effects results depend on the method to estimate the RE variance.\\n\\n    Scale estimate\\n    In fixed effects models and in random effects models without fully\\n    iterated random effects variance, the model will in general not account\\n    for all residual variance. Traditional meta-analysis uses a fixed\\n    scale equal to 1, that might not produce test statistics and\\n    confidence intervals with the correct size. Estimating the scale to account\\n    for residual variance often improves the small sample properties of\\n    inference and confidence intervals.\\n    This adjustment to the standard errors is often referred to as HKSJ\\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\\n    However, this is equivalent to estimating the scale in WLS.\\n    The results instance includes both, fixed scale and estimated scale\\n    versions of standard errors and confidence intervals.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res",
            "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'combining effect sizes for effect sizes using meta-analysis\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    effect : array\\n        mean of effect size measure for all samples\\n    variance : array\\n        variance of mean or effect size measure for all samples\\n    method_re : {\"iterated\", \"chi2\"}\\n        method that is use to compute the between random effects variance\\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\\n        estimate the random effects variance. Options for the iteration can\\n        be provided in the ``kwds``\\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\\n    row_names : list of strings (optional)\\n        names for samples or studies, will be included in results summary and\\n        table.\\n    alpha : float in (0, 1)\\n        significance level, default is 0.05, for the confidence intervals\\n\\n    Returns\\n    -------\\n    results : CombineResults\\n        Contains estimation results and intermediate statistics, and includes\\n        a method to return a summary table.\\n        Statistics from intermediate calculations might be removed at a later\\n        time.\\n\\n    Notes\\n    -----\\n    Status: Basic functionality is verified, mainly compared to R metafor\\n    package. However, API might still change.\\n\\n    This computes both fixed effects and random effects estimates. The\\n    random effects results depend on the method to estimate the RE variance.\\n\\n    Scale estimate\\n    In fixed effects models and in random effects models without fully\\n    iterated random effects variance, the model will in general not account\\n    for all residual variance. Traditional meta-analysis uses a fixed\\n    scale equal to 1, that might not produce test statistics and\\n    confidence intervals with the correct size. Estimating the scale to account\\n    for residual variance often improves the small sample properties of\\n    inference and confidence intervals.\\n    This adjustment to the standard errors is often referred to as HKSJ\\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\\n    However, this is equivalent to estimating the scale in WLS.\\n    The results instance includes both, fixed scale and estimated scale\\n    versions of standard errors and confidence intervals.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res",
            "def combine_effects(effect, variance, method_re='iterated', row_names=None, use_t=False, alpha=0.05, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'combining effect sizes for effect sizes using meta-analysis\\n\\n    This currently does not use np.asarray, all computations are possible in\\n    pandas.\\n\\n    Parameters\\n    ----------\\n    effect : array\\n        mean of effect size measure for all samples\\n    variance : array\\n        variance of mean or effect size measure for all samples\\n    method_re : {\"iterated\", \"chi2\"}\\n        method that is use to compute the between random effects variance\\n        \"iterated\" or \"pm\" uses Paule and Mandel method to iteratively\\n        estimate the random effects variance. Options for the iteration can\\n        be provided in the ``kwds``\\n        \"chi2\" or \"dl\" uses DerSimonian and Laird one-step estimator.\\n    row_names : list of strings (optional)\\n        names for samples or studies, will be included in results summary and\\n        table.\\n    alpha : float in (0, 1)\\n        significance level, default is 0.05, for the confidence intervals\\n\\n    Returns\\n    -------\\n    results : CombineResults\\n        Contains estimation results and intermediate statistics, and includes\\n        a method to return a summary table.\\n        Statistics from intermediate calculations might be removed at a later\\n        time.\\n\\n    Notes\\n    -----\\n    Status: Basic functionality is verified, mainly compared to R metafor\\n    package. However, API might still change.\\n\\n    This computes both fixed effects and random effects estimates. The\\n    random effects results depend on the method to estimate the RE variance.\\n\\n    Scale estimate\\n    In fixed effects models and in random effects models without fully\\n    iterated random effects variance, the model will in general not account\\n    for all residual variance. Traditional meta-analysis uses a fixed\\n    scale equal to 1, that might not produce test statistics and\\n    confidence intervals with the correct size. Estimating the scale to account\\n    for residual variance often improves the small sample properties of\\n    inference and confidence intervals.\\n    This adjustment to the standard errors is often referred to as HKSJ\\n    method based attributed to Hartung and Knapp and Sidik and Jonkman.\\n    However, this is equivalent to estimating the scale in WLS.\\n    The results instance includes both, fixed scale and estimated scale\\n    versions of standard errors and confidence intervals.\\n\\n    References\\n    ----------\\n    Borenstein, Michael. 2009. Introduction to Meta-Analysis.\\n        Chichester: Wiley.\\n\\n    Chen, Ding-Geng, and Karl E. Peace. 2013. Applied Meta-Analysis with R.\\n        Chapman & Hall/CRC Biostatistics Series.\\n        Boca Raton: CRC Press/Taylor & Francis Group.\\n\\n    '\n    k = len(effect)\n    if row_names is None:\n        row_names = list(range(k))\n    crit = stats.norm.isf(alpha / 2)\n    eff = effect\n    var_eff = variance\n    sd_eff = np.sqrt(var_eff)\n    weights_fe = 1 / var_eff\n    w_total_fe = weights_fe.sum(0)\n    weights_rel_fe = weights_fe / w_total_fe\n    eff_w_fe = weights_rel_fe * eff\n    mean_effect_fe = eff_w_fe.sum()\n    var_eff_w_fe = 1 / w_total_fe\n    sd_eff_w_fe = np.sqrt(var_eff_w_fe)\n    q = (weights_fe * eff ** 2).sum(0)\n    q -= (weights_fe * eff).sum() ** 2 / w_total_fe\n    df = k - 1\n    if method_re.lower() in ['iterated', 'pm']:\n        (tau2, _) = _fit_tau_iterative(eff, var_eff, **kwds)\n    elif method_re.lower() in ['chi2', 'dl']:\n        c = w_total_fe - (weights_fe ** 2).sum() / w_total_fe\n        tau2 = (q - df) / c\n    else:\n        raise ValueError('method_re should be \"iterated\" or \"chi2\"')\n    weights_re = 1 / (var_eff + tau2)\n    w_total_re = weights_re.sum(0)\n    weights_rel_re = weights_re / weights_re.sum(0)\n    eff_w_re = weights_rel_re * eff\n    mean_effect_re = eff_w_re.sum()\n    var_eff_w_re = 1 / w_total_re\n    sd_eff_w_re = np.sqrt(var_eff_w_re)\n    scale_hksj_re = (weights_re * (eff - mean_effect_re) ** 2).sum() / df\n    scale_hksj_fe = (weights_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    var_hksj_re = (weights_rel_re * (eff - mean_effect_re) ** 2).sum() / df\n    var_hksj_fe = (weights_rel_fe * (eff - mean_effect_fe) ** 2).sum() / df\n    res = CombineResults(**locals())\n    return res"
        ]
    },
    {
        "func_name": "_fit_tau_iterative",
        "original": "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    \"\"\"Paule-Mandel iterative estimate of between random effect variance\n\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\n    see also Kacker 2004\n\n    Parameters\n    ----------\n    eff : ndarray\n        effect sizes\n    var_eff : ndarray\n        variance of effect sizes\n    tau2_start : float\n        starting value for iteration\n    atol : float, default: 1e-5\n        convergence tolerance for absolute value of estimating equation\n    maxiter : int\n        maximum number of iterations\n\n    Returns\n    -------\n    tau2 : float\n        estimate of random effects variance tau squared\n    converged : bool\n        True if iteration has converged.\n\n    \"\"\"\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)",
        "mutated": [
            "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n    'Paule-Mandel iterative estimate of between random effect variance\\n\\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\\n    see also Kacker 2004\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for absolute value of estimating equation\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)",
            "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Paule-Mandel iterative estimate of between random effect variance\\n\\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\\n    see also Kacker 2004\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for absolute value of estimating equation\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)",
            "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Paule-Mandel iterative estimate of between random effect variance\\n\\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\\n    see also Kacker 2004\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for absolute value of estimating equation\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)",
            "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Paule-Mandel iterative estimate of between random effect variance\\n\\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\\n    see also Kacker 2004\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for absolute value of estimating equation\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)",
            "def _fit_tau_iterative(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Paule-Mandel iterative estimate of between random effect variance\\n\\n    implementation follows DerSimonian and Kacker 2007 Appendix 8\\n    see also Kacker 2004\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for absolute value of estimating equation\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    k = eff.shape[0]\n    converged = False\n    for i in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        m = w.dot(eff) / w.sum(0)\n        resid_sq = (eff - m) ** 2\n        q_w = w.dot(resid_sq)\n        ee = q_w - (k - 1)\n        if ee < 0:\n            tau2 = 0\n            converged = 0\n            break\n        if np.allclose(ee, 0, atol=atol):\n            converged = True\n            break\n        delta = ee / (w ** 2).dot(resid_sq)\n        tau2 += delta\n    return (tau2, converged)"
        ]
    },
    {
        "func_name": "_fit_tau_mm",
        "original": "def _fit_tau_mm(eff, var_eff, weights):\n    \"\"\"one-step method of moment estimate of between random effect variance\n\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\n\n    Parameters\n    ----------\n    eff : ndarray\n        effect sizes\n    var_eff : ndarray\n        variance of effect sizes\n    weights : ndarray\n        weights for estimating overall weighted mean\n\n    Returns\n    -------\n    tau2 : float\n        estimate of random effects variance tau squared\n\n    \"\"\"\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2",
        "mutated": [
            "def _fit_tau_mm(eff, var_eff, weights):\n    if False:\n        i = 10\n    'one-step method of moment estimate of between random effect variance\\n\\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    weights : ndarray\\n        weights for estimating overall weighted mean\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n\\n    '\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2",
            "def _fit_tau_mm(eff, var_eff, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'one-step method of moment estimate of between random effect variance\\n\\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    weights : ndarray\\n        weights for estimating overall weighted mean\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n\\n    '\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2",
            "def _fit_tau_mm(eff, var_eff, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'one-step method of moment estimate of between random effect variance\\n\\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    weights : ndarray\\n        weights for estimating overall weighted mean\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n\\n    '\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2",
            "def _fit_tau_mm(eff, var_eff, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'one-step method of moment estimate of between random effect variance\\n\\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    weights : ndarray\\n        weights for estimating overall weighted mean\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n\\n    '\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2",
            "def _fit_tau_mm(eff, var_eff, weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'one-step method of moment estimate of between random effect variance\\n\\n    implementation follows Kacker 2004 and DerSimonian and Kacker 2007 eq. 6\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    weights : ndarray\\n        weights for estimating overall weighted mean\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n\\n    '\n    w = weights\n    m = w.dot(eff) / w.sum(0)\n    resid_sq = (eff - m) ** 2\n    q_w = w.dot(resid_sq)\n    w_t = w.sum()\n    expect = w.dot(var_eff) - (w ** 2).dot(var_eff) / w_t\n    denom = w_t - (w ** 2).sum() / w_t\n    tau2 = (q_w - expect) / denom\n    return tau2"
        ]
    },
    {
        "func_name": "_fit_tau_iter_mm",
        "original": "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    \"\"\"iterated method of moment estimate of between random effect variance\n\n    This repeatedly estimates tau, updating weights in each iteration\n    see two-step estimators in DerSimonian and Kacker 2007\n\n    Parameters\n    ----------\n    eff : ndarray\n        effect sizes\n    var_eff : ndarray\n        variance of effect sizes\n    tau2_start : float\n        starting value for iteration\n    atol : float, default: 1e-5\n        convergence tolerance for change in tau2 estimate between iterations\n    maxiter : int\n        maximum number of iterations\n\n    Returns\n    -------\n    tau2 : float\n        estimate of random effects variance tau squared\n    converged : bool\n        True if iteration has converged.\n\n    \"\"\"\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)",
        "mutated": [
            "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n    'iterated method of moment estimate of between random effect variance\\n\\n    This repeatedly estimates tau, updating weights in each iteration\\n    see two-step estimators in DerSimonian and Kacker 2007\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for change in tau2 estimate between iterations\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)",
            "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'iterated method of moment estimate of between random effect variance\\n\\n    This repeatedly estimates tau, updating weights in each iteration\\n    see two-step estimators in DerSimonian and Kacker 2007\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for change in tau2 estimate between iterations\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)",
            "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'iterated method of moment estimate of between random effect variance\\n\\n    This repeatedly estimates tau, updating weights in each iteration\\n    see two-step estimators in DerSimonian and Kacker 2007\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for change in tau2 estimate between iterations\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)",
            "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'iterated method of moment estimate of between random effect variance\\n\\n    This repeatedly estimates tau, updating weights in each iteration\\n    see two-step estimators in DerSimonian and Kacker 2007\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for change in tau2 estimate between iterations\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)",
            "def _fit_tau_iter_mm(eff, var_eff, tau2_start=0, atol=1e-05, maxiter=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'iterated method of moment estimate of between random effect variance\\n\\n    This repeatedly estimates tau, updating weights in each iteration\\n    see two-step estimators in DerSimonian and Kacker 2007\\n\\n    Parameters\\n    ----------\\n    eff : ndarray\\n        effect sizes\\n    var_eff : ndarray\\n        variance of effect sizes\\n    tau2_start : float\\n        starting value for iteration\\n    atol : float, default: 1e-5\\n        convergence tolerance for change in tau2 estimate between iterations\\n    maxiter : int\\n        maximum number of iterations\\n\\n    Returns\\n    -------\\n    tau2 : float\\n        estimate of random effects variance tau squared\\n    converged : bool\\n        True if iteration has converged.\\n\\n    '\n    tau2 = tau2_start\n    converged = False\n    for _ in range(maxiter):\n        w = 1 / (var_eff + tau2)\n        tau2_new = _fit_tau_mm(eff, var_eff, w)\n        tau2_new = max(0, tau2_new)\n        delta = tau2_new - tau2\n        if np.allclose(delta, 0, atol=atol):\n            converged = True\n            break\n        tau2 = tau2_new\n    return (tau2, converged)"
        ]
    }
]