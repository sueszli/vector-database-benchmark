[
    {
        "func_name": "gen_forward",
        "original": "def gen_forward():\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)",
        "mutated": [
            "def gen_forward():\n    if False:\n        i = 10\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)",
            "def gen_forward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)",
            "def gen_forward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)",
            "def gen_forward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)",
            "def gen_forward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_forward(at::Tensor input, at::Tensor filters, int padding_l) {\\n\\n    at::DeviceGuard g(input.device());\\n    const auto minibatch = input.size(0);\\n    const auto numFeatures = input.size(1);\\n    const auto sequenceLength = input.size(2);\\n\\n    const auto numHeads = filters.size(0);\\n    const auto filterSize = filters.size(1);\\n\\n    const auto numFiltersInBlock = numFeatures / numHeads;\\n\\n    const dim3 blocks(minibatch, numFeatures);\\n\\n    auto output = at::zeros_like(input);\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n'\n    sequence_if = '\\n    if (sequenceLength <= {seq}) {{\\n        switch(filterSize) {{\\n'\n    case_k = '\\n            case {k}:\\n'\n    main_block = '\\n                if (padding_l == {pad}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_forward\", ([&] {{\\n                        lightconv_forward_kernel<{k}, {b_size}, {pad}, scalar_t>\\n                        <<<blocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                output.data<scalar_t>());\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping forward pass\" << std::endl;\\n                }\\n                break;\\n'\n    bad_filter = '\\n            default:\\n                std::cout << \"WARNING: Unsupported filter length passed - skipping forward pass\" << std::endl;\\n        }\\n'\n    con_else = '\\n    } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    final_return = '\\n    }\\n\\n    return {output};\\n}\\n'\n    with open('lightconv_cuda_forward.cu', 'w') as forward:\n        forward.write(head)\n        for seq in seqs:\n            forward.write(sequence_if.format(seq=seq))\n            for k in kernels:\n                forward.write(case_k.format(k=k))\n                for pad in [k // 2, k - 1]:\n                    forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n                forward.write(bad_padding)\n            forward.write(bad_filter)\n            forward.write(con_else)\n        forward.write(final_else)\n        for k in kernels:\n            forward.write(case_k.format(k=k))\n            for pad in [k // 2, k - 1]:\n                forward.write(main_block.format(k=k, b_size=seq, pad=pad))\n            forward.write(bad_padding)\n        forward.write(bad_filter)\n        forward.write(final_return)"
        ]
    },
    {
        "func_name": "gen_backward",
        "original": "def gen_backward():\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)",
        "mutated": [
            "def gen_backward():\n    if False:\n        i = 10\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)",
            "def gen_backward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)",
            "def gen_backward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)",
            "def gen_backward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)",
            "def gen_backward():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    head = '\\n/**\\n * Copyright (c) Facebook, Inc. and its affiliates.\\n *\\n * This source code is licensed under the MIT license found in the\\n * LICENSE file in the root directory of this source tree.\\n */\\n\\n#include \"lightconv_cuda.cuh\"\\n\\nstd::vector<at::Tensor> lightconv_cuda_backward(\\n        at::Tensor gradOutput,\\n        int padding_l,\\n        at::Tensor input,\\n        at::Tensor filters) {\\n\\n    // gradWrtInput\\n    const int minibatch = input.size(0);\\n    const int numFeatures = input.size(1);\\n    const int sequenceLength = input.size(2);\\n\\n    const int numHeads = filters.size(0);\\n    const int filterSize = filters.size(1);\\n\\n    const dim3 gradBlocks(minibatch, numFeatures);\\n    const dim3 weightGradFirstpassShortBlocks(minibatch, numHeads);\\n    const dim3 weightGradSecondpassBlocks(numHeads, filterSize);\\n\\n    const int numFiltersInBlock = numFeatures / numHeads;\\n\\n    auto gradInput = at::zeros_like(input);\\n    auto gradFilters = at::zeros_like(filters);\\n\\n    at::DeviceGuard g(input.device());\\n    auto stream = at::cuda::getCurrentCUDAStream();\\n\\n    switch(filterSize) {\\n'\n    sequence_if = '\\n            if (sequenceLength <= {seq}) {{\\n'\n    case_k = '\\n        case {k}:\\n'\n    main_block = '\\n                if (padding_l == {p}) {{\\n                    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"lightconv_backward\", ([&] {{\\n                        lightconv_grad_wrt_input_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                gradOutput.data<scalar_t>(),\\n                                filters.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                gradInput.data<scalar_t>());\\n\\n'\n    weight_grad_short = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numHeads, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_short_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<weightGradFirstpassShortBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                numHeads,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_short_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    weight_grad = '\\n                        at::Tensor tempSumGradFilters = at::zeros({{minibatch, numFeatures, filterSize}}, input.options().dtype(at::kFloat));\\n                        lightconv_grad_wrt_weights_firstpass_kernel<{k}, {b_size}, {p}, scalar_t>\\n                        <<<gradBlocks, {b_size}, 0, stream>>>(\\n                                input.data<scalar_t>(),\\n                                gradOutput.data<scalar_t>(),\\n                                minibatch,\\n                                sequenceLength,\\n                                numFeatures,\\n                                numFiltersInBlock,\\n                                tempSumGradFilters.data<float>()\\n                        );\\n\\n                        lightconv_grad_wrt_weights_secondpass_kernel<{k}, {b_size}, scalar_t>\\n                        <<<weightGradSecondpassBlocks, {b_size}, 0, stream>>>(\\n                                tempSumGradFilters.data<float>(),\\n                                minibatch,\\n                                numFiltersInBlock,\\n                                gradFilters.data<scalar_t>()\\n                        );\\n                    }}));\\n                }} else\\n'\n    bad_padding = '\\n                {\\n                    std::cout << \"WARNING: Unsupported padding size - skipping backward pass\" << std::endl;\\n                }\\n'\n    breakout = '\\n                break;\\n'\n    bad_filter = '\\n        default:\\n            std::cout << \"WARNING: Unsupported filter length passed - skipping backward pass\" << std::endl;\\n'\n    con_else = '\\n            } else\\n'\n    final_else = '\\n    {\\n        switch(filterSize) {\\n'\n    last_return = '\\n    }\\n    return {gradInput, gradFilters};\\n}\\n'\n    kernels = [3, 5, 7, 15, 31, 63, 127, 255]\n    seqs = [32 * x for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]]\n    thresh = [32, 32, 64, 128, 256, -1, -1, -1]\n    max_mem = [-1, -1, -1, -1, -1, 192, 96, 64]\n    with open('lightconv_cuda_backward.cu', 'w') as backward:\n        backward.write(head)\n        for (k, t, mem) in zip(kernels, thresh, max_mem):\n            backward.write(case_k.format(k=k))\n            for seq in seqs:\n                if (t == -1 or seq <= t) and (mem == -1 or seq < mem):\n                    backward.write(sequence_if.format(seq=seq))\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=seq, p=p))\n                        backward.write(weight_grad_short.format(k=k, b_size=seq, p=p))\n                    backward.write(bad_padding)\n                else:\n                    for p in [k // 2, k - 1]:\n                        backward.write(main_block.format(k=k, b_size=32, p=p))\n                        backward.write(weight_grad.format(k=k, b_size=32, p=p))\n                    backward.write(bad_padding)\n                    backward.write(breakout)\n                    break\n                backward.write(con_else)\n        backward.write(bad_filter)\n        backward.write(last_return)"
        ]
    }
]