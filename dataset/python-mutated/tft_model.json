[
    {
        "func_name": "__init__",
        "original": "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    \"\"\"PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\n        The implementation is built upon `pytorch-forecasting's TemporalFusionTransformer\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\n\n        Parameters\n        ----------\n        output_dim : Tuple[int, int]\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\n        variables_meta : Dict[str, Dict[str, List[str]]]\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\n        num_static_components\n            the number of static components (not variables) of the input target series. This is either equal to the\n            number of target components or 1.\n        hidden_size : int\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\n            architecture.\n        lstm_layers : int\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\n        num_attention_heads : int\n            number of attention heads (4 is a good default)\n        full_attention : bool\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\n            only queries on future part. Defaults to `False`.\n        feed_forward\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\n            Defaults to `GatedResidualNetwork`.\n        hidden_continuous_size : int\n            default for hidden size for processing continuous variables.\n        categorical_embedding_sizes : dict\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\n            of the categorical static covariates. The values are tuples of integers with\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\n        dropout : float\n            Fraction of neurons affected by Dropout.\n        add_relative_index : bool\n            Whether to add positional values to future covariates. Defaults to `False`.\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\n            It gives a value to the position of each step from input and output chunk relative to the prediction\n            point. The values are normalized with `input_chunk_length`.\n        likelihood\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\n            a ``QuantileRegression`` likelihood.\n        norm_type: str | nn.Module\n            The type of LayerNorm variant to use.\n        **kwargs\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\n        \"\"\"\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None",
        "mutated": [
            "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    if False:\n        i = 10\n    'PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\\n        The implementation is built upon `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\\n\\n        Parameters\\n        ----------\\n        output_dim : Tuple[int, int]\\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\\n        variables_meta : Dict[str, Dict[str, List[str]]]\\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\\n        num_static_components\\n            the number of static components (not variables) of the input target series. This is either equal to the\\n            number of target components or 1.\\n        hidden_size : int\\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers : int\\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads : int\\n            number of attention heads (4 is a good default)\\n        full_attention : bool\\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\\n            only queries on future part. Defaults to `False`.\\n        feed_forward\\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\\n            Defaults to `GatedResidualNetwork`.\\n        hidden_continuous_size : int\\n            default for hidden size for processing continuous variables.\\n        categorical_embedding_sizes : dict\\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. The values are tuples of integers with\\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        dropout : float\\n            Fraction of neurons affected by Dropout.\\n        add_relative_index : bool\\n            Whether to add positional values to future covariates. Defaults to `False`.\\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\\n            It gives a value to the position of each step from input and output chunk relative to the prediction\\n            point. The values are normalized with `input_chunk_length`.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.\\n        **kwargs\\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\\n        '\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None",
            "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\\n        The implementation is built upon `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\\n\\n        Parameters\\n        ----------\\n        output_dim : Tuple[int, int]\\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\\n        variables_meta : Dict[str, Dict[str, List[str]]]\\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\\n        num_static_components\\n            the number of static components (not variables) of the input target series. This is either equal to the\\n            number of target components or 1.\\n        hidden_size : int\\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers : int\\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads : int\\n            number of attention heads (4 is a good default)\\n        full_attention : bool\\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\\n            only queries on future part. Defaults to `False`.\\n        feed_forward\\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\\n            Defaults to `GatedResidualNetwork`.\\n        hidden_continuous_size : int\\n            default for hidden size for processing continuous variables.\\n        categorical_embedding_sizes : dict\\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. The values are tuples of integers with\\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        dropout : float\\n            Fraction of neurons affected by Dropout.\\n        add_relative_index : bool\\n            Whether to add positional values to future covariates. Defaults to `False`.\\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\\n            It gives a value to the position of each step from input and output chunk relative to the prediction\\n            point. The values are normalized with `input_chunk_length`.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.\\n        **kwargs\\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\\n        '\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None",
            "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\\n        The implementation is built upon `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\\n\\n        Parameters\\n        ----------\\n        output_dim : Tuple[int, int]\\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\\n        variables_meta : Dict[str, Dict[str, List[str]]]\\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\\n        num_static_components\\n            the number of static components (not variables) of the input target series. This is either equal to the\\n            number of target components or 1.\\n        hidden_size : int\\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers : int\\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads : int\\n            number of attention heads (4 is a good default)\\n        full_attention : bool\\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\\n            only queries on future part. Defaults to `False`.\\n        feed_forward\\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\\n            Defaults to `GatedResidualNetwork`.\\n        hidden_continuous_size : int\\n            default for hidden size for processing continuous variables.\\n        categorical_embedding_sizes : dict\\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. The values are tuples of integers with\\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        dropout : float\\n            Fraction of neurons affected by Dropout.\\n        add_relative_index : bool\\n            Whether to add positional values to future covariates. Defaults to `False`.\\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\\n            It gives a value to the position of each step from input and output chunk relative to the prediction\\n            point. The values are normalized with `input_chunk_length`.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.\\n        **kwargs\\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\\n        '\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None",
            "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\\n        The implementation is built upon `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\\n\\n        Parameters\\n        ----------\\n        output_dim : Tuple[int, int]\\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\\n        variables_meta : Dict[str, Dict[str, List[str]]]\\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\\n        num_static_components\\n            the number of static components (not variables) of the input target series. This is either equal to the\\n            number of target components or 1.\\n        hidden_size : int\\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers : int\\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads : int\\n            number of attention heads (4 is a good default)\\n        full_attention : bool\\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\\n            only queries on future part. Defaults to `False`.\\n        feed_forward\\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\\n            Defaults to `GatedResidualNetwork`.\\n        hidden_continuous_size : int\\n            default for hidden size for processing continuous variables.\\n        categorical_embedding_sizes : dict\\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. The values are tuples of integers with\\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        dropout : float\\n            Fraction of neurons affected by Dropout.\\n        add_relative_index : bool\\n            Whether to add positional values to future covariates. Defaults to `False`.\\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\\n            It gives a value to the position of each step from input and output chunk relative to the prediction\\n            point. The values are normalized with `input_chunk_length`.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.\\n        **kwargs\\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\\n        '\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None",
            "def __init__(self, output_dim: Tuple[int, int], variables_meta: Dict[str, Dict[str, List[str]]], num_static_components: int, hidden_size: Union[int, List[int]], lstm_layers: int, num_attention_heads: int, full_attention: bool, feed_forward: str, hidden_continuous_size: int, categorical_embedding_sizes: Dict[str, Tuple[int, int]], dropout: float, add_relative_index: bool, norm_type: Union[str, nn.Module], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'PyTorch module implementing the TFT architecture from `this paper <https://arxiv.org/pdf/1912.09363.pdf>`_\\n        The implementation is built upon `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_.\\n\\n        Parameters\\n        ----------\\n        output_dim : Tuple[int, int]\\n            shape of output given by (n_targets, loss_size). (loss_size corresponds to nr_params in other models).\\n        variables_meta : Dict[str, Dict[str, List[str]]]\\n            dict containing variable encoder, decoder variable names for mapping tensors in `_TFTModule.forward()`\\n        num_static_components\\n            the number of static components (not variables) of the input target series. This is either equal to the\\n            number of target components or 1.\\n        hidden_size : int\\n            hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers : int\\n            number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads : int\\n            number of attention heads (4 is a good default)\\n        full_attention : bool\\n            If `True`, applies multi-head attention query on past (encoder) and future (decoder) parts. Otherwise,\\n            only queries on future part. Defaults to `False`.\\n        feed_forward\\n            Set the feedforward network block. default `GatedResidualNetwork` or one of the  glu variant.\\n            Defaults to `GatedResidualNetwork`.\\n        hidden_continuous_size : int\\n            default for hidden size for processing continuous variables.\\n        categorical_embedding_sizes : dict\\n            A dictionary containing embedding sizes for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. The values are tuples of integers with\\n            `(number of unique categories, embedding size)`. For example `{\"some_column\": (64, 8)}`.\\n            Note that `TorchForecastingModels` can only handle numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        dropout : float\\n            Fraction of neurons affected by Dropout.\\n        add_relative_index : bool\\n            Whether to add positional values to future covariates. Defaults to `False`.\\n            This allows to use the TFTModel without having to pass future_covariates to `fit()` and `train()`.\\n            It gives a value to the position of each step from input and output chunk relative to the prediction\\n            point. The values are normalized with `input_chunk_length`.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.\\n        **kwargs\\n            all parameters required for :class:`darts.model.forecasting_models.PLForecastingModule` base class.\\n        '\n    super().__init__(**kwargs)\n    (self.n_targets, self.loss_size) = output_dim\n    self.variables_meta = variables_meta\n    self.num_static_components = num_static_components\n    self.hidden_size = hidden_size\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.add_relative_index = add_relative_index\n    if isinstance(norm_type, str):\n        try:\n            self.layer_norm = getattr(layer_norm_variants, norm_type)\n        except AttributeError:\n            raise_log(AttributeError('please provide a valid layer norm type'))\n    else:\n        self.layer_norm = norm_type\n    self.batch_size_last = -1\n    self.attention_mask = None\n    self.relative_index = None\n    self.input_embeddings = _MultiEmbedding(embedding_sizes=categorical_embedding_sizes, variable_names=self.categorical_static_variables)\n    self.prescalers_linear = {name: nn.Linear(1 if name not in self.numeric_static_variables else self.num_static_components, self.hidden_continuous_size) for name in self.reals}\n    static_input_sizes = {name: self.input_embeddings.output_size[name] for name in self.categorical_static_variables}\n    static_input_sizes.update({name: self.hidden_continuous_size for name in self.numeric_static_variables})\n    self.static_covariates_vsn = _VariableSelectionNetwork(input_sizes=static_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={name: True for name in self.categorical_static_variables}, dropout=self.dropout, prescalers=self.prescalers_linear, single_variable_grns={}, context_size=None, layer_norm=self.layer_norm)\n    encoder_input_sizes = {name: self.hidden_continuous_size for name in self.encoder_variables}\n    decoder_input_sizes = {name: self.hidden_continuous_size for name in self.decoder_variables}\n    self.encoder_vsn = _VariableSelectionNetwork(input_sizes=encoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.decoder_vsn = _VariableSelectionNetwork(input_sizes=decoder_input_sizes, hidden_size=self.hidden_size, input_embedding_flags={}, dropout=self.dropout, context_size=self.hidden_size, prescalers=self.prescalers_linear, single_variable_grns={}, layer_norm=self.layer_norm)\n    self.static_context_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_hidden_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_cell_encoder_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.static_context_enrichment = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    self.lstm_encoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.lstm_decoder = _LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=self.dropout if self.lstm_layers > 1 else 0, batch_first=True)\n    self.post_lstm_gan = _GateAddNorm(input_size=self.hidden_size, dropout=dropout, layer_norm=self.layer_norm)\n    self.static_enrichment_grn = _GatedResidualNetwork(input_size=self.hidden_size, hidden_size=self.hidden_size, output_size=self.hidden_size, dropout=self.dropout, context_size=self.hidden_size, layer_norm=self.layer_norm)\n    self.multihead_attn = _InterpretableMultiHeadAttention(d_model=self.hidden_size, n_head=self.num_attention_heads, dropout=self.dropout)\n    self.post_attn_gan = _GateAddNorm(self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    if self.feed_forward == 'GatedResidualNetwork':\n        self.feed_forward_block = _GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, dropout=self.dropout, layer_norm=self.layer_norm)\n    else:\n        raise_if_not(self.feed_forward in GLU_FFN, f\"'{self.feed_forward}' is not in {GLU_FFN + ['GatedResidualNetwork']}\")\n        self.feed_forward_block = getattr(glu_variants, self.feed_forward)(d_model=self.hidden_size, d_ff=self.hidden_size * 4, dropout=dropout)\n    self.pre_output_gan = _GateAddNorm(self.hidden_size, dropout=None, layer_norm=self.layer_norm)\n    self.output_layer = nn.Linear(self.hidden_size, self.n_targets * self.loss_size)\n    self._attn_out_weights = None\n    self._static_covariate_var = None\n    self._encoder_sparse_weights = None\n    self._decoder_sparse_weights = None"
        ]
    },
    {
        "func_name": "reals",
        "original": "@property\ndef reals(self) -> List[str]:\n    \"\"\"\n        List of all continuous variables in model\n        \"\"\"\n    return self.variables_meta['model_config']['reals_input']",
        "mutated": [
            "@property\ndef reals(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of all continuous variables in model\\n        '\n    return self.variables_meta['model_config']['reals_input']",
            "@property\ndef reals(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of all continuous variables in model\\n        '\n    return self.variables_meta['model_config']['reals_input']",
            "@property\ndef reals(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of all continuous variables in model\\n        '\n    return self.variables_meta['model_config']['reals_input']",
            "@property\ndef reals(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of all continuous variables in model\\n        '\n    return self.variables_meta['model_config']['reals_input']",
            "@property\ndef reals(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of all continuous variables in model\\n        '\n    return self.variables_meta['model_config']['reals_input']"
        ]
    },
    {
        "func_name": "static_variables",
        "original": "@property\ndef static_variables(self) -> List[str]:\n    \"\"\"\n        List of all static variables in model\n        \"\"\"\n    return self.variables_meta['model_config']['static_input']",
        "mutated": [
            "@property\ndef static_variables(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of all static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input']",
            "@property\ndef static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of all static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input']",
            "@property\ndef static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of all static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input']",
            "@property\ndef static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of all static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input']",
            "@property\ndef static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of all static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input']"
        ]
    },
    {
        "func_name": "numeric_static_variables",
        "original": "@property\ndef numeric_static_variables(self) -> List[str]:\n    \"\"\"\n        List of numeric static variables in model\n        \"\"\"\n    return self.variables_meta['model_config']['static_input_numeric']",
        "mutated": [
            "@property\ndef numeric_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of numeric static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_numeric']",
            "@property\ndef numeric_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of numeric static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_numeric']",
            "@property\ndef numeric_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of numeric static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_numeric']",
            "@property\ndef numeric_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of numeric static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_numeric']",
            "@property\ndef numeric_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of numeric static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_numeric']"
        ]
    },
    {
        "func_name": "categorical_static_variables",
        "original": "@property\ndef categorical_static_variables(self) -> List[str]:\n    \"\"\"\n        List of categorical static variables in model\n        \"\"\"\n    return self.variables_meta['model_config']['static_input_categorical']",
        "mutated": [
            "@property\ndef categorical_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of categorical static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_categorical']",
            "@property\ndef categorical_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of categorical static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_categorical']",
            "@property\ndef categorical_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of categorical static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_categorical']",
            "@property\ndef categorical_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of categorical static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_categorical']",
            "@property\ndef categorical_static_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of categorical static variables in model\\n        '\n    return self.variables_meta['model_config']['static_input_categorical']"
        ]
    },
    {
        "func_name": "encoder_variables",
        "original": "@property\ndef encoder_variables(self) -> List[str]:\n    \"\"\"\n        List of all encoder variables in model (excluding static variables)\n        \"\"\"\n    return self.variables_meta['model_config']['time_varying_encoder_input']",
        "mutated": [
            "@property\ndef encoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of all encoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_encoder_input']",
            "@property\ndef encoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of all encoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_encoder_input']",
            "@property\ndef encoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of all encoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_encoder_input']",
            "@property\ndef encoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of all encoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_encoder_input']",
            "@property\ndef encoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of all encoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_encoder_input']"
        ]
    },
    {
        "func_name": "decoder_variables",
        "original": "@property\ndef decoder_variables(self) -> List[str]:\n    \"\"\"\n        List of all decoder variables in model (excluding static variables)\n        \"\"\"\n    return self.variables_meta['model_config']['time_varying_decoder_input']",
        "mutated": [
            "@property\ndef decoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n    '\\n        List of all decoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_decoder_input']",
            "@property\ndef decoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        List of all decoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_decoder_input']",
            "@property\ndef decoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        List of all decoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_decoder_input']",
            "@property\ndef decoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        List of all decoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_decoder_input']",
            "@property\ndef decoder_variables(self) -> List[str]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        List of all decoder variables in model (excluding static variables)\\n        '\n    return self.variables_meta['model_config']['time_varying_decoder_input']"
        ]
    },
    {
        "func_name": "expand_static_context",
        "original": "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    \"\"\"\n        add time dimension to static context\n        \"\"\"\n    return context[:, None].expand(-1, time_steps, -1)",
        "mutated": [
            "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n        add time dimension to static context\\n        '\n    return context[:, None].expand(-1, time_steps, -1)",
            "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        add time dimension to static context\\n        '\n    return context[:, None].expand(-1, time_steps, -1)",
            "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        add time dimension to static context\\n        '\n    return context[:, None].expand(-1, time_steps, -1)",
            "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        add time dimension to static context\\n        '\n    return context[:, None].expand(-1, time_steps, -1)",
            "@staticmethod\ndef expand_static_context(context: torch.Tensor, time_steps: int) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        add time dimension to static context\\n        '\n    return context[:, None].expand(-1, time_steps, -1)"
        ]
    },
    {
        "func_name": "get_relative_index",
        "original": "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    \"\"\"\n        Returns scaled time index relative to prediction point.\n        \"\"\"\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)",
        "mutated": [
            "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n        Returns scaled time index relative to prediction point.\\n        '\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)",
            "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns scaled time index relative to prediction point.\\n        '\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)",
            "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns scaled time index relative to prediction point.\\n        '\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)",
            "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns scaled time index relative to prediction point.\\n        '\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)",
            "@staticmethod\ndef get_relative_index(encoder_length: int, decoder_length: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns scaled time index relative to prediction point.\\n        '\n    index = torch.arange(encoder_length + decoder_length, dtype=dtype, device=device)\n    prediction_index = encoder_length - 1\n    index[:encoder_length] = index[:encoder_length] / prediction_index\n    index[encoder_length:] = index[encoder_length:] / prediction_index\n    return index.reshape(1, len(index), 1).repeat(batch_size, 1, 1)"
        ]
    },
    {
        "func_name": "get_attention_mask_full",
        "original": "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    \"\"\"\n        Returns causal mask to apply for self-attention layer.\n        \"\"\"\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1",
        "mutated": [
            "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n        Returns causal mask to apply for self-attention layer.\\n        '\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1",
            "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns causal mask to apply for self-attention layer.\\n        '\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1",
            "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns causal mask to apply for self-attention layer.\\n        '\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1",
            "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns causal mask to apply for self-attention layer.\\n        '\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1",
            "@staticmethod\ndef get_attention_mask_full(time_steps: int, batch_size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns causal mask to apply for self-attention layer.\\n        '\n    eye = torch.eye(time_steps, dtype=dtype, device=device)\n    mask = torch.cumsum(eye.unsqueeze(0).repeat(batch_size, 1, 1), dim=1)\n    return mask < 1"
        ]
    },
    {
        "func_name": "get_attention_mask_future",
        "original": "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    \"\"\"\n        Returns causal mask to apply for self-attention layer that acts on future input only.\n        The model will attend to all `False` values.\n        \"\"\"\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask",
        "mutated": [
            "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    if False:\n        i = 10\n    '\\n        Returns causal mask to apply for self-attention layer that acts on future input only.\\n        The model will attend to all `False` values.\\n        '\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask",
            "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns causal mask to apply for self-attention layer that acts on future input only.\\n        The model will attend to all `False` values.\\n        '\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask",
            "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns causal mask to apply for self-attention layer that acts on future input only.\\n        The model will attend to all `False` values.\\n        '\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask",
            "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns causal mask to apply for self-attention layer that acts on future input only.\\n        The model will attend to all `False` values.\\n        '\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask",
            "@staticmethod\ndef get_attention_mask_future(encoder_length: int, decoder_length: int, batch_size: int, device: str, full_attention: bool) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns causal mask to apply for self-attention layer that acts on future input only.\\n        The model will attend to all `False` values.\\n        '\n    if full_attention:\n        decoder_mask = torch.zeros((decoder_length, decoder_length), dtype=torch.bool, device=device)\n    else:\n        attend_step = torch.arange(decoder_length, device=device)\n        predict_step = torch.arange(0, decoder_length, device=device)[:, None]\n        decoder_mask = attend_step >= predict_step\n    encoder_mask = torch.zeros(batch_size, encoder_length, dtype=torch.bool, device=device)\n    mask = torch.cat((encoder_mask.unsqueeze(1).expand(-1, decoder_length, -1), decoder_mask.unsqueeze(0).expand(batch_size, -1, -1)), dim=2)\n    return mask"
        ]
    },
    {
        "func_name": "forward",
        "original": "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    \"\"\"TFT model forward pass.\n\n        Parameters\n        ----------\n        x_in\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\n\n        Returns\n        -------\n        torch.Tensor\n            the output tensor\n        \"\"\"\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out",
        "mutated": [
            "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    if False:\n        i = 10\n    'TFT model forward pass.\\n\\n        Parameters\\n        ----------\\n        x_in\\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\\n\\n        Returns\\n        -------\\n        torch.Tensor\\n            the output tensor\\n        '\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out",
            "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'TFT model forward pass.\\n\\n        Parameters\\n        ----------\\n        x_in\\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\\n\\n        Returns\\n        -------\\n        torch.Tensor\\n            the output tensor\\n        '\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out",
            "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'TFT model forward pass.\\n\\n        Parameters\\n        ----------\\n        x_in\\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\\n\\n        Returns\\n        -------\\n        torch.Tensor\\n            the output tensor\\n        '\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out",
            "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'TFT model forward pass.\\n\\n        Parameters\\n        ----------\\n        x_in\\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\\n\\n        Returns\\n        -------\\n        torch.Tensor\\n            the output tensor\\n        '\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out",
            "@io_processor\ndef forward(self, x_in: Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]) -> torch.Tensor:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'TFT model forward pass.\\n\\n        Parameters\\n        ----------\\n        x_in\\n            comes as tuple `(x_past, x_future, x_static)` where `x_past` is the input/past chunk and `x_future`\\n            is the output/future chunk. Input dimensions are `(n_samples, n_time_steps, n_variables)`\\n\\n        Returns\\n        -------\\n        torch.Tensor\\n            the output tensor\\n        '\n    (x_cont_past, x_cont_future, x_static) = x_in\n    (dim_samples, dim_time, dim_variable) = (0, 1, 2)\n    device = x_in[0].device\n    batch_size = x_cont_past.shape[dim_samples]\n    encoder_length = self.input_chunk_length\n    decoder_length = self.output_chunk_length\n    time_steps = encoder_length + decoder_length\n    if batch_size != self.batch_size_last:\n        self.attention_mask = self.get_attention_mask_future(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, full_attention=self.full_attention)\n        if self.add_relative_index:\n            self.relative_index = self.get_relative_index(encoder_length=encoder_length, decoder_length=decoder_length, batch_size=batch_size, device=device, dtype=x_cont_past.dtype)\n        self.batch_size_last = batch_size\n    if self.add_relative_index:\n        x_cont_past = torch.cat([ts[:, :encoder_length, :] for ts in [x_cont_past, self.relative_index] if ts is not None], dim=dim_variable)\n        x_cont_future = torch.cat([ts[:, -decoder_length:, :] for ts in [x_cont_future, self.relative_index] if ts is not None], dim=dim_variable)\n    input_vectors_past = {name: x_cont_past[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.encoder_variables)}\n    input_vectors_future = {name: x_cont_future[..., idx].unsqueeze(-1) for (idx, name) in enumerate(self.decoder_variables)}\n    if self.static_variables:\n        if self.categorical_static_variables:\n            static_embedding = self.input_embeddings(torch.cat([x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.categorical_static_variables], dim=1).int())\n        else:\n            static_embedding = {}\n        static_embedding.update({name: x_static[:, :, idx] for (idx, name) in enumerate(self.static_variables) if name in self.numeric_static_variables})\n        (static_embedding, static_covariate_var) = self.static_covariates_vsn(static_embedding)\n    else:\n        static_embedding = torch.zeros((x_cont_past.shape[0], self.hidden_size), dtype=x_cont_past.dtype, device=device)\n        static_covariate_var = None\n    static_context_expanded = self.expand_static_context(context=self.static_context_grn(static_embedding), time_steps=time_steps)\n    embeddings_varying_encoder = {name: input_vectors_past[name] for name in self.encoder_variables}\n    (embeddings_varying_encoder, encoder_sparse_weights) = self.encoder_vsn(x=embeddings_varying_encoder, context=static_context_expanded[:, :encoder_length])\n    embeddings_varying_decoder = {name: input_vectors_future[name] for name in self.decoder_variables}\n    (embeddings_varying_decoder, decoder_sparse_weights) = self.decoder_vsn(x=embeddings_varying_decoder, context=static_context_expanded[:, encoder_length:])\n    input_hidden = self.static_context_hidden_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    input_cell = self.static_context_cell_encoder_grn(static_embedding).expand(self.lstm_layers, -1, -1).contiguous()\n    (encoder_out, (hidden, cell)) = self.lstm_encoder(input=embeddings_varying_encoder, hx=(input_hidden, input_cell))\n    (decoder_out, _) = self.lstm_decoder(input=embeddings_varying_decoder, hx=(hidden, cell))\n    lstm_layer = torch.cat([encoder_out, decoder_out], dim=dim_time)\n    input_embeddings = torch.cat([embeddings_varying_encoder, embeddings_varying_decoder], dim=dim_time)\n    lstm_out = self.post_lstm_gan(x=lstm_layer, skip=input_embeddings)\n    static_context_enriched = self.static_context_enrichment(static_embedding)\n    attn_input = self.static_enrichment_grn(x=lstm_out, context=self.expand_static_context(context=static_context_enriched, time_steps=time_steps))\n    (attn_out, attn_out_weights) = self.multihead_attn(q=attn_input[:, encoder_length:], k=attn_input, v=attn_input, mask=self.attention_mask)\n    attn_out = self.post_attn_gan(x=attn_out, skip=attn_input[:, encoder_length:])\n    out = self.feed_forward_block(x=attn_out)\n    out = self.pre_output_gan(x=out, skip=lstm_out[:, encoder_length:])\n    out = self.output_layer(out)\n    out = out.view(batch_size, self.output_chunk_length, self.n_targets, self.loss_size)\n    self._attn_out_weights = attn_out_weights\n    self._static_covariate_var = static_covariate_var\n    self._encoder_sparse_weights = encoder_sparse_weights\n    self._decoder_sparse_weights = decoder_sparse_weights\n    return out"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    \"\"\"Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\n\n        This is an implementation of the TFT architecture, as outlined in [1]_.\n\n        The internal sub models are adopted from `pytorch-forecasting's TemporalFusionTransformer\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\n\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\n\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\n        :func:`predict()`.\n\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\n\n        Parameters\n        ----------\n        input_chunk_length\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\n        output_chunk_length\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\n        hidden_size\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\n            architecture.\n        lstm_layers\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\n        num_attention_heads\n            Number of attention heads (4 is a good default)\n        full_attention\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\n            current, and future time steps. Defaults to ``False``.\n        feed_forward\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant's\n            FeedForward Network (FFN)[2]. The glu variant's FeedForward Network are a series of FFNs designed to work\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\n        dropout\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\n            prediction time).\n        hidden_continuous_size\n            Default for hidden size for processing continuous variables\n        categorical_embedding_sizes\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\n            ``min(round(1.6 * n**0.56), 100)``.\n            For a tuple of integers, give (number of unique categories, embedding size). For example\n            ``{\"some_column\": (64, 8)}``.\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\n        add_relative_index\n            Whether to add positional values to future covariates. Defaults to ``False``.\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\n            to the prediction point. The values are normalized with ``input_chunk_length``.\n        loss_fn: nn.Module\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\n            `likelihood`` to None and give a ``loss_fn`` argument.\n        likelihood\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\n            a ``QuantileRegression`` likelihood.\n        norm_type: str | nn.Module\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\n        use_static_covariates\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\n        **kwargs\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\n            Darts' :class:`TorchForecastingModel`.\n\n        torch_metrics\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\n        optimizer_cls\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\n        optimizer_kwargs\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{'lr': 1e-3}``\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\n            will be used. Default: ``None``.\n        lr_scheduler_cls\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\n            to using a constant learning rate. Default: ``None``.\n        lr_scheduler_kwargs\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\n        use_reversible_instance_norm\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\n            It is only applied to the features of the target series and not the covariates.\n        batch_size\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\n        n_epochs\n            Number of epochs over which to train the model. Default: ``100``.\n        model_name\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\n            spawned at the same time by different processes to share the same model_name). E.g.,\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\n        work_dir\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\n            Default: current working directory.\n        log_tensorboard\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\n        nr_epochs_val_period\n            Number of epochs to wait before evaluating the validation loss (if a validation\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\n        force_reset\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\n            be discarded). Default: ``False``.\n        save_checkpoints\n            Whether or not to automatically save the untrained model and checkpoints from training.\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\n        add_encoders\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\n            will be used as index encoders. Additionally, a transformer such as Darts' :class:`Scaler` can be added to\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\n            model creation.\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\n\n            .. highlight:: python\n            .. code-block:: python\n\n                def encode_year(idx):\n                    return (idx.year - 1950) / 50\n\n                add_encoders={\n                    'cyclic': {'future': ['month']},\n                    'datetime_attribute': {'future': ['hour', 'dayofweek']},\n                    'position': {'past': ['relative'], 'future': ['relative']},\n                    'custom': {'past': [encode_year]},\n                    'transformer': Scaler(),\n                    'tz': 'CET'\n                }\n            ..\n        random_state\n            Control the randomness of the weight's initialization. Check this\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\n            Default: ``None``.\n        pl_trainer_kwargs\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\n            that performs the training, validation and prediction processes. These presets include automatic\n            checkpointing, tensorboard logging, setting the torch device and more.\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\n            object. Check the `PL Trainer documentation\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\n            supported kwargs. Default: ``None``.\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\n            dict:\n\n\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\n\n            For more info, see here:\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\n\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts'\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\n            specifications. For more information on callbacks, visit:\n            `PyTorch Lightning Callbacks\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\n\n            .. highlight:: python\n            .. code-block:: python\n\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\n                # a period of 5 epochs (`patience`)\n                my_stopper = EarlyStopping(\n                    monitor=\"val_loss\",\n                    patience=5,\n                    min_delta=0.05,\n                    mode='min',\n                )\n\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\n            ..\n\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\n        show_warnings\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\n            your forecasting use case. Default: ``False``.\n\n        References\n        ----------\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\n\n        Examples\n        --------\n        >>> from darts.datasets import WeatherDataset\n        >>> from darts.models import TFTModel\n        >>> series = WeatherDataset().load()\n        >>> # predicting atmospheric pressure\n        >>> target = series['p (mbar)'][:100]\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\n        >>> past_cov = series['rain (mm)'][:100]\n        >>> # future temperatures (pretending this component is a forecast)\n        >>> future_cov = series['T (degC)'][:106]\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\n        >>> model = TFTModel(\n        >>>     input_chunk_length=6,\n        >>>     output_chunk_length=6,\n        >>>     n_epochs=5,\n        >>> )\n        >>> # future_covariates are mandatory for `TFTModel`\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\n        >>> pred = model.predict(6, num_samples=100)\n        >>> # shape : (forecast horizon, components, num_samples)\n        >>> pred.all_values().shape\n        (6, 1, 100)\n        >>> # showing the first 3 samples for each timestamp\n        >>> pred.all_values()[:,:,:3]\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\n               [[ 0.02928407, -0.40867163,  1.19650033]],\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\n               [[-0.83076568, -0.25780816, -0.28318784]]])\n\n        .. note::\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\n        \"\"\"\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates",
        "mutated": [
            "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    if False:\n        i = 10\n    'Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\\n\\n        This is an implementation of the TFT architecture, as outlined in [1]_.\\n\\n        The internal sub models are adopted from `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\\n\\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\\n\\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\\n        :func:`predict()`.\\n\\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\\n\\n        Parameters\\n        ----------\\n        input_chunk_length\\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\\n        output_chunk_length\\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\\n        hidden_size\\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers\\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads\\n            Number of attention heads (4 is a good default)\\n        full_attention\\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\\n            current, and future time steps. Defaults to ``False``.\\n        feed_forward\\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant\\'s\\n            FeedForward Network (FFN)[2]. The glu variant\\'s FeedForward Network are a series of FFNs designed to work\\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\\n        dropout\\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\\n            prediction time).\\n        hidden_continuous_size\\n            Default for hidden size for processing continuous variables\\n        categorical_embedding_sizes\\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\\n            ``min(round(1.6 * n**0.56), 100)``.\\n            For a tuple of integers, give (number of unique categories, embedding size). For example\\n            ``{\"some_column\": (64, 8)}``.\\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        add_relative_index\\n            Whether to add positional values to future covariates. Defaults to ``False``.\\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\\n            to the prediction point. The values are normalized with ``input_chunk_length``.\\n        loss_fn: nn.Module\\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\\n            `likelihood`` to None and give a ``loss_fn`` argument.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\\n        use_static_covariates\\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\\n        **kwargs\\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\\n            Darts\\' :class:`TorchForecastingModel`.\\n\\n        torch_metrics\\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\\n        optimizer_cls\\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\\n        optimizer_kwargs\\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{\\'lr\\': 1e-3}``\\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\\n            will be used. Default: ``None``.\\n        lr_scheduler_cls\\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\\n            to using a constant learning rate. Default: ``None``.\\n        lr_scheduler_kwargs\\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\\n        use_reversible_instance_norm\\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\\n            It is only applied to the features of the target series and not the covariates.\\n        batch_size\\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\\n        n_epochs\\n            Number of epochs over which to train the model. Default: ``100``.\\n        model_name\\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\\n            spawned at the same time by different processes to share the same model_name). E.g.,\\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\\n        work_dir\\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\\n            Default: current working directory.\\n        log_tensorboard\\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\\n        nr_epochs_val_period\\n            Number of epochs to wait before evaluating the validation loss (if a validation\\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\\n        force_reset\\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\\n            be discarded). Default: ``False``.\\n        save_checkpoints\\n            Whether or not to automatically save the untrained model and checkpoints from training.\\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\\n        add_encoders\\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\\n            will be used as index encoders. Additionally, a transformer such as Darts\\' :class:`Scaler` can be added to\\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\\n            model creation.\\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                def encode_year(idx):\\n                    return (idx.year - 1950) / 50\\n\\n                add_encoders={\\n                    \\'cyclic\\': {\\'future\\': [\\'month\\']},\\n                    \\'datetime_attribute\\': {\\'future\\': [\\'hour\\', \\'dayofweek\\']},\\n                    \\'position\\': {\\'past\\': [\\'relative\\'], \\'future\\': [\\'relative\\']},\\n                    \\'custom\\': {\\'past\\': [encode_year]},\\n                    \\'transformer\\': Scaler(),\\n                    \\'tz\\': \\'CET\\'\\n                }\\n            ..\\n        random_state\\n            Control the randomness of the weight\\'s initialization. Check this\\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\\n            Default: ``None``.\\n        pl_trainer_kwargs\\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\\n            that performs the training, validation and prediction processes. These presets include automatic\\n            checkpointing, tensorboard logging, setting the torch device and more.\\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\\n            object. Check the `PL Trainer documentation\\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\\n            supported kwargs. Default: ``None``.\\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\\n            dict:\\n\\n\\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\\n\\n            For more info, see here:\\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\\n\\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts\\'\\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\\n            specifications. For more information on callbacks, visit:\\n            `PyTorch Lightning Callbacks\\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\\n\\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\\n                # a period of 5 epochs (`patience`)\\n                my_stopper = EarlyStopping(\\n                    monitor=\"val_loss\",\\n                    patience=5,\\n                    min_delta=0.05,\\n                    mode=\\'min\\',\\n                )\\n\\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\\n            ..\\n\\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\\n        show_warnings\\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\\n            your forecasting use case. Default: ``False``.\\n\\n        References\\n        ----------\\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import WeatherDataset\\n        >>> from darts.models import TFTModel\\n        >>> series = WeatherDataset().load()\\n        >>> # predicting atmospheric pressure\\n        >>> target = series[\\'p (mbar)\\'][:100]\\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\\n        >>> past_cov = series[\\'rain (mm)\\'][:100]\\n        >>> # future temperatures (pretending this component is a forecast)\\n        >>> future_cov = series[\\'T (degC)\\'][:106]\\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=6,\\n        >>>     output_chunk_length=6,\\n        >>>     n_epochs=5,\\n        >>> )\\n        >>> # future_covariates are mandatory for `TFTModel`\\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\\n        >>> pred = model.predict(6, num_samples=100)\\n        >>> # shape : (forecast horizon, components, num_samples)\\n        >>> pred.all_values().shape\\n        (6, 1, 100)\\n        >>> # showing the first 3 samples for each timestamp\\n        >>> pred.all_values()[:,:,:3]\\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\\n               [[ 0.02928407, -0.40867163,  1.19650033]],\\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\\n               [[-0.83076568, -0.25780816, -0.28318784]]])\\n\\n        .. note::\\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\\n        '\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates",
            "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\\n\\n        This is an implementation of the TFT architecture, as outlined in [1]_.\\n\\n        The internal sub models are adopted from `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\\n\\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\\n\\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\\n        :func:`predict()`.\\n\\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\\n\\n        Parameters\\n        ----------\\n        input_chunk_length\\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\\n        output_chunk_length\\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\\n        hidden_size\\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers\\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads\\n            Number of attention heads (4 is a good default)\\n        full_attention\\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\\n            current, and future time steps. Defaults to ``False``.\\n        feed_forward\\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant\\'s\\n            FeedForward Network (FFN)[2]. The glu variant\\'s FeedForward Network are a series of FFNs designed to work\\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\\n        dropout\\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\\n            prediction time).\\n        hidden_continuous_size\\n            Default for hidden size for processing continuous variables\\n        categorical_embedding_sizes\\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\\n            ``min(round(1.6 * n**0.56), 100)``.\\n            For a tuple of integers, give (number of unique categories, embedding size). For example\\n            ``{\"some_column\": (64, 8)}``.\\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        add_relative_index\\n            Whether to add positional values to future covariates. Defaults to ``False``.\\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\\n            to the prediction point. The values are normalized with ``input_chunk_length``.\\n        loss_fn: nn.Module\\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\\n            `likelihood`` to None and give a ``loss_fn`` argument.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\\n        use_static_covariates\\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\\n        **kwargs\\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\\n            Darts\\' :class:`TorchForecastingModel`.\\n\\n        torch_metrics\\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\\n        optimizer_cls\\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\\n        optimizer_kwargs\\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{\\'lr\\': 1e-3}``\\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\\n            will be used. Default: ``None``.\\n        lr_scheduler_cls\\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\\n            to using a constant learning rate. Default: ``None``.\\n        lr_scheduler_kwargs\\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\\n        use_reversible_instance_norm\\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\\n            It is only applied to the features of the target series and not the covariates.\\n        batch_size\\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\\n        n_epochs\\n            Number of epochs over which to train the model. Default: ``100``.\\n        model_name\\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\\n            spawned at the same time by different processes to share the same model_name). E.g.,\\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\\n        work_dir\\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\\n            Default: current working directory.\\n        log_tensorboard\\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\\n        nr_epochs_val_period\\n            Number of epochs to wait before evaluating the validation loss (if a validation\\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\\n        force_reset\\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\\n            be discarded). Default: ``False``.\\n        save_checkpoints\\n            Whether or not to automatically save the untrained model and checkpoints from training.\\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\\n        add_encoders\\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\\n            will be used as index encoders. Additionally, a transformer such as Darts\\' :class:`Scaler` can be added to\\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\\n            model creation.\\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                def encode_year(idx):\\n                    return (idx.year - 1950) / 50\\n\\n                add_encoders={\\n                    \\'cyclic\\': {\\'future\\': [\\'month\\']},\\n                    \\'datetime_attribute\\': {\\'future\\': [\\'hour\\', \\'dayofweek\\']},\\n                    \\'position\\': {\\'past\\': [\\'relative\\'], \\'future\\': [\\'relative\\']},\\n                    \\'custom\\': {\\'past\\': [encode_year]},\\n                    \\'transformer\\': Scaler(),\\n                    \\'tz\\': \\'CET\\'\\n                }\\n            ..\\n        random_state\\n            Control the randomness of the weight\\'s initialization. Check this\\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\\n            Default: ``None``.\\n        pl_trainer_kwargs\\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\\n            that performs the training, validation and prediction processes. These presets include automatic\\n            checkpointing, tensorboard logging, setting the torch device and more.\\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\\n            object. Check the `PL Trainer documentation\\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\\n            supported kwargs. Default: ``None``.\\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\\n            dict:\\n\\n\\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\\n\\n            For more info, see here:\\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\\n\\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts\\'\\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\\n            specifications. For more information on callbacks, visit:\\n            `PyTorch Lightning Callbacks\\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\\n\\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\\n                # a period of 5 epochs (`patience`)\\n                my_stopper = EarlyStopping(\\n                    monitor=\"val_loss\",\\n                    patience=5,\\n                    min_delta=0.05,\\n                    mode=\\'min\\',\\n                )\\n\\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\\n            ..\\n\\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\\n        show_warnings\\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\\n            your forecasting use case. Default: ``False``.\\n\\n        References\\n        ----------\\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import WeatherDataset\\n        >>> from darts.models import TFTModel\\n        >>> series = WeatherDataset().load()\\n        >>> # predicting atmospheric pressure\\n        >>> target = series[\\'p (mbar)\\'][:100]\\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\\n        >>> past_cov = series[\\'rain (mm)\\'][:100]\\n        >>> # future temperatures (pretending this component is a forecast)\\n        >>> future_cov = series[\\'T (degC)\\'][:106]\\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=6,\\n        >>>     output_chunk_length=6,\\n        >>>     n_epochs=5,\\n        >>> )\\n        >>> # future_covariates are mandatory for `TFTModel`\\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\\n        >>> pred = model.predict(6, num_samples=100)\\n        >>> # shape : (forecast horizon, components, num_samples)\\n        >>> pred.all_values().shape\\n        (6, 1, 100)\\n        >>> # showing the first 3 samples for each timestamp\\n        >>> pred.all_values()[:,:,:3]\\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\\n               [[ 0.02928407, -0.40867163,  1.19650033]],\\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\\n               [[-0.83076568, -0.25780816, -0.28318784]]])\\n\\n        .. note::\\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\\n        '\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates",
            "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\\n\\n        This is an implementation of the TFT architecture, as outlined in [1]_.\\n\\n        The internal sub models are adopted from `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\\n\\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\\n\\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\\n        :func:`predict()`.\\n\\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\\n\\n        Parameters\\n        ----------\\n        input_chunk_length\\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\\n        output_chunk_length\\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\\n        hidden_size\\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers\\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads\\n            Number of attention heads (4 is a good default)\\n        full_attention\\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\\n            current, and future time steps. Defaults to ``False``.\\n        feed_forward\\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant\\'s\\n            FeedForward Network (FFN)[2]. The glu variant\\'s FeedForward Network are a series of FFNs designed to work\\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\\n        dropout\\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\\n            prediction time).\\n        hidden_continuous_size\\n            Default for hidden size for processing continuous variables\\n        categorical_embedding_sizes\\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\\n            ``min(round(1.6 * n**0.56), 100)``.\\n            For a tuple of integers, give (number of unique categories, embedding size). For example\\n            ``{\"some_column\": (64, 8)}``.\\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        add_relative_index\\n            Whether to add positional values to future covariates. Defaults to ``False``.\\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\\n            to the prediction point. The values are normalized with ``input_chunk_length``.\\n        loss_fn: nn.Module\\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\\n            `likelihood`` to None and give a ``loss_fn`` argument.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\\n        use_static_covariates\\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\\n        **kwargs\\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\\n            Darts\\' :class:`TorchForecastingModel`.\\n\\n        torch_metrics\\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\\n        optimizer_cls\\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\\n        optimizer_kwargs\\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{\\'lr\\': 1e-3}``\\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\\n            will be used. Default: ``None``.\\n        lr_scheduler_cls\\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\\n            to using a constant learning rate. Default: ``None``.\\n        lr_scheduler_kwargs\\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\\n        use_reversible_instance_norm\\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\\n            It is only applied to the features of the target series and not the covariates.\\n        batch_size\\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\\n        n_epochs\\n            Number of epochs over which to train the model. Default: ``100``.\\n        model_name\\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\\n            spawned at the same time by different processes to share the same model_name). E.g.,\\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\\n        work_dir\\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\\n            Default: current working directory.\\n        log_tensorboard\\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\\n        nr_epochs_val_period\\n            Number of epochs to wait before evaluating the validation loss (if a validation\\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\\n        force_reset\\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\\n            be discarded). Default: ``False``.\\n        save_checkpoints\\n            Whether or not to automatically save the untrained model and checkpoints from training.\\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\\n        add_encoders\\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\\n            will be used as index encoders. Additionally, a transformer such as Darts\\' :class:`Scaler` can be added to\\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\\n            model creation.\\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                def encode_year(idx):\\n                    return (idx.year - 1950) / 50\\n\\n                add_encoders={\\n                    \\'cyclic\\': {\\'future\\': [\\'month\\']},\\n                    \\'datetime_attribute\\': {\\'future\\': [\\'hour\\', \\'dayofweek\\']},\\n                    \\'position\\': {\\'past\\': [\\'relative\\'], \\'future\\': [\\'relative\\']},\\n                    \\'custom\\': {\\'past\\': [encode_year]},\\n                    \\'transformer\\': Scaler(),\\n                    \\'tz\\': \\'CET\\'\\n                }\\n            ..\\n        random_state\\n            Control the randomness of the weight\\'s initialization. Check this\\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\\n            Default: ``None``.\\n        pl_trainer_kwargs\\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\\n            that performs the training, validation and prediction processes. These presets include automatic\\n            checkpointing, tensorboard logging, setting the torch device and more.\\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\\n            object. Check the `PL Trainer documentation\\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\\n            supported kwargs. Default: ``None``.\\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\\n            dict:\\n\\n\\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\\n\\n            For more info, see here:\\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\\n\\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts\\'\\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\\n            specifications. For more information on callbacks, visit:\\n            `PyTorch Lightning Callbacks\\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\\n\\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\\n                # a period of 5 epochs (`patience`)\\n                my_stopper = EarlyStopping(\\n                    monitor=\"val_loss\",\\n                    patience=5,\\n                    min_delta=0.05,\\n                    mode=\\'min\\',\\n                )\\n\\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\\n            ..\\n\\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\\n        show_warnings\\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\\n            your forecasting use case. Default: ``False``.\\n\\n        References\\n        ----------\\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import WeatherDataset\\n        >>> from darts.models import TFTModel\\n        >>> series = WeatherDataset().load()\\n        >>> # predicting atmospheric pressure\\n        >>> target = series[\\'p (mbar)\\'][:100]\\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\\n        >>> past_cov = series[\\'rain (mm)\\'][:100]\\n        >>> # future temperatures (pretending this component is a forecast)\\n        >>> future_cov = series[\\'T (degC)\\'][:106]\\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=6,\\n        >>>     output_chunk_length=6,\\n        >>>     n_epochs=5,\\n        >>> )\\n        >>> # future_covariates are mandatory for `TFTModel`\\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\\n        >>> pred = model.predict(6, num_samples=100)\\n        >>> # shape : (forecast horizon, components, num_samples)\\n        >>> pred.all_values().shape\\n        (6, 1, 100)\\n        >>> # showing the first 3 samples for each timestamp\\n        >>> pred.all_values()[:,:,:3]\\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\\n               [[ 0.02928407, -0.40867163,  1.19650033]],\\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\\n               [[-0.83076568, -0.25780816, -0.28318784]]])\\n\\n        .. note::\\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\\n        '\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates",
            "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\\n\\n        This is an implementation of the TFT architecture, as outlined in [1]_.\\n\\n        The internal sub models are adopted from `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\\n\\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\\n\\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\\n        :func:`predict()`.\\n\\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\\n\\n        Parameters\\n        ----------\\n        input_chunk_length\\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\\n        output_chunk_length\\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\\n        hidden_size\\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers\\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads\\n            Number of attention heads (4 is a good default)\\n        full_attention\\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\\n            current, and future time steps. Defaults to ``False``.\\n        feed_forward\\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant\\'s\\n            FeedForward Network (FFN)[2]. The glu variant\\'s FeedForward Network are a series of FFNs designed to work\\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\\n        dropout\\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\\n            prediction time).\\n        hidden_continuous_size\\n            Default for hidden size for processing continuous variables\\n        categorical_embedding_sizes\\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\\n            ``min(round(1.6 * n**0.56), 100)``.\\n            For a tuple of integers, give (number of unique categories, embedding size). For example\\n            ``{\"some_column\": (64, 8)}``.\\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        add_relative_index\\n            Whether to add positional values to future covariates. Defaults to ``False``.\\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\\n            to the prediction point. The values are normalized with ``input_chunk_length``.\\n        loss_fn: nn.Module\\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\\n            `likelihood`` to None and give a ``loss_fn`` argument.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\\n        use_static_covariates\\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\\n        **kwargs\\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\\n            Darts\\' :class:`TorchForecastingModel`.\\n\\n        torch_metrics\\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\\n        optimizer_cls\\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\\n        optimizer_kwargs\\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{\\'lr\\': 1e-3}``\\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\\n            will be used. Default: ``None``.\\n        lr_scheduler_cls\\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\\n            to using a constant learning rate. Default: ``None``.\\n        lr_scheduler_kwargs\\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\\n        use_reversible_instance_norm\\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\\n            It is only applied to the features of the target series and not the covariates.\\n        batch_size\\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\\n        n_epochs\\n            Number of epochs over which to train the model. Default: ``100``.\\n        model_name\\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\\n            spawned at the same time by different processes to share the same model_name). E.g.,\\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\\n        work_dir\\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\\n            Default: current working directory.\\n        log_tensorboard\\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\\n        nr_epochs_val_period\\n            Number of epochs to wait before evaluating the validation loss (if a validation\\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\\n        force_reset\\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\\n            be discarded). Default: ``False``.\\n        save_checkpoints\\n            Whether or not to automatically save the untrained model and checkpoints from training.\\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\\n        add_encoders\\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\\n            will be used as index encoders. Additionally, a transformer such as Darts\\' :class:`Scaler` can be added to\\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\\n            model creation.\\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                def encode_year(idx):\\n                    return (idx.year - 1950) / 50\\n\\n                add_encoders={\\n                    \\'cyclic\\': {\\'future\\': [\\'month\\']},\\n                    \\'datetime_attribute\\': {\\'future\\': [\\'hour\\', \\'dayofweek\\']},\\n                    \\'position\\': {\\'past\\': [\\'relative\\'], \\'future\\': [\\'relative\\']},\\n                    \\'custom\\': {\\'past\\': [encode_year]},\\n                    \\'transformer\\': Scaler(),\\n                    \\'tz\\': \\'CET\\'\\n                }\\n            ..\\n        random_state\\n            Control the randomness of the weight\\'s initialization. Check this\\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\\n            Default: ``None``.\\n        pl_trainer_kwargs\\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\\n            that performs the training, validation and prediction processes. These presets include automatic\\n            checkpointing, tensorboard logging, setting the torch device and more.\\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\\n            object. Check the `PL Trainer documentation\\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\\n            supported kwargs. Default: ``None``.\\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\\n            dict:\\n\\n\\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\\n\\n            For more info, see here:\\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\\n\\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts\\'\\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\\n            specifications. For more information on callbacks, visit:\\n            `PyTorch Lightning Callbacks\\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\\n\\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\\n                # a period of 5 epochs (`patience`)\\n                my_stopper = EarlyStopping(\\n                    monitor=\"val_loss\",\\n                    patience=5,\\n                    min_delta=0.05,\\n                    mode=\\'min\\',\\n                )\\n\\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\\n            ..\\n\\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\\n        show_warnings\\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\\n            your forecasting use case. Default: ``False``.\\n\\n        References\\n        ----------\\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import WeatherDataset\\n        >>> from darts.models import TFTModel\\n        >>> series = WeatherDataset().load()\\n        >>> # predicting atmospheric pressure\\n        >>> target = series[\\'p (mbar)\\'][:100]\\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\\n        >>> past_cov = series[\\'rain (mm)\\'][:100]\\n        >>> # future temperatures (pretending this component is a forecast)\\n        >>> future_cov = series[\\'T (degC)\\'][:106]\\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=6,\\n        >>>     output_chunk_length=6,\\n        >>>     n_epochs=5,\\n        >>> )\\n        >>> # future_covariates are mandatory for `TFTModel`\\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\\n        >>> pred = model.predict(6, num_samples=100)\\n        >>> # shape : (forecast horizon, components, num_samples)\\n        >>> pred.all_values().shape\\n        (6, 1, 100)\\n        >>> # showing the first 3 samples for each timestamp\\n        >>> pred.all_values()[:,:,:3]\\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\\n               [[ 0.02928407, -0.40867163,  1.19650033]],\\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\\n               [[-0.83076568, -0.25780816, -0.28318784]]])\\n\\n        .. note::\\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\\n        '\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates",
            "def __init__(self, input_chunk_length: int, output_chunk_length: int, hidden_size: Union[int, List[int]]=16, lstm_layers: int=1, num_attention_heads: int=4, full_attention: bool=False, feed_forward: str='GatedResidualNetwork', dropout: float=0.1, hidden_continuous_size: int=8, categorical_embedding_sizes: Optional[Dict[str, Union[int, Tuple[int, int]]]]=None, add_relative_index: bool=False, loss_fn: Optional[nn.Module]=None, likelihood: Optional[Likelihood]=None, norm_type: Union[str, nn.Module]='LayerNorm', use_static_covariates: bool=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Temporal Fusion Transformers (TFT) for Interpretable Time Series Forecasting.\\n\\n        This is an implementation of the TFT architecture, as outlined in [1]_.\\n\\n        The internal sub models are adopted from `pytorch-forecasting\\'s TemporalFusionTransformer\\n        <https://pytorch-forecasting.readthedocs.io/en/latest/models.html>`_ implementation.\\n\\n        This model supports mixed covariates (includes past covariates known for ``input_chunk_length``\\n        points before prediction time and future covariates known for ``output_chunk_length`` after prediction time).\\n\\n        The TFT applies multi-head attention queries on future inputs from mandatory ``future_covariates``.\\n        Specifying future encoders with ``add_encoders`` (read below) can automatically generate future covariates\\n        and allows to use the model without having to pass any ``future_covariates`` to :func:`fit()` and\\n        :func:`predict()`.\\n\\n        By default, this model uses the ``QuantileRegression`` likelihood, which means that its forecasts are\\n        probabilistic; it is recommended to call :func`predict()` with ``num_samples >> 1`` to get meaningful results.\\n\\n        Parameters\\n        ----------\\n        input_chunk_length\\n            Encoder length; number of past time steps that are fed to the forecasting module at prediction time.\\n        output_chunk_length\\n            Decoder length; number of future time steps that are fed to the forecasting module at prediction time.\\n        hidden_size\\n            Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT\\n            architecture.\\n        lstm_layers\\n            Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\\n        num_attention_heads\\n            Number of attention heads (4 is a good default)\\n        full_attention\\n            If ``False``, only attends to previous time steps in the decoder. If ``True`` attends to previous,\\n            current, and future time steps. Defaults to ``False``.\\n        feed_forward\\n            A feedforward network is a fully-connected layer with an activation. TFT Can be one of the glu variant\\'s\\n            FeedForward Network (FFN)[2]. The glu variant\\'s FeedForward Network are a series of FFNs designed to work\\n            better with Transformer based models. Defaults to ``\"GatedResidualNetwork\"``. [\"GLU\", \"Bilinear\", \"ReGLU\",\\n            \"GEGLU\", \"SwiGLU\", \"ReLU\", \"GELU\"] or the TFT original FeedForward Network [\"GatedResidualNetwork\"].\\n        dropout\\n            Fraction of neurons affected by dropout. This is compatible with Monte Carlo dropout\\n            at inference time for model uncertainty estimation (enabled with ``mc_dropout=True`` at\\n            prediction time).\\n        hidden_continuous_size\\n            Default for hidden size for processing continuous variables\\n        categorical_embedding_sizes\\n            A dictionary used to construct embeddings for categorical static covariates. The keys are the column names\\n            of the categorical static covariates. Each value is either a single integer or a tuple of integers.\\n            For a single integer give the number of unique categories (n) of the corresponding variable. For example\\n            ``{\"some_column\": 64}``. The embedding size will be automatically determined by\\n            ``min(round(1.6 * n**0.56), 100)``.\\n            For a tuple of integers, give (number of unique categories, embedding size). For example\\n            ``{\"some_column\": (64, 8)}``.\\n            Note that ``TorchForecastingModels`` only support numeric data. Consider transforming/encoding your data\\n            with `darts.dataprocessing.transformers.static_covariates_transformer.StaticCovariatesTransformer`.\\n        add_relative_index\\n            Whether to add positional values to future covariates. Defaults to ``False``.\\n            This allows to use the TFTModel without having to pass future_covariates to :func:`fit()` and\\n            :func:`train()`. It gives a value to the position of each step from input and output chunk relative\\n            to the prediction point. The values are normalized with ``input_chunk_length``.\\n        loss_fn: nn.Module\\n            PyTorch loss function used for training. By default, the TFT model is probabilistic and uses a\\n            ``likelihood`` instead (``QuantileRegression``). To make the model deterministic, you can set the `\\n            `likelihood`` to None and give a ``loss_fn`` argument.\\n        likelihood\\n            The likelihood model to be used for probabilistic forecasts. By default, the TFT uses\\n            a ``QuantileRegression`` likelihood.\\n        norm_type: str | nn.Module\\n            The type of LayerNorm variant to use.  Default: ``LayerNorm``. Available options are\\n            [\"LayerNorm\", \"RMSNorm\", \"LayerNormNoBias\"], or provide a custom nn.Module.\\n        use_static_covariates\\n            Whether the model should use static covariate information in case the input `series` passed to ``fit()``\\n            contain static covariates. If ``True``, and static covariates are available at fitting time, will enforce\\n            that all target `series` have the same static covariate dimensionality in ``fit()`` and ``predict()``.\\n        **kwargs\\n            Optional arguments to initialize the pytorch_lightning.Module, pytorch_lightning.Trainer, and\\n            Darts\\' :class:`TorchForecastingModel`.\\n\\n        torch_metrics\\n            A torch metric or a ``MetricCollection`` used for evaluation. A full list of available metrics can be found\\n            at https://torchmetrics.readthedocs.io/en/latest/. Default: ``None``.\\n        optimizer_cls\\n            The PyTorch optimizer class to be used. Default: ``torch.optim.Adam``.\\n        optimizer_kwargs\\n            Optionally, some keyword arguments for the PyTorch optimizer (e.g., ``{\\'lr\\': 1e-3}``\\n            for specifying a learning rate). Otherwise, the default values of the selected ``optimizer_cls``\\n            will be used. Default: ``None``.\\n        lr_scheduler_cls\\n            Optionally, the PyTorch learning rate scheduler class to be used. Specifying ``None`` corresponds\\n            to using a constant learning rate. Default: ``None``.\\n        lr_scheduler_kwargs\\n            Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: ``None``.\\n        use_reversible_instance_norm\\n            Whether to use reversible instance normalization `RINorm` against distribution shift as shown in [3]_.\\n            It is only applied to the features of the target series and not the covariates.\\n        batch_size\\n            Number of time series (input and output sequences) used in each training pass. Default: ``32``.\\n        n_epochs\\n            Number of epochs over which to train the model. Default: ``100``.\\n        model_name\\n            Name of the model. Used for creating checkpoints and saving tensorboard data. If not specified,\\n            defaults to the following string ``\"YYYY-mm-dd_HH_MM_SS_torch_model_run_PID\"``, where the initial part\\n            of the name is formatted with the local date and time, while PID is the processed ID (preventing models\\n            spawned at the same time by different processes to share the same model_name). E.g.,\\n            ``\"2021-06-14_09_53_32_torch_model_run_44607\"``.\\n        work_dir\\n            Path of the working directory, where to save checkpoints and Tensorboard summaries.\\n            Default: current working directory.\\n        log_tensorboard\\n            If set, use Tensorboard to log the different parameters. The logs will be located in:\\n            ``\"{work_dir}/darts_logs/{model_name}/logs/\"``. Default: ``False``.\\n        nr_epochs_val_period\\n            Number of epochs to wait before evaluating the validation loss (if a validation\\n            ``TimeSeries`` is passed to the :func:`fit()` method). Default: ``1``.\\n        force_reset\\n            If set to ``True``, any previously-existing model with the same name will be reset (all checkpoints will\\n            be discarded). Default: ``False``.\\n        save_checkpoints\\n            Whether or not to automatically save the untrained model and checkpoints from training.\\n            To load the model from checkpoint, call :func:`MyModelClass.load_from_checkpoint()`, where\\n            :class:`MyModelClass` is the :class:`TorchForecastingModel` class that was used (such as :class:`TFTModel`,\\n            :class:`NBEATSModel`, etc.). If set to ``False``, the model can still be manually saved using\\n            :func:`save()` and loaded using :func:`load()`. Default: ``False``.\\n        add_encoders\\n            A large number of past and future covariates can be automatically generated with `add_encoders`.\\n            This can be done by adding multiple pre-defined index encoders and/or custom user-made functions that\\n            will be used as index encoders. Additionally, a transformer such as Darts\\' :class:`Scaler` can be added to\\n            transform the generated covariates. This happens all under one hood and only needs to be specified at\\n            model creation.\\n            Read :meth:`SequentialEncoder <darts.dataprocessing.encoders.SequentialEncoder>` to find out more about\\n            ``add_encoders``. Default: ``None``. An example showing some of ``add_encoders`` features:\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                def encode_year(idx):\\n                    return (idx.year - 1950) / 50\\n\\n                add_encoders={\\n                    \\'cyclic\\': {\\'future\\': [\\'month\\']},\\n                    \\'datetime_attribute\\': {\\'future\\': [\\'hour\\', \\'dayofweek\\']},\\n                    \\'position\\': {\\'past\\': [\\'relative\\'], \\'future\\': [\\'relative\\']},\\n                    \\'custom\\': {\\'past\\': [encode_year]},\\n                    \\'transformer\\': Scaler(),\\n                    \\'tz\\': \\'CET\\'\\n                }\\n            ..\\n        random_state\\n            Control the randomness of the weight\\'s initialization. Check this\\n            `link <https://scikit-learn.org/stable/glossary.html#term-random_state>`_ for more details.\\n            Default: ``None``.\\n        pl_trainer_kwargs\\n            By default :class:`TorchForecastingModel` creates a PyTorch Lightning Trainer with several useful presets\\n            that performs the training, validation and prediction processes. These presets include automatic\\n            checkpointing, tensorboard logging, setting the torch device and more.\\n            With ``pl_trainer_kwargs`` you can add additional kwargs to instantiate the PyTorch Lightning trainer\\n            object. Check the `PL Trainer documentation\\n            <https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html>`_ for more information about the\\n            supported kwargs. Default: ``None``.\\n            Running on GPU(s) is also possible using ``pl_trainer_kwargs`` by specifying keys ``\"accelerator\",\\n            \"devices\", and \"auto_select_gpus\"``. Some examples for setting the devices inside the ``pl_trainer_kwargs``\\n            dict:\\n\\n\\n            - ``{\"accelerator\": \"cpu\"}`` for CPU,\\n            - ``{\"accelerator\": \"gpu\", \"devices\": [i]}`` to use only GPU ``i`` (``i`` must be an integer),\\n            - ``{\"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}`` to use all available GPUS.\\n\\n            For more info, see here:\\n            https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags , and\\n            https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_basic.html#train-on-multiple-gpus\\n\\n            With parameter ``\"callbacks\"`` you can add custom or PyTorch-Lightning built-in callbacks to Darts\\'\\n            :class:`TorchForecastingModel`. Below is an example for adding EarlyStopping to the training process.\\n            The model will stop training early if the validation loss `val_loss` does not improve beyond\\n            specifications. For more information on callbacks, visit:\\n            `PyTorch Lightning Callbacks\\n            <https://pytorch-lightning.readthedocs.io/en/stable/extensions/callbacks.html>`_\\n\\n            .. highlight:: python\\n            .. code-block:: python\\n\\n                from pytorch_lightning.callbacks.early_stopping import EarlyStopping\\n\\n                # stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\\n                # a period of 5 epochs (`patience`)\\n                my_stopper = EarlyStopping(\\n                    monitor=\"val_loss\",\\n                    patience=5,\\n                    min_delta=0.05,\\n                    mode=\\'min\\',\\n                )\\n\\n                pl_trainer_kwargs={\"callbacks\": [my_stopper]}\\n            ..\\n\\n            Note that you can also use a custom PyTorch Lightning Trainer for training and prediction with optional\\n            parameter ``trainer`` in :func:`fit()` and :func:`predict()`.\\n        show_warnings\\n            whether to show warnings raised from PyTorch Lightning. Useful to detect potential issues of\\n            your forecasting use case. Default: ``False``.\\n\\n        References\\n        ----------\\n        .. [1] https://arxiv.org/pdf/1912.09363.pdf\\n        .. [2] Shazeer, Noam, \"GLU Variants Improve Transformer\", 2020. arVix https://arxiv.org/abs/2002.05202.\\n        .. [3] T. Kim et al. \"Reversible Instance Normalization for Accurate Time-Series Forecasting against\\n                Distribution Shift\", https://openreview.net/forum?id=cGDAkQo1C0p\\n\\n        Examples\\n        --------\\n        >>> from darts.datasets import WeatherDataset\\n        >>> from darts.models import TFTModel\\n        >>> series = WeatherDataset().load()\\n        >>> # predicting atmospheric pressure\\n        >>> target = series[\\'p (mbar)\\'][:100]\\n        >>> # optionally, past observed rainfall (pretending to be unknown beyond index 100)\\n        >>> past_cov = series[\\'rain (mm)\\'][:100]\\n        >>> # future temperatures (pretending this component is a forecast)\\n        >>> future_cov = series[\\'T (degC)\\'][:106]\\n        >>> # by default, TFTModel is trained using a `QuantileRegression` making it a probabilistic forecasting model\\n        >>> model = TFTModel(\\n        >>>     input_chunk_length=6,\\n        >>>     output_chunk_length=6,\\n        >>>     n_epochs=5,\\n        >>> )\\n        >>> # future_covariates are mandatory for `TFTModel`\\n        >>> model.fit(target, past_covariates=past_cov, future_covariates=future_cov)\\n        >>> # TFTModel is probabilistic by definition; using `num_samples >> 1` to generate probabilistic forecasts\\n        >>> pred = model.predict(6, num_samples=100)\\n        >>> # shape : (forecast horizon, components, num_samples)\\n        >>> pred.all_values().shape\\n        (6, 1, 100)\\n        >>> # showing the first 3 samples for each timestamp\\n        >>> pred.all_values()[:,:,:3]\\n        array([[[-0.06414202, -0.7188093 ,  0.52541292]],\\n               [[ 0.02928407, -0.40867163,  1.19650033]],\\n               [[ 0.77252372, -0.50859694,  0.360166  ]],\\n               [[ 0.9586113 ,  1.24147138, -0.01625545]],\\n               [[ 1.06863863,  0.2987822 , -0.69213369]],\\n               [[-0.83076568, -0.25780816, -0.28318784]]])\\n\\n        .. note::\\n            `TFT example notebook <https://unit8co.github.io/darts/examples/13-TFT-examples.html>`_ presents\\n            techniques that can be used to improve the forecasts quality compared to this simple usage example.\\n        '\n    model_kwargs = {key: val for (key, val) in self.model_params.items()}\n    if likelihood is None and loss_fn is None:\n        model_kwargs['loss_fn'] = None\n        model_kwargs['likelihood'] = QuantileRegression()\n    super().__init__(**self._extract_torch_model_params(**model_kwargs))\n    self.pl_module_params = self._extract_pl_module_params(**model_kwargs)\n    self.hidden_size = hidden_size\n    self.lstm_layers = lstm_layers\n    self.num_attention_heads = num_attention_heads\n    self.full_attention = full_attention\n    self.feed_forward = feed_forward\n    self.dropout = dropout\n    self.hidden_continuous_size = hidden_continuous_size\n    self.categorical_embedding_sizes = categorical_embedding_sizes if categorical_embedding_sizes is not None else {}\n    self.add_relative_index = add_relative_index\n    self.output_dim: Optional[Tuple[int, int]] = None\n    self.norm_type = norm_type\n    self._considers_static_covariates = use_static_covariates"
        ]
    },
    {
        "func_name": "_create_model",
        "original": "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    \"\"\"\n        `train_sample` contains the following tensors:\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\n            future_target)\n\n            each tensor has shape (n_timesteps, n_variables)\n            - past/historic tensors have shape (input_chunk_length, n_variables)\n            - future tensors have shape (output_chunk_length, n_variables)\n            - static covariates have shape (component, static variable)\n\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\n            time_varying_knowns : future_covariates (including historic_future_covariates)\n            time_varying_unknowns : past_targets, past_covariates\n\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\n            time_varying_decoders : [historic_future_covariates, future_covariates]\n\n        `variable_meta` is used in TFT to access specific variables\n        \"\"\"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)",
        "mutated": [
            "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    if False:\n        i = 10\n    \"\\n        `train_sample` contains the following tensors:\\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\\n            future_target)\\n\\n            each tensor has shape (n_timesteps, n_variables)\\n            - past/historic tensors have shape (input_chunk_length, n_variables)\\n            - future tensors have shape (output_chunk_length, n_variables)\\n            - static covariates have shape (component, static variable)\\n\\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\\n            time_varying_knowns : future_covariates (including historic_future_covariates)\\n            time_varying_unknowns : past_targets, past_covariates\\n\\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\\n            time_varying_decoders : [historic_future_covariates, future_covariates]\\n\\n        `variable_meta` is used in TFT to access specific variables\\n        \"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)",
            "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        `train_sample` contains the following tensors:\\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\\n            future_target)\\n\\n            each tensor has shape (n_timesteps, n_variables)\\n            - past/historic tensors have shape (input_chunk_length, n_variables)\\n            - future tensors have shape (output_chunk_length, n_variables)\\n            - static covariates have shape (component, static variable)\\n\\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\\n            time_varying_knowns : future_covariates (including historic_future_covariates)\\n            time_varying_unknowns : past_targets, past_covariates\\n\\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\\n            time_varying_decoders : [historic_future_covariates, future_covariates]\\n\\n        `variable_meta` is used in TFT to access specific variables\\n        \"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)",
            "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        `train_sample` contains the following tensors:\\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\\n            future_target)\\n\\n            each tensor has shape (n_timesteps, n_variables)\\n            - past/historic tensors have shape (input_chunk_length, n_variables)\\n            - future tensors have shape (output_chunk_length, n_variables)\\n            - static covariates have shape (component, static variable)\\n\\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\\n            time_varying_knowns : future_covariates (including historic_future_covariates)\\n            time_varying_unknowns : past_targets, past_covariates\\n\\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\\n            time_varying_decoders : [historic_future_covariates, future_covariates]\\n\\n        `variable_meta` is used in TFT to access specific variables\\n        \"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)",
            "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        `train_sample` contains the following tensors:\\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\\n            future_target)\\n\\n            each tensor has shape (n_timesteps, n_variables)\\n            - past/historic tensors have shape (input_chunk_length, n_variables)\\n            - future tensors have shape (output_chunk_length, n_variables)\\n            - static covariates have shape (component, static variable)\\n\\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\\n            time_varying_knowns : future_covariates (including historic_future_covariates)\\n            time_varying_unknowns : past_targets, past_covariates\\n\\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\\n            time_varying_decoders : [historic_future_covariates, future_covariates]\\n\\n        `variable_meta` is used in TFT to access specific variables\\n        \"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)",
            "def _create_model(self, train_sample: MixedCovariatesTrainTensorType) -> nn.Module:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        `train_sample` contains the following tensors:\\n            (past_target, past_covariates, historic_future_covariates, future_covariates, static_covariates,\\n            future_target)\\n\\n            each tensor has shape (n_timesteps, n_variables)\\n            - past/historic tensors have shape (input_chunk_length, n_variables)\\n            - future tensors have shape (output_chunk_length, n_variables)\\n            - static covariates have shape (component, static variable)\\n\\n        Darts Interpretation of pytorch-forecasting's TimeSeriesDataSet:\\n            time_varying_knowns : future_covariates (including historic_future_covariates)\\n            time_varying_unknowns : past_targets, past_covariates\\n\\n            time_varying_encoders : [past_targets, past_covariates, historic_future_covariates, future_covariates]\\n            time_varying_decoders : [historic_future_covariates, future_covariates]\\n\\n        `variable_meta` is used in TFT to access specific variables\\n        \"\n    (past_target, past_covariate, historic_future_covariate, future_covariate, static_covariates, future_target) = train_sample\n    if self.add_relative_index:\n        time_steps = self.input_chunk_length + self.output_chunk_length\n        expand_future_covariate = np.arange(time_steps).reshape((time_steps, 1))\n        historic_future_covariate = np.concatenate([ts[:self.input_chunk_length] for ts in [historic_future_covariate, expand_future_covariate] if ts is not None], axis=1)\n        future_covariate = np.concatenate([ts[-self.output_chunk_length:] for ts in [future_covariate, expand_future_covariate] if ts is not None], axis=1)\n    self.output_dim = (future_target.shape[1], 1) if self.likelihood is None else (future_target.shape[1], self.likelihood.num_parameters)\n    tensors = [past_target, past_covariate, historic_future_covariate, future_covariate, future_target, static_covariates]\n    type_names = ['past_target', 'past_covariate', 'historic_future_covariate', 'future_covariate', 'future_target', 'static_covariate']\n    variable_names = ['target', 'past_covariate', 'future_covariate', 'future_covariate', 'target', 'static_covariate']\n    variables_meta = {'input': {type_name: [f'{var_name}_{i}' for i in range(tensor.shape[1])] for (type_name, var_name, tensor) in zip(type_names, variable_names, tensors) if tensor is not None}, 'model_config': {}}\n    reals_input = []\n    categorical_input = []\n    time_varying_encoder_input = []\n    time_varying_decoder_input = []\n    static_input = []\n    static_input_numeric = []\n    static_input_categorical = []\n    categorical_embedding_sizes = {}\n    for input_var in type_names:\n        if input_var in variables_meta['input']:\n            vars_meta = variables_meta['input'][input_var]\n            if input_var in ['past_target', 'past_covariate', 'historic_future_covariate']:\n                time_varying_encoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['future_covariate']:\n                time_varying_decoder_input += vars_meta\n                reals_input += vars_meta\n            elif input_var in ['static_covariate']:\n                if self.static_covariates is None:\n                    static_cols = pd.Index([i for i in range(static_covariates.shape[1])])\n                else:\n                    static_cols = self.static_covariates.columns\n                numeric_mask = ~static_cols.isin(self.categorical_embedding_sizes)\n                for (idx, (static_var, col_name, is_numeric)) in enumerate(zip(vars_meta, static_cols, numeric_mask)):\n                    static_input.append(static_var)\n                    if is_numeric:\n                        static_input_numeric.append(static_var)\n                        reals_input.append(static_var)\n                    else:\n                        embedding = self.categorical_embedding_sizes[col_name]\n                        raise_if_not(isinstance(embedding, (int, tuple)), 'Dict values of `categorical_embedding_sizes` must either be integers or tuples. Read the TFTModel documentation for more information.', logger)\n                        if isinstance(embedding, int):\n                            embedding = (embedding, get_embedding_size(n=embedding))\n                        categorical_embedding_sizes[vars_meta[idx]] = embedding\n                        static_input_categorical.append(static_var)\n                        categorical_input.append(static_var)\n    variables_meta['model_config']['reals_input'] = list(dict.fromkeys(reals_input))\n    variables_meta['model_config']['categorical_input'] = list(dict.fromkeys(categorical_input))\n    variables_meta['model_config']['time_varying_encoder_input'] = list(dict.fromkeys(time_varying_encoder_input))\n    variables_meta['model_config']['time_varying_decoder_input'] = list(dict.fromkeys(time_varying_decoder_input))\n    variables_meta['model_config']['static_input'] = list(dict.fromkeys(static_input))\n    variables_meta['model_config']['static_input_numeric'] = list(dict.fromkeys(static_input_numeric))\n    variables_meta['model_config']['static_input_categorical'] = list(dict.fromkeys(static_input_categorical))\n    n_static_components = len(static_covariates) if static_covariates is not None else 0\n    self.categorical_embedding_sizes = categorical_embedding_sizes\n    return _TFTModule(output_dim=self.output_dim, variables_meta=variables_meta, num_static_components=n_static_components, hidden_size=self.hidden_size, lstm_layers=self.lstm_layers, dropout=self.dropout, num_attention_heads=self.num_attention_heads, full_attention=self.full_attention, feed_forward=self.feed_forward, hidden_continuous_size=self.hidden_continuous_size, categorical_embedding_sizes=self.categorical_embedding_sizes, add_relative_index=self.add_relative_index, norm_type=self.norm_type, **self.pl_module_params)"
        ]
    },
    {
        "func_name": "_build_train_dataset",
        "original": "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)",
        "mutated": [
            "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    if False:\n        i = 10\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)",
            "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)",
            "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)",
            "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)",
            "def _build_train_dataset(self, target: Sequence[TimeSeries], past_covariates: Optional[Sequence[TimeSeries]], future_covariates: Optional[Sequence[TimeSeries]], max_samples_per_ts: Optional[int]) -> MixedCovariatesSequentialDataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    raise_if(future_covariates is None and (not self.add_relative_index), 'TFTModel requires future covariates. The model applies multi-head attention queries on future inputs. Consider specifying a future encoder with `add_encoders` or setting `add_relative_index` to `True` at model creation (read TFT model docs for more information). These will automatically generate `future_covariates` from indexes.', logger)\n    return MixedCovariatesSequentialDataset(target_series=target, past_covariates=past_covariates, future_covariates=future_covariates, input_chunk_length=self.input_chunk_length, output_chunk_length=self.output_chunk_length, max_samples_per_ts=max_samples_per_ts, use_static_covariates=self.uses_static_covariates)"
        ]
    },
    {
        "func_name": "_verify_train_dataset_type",
        "original": "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')",
        "mutated": [
            "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    if False:\n        i = 10\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')",
            "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')",
            "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')",
            "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')",
            "def _verify_train_dataset_type(self, train_dataset: TrainingDataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    raise_if_not(isinstance(train_dataset, MixedCovariatesTrainingDataset), 'TFTModel requires a training dataset of type MixedCovariatesTrainingDataset.')"
        ]
    },
    {
        "func_name": "supports_multivariate",
        "original": "@property\ndef supports_multivariate(self) -> bool:\n    return True",
        "mutated": [
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n    return True",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return True",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return True",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return True",
            "@property\ndef supports_multivariate(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return True"
        ]
    },
    {
        "func_name": "supports_static_covariates",
        "original": "@property\ndef supports_static_covariates(self) -> bool:\n    return True",
        "mutated": [
            "@property\ndef supports_static_covariates(self) -> bool:\n    if False:\n        i = 10\n    return True",
            "@property\ndef supports_static_covariates(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return True",
            "@property\ndef supports_static_covariates(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return True",
            "@property\ndef supports_static_covariates(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return True",
            "@property\ndef supports_static_covariates(self) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return True"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, n, *args, **kwargs):\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]",
        "mutated": [
            "def predict(self, n, *args, **kwargs):\n    if False:\n        i = 10\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]",
            "def predict(self, n, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]",
            "def predict(self, n, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]",
            "def predict(self, n, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]",
            "def predict(self, n, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if n >= self.output_chunk_length:\n        return super().predict(n, *args, **kwargs)\n    else:\n        return super().predict(self.output_chunk_length, *args, **kwargs)[:n]"
        ]
    }
]