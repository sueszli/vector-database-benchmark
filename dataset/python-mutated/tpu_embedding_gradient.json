[
    {
        "func_name": "get_gradients_through_compute_gradients",
        "original": "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    \"\"\"Compute gradients to send to TPU embedding.\n\n  Args:\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\n      Used to call compute_gradients().\n    loss: a Tensor to call optimizer.compute_gradients() on.\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\n\n  Returns:\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\n      the loss wrt the activations of the features.\n  \"\"\"\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict",
        "mutated": [
            "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    if False:\n        i = 10\n    'Compute gradients to send to TPU embedding.\\n\\n  Args:\\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\\n      Used to call compute_gradients().\\n    loss: a Tensor to call optimizer.compute_gradients() on.\\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\\n\\n  Returns:\\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\\n      the loss wrt the activations of the features.\\n  '\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict",
            "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute gradients to send to TPU embedding.\\n\\n  Args:\\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\\n      Used to call compute_gradients().\\n    loss: a Tensor to call optimizer.compute_gradients() on.\\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\\n\\n  Returns:\\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\\n      the loss wrt the activations of the features.\\n  '\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict",
            "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute gradients to send to TPU embedding.\\n\\n  Args:\\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\\n      Used to call compute_gradients().\\n    loss: a Tensor to call optimizer.compute_gradients() on.\\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\\n\\n  Returns:\\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\\n      the loss wrt the activations of the features.\\n  '\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict",
            "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute gradients to send to TPU embedding.\\n\\n  Args:\\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\\n      Used to call compute_gradients().\\n    loss: a Tensor to call optimizer.compute_gradients() on.\\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\\n\\n  Returns:\\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\\n      the loss wrt the activations of the features.\\n  '\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict",
            "def get_gradients_through_compute_gradients(optimizer, loss, activations):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute gradients to send to TPU embedding.\\n\\n  Args:\\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\\n      Used to call compute_gradients().\\n    loss: a Tensor to call optimizer.compute_gradients() on.\\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\\n\\n  Returns:\\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\\n      the loss wrt the activations of the features.\\n  '\n    activation_list = activations.values()\n    grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n    grads = [grad for (grad, _) in grads_and_vars]\n    feature_to_gradient_dict = collections.OrderedDict(zip(activations.keys(), grads))\n    return feature_to_gradient_dict"
        ]
    },
    {
        "func_name": "create_dummy_table_variables",
        "original": "def create_dummy_table_variables(tpu_embedding):\n    \"\"\"Create dummy embedding table variables.\n\n  The sole purpose of these dummy variables are to trigger gradient\n  calculation wrt them so that the gradients wrt activation can be captured\n  and later sent to TPU embedding.\n\n  Args:\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\n      with tpu_embedding.\n\n  Returns:\n    A tuple of dummy variables and their initializer.\n\n  Raises:\n    RuntimeError: if collection to store gradients already exists and is not\n    empty.\n  \"\"\"\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))",
        "mutated": [
            "def create_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n    'Create dummy embedding table variables.\\n\\n  The sole purpose of these dummy variables are to trigger gradient\\n  calculation wrt them so that the gradients wrt activation can be captured\\n  and later sent to TPU embedding.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\\n      with tpu_embedding.\\n\\n  Returns:\\n    A tuple of dummy variables and their initializer.\\n\\n  Raises:\\n    RuntimeError: if collection to store gradients already exists and is not\\n    empty.\\n  '\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))",
            "def create_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create dummy embedding table variables.\\n\\n  The sole purpose of these dummy variables are to trigger gradient\\n  calculation wrt them so that the gradients wrt activation can be captured\\n  and later sent to TPU embedding.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\\n      with tpu_embedding.\\n\\n  Returns:\\n    A tuple of dummy variables and their initializer.\\n\\n  Raises:\\n    RuntimeError: if collection to store gradients already exists and is not\\n    empty.\\n  '\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))",
            "def create_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create dummy embedding table variables.\\n\\n  The sole purpose of these dummy variables are to trigger gradient\\n  calculation wrt them so that the gradients wrt activation can be captured\\n  and later sent to TPU embedding.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\\n      with tpu_embedding.\\n\\n  Returns:\\n    A tuple of dummy variables and their initializer.\\n\\n  Raises:\\n    RuntimeError: if collection to store gradients already exists and is not\\n    empty.\\n  '\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))",
            "def create_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create dummy embedding table variables.\\n\\n  The sole purpose of these dummy variables are to trigger gradient\\n  calculation wrt them so that the gradients wrt activation can be captured\\n  and later sent to TPU embedding.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\\n      with tpu_embedding.\\n\\n  Returns:\\n    A tuple of dummy variables and their initializer.\\n\\n  Raises:\\n    RuntimeError: if collection to store gradients already exists and is not\\n    empty.\\n  '\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))",
            "def create_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create dummy embedding table variables.\\n\\n  The sole purpose of these dummy variables are to trigger gradient\\n  calculation wrt them so that the gradients wrt activation can be captured\\n  and later sent to TPU embedding.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\\n      with tpu_embedding.\\n\\n  Returns:\\n    A tuple of dummy variables and their initializer.\\n\\n  Raises:\\n    RuntimeError: if collection to store gradients already exists and is not\\n    empty.\\n  '\n    dummy_table_variables = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_features_dict):\n        dummy_table_variables[table] = variable_scope.get_variable('tpu_embedding_dummy_table_variable_{}'.format(table), dtype=dtypes.float32, shape=[1], use_resource=True, trainable=True, collections=['tpu_embedding_dummy_table_variables'])\n        g = ops.get_default_graph()\n        table_gradients = g.get_collection_ref('tpu_embedding_gradients_table_{}'.format(table_id))\n        if table_gradients:\n            raise RuntimeError('tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n        num_features = len(tpu_embedding.table_to_features_dict[table])\n        table_gradients.extend([None for _ in range(num_features)])\n    return (dummy_table_variables, variables.variables_initializer(dummy_table_variables.values(), name='tpu_embedding_dummy_table_variables_init'))"
        ]
    },
    {
        "func_name": "hook_dummy_table_variables_to_activations",
        "original": "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    \"\"\"Have activations depend on dummy table variables for gradient intercept.\n\n  Args:\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\n      tpu_embedding.\n    activations: An OrderedDict of feature name String to activation tensors.\n    dummy_table_variables: An OrderedDict of table name String to dummy table\n      variables.\n\n  Returns:\n    An OrderedDict of feature name String to activation tensors, which can be\n      used just as the activations input.\n  \"\"\"\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations",
        "mutated": [
            "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    if False:\n        i = 10\n    'Have activations depend on dummy table variables for gradient intercept.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\\n      tpu_embedding.\\n    activations: An OrderedDict of feature name String to activation tensors.\\n    dummy_table_variables: An OrderedDict of table name String to dummy table\\n      variables.\\n\\n  Returns:\\n    An OrderedDict of feature name String to activation tensors, which can be\\n      used just as the activations input.\\n  '\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations",
            "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Have activations depend on dummy table variables for gradient intercept.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\\n      tpu_embedding.\\n    activations: An OrderedDict of feature name String to activation tensors.\\n    dummy_table_variables: An OrderedDict of table name String to dummy table\\n      variables.\\n\\n  Returns:\\n    An OrderedDict of feature name String to activation tensors, which can be\\n      used just as the activations input.\\n  '\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations",
            "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Have activations depend on dummy table variables for gradient intercept.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\\n      tpu_embedding.\\n    activations: An OrderedDict of feature name String to activation tensors.\\n    dummy_table_variables: An OrderedDict of table name String to dummy table\\n      variables.\\n\\n  Returns:\\n    An OrderedDict of feature name String to activation tensors, which can be\\n      used just as the activations input.\\n  '\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations",
            "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Have activations depend on dummy table variables for gradient intercept.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\\n      tpu_embedding.\\n    activations: An OrderedDict of feature name String to activation tensors.\\n    dummy_table_variables: An OrderedDict of table name String to dummy table\\n      variables.\\n\\n  Returns:\\n    An OrderedDict of feature name String to activation tensors, which can be\\n      used just as the activations input.\\n  '\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations",
            "def hook_dummy_table_variables_to_activations(tpu_embedding, activations, dummy_table_variables):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Have activations depend on dummy table variables for gradient intercept.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\\n      tpu_embedding.\\n    activations: An OrderedDict of feature name String to activation tensors.\\n    dummy_table_variables: An OrderedDict of table name String to dummy table\\n      variables.\\n\\n  Returns:\\n    An OrderedDict of feature name String to activation tensors, which can be\\n      used just as the activations input.\\n  '\n    new_activations = collections.OrderedDict()\n    for feature in activations:\n        table = tpu_embedding.feature_to_config_dict[feature].table_id\n        new_activations[feature] = tpu_ops.tpu_embedding_activations(dummy_table_variables[table], activations[feature], table_id=list(tpu_embedding.table_to_config_dict).index(table), lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n    return new_activations"
        ]
    },
    {
        "func_name": "get_gradients_through_dummy_table_variables",
        "original": "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    \"\"\"Get gradients wrt the activations of each feature.\n\n  Args:\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\n      tpu_embedding.\n\n  Returns:\n    An OrderedDict mapping feature name to gradient.\n\n  Raises:\n    ValueError: if some gradients are not defined.\n  \"\"\"\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict",
        "mutated": [
            "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n    'Get gradients wrt the activations of each feature.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\\n      tpu_embedding.\\n\\n  Returns:\\n    An OrderedDict mapping feature name to gradient.\\n\\n  Raises:\\n    ValueError: if some gradients are not defined.\\n  '\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict",
            "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get gradients wrt the activations of each feature.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\\n      tpu_embedding.\\n\\n  Returns:\\n    An OrderedDict mapping feature name to gradient.\\n\\n  Raises:\\n    ValueError: if some gradients are not defined.\\n  '\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict",
            "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get gradients wrt the activations of each feature.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\\n      tpu_embedding.\\n\\n  Returns:\\n    An OrderedDict mapping feature name to gradient.\\n\\n  Raises:\\n    ValueError: if some gradients are not defined.\\n  '\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict",
            "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get gradients wrt the activations of each feature.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\\n      tpu_embedding.\\n\\n  Returns:\\n    An OrderedDict mapping feature name to gradient.\\n\\n  Raises:\\n    ValueError: if some gradients are not defined.\\n  '\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict",
            "def get_gradients_through_dummy_table_variables(tpu_embedding):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get gradients wrt the activations of each feature.\\n\\n  Args:\\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\\n      tpu_embedding.\\n\\n  Returns:\\n    An OrderedDict mapping feature name to gradient.\\n\\n  Raises:\\n    ValueError: if some gradients are not defined.\\n  '\n    g = ops.get_default_graph()\n    gradients_found = False\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        if any((gradient is None for gradient in table_gradients)):\n            logging.warn('Table {} with id {} has undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used, or tf.stop_gradient() is applied. Gradients of zeros are sent back to TPUEmbedding instead. Gradients of zeros and no gradients are equivalent for SGD, AdaGrad, FTRL, etc, but might differ for other optimizers due to implementation of TPU embedding optimizers.'.format(table, table_id))\n        gradients_found = gradients_found or any((gradient is not None for gradient in table_gradients))\n    if not gradients_found:\n        logging.warn('All tables have undefined gradients: this is probably because the model asked TPUEmbedding to compute activations that were not used. If all TPUEmbedding features have stop_gradients, consider using the INFERENCE mode instead.')\n    feature_to_gradient_dict = collections.OrderedDict()\n    for (table_id, table) in enumerate(tpu_embedding.table_to_config_dict):\n        table_gradients = g.get_collection('tpu_embedding_gradients_table_{}'.format(table_id))\n        for (feature, gradient) in zip(tpu_embedding.table_to_features_dict[table], table_gradients):\n            if gradient is not None:\n                feature_to_gradient_dict[feature] = gradient\n            else:\n                dimension = tpu_embedding.table_to_config_dict[table].dimension\n                batch_size = tpu_embedding.batch_size_per_core\n                max_sequence_length = tpu_embedding.feature_to_config_dict[feature].max_sequence_length\n                if max_sequence_length:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, max_sequence_length, dimension])\n                else:\n                    feature_to_gradient_dict[feature] = array_ops.zeros([batch_size, dimension])\n    return feature_to_gradient_dict"
        ]
    }
]