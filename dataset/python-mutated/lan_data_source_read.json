[
    {
        "func_name": "__init__",
        "original": "def __init__(self) -> None:\n    self.ser = CloudPickleSerializer()",
        "mutated": [
            "def __init__(self) -> None:\n    if False:\n        i = 10\n    self.ser = CloudPickleSerializer()",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.ser = CloudPickleSerializer()",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.ser = CloudPickleSerializer()",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.ser = CloudPickleSerializer()",
            "def __init__(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.ser = CloudPickleSerializer()"
        ]
    },
    {
        "func_name": "eval",
        "original": "def eval(self, partition_bytes: Any) -> Iterator:\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)",
        "mutated": [
            "def eval(self, partition_bytes: Any) -> Iterator:\n    if False:\n        i = 10\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)",
            "def eval(self, partition_bytes: Any) -> Iterator:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)",
            "def eval(self, partition_bytes: Any) -> Iterator:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)",
            "def eval(self, partition_bytes: Any) -> Iterator:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)",
            "def eval(self, partition_bytes: Any) -> Iterator:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    partition = self.ser.loads(partition_bytes)\n    yield from reader.read(partition)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(infile: IO, outfile: IO) -> None:\n    \"\"\"\n    Main method for planning a data source read.\n\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\n\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\n    information to this process via the socket:\n    - a `DataSource` instance representing the data source\n    - a `StructType` instance representing the output schema of the data source\n\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\n    constructs a Python UDTF using the `read()` method of the reader.\n\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\n    \"\"\"\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
        "mutated": [
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n    '\\n    Main method for planning a data source read.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\\n\\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\\n    information to this process via the socket:\\n    - a `DataSource` instance representing the data source\\n    - a `StructType` instance representing the output schema of the data source\\n\\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\\n    constructs a Python UDTF using the `read()` method of the reader.\\n\\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Main method for planning a data source read.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\\n\\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\\n    information to this process via the socket:\\n    - a `DataSource` instance representing the data source\\n    - a `StructType` instance representing the output schema of the data source\\n\\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\\n    constructs a Python UDTF using the `read()` method of the reader.\\n\\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Main method for planning a data source read.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\\n\\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\\n    information to this process via the socket:\\n    - a `DataSource` instance representing the data source\\n    - a `StructType` instance representing the output schema of the data source\\n\\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\\n    constructs a Python UDTF using the `read()` method of the reader.\\n\\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Main method for planning a data source read.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\\n\\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\\n    information to this process via the socket:\\n    - a `DataSource` instance representing the data source\\n    - a `StructType` instance representing the output schema of the data source\\n\\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\\n    constructs a Python UDTF using the `read()` method of the reader.\\n\\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)",
            "def main(infile: IO, outfile: IO) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Main method for planning a data source read.\\n\\n    This process is invoked from the `UserDefinedPythonDataSourceReadRunner.runInPython`\\n    method in the optimizer rule `PlanPythonDataSourceScan` in JVM. This process is responsible\\n    for creating a `DataSourceReader` object and send the information needed back to the JVM.\\n\\n    The infile and outfile are connected to the JVM via a socket. The JVM sends the following\\n    information to this process via the socket:\\n    - a `DataSource` instance representing the data source\\n    - a `StructType` instance representing the output schema of the data source\\n\\n    This process then creates a `DataSourceReader` instance by calling the `reader` method\\n    on the `DataSource` instance. Then it calls the `partitions()` method of the reader and\\n    constructs a Python UDTF using the `read()` method of the reader.\\n\\n    The partition values and the UDTF are then serialized and sent back to the JVM via the socket.\\n    '\n    try:\n        check_python_version(infile)\n        memory_limit_mb = int(os.environ.get('PYSPARK_PLANNER_MEMORY_MB', '-1'))\n        setup_memory_limits(memory_limit_mb)\n        setup_spark_files(infile)\n        setup_broadcasts(infile)\n        _accumulatorRegistry.clear()\n        data_source = read_command(pickleSer, infile)\n        if not isinstance(data_source, DataSource):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source instance of type 'DataSource'\", 'actual': f\"'{type(data_source).__name__}'\"})\n        schema_json = utf8_deserializer.loads(infile)\n        schema = _parse_datatype_json_string(schema_json)\n        if not isinstance(schema, StructType):\n            raise PySparkAssertionError(error_class='PYTHON_DATA_SOURCE_TYPE_MISMATCH', message_parameters={'expected': \"a Python data source schema of type 'StructType'\", 'actual': f\"'{type(schema).__name__}'\"})\n        try:\n            reader = data_source.reader(schema=schema)\n        except NotImplementedError:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_METHOD_NOT_IMPLEMENTED', message_parameters={'type': 'reader', 'method': 'reader'})\n        except Exception as e:\n            raise PySparkRuntimeError(error_class='PYTHON_DATA_SOURCE_CREATE_ERROR', message_parameters={'type': 'reader', 'error': str(e)})\n        partitions = list(reader.partitions() or [])\n        if len(partitions) == 0:\n            partitions = [None]\n\n        class PythonDataSourceReaderUDTF:\n\n            def __init__(self) -> None:\n                self.ser = CloudPickleSerializer()\n\n            def eval(self, partition_bytes: Any) -> Iterator:\n                partition = self.ser.loads(partition_bytes)\n                yield from reader.read(partition)\n        command = PythonDataSourceReaderUDTF\n        pickleSer._write_with_length(command, outfile)\n        write_int(len(partitions), outfile)\n        for partition in partitions:\n            pickleSer._write_with_length(partition, outfile)\n    except BaseException as e:\n        handle_worker_exception(e, outfile)\n        sys.exit(-1)\n    send_accumulator_updates(outfile)\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n        write_int(SpecialLengths.END_OF_STREAM, outfile)\n    else:\n        write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)\n        sys.exit(-1)"
        ]
    }
]