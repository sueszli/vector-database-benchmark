[
    {
        "func_name": "entropy",
        "original": "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    \"\"\"\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\n\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\n    ``H = -sum(pk * log(pk))``.\n\n    If `qk` is not None, then compute the relative entropy\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\n    as the Kullback-Leibler divergence.\n\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\n\n    Parameters\n    ----------\n    pk : array_like\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\n        element ``i`` is the  (possibly unnormalized) probability of event\n        ``i``.\n    qk : array_like, optional\n        Sequence against which the relative entropy is computed. Should be in\n        the same format as `pk`.\n    base : float, optional\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\n    axis : int, optional\n        The axis along which the entropy is calculated. Default is 0.\n\n    Returns\n    -------\n    S : {float, array_like}\n        The calculated entropy.\n\n    Notes\n    -----\n    Informally, the Shannon entropy quantifies the expected uncertainty\n    inherent in the possible outcomes of a discrete random variable.\n    For example,\n    if messages consisting of sequences of symbols from a set are to be\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\n    ``H(pk)`` gives a tight lower bound for the average number of units of\n    information needed per symbol if the symbols occur with frequencies\n    governed by the discrete distribution `pk` [1]_. The choice of base\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\n\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\n    number of units of information needed per symbol if the encoding is\n    optimized for the probability distribution `qk` instead of the true\n    distribution `pk`. Informally, the relative entropy quantifies the expected\n    excess in surprise experienced if one believes the true distribution is\n    `qk` when it is actually `pk`.\n\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\n    number of units of information needed per symbol if an encoding is\n    optimized for the probability distribution `qk` when the true distribution\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\n    using two calls to the function (see Examples).\n\n    See [2]_ for more information.\n\n    References\n    ----------\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\n           Bell System Technical Journal, 27: 379-423.\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\n           Theory (Wiley Series in Telecommunications and Signal Processing).\n           Wiley-Interscience, USA.\n\n\n    Examples\n    --------\n    The outcome of a fair coin is the most uncertain:\n\n    >>> import numpy as np\n    >>> from scipy.stats import entropy\n    >>> base = 2  # work in units of bits\n    >>> pk = np.array([1/2, 1/2])  # fair coin\n    >>> H = entropy(pk, base=base)\n    >>> H\n    1.0\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\n    True\n\n    The outcome of a biased coin is less uncertain:\n\n    >>> qk = np.array([9/10, 1/10])  # biased coin\n    >>> entropy(qk, base=base)\n    0.46899559358928117\n\n    The relative entropy between the fair coin and biased coin is calculated\n    as:\n\n    >>> D = entropy(pk, qk, base=base)\n    >>> D\n    0.7369655941662062\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\n    True\n\n    The cross entropy can be calculated as the sum of the entropy and\n    relative entropy`:\n\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\n    >>> CE\n    1.736965594166206\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\n    True\n\n    \"\"\"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S",
        "mutated": [
            "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    if False:\n        i = 10\n    \"\\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\\n\\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\\n    ``H = -sum(pk * log(pk))``.\\n\\n    If `qk` is not None, then compute the relative entropy\\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\\n    as the Kullback-Leibler divergence.\\n\\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\\n\\n    Parameters\\n    ----------\\n    pk : array_like\\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\\n        element ``i`` is the  (possibly unnormalized) probability of event\\n        ``i``.\\n    qk : array_like, optional\\n        Sequence against which the relative entropy is computed. Should be in\\n        the same format as `pk`.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the entropy is calculated. Default is 0.\\n\\n    Returns\\n    -------\\n    S : {float, array_like}\\n        The calculated entropy.\\n\\n    Notes\\n    -----\\n    Informally, the Shannon entropy quantifies the expected uncertainty\\n    inherent in the possible outcomes of a discrete random variable.\\n    For example,\\n    if messages consisting of sequences of symbols from a set are to be\\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\\n    ``H(pk)`` gives a tight lower bound for the average number of units of\\n    information needed per symbol if the symbols occur with frequencies\\n    governed by the discrete distribution `pk` [1]_. The choice of base\\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\\n\\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\\n    number of units of information needed per symbol if the encoding is\\n    optimized for the probability distribution `qk` instead of the true\\n    distribution `pk`. Informally, the relative entropy quantifies the expected\\n    excess in surprise experienced if one believes the true distribution is\\n    `qk` when it is actually `pk`.\\n\\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\\n    number of units of information needed per symbol if an encoding is\\n    optimized for the probability distribution `qk` when the true distribution\\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\\n    using two calls to the function (see Examples).\\n\\n    See [2]_ for more information.\\n\\n    References\\n    ----------\\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\\n           Bell System Technical Journal, 27: 379-423.\\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\\n           Theory (Wiley Series in Telecommunications and Signal Processing).\\n           Wiley-Interscience, USA.\\n\\n\\n    Examples\\n    --------\\n    The outcome of a fair coin is the most uncertain:\\n\\n    >>> import numpy as np\\n    >>> from scipy.stats import entropy\\n    >>> base = 2  # work in units of bits\\n    >>> pk = np.array([1/2, 1/2])  # fair coin\\n    >>> H = entropy(pk, base=base)\\n    >>> H\\n    1.0\\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\\n    True\\n\\n    The outcome of a biased coin is less uncertain:\\n\\n    >>> qk = np.array([9/10, 1/10])  # biased coin\\n    >>> entropy(qk, base=base)\\n    0.46899559358928117\\n\\n    The relative entropy between the fair coin and biased coin is calculated\\n    as:\\n\\n    >>> D = entropy(pk, qk, base=base)\\n    >>> D\\n    0.7369655941662062\\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\\n    True\\n\\n    The cross entropy can be calculated as the sum of the entropy and\\n    relative entropy`:\\n\\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\\n    >>> CE\\n    1.736965594166206\\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\\n    True\\n\\n    \"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S",
            "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\\n\\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\\n    ``H = -sum(pk * log(pk))``.\\n\\n    If `qk` is not None, then compute the relative entropy\\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\\n    as the Kullback-Leibler divergence.\\n\\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\\n\\n    Parameters\\n    ----------\\n    pk : array_like\\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\\n        element ``i`` is the  (possibly unnormalized) probability of event\\n        ``i``.\\n    qk : array_like, optional\\n        Sequence against which the relative entropy is computed. Should be in\\n        the same format as `pk`.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the entropy is calculated. Default is 0.\\n\\n    Returns\\n    -------\\n    S : {float, array_like}\\n        The calculated entropy.\\n\\n    Notes\\n    -----\\n    Informally, the Shannon entropy quantifies the expected uncertainty\\n    inherent in the possible outcomes of a discrete random variable.\\n    For example,\\n    if messages consisting of sequences of symbols from a set are to be\\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\\n    ``H(pk)`` gives a tight lower bound for the average number of units of\\n    information needed per symbol if the symbols occur with frequencies\\n    governed by the discrete distribution `pk` [1]_. The choice of base\\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\\n\\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\\n    number of units of information needed per symbol if the encoding is\\n    optimized for the probability distribution `qk` instead of the true\\n    distribution `pk`. Informally, the relative entropy quantifies the expected\\n    excess in surprise experienced if one believes the true distribution is\\n    `qk` when it is actually `pk`.\\n\\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\\n    number of units of information needed per symbol if an encoding is\\n    optimized for the probability distribution `qk` when the true distribution\\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\\n    using two calls to the function (see Examples).\\n\\n    See [2]_ for more information.\\n\\n    References\\n    ----------\\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\\n           Bell System Technical Journal, 27: 379-423.\\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\\n           Theory (Wiley Series in Telecommunications and Signal Processing).\\n           Wiley-Interscience, USA.\\n\\n\\n    Examples\\n    --------\\n    The outcome of a fair coin is the most uncertain:\\n\\n    >>> import numpy as np\\n    >>> from scipy.stats import entropy\\n    >>> base = 2  # work in units of bits\\n    >>> pk = np.array([1/2, 1/2])  # fair coin\\n    >>> H = entropy(pk, base=base)\\n    >>> H\\n    1.0\\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\\n    True\\n\\n    The outcome of a biased coin is less uncertain:\\n\\n    >>> qk = np.array([9/10, 1/10])  # biased coin\\n    >>> entropy(qk, base=base)\\n    0.46899559358928117\\n\\n    The relative entropy between the fair coin and biased coin is calculated\\n    as:\\n\\n    >>> D = entropy(pk, qk, base=base)\\n    >>> D\\n    0.7369655941662062\\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\\n    True\\n\\n    The cross entropy can be calculated as the sum of the entropy and\\n    relative entropy`:\\n\\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\\n    >>> CE\\n    1.736965594166206\\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\\n    True\\n\\n    \"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S",
            "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\\n\\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\\n    ``H = -sum(pk * log(pk))``.\\n\\n    If `qk` is not None, then compute the relative entropy\\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\\n    as the Kullback-Leibler divergence.\\n\\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\\n\\n    Parameters\\n    ----------\\n    pk : array_like\\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\\n        element ``i`` is the  (possibly unnormalized) probability of event\\n        ``i``.\\n    qk : array_like, optional\\n        Sequence against which the relative entropy is computed. Should be in\\n        the same format as `pk`.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the entropy is calculated. Default is 0.\\n\\n    Returns\\n    -------\\n    S : {float, array_like}\\n        The calculated entropy.\\n\\n    Notes\\n    -----\\n    Informally, the Shannon entropy quantifies the expected uncertainty\\n    inherent in the possible outcomes of a discrete random variable.\\n    For example,\\n    if messages consisting of sequences of symbols from a set are to be\\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\\n    ``H(pk)`` gives a tight lower bound for the average number of units of\\n    information needed per symbol if the symbols occur with frequencies\\n    governed by the discrete distribution `pk` [1]_. The choice of base\\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\\n\\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\\n    number of units of information needed per symbol if the encoding is\\n    optimized for the probability distribution `qk` instead of the true\\n    distribution `pk`. Informally, the relative entropy quantifies the expected\\n    excess in surprise experienced if one believes the true distribution is\\n    `qk` when it is actually `pk`.\\n\\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\\n    number of units of information needed per symbol if an encoding is\\n    optimized for the probability distribution `qk` when the true distribution\\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\\n    using two calls to the function (see Examples).\\n\\n    See [2]_ for more information.\\n\\n    References\\n    ----------\\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\\n           Bell System Technical Journal, 27: 379-423.\\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\\n           Theory (Wiley Series in Telecommunications and Signal Processing).\\n           Wiley-Interscience, USA.\\n\\n\\n    Examples\\n    --------\\n    The outcome of a fair coin is the most uncertain:\\n\\n    >>> import numpy as np\\n    >>> from scipy.stats import entropy\\n    >>> base = 2  # work in units of bits\\n    >>> pk = np.array([1/2, 1/2])  # fair coin\\n    >>> H = entropy(pk, base=base)\\n    >>> H\\n    1.0\\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\\n    True\\n\\n    The outcome of a biased coin is less uncertain:\\n\\n    >>> qk = np.array([9/10, 1/10])  # biased coin\\n    >>> entropy(qk, base=base)\\n    0.46899559358928117\\n\\n    The relative entropy between the fair coin and biased coin is calculated\\n    as:\\n\\n    >>> D = entropy(pk, qk, base=base)\\n    >>> D\\n    0.7369655941662062\\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\\n    True\\n\\n    The cross entropy can be calculated as the sum of the entropy and\\n    relative entropy`:\\n\\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\\n    >>> CE\\n    1.736965594166206\\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\\n    True\\n\\n    \"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S",
            "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\\n\\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\\n    ``H = -sum(pk * log(pk))``.\\n\\n    If `qk` is not None, then compute the relative entropy\\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\\n    as the Kullback-Leibler divergence.\\n\\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\\n\\n    Parameters\\n    ----------\\n    pk : array_like\\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\\n        element ``i`` is the  (possibly unnormalized) probability of event\\n        ``i``.\\n    qk : array_like, optional\\n        Sequence against which the relative entropy is computed. Should be in\\n        the same format as `pk`.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the entropy is calculated. Default is 0.\\n\\n    Returns\\n    -------\\n    S : {float, array_like}\\n        The calculated entropy.\\n\\n    Notes\\n    -----\\n    Informally, the Shannon entropy quantifies the expected uncertainty\\n    inherent in the possible outcomes of a discrete random variable.\\n    For example,\\n    if messages consisting of sequences of symbols from a set are to be\\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\\n    ``H(pk)`` gives a tight lower bound for the average number of units of\\n    information needed per symbol if the symbols occur with frequencies\\n    governed by the discrete distribution `pk` [1]_. The choice of base\\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\\n\\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\\n    number of units of information needed per symbol if the encoding is\\n    optimized for the probability distribution `qk` instead of the true\\n    distribution `pk`. Informally, the relative entropy quantifies the expected\\n    excess in surprise experienced if one believes the true distribution is\\n    `qk` when it is actually `pk`.\\n\\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\\n    number of units of information needed per symbol if an encoding is\\n    optimized for the probability distribution `qk` when the true distribution\\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\\n    using two calls to the function (see Examples).\\n\\n    See [2]_ for more information.\\n\\n    References\\n    ----------\\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\\n           Bell System Technical Journal, 27: 379-423.\\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\\n           Theory (Wiley Series in Telecommunications and Signal Processing).\\n           Wiley-Interscience, USA.\\n\\n\\n    Examples\\n    --------\\n    The outcome of a fair coin is the most uncertain:\\n\\n    >>> import numpy as np\\n    >>> from scipy.stats import entropy\\n    >>> base = 2  # work in units of bits\\n    >>> pk = np.array([1/2, 1/2])  # fair coin\\n    >>> H = entropy(pk, base=base)\\n    >>> H\\n    1.0\\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\\n    True\\n\\n    The outcome of a biased coin is less uncertain:\\n\\n    >>> qk = np.array([9/10, 1/10])  # biased coin\\n    >>> entropy(qk, base=base)\\n    0.46899559358928117\\n\\n    The relative entropy between the fair coin and biased coin is calculated\\n    as:\\n\\n    >>> D = entropy(pk, qk, base=base)\\n    >>> D\\n    0.7369655941662062\\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\\n    True\\n\\n    The cross entropy can be calculated as the sum of the entropy and\\n    relative entropy`:\\n\\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\\n    >>> CE\\n    1.736965594166206\\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\\n    True\\n\\n    \"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S",
            "@_axis_nan_policy_factory(lambda x: x, n_samples=lambda kwgs: 2 if 'qk' in kwgs and kwgs['qk'] is not None else 1, n_outputs=1, result_to_tuple=lambda x: (x,), paired=True, too_small=-1)\ndef entropy(pk: np.typing.ArrayLike, qk: np.typing.ArrayLike | None=None, base: float | None=None, axis: int=0) -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Calculate the Shannon entropy/relative entropy of given distribution(s).\\n\\n    If only probabilities `pk` are given, the Shannon entropy is calculated as\\n    ``H = -sum(pk * log(pk))``.\\n\\n    If `qk` is not None, then compute the relative entropy\\n    ``D = sum(pk * log(pk / qk))``. This quantity is also known\\n    as the Kullback-Leibler divergence.\\n\\n    This routine will normalize `pk` and `qk` if they don't sum to 1.\\n\\n    Parameters\\n    ----------\\n    pk : array_like\\n        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\\n        element ``i`` is the  (possibly unnormalized) probability of event\\n        ``i``.\\n    qk : array_like, optional\\n        Sequence against which the relative entropy is computed. Should be in\\n        the same format as `pk`.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the entropy is calculated. Default is 0.\\n\\n    Returns\\n    -------\\n    S : {float, array_like}\\n        The calculated entropy.\\n\\n    Notes\\n    -----\\n    Informally, the Shannon entropy quantifies the expected uncertainty\\n    inherent in the possible outcomes of a discrete random variable.\\n    For example,\\n    if messages consisting of sequences of symbols from a set are to be\\n    encoded and transmitted over a noiseless channel, then the Shannon entropy\\n    ``H(pk)`` gives a tight lower bound for the average number of units of\\n    information needed per symbol if the symbols occur with frequencies\\n    governed by the discrete distribution `pk` [1]_. The choice of base\\n    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\\n\\n    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\\n    number of units of information needed per symbol if the encoding is\\n    optimized for the probability distribution `qk` instead of the true\\n    distribution `pk`. Informally, the relative entropy quantifies the expected\\n    excess in surprise experienced if one believes the true distribution is\\n    `qk` when it is actually `pk`.\\n\\n    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\\n    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\\n    the formula ``CE = -sum(pk * log(qk))``. It gives the average\\n    number of units of information needed per symbol if an encoding is\\n    optimized for the probability distribution `qk` when the true distribution\\n    is `pk`. It is not computed directly by `entropy`, but it can be computed\\n    using two calls to the function (see Examples).\\n\\n    See [2]_ for more information.\\n\\n    References\\n    ----------\\n    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\\n           Bell System Technical Journal, 27: 379-423.\\n           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\\n    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\\n           Theory (Wiley Series in Telecommunications and Signal Processing).\\n           Wiley-Interscience, USA.\\n\\n\\n    Examples\\n    --------\\n    The outcome of a fair coin is the most uncertain:\\n\\n    >>> import numpy as np\\n    >>> from scipy.stats import entropy\\n    >>> base = 2  # work in units of bits\\n    >>> pk = np.array([1/2, 1/2])  # fair coin\\n    >>> H = entropy(pk, base=base)\\n    >>> H\\n    1.0\\n    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\\n    True\\n\\n    The outcome of a biased coin is less uncertain:\\n\\n    >>> qk = np.array([9/10, 1/10])  # biased coin\\n    >>> entropy(qk, base=base)\\n    0.46899559358928117\\n\\n    The relative entropy between the fair coin and biased coin is calculated\\n    as:\\n\\n    >>> D = entropy(pk, qk, base=base)\\n    >>> D\\n    0.7369655941662062\\n    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\\n    True\\n\\n    The cross entropy can be calculated as the sum of the entropy and\\n    relative entropy`:\\n\\n    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\\n    >>> CE\\n    1.736965594166206\\n    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\\n    True\\n\\n    \"\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    pk = np.asarray(pk)\n    pk = 1.0 * pk / np.sum(pk, axis=axis, keepdims=True)\n    if qk is None:\n        vec = special.entr(pk)\n    else:\n        qk = np.asarray(qk)\n        (pk, qk) = np.broadcast_arrays(pk, qk)\n        qk = 1.0 * qk / np.sum(qk, axis=axis, keepdims=True)\n        vec = special.rel_entr(pk, qk)\n    S = np.sum(vec, axis=axis)\n    if base is not None:\n        S /= np.log(base)\n    return S"
        ]
    },
    {
        "func_name": "_differential_entropy_is_too_small",
        "original": "def _differential_entropy_is_too_small(samples, kwargs):\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False",
        "mutated": [
            "def _differential_entropy_is_too_small(samples, kwargs):\n    if False:\n        i = 10\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False",
            "def _differential_entropy_is_too_small(samples, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False",
            "def _differential_entropy_is_too_small(samples, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False",
            "def _differential_entropy_is_too_small(samples, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False",
            "def _differential_entropy_is_too_small(samples, kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    values = samples[0]\n    n = values.shape[-1]\n    window_length = kwargs.get('window_length', math.floor(math.sqrt(n) + 0.5))\n    if not 2 <= 2 * window_length < n:\n        return True\n    return False"
        ]
    },
    {
        "func_name": "differential_entropy",
        "original": "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    \"\"\"Given a sample of a distribution, estimate the differential entropy.\n\n    Several estimation methods are available using the `method` parameter. By\n    default, a method is selected based the size of the sample.\n\n    Parameters\n    ----------\n    values : sequence\n        Sample from a continuous distribution.\n    window_length : int, optional\n        Window length for computing Vasicek estimate. Must be an integer\n        between 1 and half of the sample size. If ``None`` (the default), it\n        uses the heuristic value\n\n        .. math::\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\n\n        where :math:`n` is the sample size. This heuristic was originally\n        proposed in [2]_ and has become common in the literature.\n    base : float, optional\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\n    axis : int, optional\n        The axis along which the differential entropy is calculated.\n        Default is 0.\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\n        The method used to estimate the differential entropy from the sample.\n        Default is ``'auto'``.  See Notes for more information.\n\n    Returns\n    -------\n    entropy : float\n        The calculated differential entropy.\n\n    Notes\n    -----\n    This function will converge to the true differential entropy in the limit\n\n    .. math::\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\n\n    The optimal choice of ``window_length`` for a given sample size depends on\n    the (unknown) distribution. Typically, the smoother the density of the\n    distribution, the larger the optimal value of ``window_length`` [1]_.\n\n    The following options are available for the `method` parameter.\n\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\n      one of the first and most influential estimators of differential entropy.\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\n      is not only consistent but, under some conditions, asymptotically normal.\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\n      in simulation to have smaller bias and mean squared error than\n      the Vasicek estimator.\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\n      regression. In a simulation study, it had consistently smaller mean\n      square error than the Vasiceck estimator, but it is more expensive to\n      compute.\n    * ``'auto'`` selects the method automatically (default). Currently,\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\n      samples, but this behavior is subject to change in future versions.\n\n    All estimators are implemented as described in [6]_.\n\n    References\n    ----------\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\n           Journal of the Royal Statistical Society:\n           Series B (Methodological), 38(1), 54-59.\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\n           goodness-of-fit test for exponentiality. Communications in\n           Statistics-Theory and Methods, 28(5), 1183-1202.\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\n           class of statistics based on spacings. Scandinavian Journal of\n           Statistics, 61-72.\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\n           Annals of Data Science, 2(2), 231-241.\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import differential_entropy, norm\n\n    Entropy of a standard normal distribution:\n\n    >>> rng = np.random.default_rng()\n    >>> values = rng.standard_normal(100)\n    >>> differential_entropy(values)\n    1.3407817436640392\n\n    Compare with the true entropy:\n\n    >>> float(norm.entropy())\n    1.4189385332046727\n\n    For several sample sizes between 5 and 1000, compare the accuracy of\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\n    compare the root mean squared error (over 1000 trials) between the estimate\n    and the true differential entropy of the distribution.\n\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>>\n    >>>\n    >>> def rmse(res, expected):\n    ...     '''Root mean squared error'''\n    ...     return np.sqrt(np.mean((res - expected)**2))\n    >>>\n    >>>\n    >>> a, b = np.log10(5), np.log10(1000)\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\n    >>> reps = 1000  # number of repetitions for each sample size\n    >>> expected = stats.expon.entropy()\n    >>>\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\n    >>> for method in method_errors:\n    ...     for n in ns:\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\n    ...        error = rmse(res, expected)\n    ...        method_errors[method].append(error)\n    >>>\n    >>> for method, errors in method_errors.items():\n    ...     plt.loglog(ns, errors, label=method)\n    >>>\n    >>> plt.legend()\n    >>> plt.xlabel('sample size')\n    >>> plt.ylabel('RMSE (1000 trials)')\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\n\n    \"\"\"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res",
        "mutated": [
            "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    if False:\n        i = 10\n    \"Given a sample of a distribution, estimate the differential entropy.\\n\\n    Several estimation methods are available using the `method` parameter. By\\n    default, a method is selected based the size of the sample.\\n\\n    Parameters\\n    ----------\\n    values : sequence\\n        Sample from a continuous distribution.\\n    window_length : int, optional\\n        Window length for computing Vasicek estimate. Must be an integer\\n        between 1 and half of the sample size. If ``None`` (the default), it\\n        uses the heuristic value\\n\\n        .. math::\\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\\n\\n        where :math:`n` is the sample size. This heuristic was originally\\n        proposed in [2]_ and has become common in the literature.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the differential entropy is calculated.\\n        Default is 0.\\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\\n        The method used to estimate the differential entropy from the sample.\\n        Default is ``'auto'``.  See Notes for more information.\\n\\n    Returns\\n    -------\\n    entropy : float\\n        The calculated differential entropy.\\n\\n    Notes\\n    -----\\n    This function will converge to the true differential entropy in the limit\\n\\n    .. math::\\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\\n\\n    The optimal choice of ``window_length`` for a given sample size depends on\\n    the (unknown) distribution. Typically, the smoother the density of the\\n    distribution, the larger the optimal value of ``window_length`` [1]_.\\n\\n    The following options are available for the `method` parameter.\\n\\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\\n      one of the first and most influential estimators of differential entropy.\\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\\n      is not only consistent but, under some conditions, asymptotically normal.\\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\\n      in simulation to have smaller bias and mean squared error than\\n      the Vasicek estimator.\\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\\n      regression. In a simulation study, it had consistently smaller mean\\n      square error than the Vasiceck estimator, but it is more expensive to\\n      compute.\\n    * ``'auto'`` selects the method automatically (default). Currently,\\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\\n      samples, but this behavior is subject to change in future versions.\\n\\n    All estimators are implemented as described in [6]_.\\n\\n    References\\n    ----------\\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\\n           Journal of the Royal Statistical Society:\\n           Series B (Methodological), 38(1), 54-59.\\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\\n           goodness-of-fit test for exponentiality. Communications in\\n           Statistics-Theory and Methods, 28(5), 1183-1202.\\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\\n           class of statistics based on spacings. Scandinavian Journal of\\n           Statistics, 61-72.\\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\\n           Annals of Data Science, 2(2), 231-241.\\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy.stats import differential_entropy, norm\\n\\n    Entropy of a standard normal distribution:\\n\\n    >>> rng = np.random.default_rng()\\n    >>> values = rng.standard_normal(100)\\n    >>> differential_entropy(values)\\n    1.3407817436640392\\n\\n    Compare with the true entropy:\\n\\n    >>> float(norm.entropy())\\n    1.4189385332046727\\n\\n    For several sample sizes between 5 and 1000, compare the accuracy of\\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\\n    compare the root mean squared error (over 1000 trials) between the estimate\\n    and the true differential entropy of the distribution.\\n\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n    >>>\\n    >>>\\n    >>> def rmse(res, expected):\\n    ...     '''Root mean squared error'''\\n    ...     return np.sqrt(np.mean((res - expected)**2))\\n    >>>\\n    >>>\\n    >>> a, b = np.log10(5), np.log10(1000)\\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\\n    >>> reps = 1000  # number of repetitions for each sample size\\n    >>> expected = stats.expon.entropy()\\n    >>>\\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\\n    >>> for method in method_errors:\\n    ...     for n in ns:\\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\\n    ...        error = rmse(res, expected)\\n    ...        method_errors[method].append(error)\\n    >>>\\n    >>> for method, errors in method_errors.items():\\n    ...     plt.loglog(ns, errors, label=method)\\n    >>>\\n    >>> plt.legend()\\n    >>> plt.xlabel('sample size')\\n    >>> plt.ylabel('RMSE (1000 trials)')\\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\\n\\n    \"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res",
            "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Given a sample of a distribution, estimate the differential entropy.\\n\\n    Several estimation methods are available using the `method` parameter. By\\n    default, a method is selected based the size of the sample.\\n\\n    Parameters\\n    ----------\\n    values : sequence\\n        Sample from a continuous distribution.\\n    window_length : int, optional\\n        Window length for computing Vasicek estimate. Must be an integer\\n        between 1 and half of the sample size. If ``None`` (the default), it\\n        uses the heuristic value\\n\\n        .. math::\\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\\n\\n        where :math:`n` is the sample size. This heuristic was originally\\n        proposed in [2]_ and has become common in the literature.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the differential entropy is calculated.\\n        Default is 0.\\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\\n        The method used to estimate the differential entropy from the sample.\\n        Default is ``'auto'``.  See Notes for more information.\\n\\n    Returns\\n    -------\\n    entropy : float\\n        The calculated differential entropy.\\n\\n    Notes\\n    -----\\n    This function will converge to the true differential entropy in the limit\\n\\n    .. math::\\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\\n\\n    The optimal choice of ``window_length`` for a given sample size depends on\\n    the (unknown) distribution. Typically, the smoother the density of the\\n    distribution, the larger the optimal value of ``window_length`` [1]_.\\n\\n    The following options are available for the `method` parameter.\\n\\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\\n      one of the first and most influential estimators of differential entropy.\\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\\n      is not only consistent but, under some conditions, asymptotically normal.\\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\\n      in simulation to have smaller bias and mean squared error than\\n      the Vasicek estimator.\\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\\n      regression. In a simulation study, it had consistently smaller mean\\n      square error than the Vasiceck estimator, but it is more expensive to\\n      compute.\\n    * ``'auto'`` selects the method automatically (default). Currently,\\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\\n      samples, but this behavior is subject to change in future versions.\\n\\n    All estimators are implemented as described in [6]_.\\n\\n    References\\n    ----------\\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\\n           Journal of the Royal Statistical Society:\\n           Series B (Methodological), 38(1), 54-59.\\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\\n           goodness-of-fit test for exponentiality. Communications in\\n           Statistics-Theory and Methods, 28(5), 1183-1202.\\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\\n           class of statistics based on spacings. Scandinavian Journal of\\n           Statistics, 61-72.\\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\\n           Annals of Data Science, 2(2), 231-241.\\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy.stats import differential_entropy, norm\\n\\n    Entropy of a standard normal distribution:\\n\\n    >>> rng = np.random.default_rng()\\n    >>> values = rng.standard_normal(100)\\n    >>> differential_entropy(values)\\n    1.3407817436640392\\n\\n    Compare with the true entropy:\\n\\n    >>> float(norm.entropy())\\n    1.4189385332046727\\n\\n    For several sample sizes between 5 and 1000, compare the accuracy of\\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\\n    compare the root mean squared error (over 1000 trials) between the estimate\\n    and the true differential entropy of the distribution.\\n\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n    >>>\\n    >>>\\n    >>> def rmse(res, expected):\\n    ...     '''Root mean squared error'''\\n    ...     return np.sqrt(np.mean((res - expected)**2))\\n    >>>\\n    >>>\\n    >>> a, b = np.log10(5), np.log10(1000)\\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\\n    >>> reps = 1000  # number of repetitions for each sample size\\n    >>> expected = stats.expon.entropy()\\n    >>>\\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\\n    >>> for method in method_errors:\\n    ...     for n in ns:\\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\\n    ...        error = rmse(res, expected)\\n    ...        method_errors[method].append(error)\\n    >>>\\n    >>> for method, errors in method_errors.items():\\n    ...     plt.loglog(ns, errors, label=method)\\n    >>>\\n    >>> plt.legend()\\n    >>> plt.xlabel('sample size')\\n    >>> plt.ylabel('RMSE (1000 trials)')\\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\\n\\n    \"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res",
            "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Given a sample of a distribution, estimate the differential entropy.\\n\\n    Several estimation methods are available using the `method` parameter. By\\n    default, a method is selected based the size of the sample.\\n\\n    Parameters\\n    ----------\\n    values : sequence\\n        Sample from a continuous distribution.\\n    window_length : int, optional\\n        Window length for computing Vasicek estimate. Must be an integer\\n        between 1 and half of the sample size. If ``None`` (the default), it\\n        uses the heuristic value\\n\\n        .. math::\\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\\n\\n        where :math:`n` is the sample size. This heuristic was originally\\n        proposed in [2]_ and has become common in the literature.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the differential entropy is calculated.\\n        Default is 0.\\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\\n        The method used to estimate the differential entropy from the sample.\\n        Default is ``'auto'``.  See Notes for more information.\\n\\n    Returns\\n    -------\\n    entropy : float\\n        The calculated differential entropy.\\n\\n    Notes\\n    -----\\n    This function will converge to the true differential entropy in the limit\\n\\n    .. math::\\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\\n\\n    The optimal choice of ``window_length`` for a given sample size depends on\\n    the (unknown) distribution. Typically, the smoother the density of the\\n    distribution, the larger the optimal value of ``window_length`` [1]_.\\n\\n    The following options are available for the `method` parameter.\\n\\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\\n      one of the first and most influential estimators of differential entropy.\\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\\n      is not only consistent but, under some conditions, asymptotically normal.\\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\\n      in simulation to have smaller bias and mean squared error than\\n      the Vasicek estimator.\\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\\n      regression. In a simulation study, it had consistently smaller mean\\n      square error than the Vasiceck estimator, but it is more expensive to\\n      compute.\\n    * ``'auto'`` selects the method automatically (default). Currently,\\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\\n      samples, but this behavior is subject to change in future versions.\\n\\n    All estimators are implemented as described in [6]_.\\n\\n    References\\n    ----------\\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\\n           Journal of the Royal Statistical Society:\\n           Series B (Methodological), 38(1), 54-59.\\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\\n           goodness-of-fit test for exponentiality. Communications in\\n           Statistics-Theory and Methods, 28(5), 1183-1202.\\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\\n           class of statistics based on spacings. Scandinavian Journal of\\n           Statistics, 61-72.\\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\\n           Annals of Data Science, 2(2), 231-241.\\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy.stats import differential_entropy, norm\\n\\n    Entropy of a standard normal distribution:\\n\\n    >>> rng = np.random.default_rng()\\n    >>> values = rng.standard_normal(100)\\n    >>> differential_entropy(values)\\n    1.3407817436640392\\n\\n    Compare with the true entropy:\\n\\n    >>> float(norm.entropy())\\n    1.4189385332046727\\n\\n    For several sample sizes between 5 and 1000, compare the accuracy of\\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\\n    compare the root mean squared error (over 1000 trials) between the estimate\\n    and the true differential entropy of the distribution.\\n\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n    >>>\\n    >>>\\n    >>> def rmse(res, expected):\\n    ...     '''Root mean squared error'''\\n    ...     return np.sqrt(np.mean((res - expected)**2))\\n    >>>\\n    >>>\\n    >>> a, b = np.log10(5), np.log10(1000)\\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\\n    >>> reps = 1000  # number of repetitions for each sample size\\n    >>> expected = stats.expon.entropy()\\n    >>>\\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\\n    >>> for method in method_errors:\\n    ...     for n in ns:\\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\\n    ...        error = rmse(res, expected)\\n    ...        method_errors[method].append(error)\\n    >>>\\n    >>> for method, errors in method_errors.items():\\n    ...     plt.loglog(ns, errors, label=method)\\n    >>>\\n    >>> plt.legend()\\n    >>> plt.xlabel('sample size')\\n    >>> plt.ylabel('RMSE (1000 trials)')\\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\\n\\n    \"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res",
            "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Given a sample of a distribution, estimate the differential entropy.\\n\\n    Several estimation methods are available using the `method` parameter. By\\n    default, a method is selected based the size of the sample.\\n\\n    Parameters\\n    ----------\\n    values : sequence\\n        Sample from a continuous distribution.\\n    window_length : int, optional\\n        Window length for computing Vasicek estimate. Must be an integer\\n        between 1 and half of the sample size. If ``None`` (the default), it\\n        uses the heuristic value\\n\\n        .. math::\\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\\n\\n        where :math:`n` is the sample size. This heuristic was originally\\n        proposed in [2]_ and has become common in the literature.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the differential entropy is calculated.\\n        Default is 0.\\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\\n        The method used to estimate the differential entropy from the sample.\\n        Default is ``'auto'``.  See Notes for more information.\\n\\n    Returns\\n    -------\\n    entropy : float\\n        The calculated differential entropy.\\n\\n    Notes\\n    -----\\n    This function will converge to the true differential entropy in the limit\\n\\n    .. math::\\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\\n\\n    The optimal choice of ``window_length`` for a given sample size depends on\\n    the (unknown) distribution. Typically, the smoother the density of the\\n    distribution, the larger the optimal value of ``window_length`` [1]_.\\n\\n    The following options are available for the `method` parameter.\\n\\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\\n      one of the first and most influential estimators of differential entropy.\\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\\n      is not only consistent but, under some conditions, asymptotically normal.\\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\\n      in simulation to have smaller bias and mean squared error than\\n      the Vasicek estimator.\\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\\n      regression. In a simulation study, it had consistently smaller mean\\n      square error than the Vasiceck estimator, but it is more expensive to\\n      compute.\\n    * ``'auto'`` selects the method automatically (default). Currently,\\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\\n      samples, but this behavior is subject to change in future versions.\\n\\n    All estimators are implemented as described in [6]_.\\n\\n    References\\n    ----------\\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\\n           Journal of the Royal Statistical Society:\\n           Series B (Methodological), 38(1), 54-59.\\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\\n           goodness-of-fit test for exponentiality. Communications in\\n           Statistics-Theory and Methods, 28(5), 1183-1202.\\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\\n           class of statistics based on spacings. Scandinavian Journal of\\n           Statistics, 61-72.\\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\\n           Annals of Data Science, 2(2), 231-241.\\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy.stats import differential_entropy, norm\\n\\n    Entropy of a standard normal distribution:\\n\\n    >>> rng = np.random.default_rng()\\n    >>> values = rng.standard_normal(100)\\n    >>> differential_entropy(values)\\n    1.3407817436640392\\n\\n    Compare with the true entropy:\\n\\n    >>> float(norm.entropy())\\n    1.4189385332046727\\n\\n    For several sample sizes between 5 and 1000, compare the accuracy of\\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\\n    compare the root mean squared error (over 1000 trials) between the estimate\\n    and the true differential entropy of the distribution.\\n\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n    >>>\\n    >>>\\n    >>> def rmse(res, expected):\\n    ...     '''Root mean squared error'''\\n    ...     return np.sqrt(np.mean((res - expected)**2))\\n    >>>\\n    >>>\\n    >>> a, b = np.log10(5), np.log10(1000)\\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\\n    >>> reps = 1000  # number of repetitions for each sample size\\n    >>> expected = stats.expon.entropy()\\n    >>>\\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\\n    >>> for method in method_errors:\\n    ...     for n in ns:\\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\\n    ...        error = rmse(res, expected)\\n    ...        method_errors[method].append(error)\\n    >>>\\n    >>> for method, errors in method_errors.items():\\n    ...     plt.loglog(ns, errors, label=method)\\n    >>>\\n    >>> plt.legend()\\n    >>> plt.xlabel('sample size')\\n    >>> plt.ylabel('RMSE (1000 trials)')\\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\\n\\n    \"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res",
            "@_axis_nan_policy_factory(lambda x: x, n_outputs=1, result_to_tuple=lambda x: (x,), too_small=_differential_entropy_is_too_small)\ndef differential_entropy(values: np.typing.ArrayLike, *, window_length: int | None=None, base: float | None=None, axis: int=0, method: str='auto') -> np.number | np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Given a sample of a distribution, estimate the differential entropy.\\n\\n    Several estimation methods are available using the `method` parameter. By\\n    default, a method is selected based the size of the sample.\\n\\n    Parameters\\n    ----------\\n    values : sequence\\n        Sample from a continuous distribution.\\n    window_length : int, optional\\n        Window length for computing Vasicek estimate. Must be an integer\\n        between 1 and half of the sample size. If ``None`` (the default), it\\n        uses the heuristic value\\n\\n        .. math::\\n            \\\\left \\\\lfloor \\\\sqrt{n} + 0.5 \\\\right \\\\rfloor\\n\\n        where :math:`n` is the sample size. This heuristic was originally\\n        proposed in [2]_ and has become common in the literature.\\n    base : float, optional\\n        The logarithmic base to use, defaults to ``e`` (natural logarithm).\\n    axis : int, optional\\n        The axis along which the differential entropy is calculated.\\n        Default is 0.\\n    method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\\n        The method used to estimate the differential entropy from the sample.\\n        Default is ``'auto'``.  See Notes for more information.\\n\\n    Returns\\n    -------\\n    entropy : float\\n        The calculated differential entropy.\\n\\n    Notes\\n    -----\\n    This function will converge to the true differential entropy in the limit\\n\\n    .. math::\\n        n \\\\to \\\\infty, \\\\quad m \\\\to \\\\infty, \\\\quad \\\\frac{m}{n} \\\\to 0\\n\\n    The optimal choice of ``window_length`` for a given sample size depends on\\n    the (unknown) distribution. Typically, the smoother the density of the\\n    distribution, the larger the optimal value of ``window_length`` [1]_.\\n\\n    The following options are available for the `method` parameter.\\n\\n    * ``'vasicek'`` uses the estimator presented in [1]_. This is\\n      one of the first and most influential estimators of differential entropy.\\n    * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\\n      is not only consistent but, under some conditions, asymptotically normal.\\n    * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\\n      in simulation to have smaller bias and mean squared error than\\n      the Vasicek estimator.\\n    * ``'correa'`` uses the estimator presented in [5]_ based on local linear\\n      regression. In a simulation study, it had consistently smaller mean\\n      square error than the Vasiceck estimator, but it is more expensive to\\n      compute.\\n    * ``'auto'`` selects the method automatically (default). Currently,\\n      this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\\n      for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\\n      samples, but this behavior is subject to change in future versions.\\n\\n    All estimators are implemented as described in [6]_.\\n\\n    References\\n    ----------\\n    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\\n           Journal of the Royal Statistical Society:\\n           Series B (Methodological), 38(1), 54-59.\\n    .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\\n           goodness-of-fit test for exponentiality. Communications in\\n           Statistics-Theory and Methods, 28(5), 1183-1202.\\n    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\\n           class of statistics based on spacings. Scandinavian Journal of\\n           Statistics, 61-72.\\n    .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\\n           of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\\n    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\\n           in Statistics-Theory and Methods, 24(10), 2439-2449.\\n    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\\n           Annals of Data Science, 2(2), 231-241.\\n           https://link.springer.com/article/10.1007/s40745-015-0045-9\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> from scipy.stats import differential_entropy, norm\\n\\n    Entropy of a standard normal distribution:\\n\\n    >>> rng = np.random.default_rng()\\n    >>> values = rng.standard_normal(100)\\n    >>> differential_entropy(values)\\n    1.3407817436640392\\n\\n    Compare with the true entropy:\\n\\n    >>> float(norm.entropy())\\n    1.4189385332046727\\n\\n    For several sample sizes between 5 and 1000, compare the accuracy of\\n    the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\\n    compare the root mean squared error (over 1000 trials) between the estimate\\n    and the true differential entropy of the distribution.\\n\\n    >>> from scipy import stats\\n    >>> import matplotlib.pyplot as plt\\n    >>>\\n    >>>\\n    >>> def rmse(res, expected):\\n    ...     '''Root mean squared error'''\\n    ...     return np.sqrt(np.mean((res - expected)**2))\\n    >>>\\n    >>>\\n    >>> a, b = np.log10(5), np.log10(1000)\\n    >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\\n    >>> reps = 1000  # number of repetitions for each sample size\\n    >>> expected = stats.expon.entropy()\\n    >>>\\n    >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\\n    >>> for method in method_errors:\\n    ...     for n in ns:\\n    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\\n    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\\n    ...        error = rmse(res, expected)\\n    ...        method_errors[method].append(error)\\n    >>>\\n    >>> for method, errors in method_errors.items():\\n    ...     plt.loglog(ns, errors, label=method)\\n    >>>\\n    >>> plt.legend()\\n    >>> plt.xlabel('sample size')\\n    >>> plt.ylabel('RMSE (1000 trials)')\\n    >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\\n\\n    \"\n    values = np.asarray(values)\n    values = np.moveaxis(values, axis, -1)\n    n = values.shape[-1]\n    if window_length is None:\n        window_length = math.floor(math.sqrt(n) + 0.5)\n    if not 2 <= 2 * window_length < n:\n        raise ValueError(f'Window length ({window_length}) must be positive and less than half the sample size ({n}).')\n    if base is not None and base <= 0:\n        raise ValueError('`base` must be a positive number or `None`.')\n    sorted_data = np.sort(values, axis=-1)\n    methods = {'vasicek': _vasicek_entropy, 'van es': _van_es_entropy, 'correa': _correa_entropy, 'ebrahimi': _ebrahimi_entropy, 'auto': _vasicek_entropy}\n    method = method.lower()\n    if method not in methods:\n        message = f'`method` must be one of {set(methods)}'\n        raise ValueError(message)\n    if method == 'auto':\n        if n <= 10:\n            method = 'van es'\n        elif n <= 1000:\n            method = 'ebrahimi'\n        else:\n            method = 'vasicek'\n    res = methods[method](sorted_data, window_length)\n    if base is not None:\n        res /= np.log(base)\n    return res"
        ]
    },
    {
        "func_name": "_pad_along_last_axis",
        "original": "def _pad_along_last_axis(X, m):\n    \"\"\"Pad the data for computing the rolling window difference.\"\"\"\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)",
        "mutated": [
            "def _pad_along_last_axis(X, m):\n    if False:\n        i = 10\n    'Pad the data for computing the rolling window difference.'\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)",
            "def _pad_along_last_axis(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Pad the data for computing the rolling window difference.'\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)",
            "def _pad_along_last_axis(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Pad the data for computing the rolling window difference.'\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)",
            "def _pad_along_last_axis(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Pad the data for computing the rolling window difference.'\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)",
            "def _pad_along_last_axis(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Pad the data for computing the rolling window difference.'\n    shape = np.array(X.shape)\n    shape[-1] = m\n    Xl = np.broadcast_to(X[..., [0]], shape)\n    Xr = np.broadcast_to(X[..., [-1]], shape)\n    return np.concatenate((Xl, X, Xr), axis=-1)"
        ]
    },
    {
        "func_name": "_vasicek_entropy",
        "original": "def _vasicek_entropy(X, m):\n    \"\"\"Compute the Vasicek estimator as described in [6] Eq. 1.3.\"\"\"\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)",
        "mutated": [
            "def _vasicek_entropy(X, m):\n    if False:\n        i = 10\n    'Compute the Vasicek estimator as described in [6] Eq. 1.3.'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)",
            "def _vasicek_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the Vasicek estimator as described in [6] Eq. 1.3.'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)",
            "def _vasicek_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the Vasicek estimator as described in [6] Eq. 1.3.'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)",
            "def _vasicek_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the Vasicek estimator as described in [6] Eq. 1.3.'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)",
            "def _vasicek_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the Vasicek estimator as described in [6] Eq. 1.3.'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    logs = np.log(n / (2 * m) * differences)\n    return np.mean(logs, axis=-1)"
        ]
    },
    {
        "func_name": "_van_es_entropy",
        "original": "def _van_es_entropy(X, m):\n    \"\"\"Compute the van Es estimator as described in [6].\"\"\"\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)",
        "mutated": [
            "def _van_es_entropy(X, m):\n    if False:\n        i = 10\n    'Compute the van Es estimator as described in [6].'\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)",
            "def _van_es_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the van Es estimator as described in [6].'\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)",
            "def _van_es_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the van Es estimator as described in [6].'\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)",
            "def _van_es_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the van Es estimator as described in [6].'\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)",
            "def _van_es_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the van Es estimator as described in [6].'\n    n = X.shape[-1]\n    difference = X[..., m:] - X[..., :-m]\n    term1 = 1 / (n - m) * np.sum(np.log((n + 1) / m * difference), axis=-1)\n    k = np.arange(m, n + 1)\n    return term1 + np.sum(1 / k) + np.log(m) - np.log(n + 1)"
        ]
    },
    {
        "func_name": "_ebrahimi_entropy",
        "original": "def _ebrahimi_entropy(X, m):\n    \"\"\"Compute the Ebrahimi estimator as described in [6].\"\"\"\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)",
        "mutated": [
            "def _ebrahimi_entropy(X, m):\n    if False:\n        i = 10\n    'Compute the Ebrahimi estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)",
            "def _ebrahimi_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the Ebrahimi estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)",
            "def _ebrahimi_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the Ebrahimi estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)",
            "def _ebrahimi_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the Ebrahimi estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)",
            "def _ebrahimi_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the Ebrahimi estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    differences = X[..., 2 * m:] - X[..., :-2 * m]\n    i = np.arange(1, n + 1).astype(float)\n    ci = np.ones_like(i) * 2\n    ci[i <= m] = 1 + (i[i <= m] - 1) / m\n    ci[i >= n - m + 1] = 1 + (n - i[i >= n - m + 1]) / m\n    logs = np.log(n * differences / (ci * m))\n    return np.mean(logs, axis=-1)"
        ]
    },
    {
        "func_name": "_correa_entropy",
        "original": "def _correa_entropy(X, m):\n    \"\"\"Compute the Correa estimator as described in [6].\"\"\"\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)",
        "mutated": [
            "def _correa_entropy(X, m):\n    if False:\n        i = 10\n    'Compute the Correa estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)",
            "def _correa_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the Correa estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)",
            "def _correa_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the Correa estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)",
            "def _correa_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the Correa estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)",
            "def _correa_entropy(X, m):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the Correa estimator as described in [6].'\n    n = X.shape[-1]\n    X = _pad_along_last_axis(X, m)\n    i = np.arange(1, n + 1)\n    dj = np.arange(-m, m + 1)[:, None]\n    j = i + dj\n    j0 = j + m - 1\n    Xibar = np.mean(X[..., j0], axis=-2, keepdims=True)\n    difference = X[..., j0] - Xibar\n    num = np.sum(difference * dj, axis=-2)\n    den = n * np.sum(difference ** 2, axis=-2)\n    return -np.mean(np.log(num / den), axis=-1)"
        ]
    }
]