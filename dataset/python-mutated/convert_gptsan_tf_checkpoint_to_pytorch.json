[
    {
        "func_name": "convert_tf_gptsan_to_pt",
        "original": "def convert_tf_gptsan_to_pt(args):\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)",
        "mutated": [
            "def convert_tf_gptsan_to_pt(args):\n    if False:\n        i = 10\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)",
            "def convert_tf_gptsan_to_pt(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)",
            "def convert_tf_gptsan_to_pt(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)",
            "def convert_tf_gptsan_to_pt(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)",
            "def convert_tf_gptsan_to_pt(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parameter_file = os.path.join(args.tf_model_dir, 'parameters.json')\n    params = json.loads(open(parameter_file).read())\n    if not params:\n        raise ValueError(f'It seems that the json file at {parameter_file} is empty. Make sure you have a correct json file.')\n    if not args.output.endswith('.pt'):\n        args.output = args.output + '.pt'\n    new_state = OrderedDict()\n    with tf.device('/CPU:0'):\n        reader = tf.train.load_checkpoint(args.tf_model_dir)\n        shapes = reader.get_variable_to_shape_map()\n        for key_name in shapes.keys():\n            vnp = reader.get_tensor(key_name).astype(np.float16)\n            if key_name.endswith('/adam_m') or key_name.endswith('/adam_v'):\n                continue\n            if key_name.startswith('pasts/'):\n                if key_name.startswith('pasts/mlp'):\n                    player = int(key_name[9])\n                elif key_name.startswith('pasts/out'):\n                    player = 8\n                name = 'model.sqout.%d.weight' % (player * 2)\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/moe'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/switch_gating/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.router.classifier.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/softmlp/kernel'):\n                    name = 'model.blocks.%d.feed_forward.soft_bypass_mlp.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/wo/kernel') or key_name.endswith('/wi/kernel'):\n                    nlayer = key_name[-9:-7]\n                    for i in range(16):\n                        name = 'model.blocks.%d.feed_forward.mlp.experts.expert_%d.%s.weight' % (player, i, nlayer)\n                        state = vnp[i].transpose([1, 0]).copy()\n                        new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/mlp'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/p1/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p1/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wi.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/kernel'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.weight' % player\n                    state = vnp.transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/p2/bias'):\n                    name = 'model.blocks.%d.feed_forward.mlp.wo.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/ln'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.feed_forward.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.feed_forward.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/att'):\n                player = int(key_name[9:].split('/')[0])\n                if key_name.endswith('/qkv/kernel'):\n                    state = vnp.copy()\n                    state_q = state[:, 0, :, :]\n                    state_k = state[:, 1, :, :]\n                    state_v = state[:, 2, :, :]\n                    state_q = state_q.reshape([state_q.shape[0], state_q.shape[1] * state_q.shape[2]]).transpose([1, 0]).copy()\n                    state_k = state_k.reshape([state_k.shape[0], state_k.shape[1] * state_k.shape[2]]).transpose([1, 0]).copy()\n                    state_v = state_v.reshape([state_v.shape[0], state_v.shape[1] * state_v.shape[2]]).transpose([1, 0]).copy()\n                    name = 'model.blocks.%d.self_attn.self_attn.q_proj.weight' % player\n                    new_state[name] = torch.tensor(state_q)\n                    name = 'model.blocks.%d.self_attn.self_attn.k_proj.weight' % player\n                    new_state[name] = torch.tensor(state_k)\n                    name = 'model.blocks.%d.self_attn.self_attn.v_proj.weight' % player\n                    new_state[name] = torch.tensor(state_v)\n                elif key_name.endswith('/o/kernel'):\n                    name = 'model.blocks.%d.self_attn.self_attn.out_proj.weight' % player\n                    state = vnp.reshape([vnp.shape[0] * vnp.shape[1], vnp.shape[2]]).transpose([1, 0]).copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/an'):\n                player = int(key_name[8:].split('/')[0])\n                if key_name.endswith('/b'):\n                    name = 'model.blocks.%d.self_attn.norm.bias' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n                elif key_name.endswith('/g'):\n                    name = 'model.blocks.%d.self_attn.norm.weight' % player\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wte') or key_name.startswith('model/wpe') or key_name.startswith('model/ete'):\n                nlayer = {'wte': 'embed_tokens', 'wpe': 'position_embeddings', 'ete': 'extra_position_embeddings'}[key_name[-3:]]\n                name = 'model.%s.weight' % nlayer\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n                if key_name.startswith('model/wte'):\n                    name = 'lm_head.weight'\n                    state = vnp.copy()\n                    new_state[name] = torch.tensor(state)\n            elif key_name.startswith('model/wob'):\n                name = 'final_logits_bias'\n                state = vnp.copy()\n                state = state.reshape((1, -1))\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense/kernel':\n                name = 'model.last_project.weight'\n                state = vnp.transpose([1, 0]).copy()\n                new_state[name] = torch.tensor(state)\n            elif key_name == 'model/dense_1/bias':\n                name = 'model.last_project.bias'\n                state = vnp.copy()\n                new_state[name] = torch.tensor(state)\n    torch.save(new_state, args.output)"
        ]
    }
]