[
    {
        "func_name": "loss_coteaching",
        "original": "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    \"\"\"Co-Teaching Loss function.\n\n    Parameters\n    ----------\n    y_1 : Tensor array\n      Output logits from model 1\n\n    y_2 : Tensor array\n      Output logits from model 2\n\n    t : np.ndarray\n      List of Noisy Labels (t means targets)\n\n    forget_rate : float\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\n      Just use rate_schedule[epoch] for this value\n\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\n      A np.torch.tensor list of length number of classes with weights\n    \"\"\"\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)",
        "mutated": [
            "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    if False:\n        i = 10\n    'Co-Teaching Loss function.\\n\\n    Parameters\\n    ----------\\n    y_1 : Tensor array\\n      Output logits from model 1\\n\\n    y_2 : Tensor array\\n      Output logits from model 2\\n\\n    t : np.ndarray\\n      List of Noisy Labels (t means targets)\\n\\n    forget_rate : float\\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\\n      Just use rate_schedule[epoch] for this value\\n\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    '\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)",
            "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Co-Teaching Loss function.\\n\\n    Parameters\\n    ----------\\n    y_1 : Tensor array\\n      Output logits from model 1\\n\\n    y_2 : Tensor array\\n      Output logits from model 2\\n\\n    t : np.ndarray\\n      List of Noisy Labels (t means targets)\\n\\n    forget_rate : float\\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\\n      Just use rate_schedule[epoch] for this value\\n\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    '\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)",
            "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Co-Teaching Loss function.\\n\\n    Parameters\\n    ----------\\n    y_1 : Tensor array\\n      Output logits from model 1\\n\\n    y_2 : Tensor array\\n      Output logits from model 2\\n\\n    t : np.ndarray\\n      List of Noisy Labels (t means targets)\\n\\n    forget_rate : float\\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\\n      Just use rate_schedule[epoch] for this value\\n\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    '\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)",
            "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Co-Teaching Loss function.\\n\\n    Parameters\\n    ----------\\n    y_1 : Tensor array\\n      Output logits from model 1\\n\\n    y_2 : Tensor array\\n      Output logits from model 2\\n\\n    t : np.ndarray\\n      List of Noisy Labels (t means targets)\\n\\n    forget_rate : float\\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\\n      Just use rate_schedule[epoch] for this value\\n\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    '\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)",
            "def loss_coteaching(y_1, y_2, t, forget_rate, class_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Co-Teaching Loss function.\\n\\n    Parameters\\n    ----------\\n    y_1 : Tensor array\\n      Output logits from model 1\\n\\n    y_2 : Tensor array\\n      Output logits from model 2\\n\\n    t : np.ndarray\\n      List of Noisy Labels (t means targets)\\n\\n    forget_rate : float\\n      Decimal between 0 and 1 for how quickly the models forget what they learn.\\n      Just use rate_schedule[epoch] for this value\\n\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    '\n    loss_1 = F.cross_entropy(y_1, t, reduce=False, weight=class_weights)\n    ind_1_sorted = np.argsort(loss_1.data.cpu())\n    loss_1_sorted = loss_1[ind_1_sorted]\n    loss_2 = F.cross_entropy(y_2, t, reduce=False, weight=class_weights)\n    ind_2_sorted = np.argsort(loss_2.data.cpu())\n    remember_rate = 1 - forget_rate\n    num_remember = int(remember_rate * len(loss_1_sorted))\n    ind_1_update = ind_1_sorted[:num_remember]\n    ind_2_update = ind_2_sorted[:num_remember]\n    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update], weight=class_weights)\n    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update], weight=class_weights)\n    return (torch.sum(loss_1_update) / num_remember, torch.sum(loss_2_update) / num_remember)"
        ]
    },
    {
        "func_name": "initialize_lr_scheduler",
        "original": "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    \"\"\"Scheduler to adjust learning rate and betas for Adam Optimizer\"\"\"\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)",
        "mutated": [
            "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    if False:\n        i = 10\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)",
            "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)",
            "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)",
            "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)",
            "def initialize_lr_scheduler(lr=0.001, epochs=250, epoch_decay_start=80):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    mom1 = 0.9\n    mom2 = 0.9\n    alpha_plan = [lr] * epochs\n    beta1_plan = [mom1] * epochs\n    for i in range(epoch_decay_start, epochs):\n        alpha_plan[i] = float(epochs - i) / (epochs - epoch_decay_start) * lr\n        beta1_plan[i] = mom2\n    return (alpha_plan, beta1_plan)"
        ]
    },
    {
        "func_name": "adjust_learning_rate",
        "original": "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    \"\"\"Scheduler to adjust learning rate and betas for Adam Optimizer\"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)",
        "mutated": [
            "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    if False:\n        i = 10\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)",
            "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)",
            "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)",
            "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)",
            "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta1_plan):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Scheduler to adjust learning rate and betas for Adam Optimizer'\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = alpha_plan[epoch]\n        param_group['betas'] = (beta1_plan[epoch], 0.999)"
        ]
    },
    {
        "func_name": "forget_rate_scheduler",
        "original": "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    \"\"\"Tells Co-Teaching what fraction of examples to forget at each epoch.\"\"\"\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule",
        "mutated": [
            "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    if False:\n        i = 10\n    'Tells Co-Teaching what fraction of examples to forget at each epoch.'\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule",
            "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Tells Co-Teaching what fraction of examples to forget at each epoch.'\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule",
            "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Tells Co-Teaching what fraction of examples to forget at each epoch.'\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule",
            "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Tells Co-Teaching what fraction of examples to forget at each epoch.'\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule",
            "def forget_rate_scheduler(epochs, forget_rate, num_gradual, exponent):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Tells Co-Teaching what fraction of examples to forget at each epoch.'\n    forget_rate_schedule = np.ones(epochs) * forget_rate\n    forget_rate_schedule[:num_gradual] = np.linspace(0, forget_rate ** exponent, num_gradual)\n    return forget_rate_schedule"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    \"\"\"PyTorch training function.\n\n    Parameters\n    ----------\n    train_loader : torch.utils.data.DataLoader\n    epoch : int\n    model1 : PyTorch class inheriting nn.Module\n        Must define __init__ and forward(self, x,)\n    optimizer1 : PyTorch torch.optim.Adam\n    model2 : PyTorch class inheriting nn.Module\n        Must define __init__ and forward(self, x,)\n    optimizer2 : PyTorch torch.optim.Adam\n    args : parser.parse_args() object\n        Must contain num_iter_per_epoch, print_freq, and epochs\n    forget_rate_schedule : np.ndarray of length number of epochs\n        Tells Co-Teaching loss what fraction of examples to forget about.\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\n      A np.torch.tensor list of length number of classes with weights\n    accuracy : function\n        A function of the form accuracy(output, target, topk=(1,)) for\n        computing top1 and top5 accuracy given output and true targets.\"\"\"\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)",
        "mutated": [
            "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    if False:\n        i = 10\n    'PyTorch training function.\\n\\n    Parameters\\n    ----------\\n    train_loader : torch.utils.data.DataLoader\\n    epoch : int\\n    model1 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer1 : PyTorch torch.optim.Adam\\n    model2 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer2 : PyTorch torch.optim.Adam\\n    args : parser.parse_args() object\\n        Must contain num_iter_per_epoch, print_freq, and epochs\\n    forget_rate_schedule : np.ndarray of length number of epochs\\n        Tells Co-Teaching loss what fraction of examples to forget about.\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    accuracy : function\\n        A function of the form accuracy(output, target, topk=(1,)) for\\n        computing top1 and top5 accuracy given output and true targets.'\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)",
            "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'PyTorch training function.\\n\\n    Parameters\\n    ----------\\n    train_loader : torch.utils.data.DataLoader\\n    epoch : int\\n    model1 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer1 : PyTorch torch.optim.Adam\\n    model2 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer2 : PyTorch torch.optim.Adam\\n    args : parser.parse_args() object\\n        Must contain num_iter_per_epoch, print_freq, and epochs\\n    forget_rate_schedule : np.ndarray of length number of epochs\\n        Tells Co-Teaching loss what fraction of examples to forget about.\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    accuracy : function\\n        A function of the form accuracy(output, target, topk=(1,)) for\\n        computing top1 and top5 accuracy given output and true targets.'\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)",
            "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'PyTorch training function.\\n\\n    Parameters\\n    ----------\\n    train_loader : torch.utils.data.DataLoader\\n    epoch : int\\n    model1 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer1 : PyTorch torch.optim.Adam\\n    model2 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer2 : PyTorch torch.optim.Adam\\n    args : parser.parse_args() object\\n        Must contain num_iter_per_epoch, print_freq, and epochs\\n    forget_rate_schedule : np.ndarray of length number of epochs\\n        Tells Co-Teaching loss what fraction of examples to forget about.\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    accuracy : function\\n        A function of the form accuracy(output, target, topk=(1,)) for\\n        computing top1 and top5 accuracy given output and true targets.'\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)",
            "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'PyTorch training function.\\n\\n    Parameters\\n    ----------\\n    train_loader : torch.utils.data.DataLoader\\n    epoch : int\\n    model1 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer1 : PyTorch torch.optim.Adam\\n    model2 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer2 : PyTorch torch.optim.Adam\\n    args : parser.parse_args() object\\n        Must contain num_iter_per_epoch, print_freq, and epochs\\n    forget_rate_schedule : np.ndarray of length number of epochs\\n        Tells Co-Teaching loss what fraction of examples to forget about.\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    accuracy : function\\n        A function of the form accuracy(output, target, topk=(1,)) for\\n        computing top1 and top5 accuracy given output and true targets.'\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)",
            "def train(train_loader, epoch, model1, optimizer1, model2, optimizer2, args, forget_rate_schedule, class_weights, accuracy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'PyTorch training function.\\n\\n    Parameters\\n    ----------\\n    train_loader : torch.utils.data.DataLoader\\n    epoch : int\\n    model1 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer1 : PyTorch torch.optim.Adam\\n    model2 : PyTorch class inheriting nn.Module\\n        Must define __init__ and forward(self, x,)\\n    optimizer2 : PyTorch torch.optim.Adam\\n    args : parser.parse_args() object\\n        Must contain num_iter_per_epoch, print_freq, and epochs\\n    forget_rate_schedule : np.ndarray of length number of epochs\\n        Tells Co-Teaching loss what fraction of examples to forget about.\\n    class_weights : Tensor array, shape (Number of classes x 1), Default: None\\n      A np.torch.tensor list of length number of classes with weights\\n    accuracy : function\\n        A function of the form accuracy(output, target, topk=(1,)) for\\n        computing top1 and top5 accuracy given output and true targets.'\n    train_total = 0\n    train_correct = 0\n    train_total2 = 0\n    train_correct2 = 0\n    model1.train()\n    model2.train()\n    for (i, (images, labels)) in enumerate(train_loader):\n        if i == len(train_loader) - 1 and len(labels) < MINIMUM_BATCH_SIZE:\n            continue\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        logits1 = model1(images)\n        (prec1, _) = accuracy(logits1, labels, topk=(1, 5))\n        train_total += 1\n        train_correct += prec1\n        logits2 = model2(images)\n        (prec2, _) = accuracy(logits2, labels, topk=(1, 5))\n        train_total2 += 1\n        train_correct2 += prec2\n        (loss_1, loss_2) = loss_coteaching(logits1, logits2, labels, forget_rate=forget_rate_schedule[epoch], class_weights=class_weights)\n        optimizer1.zero_grad()\n        loss_1.backward()\n        optimizer1.step()\n        optimizer2.zero_grad()\n        loss_2.backward()\n        optimizer2.step()\n        if (i + 1) % args.print_freq == 0:\n            print('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f ' % (epoch + 1, args.epochs, i + 1, len(train_loader.dataset) // args.batch_size, prec1, prec2, loss_1.data.item(), loss_2.data.item()))\n    train_acc1 = float(train_correct) / float(train_total)\n    train_acc2 = float(train_correct2) / float(train_total2)\n    return (train_acc1, train_acc2)"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(test_loader, model1, model2):\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)",
        "mutated": [
            "def evaluate(test_loader, model1, model2):\n    if False:\n        i = 10\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)",
            "def evaluate(test_loader, model1, model2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)",
            "def evaluate(test_loader, model1, model2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)",
            "def evaluate(test_loader, model1, model2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)",
            "def evaluate(test_loader, model1, model2):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('Evaluating Co-Teaching Model')\n    model1.eval()\n    correct1 = 0\n    total1 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits1 = model1(images)\n        outputs1 = F.softmax(logits1, dim=1)\n        (_, pred1) = torch.max(outputs1.data, 1)\n        total1 += labels.size(0)\n        correct1 += (pred1.cpu() == labels).sum()\n    model2.eval()\n    correct2 = 0\n    total2 = 0\n    for (images, labels) in test_loader:\n        images = Variable(images).cuda()\n        logits2 = model2(images)\n        outputs2 = F.softmax(logits2, dim=1)\n        (_, pred2) = torch.max(outputs2.data, 1)\n        total2 += labels.size(0)\n        correct2 += (pred2.cpu() == labels).sum()\n    acc1 = 100 * float(correct1) / float(total1)\n    acc2 = 100 * float(correct2) / float(total2)\n    return (acc1, acc2)"
        ]
    }
]