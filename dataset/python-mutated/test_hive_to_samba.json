[
    {
        "func_name": "__init__",
        "original": "def __init__(self, *args, **kwargs):\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)",
        "mutated": [
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.conn = MockConnectionCursor()\n    self.conn.execute = MagicMock()\n    self.get_conn = MagicMock(return_value=self.conn)\n    super().__init__(*args, **kwargs)"
        ]
    },
    {
        "func_name": "get_connection",
        "original": "def get_connection(self, *args):\n    return self.conn",
        "mutated": [
            "def get_connection(self, *args):\n    if False:\n        i = 10\n    return self.conn",
            "def get_connection(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.conn",
            "def get_connection(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.conn",
            "def get_connection(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.conn",
            "def get_connection(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.conn"
        ]
    },
    {
        "func_name": "setup_method",
        "original": "def setup_method(self, method):\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)",
        "mutated": [
            "def setup_method(self, method):\n    if False:\n        i = 10\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)",
            "def setup_method(self, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)",
            "def setup_method(self, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)",
            "def setup_method(self, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)",
            "def setup_method(self, method):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.kwargs = dict(hql='hql', destination_filepath='destination_filepath', samba_conn_id='samba_default', hiveserver2_conn_id='hiveserver2_default', task_id='test_hive_to_samba_operator')\n    super().setup_method(method)"
        ]
    },
    {
        "func_name": "test_execute",
        "original": "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)",
        "mutated": [
            "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    if False:\n        i = 10\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)",
            "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)",
            "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)",
            "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)",
            "@patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.NamedTemporaryFile')\ndef test_execute(self, mock_tmp_file, mock_hive_hook, mock_samba_hook):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    type(mock_tmp_file).name = PropertyMock(return_value='tmp_file')\n    mock_tmp_file.return_value.__enter__ = Mock(return_value=mock_tmp_file)\n    context = {}\n    HiveToSambaOperator(**self.kwargs).execute(context)\n    mock_hive_hook.assert_called_once_with(hiveserver2_conn_id=self.kwargs['hiveserver2_conn_id'])\n    mock_hive_hook.return_value.to_csv.assert_called_once_with(self.kwargs['hql'], csv_filepath=mock_tmp_file.name, hive_conf=context_to_airflow_vars(context))\n    mock_samba_hook.assert_called_once_with(samba_conn_id=self.kwargs['samba_conn_id'])\n    mock_samba_hook.return_value.push_from_local.assert_called_once_with(self.kwargs['destination_filepath'], mock_tmp_file.name)"
        ]
    },
    {
        "func_name": "test_hive2samba",
        "original": "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')",
        "mutated": [
            "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    if False:\n        i = 10\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')",
            "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')",
            "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')",
            "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')",
            "@pytest.mark.skipif('AIRFLOW_RUNALL_TESTS' not in os.environ, reason='Skipped because AIRFLOW_RUNALL_TESTS is not set')\n@patch('tempfile.tempdir', '/tmp/')\n@patch('tempfile._RandomNameSequence.__next__')\n@patch('airflow.providers.apache.hive.transfers.hive_to_samba.HiveServer2Hook', side_effect=MockHiveServer2Hook)\ndef test_hive2samba(self, mock_hive_server_hook, mock_temp_dir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mock_temp_dir.return_value = 'tst'\n    samba_hook = MockSambaHook(self.kwargs['samba_conn_id'])\n    samba_hook.upload = MagicMock()\n    with patch('airflow.providers.apache.hive.transfers.hive_to_samba.SambaHook', return_value=samba_hook):\n        samba_hook.conn.upload = MagicMock()\n        op = HiveToSambaOperator(task_id='hive2samba_check', samba_conn_id='tableau_samba', hql='SELECT * FROM airflow.static_babynames LIMIT 10000', destination_filepath='test_airflow.csv', dag=self.dag)\n        op.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)\n    samba_hook.conn.upload.assert_called_with('/tmp/tmptst', 'test_airflow.csv')"
        ]
    }
]