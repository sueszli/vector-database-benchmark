[
    {
        "func_name": "train_one_step",
        "original": "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    \"\"\"Function that improves the all policies in `train_batch` on the local worker.\n\n    .. testcode::\n        :skipif: True\n\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\n        algo = [...]\n        train_batch = synchronous_parallel_sample(algo.workers)\n        # This trains the policy on one batch.\n        print(train_one_step(algo, train_batch)))\n\n    .. testoutput::\n\n        {\"default_policy\": ...}\n\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\n    \"\"\"\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info",
        "mutated": [
            "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    if False:\n        i = 10\n    'Function that improves the all policies in `train_batch` on the local worker.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\\n    '\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info",
            "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Function that improves the all policies in `train_batch` on the local worker.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\\n    '\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info",
            "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Function that improves the all policies in `train_batch` on the local worker.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\\n    '\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info",
            "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Function that improves the all policies in `train_batch` on the local worker.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\\n    '\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info",
            "@DeveloperAPI\ndef train_one_step(algorithm, train_batch, policies_to_train=None) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Function that improves the all policies in `train_batch` on the local worker.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LEARN_ON_BATCH_TIMER timer of the `algorithm` object.\\n    '\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', 0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        if num_sgd_iter > 1 or sgd_minibatch_size > 0:\n            info = do_minibatch_sgd(train_batch, {pid: local_worker.get_policy(pid) for pid in policies_to_train or local_worker.get_policies_to_train(train_batch)}, local_worker, num_sgd_iter, sgd_minibatch_size, [])\n        else:\n            info = local_worker.learn_on_batch(train_batch)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return info"
        ]
    },
    {
        "func_name": "multi_gpu_train_one_step",
        "original": "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    \"\"\"Multi-GPU version of train_one_step.\n\n    Uses the policies' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\n    actual train batch is only split once and loaded once into the GPU(s).\n\n    .. testcode::\n        :skipif: True\n\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\n        algo = [...]\n        train_batch = synchronous_parallel_sample(algo.workers)\n        # This trains the policy on one batch.\n        print(multi_gpu_train_one_step(algo, train_batch)))\n\n    .. testoutput::\n\n        {\"default_policy\": ...}\n\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\n    \"\"\"\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info",
        "mutated": [
            "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    if False:\n        i = 10\n    'Multi-GPU version of train_one_step.\\n\\n    Uses the policies\\' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\\n    actual train batch is only split once and loaded once into the GPU(s).\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(multi_gpu_train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\\n    '\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info",
            "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Multi-GPU version of train_one_step.\\n\\n    Uses the policies\\' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\\n    actual train batch is only split once and loaded once into the GPU(s).\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(multi_gpu_train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\\n    '\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info",
            "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Multi-GPU version of train_one_step.\\n\\n    Uses the policies\\' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\\n    actual train batch is only split once and loaded once into the GPU(s).\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(multi_gpu_train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\\n    '\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info",
            "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Multi-GPU version of train_one_step.\\n\\n    Uses the policies\\' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\\n    actual train batch is only split once and loaded once into the GPU(s).\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(multi_gpu_train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\\n    '\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info",
            "@DeveloperAPI\ndef multi_gpu_train_one_step(algorithm, train_batch) -> Dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Multi-GPU version of train_one_step.\\n\\n    Uses the policies\\' `load_batch_into_buffer` and `learn_on_loaded_batch` methods\\n    to be more efficient wrt CPU/GPU data transfers. For example, when doing multiple\\n    passes through a train batch (e.g. for PPO) using `config.num_sgd_iter`, the\\n    actual train batch is only split once and loaded once into the GPU(s).\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        from ray.rllib.execution.rollout_ops import synchronous_parallel_sample\\n        algo = [...]\\n        train_batch = synchronous_parallel_sample(algo.workers)\\n        # This trains the policy on one batch.\\n        print(multi_gpu_train_one_step(algo, train_batch)))\\n\\n    .. testoutput::\\n\\n        {\"default_policy\": ...}\\n\\n    Updates the NUM_ENV_STEPS_TRAINED and NUM_AGENT_STEPS_TRAINED counters as well as\\n    the LOAD_BATCH_TIMER and LEARN_ON_BATCH_TIMER timers of the Algorithm instance.\\n    '\n    if log_once('mulit_gpu_train_one_step_deprecation_warning'):\n        deprecation_warning(old='ray.rllib.execution.train_ops.multi_gpu_train_one_step')\n    config = algorithm.config\n    workers = algorithm.workers\n    local_worker = workers.local_worker()\n    num_sgd_iter = config.get('num_sgd_iter', 1)\n    sgd_minibatch_size = config.get('sgd_minibatch_size', config['train_batch_size'])\n    num_devices = int(math.ceil(config['num_gpus'] or 1))\n    per_device_batch_size = sgd_minibatch_size // num_devices\n    batch_size = per_device_batch_size * num_devices\n    assert batch_size % num_devices == 0\n    assert batch_size >= num_devices, 'Batch size too small!'\n    train_batch = train_batch.as_multi_agent()\n    load_timer = algorithm._timers[LOAD_BATCH_TIMER]\n    with load_timer:\n        num_loaded_samples = {}\n        for (policy_id, batch) in train_batch.policy_batches.items():\n            if local_worker.is_policy_to_train is not None and (not local_worker.is_policy_to_train(policy_id, train_batch)):\n                continue\n            batch.decompress_if_needed()\n            num_loaded_samples[policy_id] = local_worker.policy_map[policy_id].load_batch_into_buffer(batch, buffer_index=0)\n    learn_timer = algorithm._timers[LEARN_ON_BATCH_TIMER]\n    with learn_timer:\n        learner_info_builder = LearnerInfoBuilder(num_devices=num_devices)\n        for (policy_id, samples_per_device) in num_loaded_samples.items():\n            policy = local_worker.policy_map[policy_id]\n            num_batches = max(1, int(samples_per_device) // int(per_device_batch_size))\n            logger.debug('== sgd epochs for {} =='.format(policy_id))\n            for _ in range(num_sgd_iter):\n                permutation = np.random.permutation(num_batches)\n                for batch_index in range(num_batches):\n                    results = policy.learn_on_loaded_batch(permutation[batch_index] * per_device_batch_size, buffer_index=0)\n                    learner_info_builder.add_learn_on_batch_results(results, policy_id)\n        learner_info = learner_info_builder.finalize()\n    load_timer.push_units_processed(train_batch.count)\n    learn_timer.push_units_processed(train_batch.count)\n    algorithm._counters[NUM_ENV_STEPS_TRAINED] += train_batch.count\n    algorithm._counters[NUM_AGENT_STEPS_TRAINED] += train_batch.agent_steps()\n    if algorithm.reward_estimators:\n        learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'] = {}\n        for (name, estimator) in algorithm.reward_estimators.items():\n            learner_info[DEFAULT_POLICY_ID]['off_policy_estimation'][name] = estimator.train(train_batch)\n    return learner_info"
        ]
    }
]