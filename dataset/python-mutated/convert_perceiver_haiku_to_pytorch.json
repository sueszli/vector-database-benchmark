[
    {
        "func_name": "prepare_img",
        "original": "def prepare_img():\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im",
        "mutated": [
            "def prepare_img():\n    if False:\n        i = 10\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im",
            "def prepare_img():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    url = 'https://storage.googleapis.com/perceiver_io/dalmation.jpg'\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im"
        ]
    },
    {
        "func_name": "rename_keys",
        "original": "def rename_keys(state_dict, architecture):\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)",
        "mutated": [
            "def rename_keys(state_dict, architecture):\n    if False:\n        i = 10\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)",
            "def rename_keys(state_dict, architecture):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)",
            "def rename_keys(state_dict, architecture):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)",
            "def rename_keys(state_dict, architecture):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)",
            "def rename_keys(state_dict, architecture):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for name in list(state_dict):\n        param = state_dict.pop(name)\n        name = name.replace('embed/embeddings', 'input_preprocessor.embeddings.weight')\n        if name.startswith('trainable_position_encoding/pos_embs'):\n            name = name.replace('trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/w', 'input_preprocessor.convnet_1x1.weight')\n        name = name.replace('image_preprocessor/~/conv2_d/b', 'input_preprocessor.convnet_1x1.bias')\n        name = name.replace('image_preprocessor/~_build_network_inputs/trainable_position_encoding/pos_embs', 'input_preprocessor.position_embeddings.position_embeddings')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/w', 'input_preprocessor.positions_projection.weight')\n        name = name.replace('image_preprocessor/~_build_network_inputs/position_encoding_projector/linear/b', 'input_preprocessor.positions_projection.bias')\n        if 'counter' in name or 'hidden' in name:\n            continue\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/conv/w', 'input_preprocessor.convnet.conv.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/offset', 'input_preprocessor.convnet.batchnorm.bias')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/scale', 'input_preprocessor.convnet.batchnorm.weight')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/mean_ema/average', 'input_preprocessor.convnet.batchnorm.running_mean')\n        name = name.replace('image_preprocessor/~/conv2_d_downsample/~/batchnorm/~/var_ema/average', 'input_preprocessor.convnet.batchnorm.running_var')\n        name = name.replace('image_preprocessor/patches_linear/b', 'input_preprocessor.conv_after_patches.bias')\n        name = name.replace('image_preprocessor/patches_linear/w', 'input_preprocessor.conv_after_patches.weight')\n        name = name.replace('multimodal_preprocessor/audio_mask_token/pos_embs', 'input_preprocessor.mask.audio')\n        name = name.replace('multimodal_preprocessor/audio_padding/pos_embs', 'input_preprocessor.padding.audio')\n        name = name.replace('multimodal_preprocessor/image_mask_token/pos_embs', 'input_preprocessor.mask.image')\n        name = name.replace('multimodal_preprocessor/image_padding/pos_embs', 'input_preprocessor.padding.image')\n        name = name.replace('multimodal_preprocessor/label_mask_token/pos_embs', 'input_preprocessor.mask.label')\n        name = name.replace('multimodal_preprocessor/label_padding/pos_embs', 'input_preprocessor.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('multimodal_decoder/~decoder_query/audio_padding/pos_embs', 'decoder.padding.audio')\n        name = name.replace('multimodal_decoder/~decoder_query/image_padding/pos_embs', 'decoder.padding.image')\n        name = name.replace('multimodal_decoder/~decoder_query/label_padding/pos_embs', 'decoder.padding.label')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('multimodal_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        if architecture == 'multimodal_autoencoding':\n            name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.modalities.label.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('flow_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('flow_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name.replace('flow_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.decoder.output_position_encodings.position_embeddings')\n        name = name.replace('basic_decoder/~/trainable_position_encoding/pos_embs', 'decoder.output_position_encodings.position_embeddings')\n        name = name.replace('classification_decoder/~/basic_decoder/cross_attention/', 'decoder.decoder.decoding_cross_attention.')\n        name = name.replace('classification_decoder/~/basic_decoder/output/b', 'decoder.decoder.final_layer.bias')\n        name = name.replace('classification_decoder/~/basic_decoder/output/w', 'decoder.decoder.final_layer.weight')\n        name = name = name.replace('classification_decoder/~/basic_decoder/~/', 'decoder.decoder.')\n        name = name.replace('basic_decoder/cross_attention/', 'decoder.decoding_cross_attention.')\n        name = name.replace('basic_decoder/~/', 'decoder.')\n        name = name.replace('projection_postprocessor/linear/b', 'output_postprocessor.modalities.image.classifier.bias')\n        name = name.replace('projection_postprocessor/linear/w', 'output_postprocessor.modalities.image.classifier.weight')\n        name = name.replace('classification_postprocessor/linear/b', 'output_postprocessor.modalities.label.classifier.bias')\n        name = name.replace('classification_postprocessor/linear/w', 'output_postprocessor.modalities.label.classifier.weight')\n        name = name.replace('audio_postprocessor/linear/b', 'output_postprocessor.modalities.audio.classifier.bias')\n        name = name.replace('audio_postprocessor/linear/w', 'output_postprocessor.modalities.audio.classifier.weight')\n        name = name.replace('perceiver_encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        name = name.replace('encoder/~/trainable_position_encoding/pos_embs', 'embeddings.latents')\n        if name.startswith('perceiver_encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('perceiver_encoder/~/', 'encoder.' + suffix)\n        if name.startswith('encoder/~/'):\n            if 'self_attention' in name:\n                suffix = 'self_attends.'\n            else:\n                suffix = ''\n            name = name.replace('encoder/~/', 'encoder.' + suffix)\n        if 'offset' in name:\n            name = name.replace('offset', 'bias')\n        if 'scale' in name:\n            name = name.replace('scale', 'weight')\n        if 'cross_attention' in name and 'layer_norm_2' in name:\n            name = name.replace('layer_norm_2', 'layernorm')\n        if 'self_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'layernorm')\n        if 'cross_attention' in name and 'layer_norm_1' in name:\n            name = name.replace('layer_norm_1', 'attention.self.layernorm2')\n        if 'cross_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        if 'self_attention' in name and 'layer_norm' in name:\n            name = name.replace('layer_norm', 'attention.self.layernorm1')\n        name = name.replace('-', '.')\n        name = name.replace('/', '.')\n        if ('cross_attention' in name or 'self_attention' in name) and 'mlp' not in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'self.query.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'self.query.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'self.key.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'self.key.weight')\n            if 'linear_2.b' in name:\n                name = name.replace('linear_2.b', 'self.value.bias')\n            if 'linear_2.w' in name:\n                name = name.replace('linear_2.w', 'self.value.weight')\n            if 'linear_3.b' in name:\n                name = name.replace('linear_3.b', 'output.dense.bias')\n            if 'linear_3.w' in name:\n                name = name.replace('linear_3.w', 'output.dense.weight')\n        if 'self_attention_' in name:\n            name = name.replace('self_attention_', '')\n        if 'self_attention' in name:\n            name = name.replace('self_attention', '0')\n        if 'mlp' in name:\n            if 'linear.b' in name:\n                name = name.replace('linear.b', 'dense1.bias')\n            if 'linear.w' in name:\n                name = name.replace('linear.w', 'dense1.weight')\n            if 'linear_1.b' in name:\n                name = name.replace('linear_1.b', 'dense2.bias')\n            if 'linear_1.w' in name:\n                name = name.replace('linear_1.w', 'dense2.weight')\n        if name[-6:] == 'weight' and 'embeddings' not in name:\n            param = np.transpose(param)\n        if 'batchnorm' in name:\n            param = np.squeeze(param)\n        if 'embedding_decoder' not in name:\n            state_dict['perceiver.' + name] = torch.from_numpy(param)\n        else:\n            state_dict[name] = torch.from_numpy(param)"
        ]
    },
    {
        "func_name": "convert_perceiver_checkpoint",
        "original": "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    \"\"\"\n    Copy/paste/tweak model's weights to our Perceiver structure.\n    \"\"\"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our Perceiver structure.\\n    \"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our Perceiver structure.\\n    \"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our Perceiver structure.\\n    \"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our Perceiver structure.\\n    \"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architecture='MLM'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our Perceiver structure.\\n    \"\n    with open(pickle_file, 'rb') as f:\n        checkpoint = pickle.loads(f.read())\n    state = None\n    if isinstance(checkpoint, dict) and architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        params = checkpoint['params']\n        state = checkpoint['state']\n    else:\n        params = checkpoint\n    state_dict = {}\n    for (scope_name, parameters) in hk.data_structures.to_mutable_dict(params).items():\n        for (param_name, param) in parameters.items():\n            state_dict[scope_name + '/' + param_name] = param\n    if state is not None:\n        for (scope_name, parameters) in hk.data_structures.to_mutable_dict(state).items():\n            for (param_name, param) in parameters.items():\n                state_dict[scope_name + '/' + param_name] = param\n    rename_keys(state_dict, architecture=architecture)\n    config = PerceiverConfig()\n    subsampling = None\n    repo_id = 'huggingface/label-files'\n    if architecture == 'MLM':\n        config.qk_channels = 8 * 32\n        config.v_channels = 1280\n        model = PerceiverForMaskedLM(config)\n    elif 'image_classification' in architecture:\n        config.num_latents = 512\n        config.d_latents = 1024\n        config.d_model = 512\n        config.num_blocks = 8\n        config.num_self_attends_per_block = 6\n        config.num_cross_attention_heads = 1\n        config.num_self_attention_heads = 8\n        config.qk_channels = None\n        config.v_channels = None\n        config.num_labels = 1000\n        filename = 'imagenet-1k-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n        if architecture == 'image_classification':\n            config.image_size = 224\n            model = PerceiverForImageClassificationLearned(config)\n        elif architecture == 'image_classification_fourier':\n            config.d_model = 261\n            model = PerceiverForImageClassificationFourier(config)\n        elif architecture == 'image_classification_conv':\n            config.d_model = 322\n            model = PerceiverForImageClassificationConvProcessing(config)\n        else:\n            raise ValueError(f'Architecture {architecture} not supported')\n    elif architecture == 'optical_flow':\n        config.num_latents = 2048\n        config.d_latents = 512\n        config.d_model = 322\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 24\n        config.num_self_attention_heads = 16\n        config.num_cross_attention_heads = 1\n        model = PerceiverForOpticalFlow(config)\n    elif architecture == 'multimodal_autoencoding':\n        config.num_latents = 28 * 28 * 1\n        config.d_latents = 512\n        config.d_model = 704\n        config.num_blocks = 1\n        config.num_self_attends_per_block = 8\n        config.num_self_attention_heads = 8\n        config.num_cross_attention_heads = 1\n        config.num_labels = 700\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        nchunks = 128\n        image_chunk_size = np.prod((16, 224, 224)) // nchunks\n        audio_chunk_size = audio.shape[1] // config.samples_per_patch // nchunks\n        chunk_idx = 0\n        subsampling = {'image': torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)), 'audio': torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)), 'label': None}\n        model = PerceiverForMultimodalAutoencoding(config)\n        filename = 'kinetics700-id2label.json'\n        id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type='dataset'), 'r'))\n        id2label = {int(k): v for (k, v) in id2label.items()}\n        config.id2label = id2label\n        config.label2id = {v: k for (k, v) in id2label.items()}\n    else:\n        raise ValueError(f'Architecture {architecture} not supported')\n    model.eval()\n    model.load_state_dict(state_dict)\n    input_mask = None\n    if architecture == 'MLM':\n        tokenizer = PerceiverTokenizer.from_pretrained('/Users/NielsRogge/Documents/Perceiver/Tokenizer files')\n        text = 'This is an incomplete sentence where some words are missing.'\n        encoding = tokenizer(text, padding='max_length', return_tensors='pt')\n        encoding.input_ids[0, 51:60] = tokenizer.mask_token_id\n        inputs = encoding.input_ids\n        input_mask = encoding.attention_mask\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        image_processor = PerceiverImageProcessor()\n        image = prepare_img()\n        encoding = image_processor(image, return_tensors='pt')\n        inputs = encoding.pixel_values\n    elif architecture == 'optical_flow':\n        inputs = torch.randn(1, 2, 27, 368, 496)\n    elif architecture == 'multimodal_autoencoding':\n        images = torch.randn((1, 16, 3, 224, 224))\n        audio = torch.randn((1, 30720, 1))\n        inputs = {'image': images, 'audio': audio, 'label': torch.zeros((images.shape[0], 700))}\n    if architecture == 'multimodal_autoencoding':\n        outputs = model(inputs=inputs, attention_mask=input_mask, subsampled_output_points=subsampling)\n    else:\n        outputs = model(inputs=inputs, attention_mask=input_mask)\n    logits = outputs.logits\n    if not isinstance(logits, dict):\n        print('Shape of logits:', logits.shape)\n    else:\n        for (k, v) in logits.items():\n            print(f'Shape of logits of modality {k}', v.shape)\n    if architecture == 'MLM':\n        expected_slice = torch.tensor([[-11.8336, -11.685, -11.8483], [-12.8149, -12.5863, -12.7904], [-12.844, -12.641, -12.8646]])\n        assert torch.allclose(logits[0, :3, :3], expected_slice)\n        masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1).tolist()\n        expected_list = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n        assert masked_tokens_predictions == expected_list\n        print('Greedy predictions:')\n        print(masked_tokens_predictions)\n        print()\n        print('Predicted string:')\n        print(tokenizer.decode(masked_tokens_predictions))\n    elif architecture in ['image_classification', 'image_classification_fourier', 'image_classification_conv']:\n        print('Predicted class:', model.config.id2label[logits.argmax(-1).item()])\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    print(f'Saving model to {pytorch_dump_folder_path}')\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]