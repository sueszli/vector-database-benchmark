import logging
from typing import Dict, Any, Optional, List
import math
import numpy as np
from ray.data import Dataset
from ray.rllib.offline.estimators.off_policy_estimator import OffPolicyEstimator
from ray.rllib.offline.offline_evaluation_utils import compute_q_and_v_values
from ray.rllib.offline.offline_evaluator import OfflineEvaluator
from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel
from ray.rllib.policy import Policy
from ray.rllib.policy.sample_batch import convert_ma_batch_to_sample_batch
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils.annotations import DeveloperAPI, override
from ray.rllib.utils.typing import SampleBatchType
from ray.rllib.utils.numpy import convert_to_numpy
logger = logging.getLogger()

@DeveloperAPI
class DirectMethod(OffPolicyEstimator):
    """The Direct Method estimator.

    Let s_t, a_t, and r_t be the state, action, and reward at timestep t.

    This method trains a Q-model for the evaluation policy \\pi_e on behavior
    data generated by \\pi_b. Currently, RLlib implements this using
    Fitted-Q Evaluation (FQE). You can also implement your own model
    and pass it in as `q_model_config = {"type": your_model_class, **your_kwargs}`.

    This estimator computes the expected return for \\pi_e for an episode as:
    V^{\\pi_e}(s_0) = \\sum_{a \\in A} \\pi_e(a | s_0) Q(s_0, a)
    and returns the mean and standard deviation over episodes.

    For more information refer to https://arxiv.org/pdf/1911.06854.pdf"""

    @override(OffPolicyEstimator)
    def __init__(self, policy: Policy, gamma: float, epsilon_greedy: float=0.0, q_model_config: Optional[Dict]=None):
        if False:
            print('Hello World!')
        'Initializes a Direct Method OPE Estimator.\n\n        Args:\n            policy: Policy to evaluate.\n            gamma: Discount factor of the environment.\n            epsilon_greedy: The probability by which we act acording to a fully random\n                policy during deployment. With 1-epsilon_greedy we act according the\n                target policy.\n            q_model_config: Arguments to specify the Q-model. Must specify\n                a `type` key pointing to the Q-model class.\n                This Q-model is trained in the train() method and is used\n                to compute the state-value estimates for the DirectMethod estimator.\n                It must implement `train` and `estimate_v`.\n                TODO (Rohan138): Unify this with RLModule API.\n        '
        super().__init__(policy, gamma, epsilon_greedy)
        if hasattr(policy, 'config'):
            assert policy.config.get('framework', 'torch') == 'torch', 'Framework must be torch to use DirectMethod.'
        q_model_config = q_model_config or {}
        model_cls = q_model_config.pop('type', FQETorchModel)
        self.model = model_cls(policy=policy, gamma=gamma, **q_model_config)
        assert hasattr(self.model, 'estimate_v'), 'self.model must implement `estimate_v`!'

    @override(OffPolicyEstimator)
    def estimate_on_single_episode(self, episode: SampleBatch) -> Dict[str, Any]:
        if False:
            while True:
                i = 10
        estimates_per_epsiode = {}
        rewards = episode['rewards']
        v_behavior = 0.0
        for t in range(episode.count):
            v_behavior += rewards[t] * self.gamma ** t
        v_target = self._compute_v_target(episode[:1])
        estimates_per_epsiode['v_behavior'] = v_behavior
        estimates_per_epsiode['v_target'] = v_target
        return estimates_per_epsiode

    @override(OffPolicyEstimator)
    def estimate_on_single_step_samples(self, batch: SampleBatch) -> Dict[str, List[float]]:
        if False:
            i = 10
            return i + 15
        estimates_per_epsiode = {}
        rewards = batch['rewards']
        v_behavior = rewards
        v_target = self._compute_v_target(batch)
        estimates_per_epsiode['v_behavior'] = v_behavior
        estimates_per_epsiode['v_target'] = v_target
        return estimates_per_epsiode

    def _compute_v_target(self, init_step):
        if False:
            print('Hello World!')
        v_target = self.model.estimate_v(init_step)
        v_target = convert_to_numpy(v_target)
        return v_target

    @override(OffPolicyEstimator)
    def train(self, batch: SampleBatchType) -> Dict[str, Any]:
        if False:
            print('Hello World!')
        'Trains self.model on the given batch.\n\n        Args:\n            batch: A SampleBatchType to train on\n\n        Returns:\n            A dict with key "loss" and value as the mean training loss.\n        '
        batch = convert_ma_batch_to_sample_batch(batch)
        losses = self.model.train(batch)
        return {'loss': np.mean(losses)}

    @override(OfflineEvaluator)
    def estimate_on_dataset(self, dataset: Dataset, *, n_parallelism: int=...) -> Dict[str, Any]:
        if False:
            print('Hello World!')
        'Calculates the Direct Method estimate on the given dataset.\n\n        Note: This estimate works for only discrete action spaces for now.\n\n        Args:\n            dataset: Dataset to compute the estimate on. Each record in dataset should\n                include the following columns: `obs`, `actions`, `action_prob` and\n                `rewards`. The `obs` on each row shoud be a vector of D dimensions.\n            n_parallelism: The number of parallel workers to use.\n\n        Returns:\n            Dictionary with the following keys:\n                v_target: The estimated value of the target policy.\n                v_behavior: The estimated value of the behavior policy.\n                v_gain: The estimated gain of the target policy over the behavior\n                    policy.\n                v_std: The standard deviation of the estimated value of the target.\n        '
        batch_size = max(dataset.count() // n_parallelism, 1)
        updated_ds = dataset.map_batches(compute_q_and_v_values, batch_size=batch_size, batch_format='pandas', fn_kwargs={'model_class': self.model.__class__, 'model_state': self.model.get_state(), 'compute_q_values': False})
        v_behavior = updated_ds.mean('rewards')
        v_target = updated_ds.mean('v_values')
        v_gain_mean = v_target / v_behavior
        v_gain_ste = updated_ds.std('v_values') / v_behavior / math.sqrt(dataset.count())
        return {'v_behavior': v_behavior, 'v_target': v_target, 'v_gain_mean': v_gain_mean, 'v_gain_ste': v_gain_ste}