[
    {
        "func_name": "register_loss_scale_wrapper",
        "original": "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    \"\"\"Registers a loss scale optimizer wrapper.\n\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\n  scaling. This function registers a\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\n  that is used by `enable_mixed_precision_graph_rewrite`, where\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\n  `optimizer_cls` instance.\n\n  Args:\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\n      that wraps \"optimizer\".\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\n      which case `wrapper_fn` should be a loss scale optimizer class whose\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\n  \"\"\"\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)",
        "mutated": [
            "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    if False:\n        i = 10\n    'Registers a loss scale optimizer wrapper.\\n\\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\\n  scaling. This function registers a\\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\\n  that is used by `enable_mixed_precision_graph_rewrite`, where\\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\\n  `optimizer_cls` instance.\\n\\n  Args:\\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\\n      that wraps \"optimizer\".\\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\\n      which case `wrapper_fn` should be a loss scale optimizer class whose\\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\\n  '\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)",
            "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Registers a loss scale optimizer wrapper.\\n\\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\\n  scaling. This function registers a\\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\\n  that is used by `enable_mixed_precision_graph_rewrite`, where\\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\\n  `optimizer_cls` instance.\\n\\n  Args:\\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\\n      that wraps \"optimizer\".\\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\\n      which case `wrapper_fn` should be a loss scale optimizer class whose\\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\\n  '\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)",
            "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Registers a loss scale optimizer wrapper.\\n\\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\\n  scaling. This function registers a\\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\\n  that is used by `enable_mixed_precision_graph_rewrite`, where\\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\\n  `optimizer_cls` instance.\\n\\n  Args:\\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\\n      that wraps \"optimizer\".\\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\\n      which case `wrapper_fn` should be a loss scale optimizer class whose\\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\\n  '\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)",
            "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Registers a loss scale optimizer wrapper.\\n\\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\\n  scaling. This function registers a\\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\\n  that is used by `enable_mixed_precision_graph_rewrite`, where\\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\\n  `optimizer_cls` instance.\\n\\n  Args:\\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\\n      that wraps \"optimizer\".\\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\\n      which case `wrapper_fn` should be a loss scale optimizer class whose\\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\\n  '\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)",
            "@tf_export('__internal__.mixed_precision.register_loss_scale_wrapper', v1=[])\ndef register_loss_scale_wrapper(optimizer_cls, wrapper_fn, wrapper_cls=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Registers a loss scale optimizer wrapper.\\n\\n  `tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite`\\n  automatically wraps an optimizer with an optimizer wrapper that performs loss\\n  scaling. This function registers a\\n  `(base_cls, wrapper_fn, wrapper_cls)` triple\\n  that is used by `enable_mixed_precision_graph_rewrite`, where\\n  `wrapper_fn` is called to create a `wrapper_cls` instance that wraps an\\n  `optimizer_cls` instance.\\n\\n  Args:\\n    optimizer_cls: A base optimizer class, e.g. `tf.keras.optimizers.Optimizer`.\\n    wrapper_fn: A function that takes in arguments \"optimizer\" and\\n      \"loss_scale\", and returns a loss scale optimizer of type \"wrapper_cls\"\\n      that wraps \"optimizer\".\\n    wrapper_cls: A loss scale optimizer class. Defaults to `wrapper_fn`, in\\n      which case `wrapper_fn` should be a loss scale optimizer class whose\\n      constructor takes in arguments \"optimizer\" and \"loss_scale\".\\n  '\n    _REGISTERED_WRAPPER_OPTIMIZER_CLS[optimizer_cls] = (wrapper_fn, wrapper_cls or wrapper_fn)"
        ]
    },
    {
        "func_name": "_wrap_optimizer",
        "original": "def _wrap_optimizer(opt, loss_scale):\n    \"\"\"Wraps an optimizer with a LossScaleOptimizer.\"\"\"\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)",
        "mutated": [
            "def _wrap_optimizer(opt, loss_scale):\n    if False:\n        i = 10\n    'Wraps an optimizer with a LossScaleOptimizer.'\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)",
            "def _wrap_optimizer(opt, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Wraps an optimizer with a LossScaleOptimizer.'\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)",
            "def _wrap_optimizer(opt, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Wraps an optimizer with a LossScaleOptimizer.'\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)",
            "def _wrap_optimizer(opt, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Wraps an optimizer with a LossScaleOptimizer.'\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)",
            "def _wrap_optimizer(opt, loss_scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Wraps an optimizer with a LossScaleOptimizer.'\n    for (_, wrapper_optimizer) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.values():\n        if isinstance(opt, wrapper_optimizer):\n            raise ValueError('\"opt\" must not already be an instance of a {cls}. `enable_mixed_precision_graph_rewrite` will automatically wrap the optimizer with a {cls}.'.format(cls=wrapper_optimizer.__name__))\n    for (optimizer_cls, (wrapper_fn, _)) in _REGISTERED_WRAPPER_OPTIMIZER_CLS.items():\n        if isinstance(opt, optimizer_cls):\n            return wrapper_fn(opt, loss_scale)\n    raise ValueError('\"opt\" must be an instance of a tf.train.Optimizer or a tf.keras.optimizers.Optimizer, but got: %s' % opt)"
        ]
    },
    {
        "func_name": "enable_mixed_precision_graph_rewrite_v1",
        "original": "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    \"\"\"Enable mixed precision via a graph rewrite.\n\n  Mixed precision is the use of both float32 and float16 data types when\n  training a model to improve performance. This is achieved via a graph rewrite\n  operation and a loss-scale optimizer.\n\n  Performing arithmetic operations in float16 takes advantage of specialized\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\n  throughput. However, due to the smaller representable range, performing the\n  entire training with float16 can result in gradient underflow, that is, small\n  gradient values becoming zeroes. Instead, performing only select arithmetic\n  operations in float16 results in higher throughput and decreased training\n  time when using compatible hardware accelerators while also reducing memory\n  usage, typically without sacrificing model accuracy.\n\n  Note: While the mixed precision rewrite changes the datatype of various\n  layers throughout the model, the same accuracy reached in float32 is\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\n  update for that batch is skipped. In this case, the global step count is not\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\n  has been shown to achieve the same accuracy as float32 and, in most cases,\n  better training throughput.\n\n  Example:\n\n  ```python\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(64, activation='softmax'),\n  ])\n\n  opt = tf.keras.optimizers.SGD()\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n  model.compile(loss=\"mse\", optimizer=opt)\n\n  x_train = np.random.random((1024, 64))\n  y_train = np.random.random((1024, 64))\n  model.fit(x_train, y_train)\n  ```\n\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\n  operation before computing gradients. The function additionally returns an\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\n  underflow in the float16 tensors during the backward pass. An optimizer of\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\n  to this function, which will then be wrapped to use loss scaling.\n\n  The graph rewrite operation changes the `dtype` of certain operations in the\n  graph from float32 to float16. There are several categories of operations\n  that are either included or excluded by this rewrite operation. The following\n  categories of Ops are defined inside corresponding functions under the class\n  `AutoMixedPrecisionLists` in\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\n  auto_mixed_precision_lists.h</a>:\n\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\n  E.g. `ArgMax` and `Floor`.\n  * `AllowList`: Ops that are considered numerically safe for execution in\n  float16, and thus are always converted. E.g. `Conv2D`.\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\n  can negatively affect downstream nodes. E.g. `Softmax`.\n  * `GrayList`: Ops that are considered numerically safe for execution in\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\n\n  When this function is used, gradients should only be computed and applied\n  with the returned optimizer, either by calling `opt.minimize()` or\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\n  This is because the returned optimizer will apply loss scaling, and\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\n  float16 underflow problems.\n\n  When eager execution is enabled, the mixed precision graph rewrite is only\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\n\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\n  (such as batch size, input size, output size, and channel counts)\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\n  256. For more information, check out the\n  [NVIDIA Deep Learning Performance Guide](\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\n\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\n\n  Raises:\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\n    API can be used.\n\n  Args:\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\n      `tf.train.Optimizer`.\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\n      is recommended to keep this as its default value of `\"dynamic\"`, which\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\n\n  Returns:\n    A version of `opt` that will use loss scaling to prevent underflow.\n  \"\"\"\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt",
        "mutated": [
            "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    if False:\n        i = 10\n    'Enable mixed precision via a graph rewrite.\\n\\n  Mixed precision is the use of both float32 and float16 data types when\\n  training a model to improve performance. This is achieved via a graph rewrite\\n  operation and a loss-scale optimizer.\\n\\n  Performing arithmetic operations in float16 takes advantage of specialized\\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\\n  throughput. However, due to the smaller representable range, performing the\\n  entire training with float16 can result in gradient underflow, that is, small\\n  gradient values becoming zeroes. Instead, performing only select arithmetic\\n  operations in float16 results in higher throughput and decreased training\\n  time when using compatible hardware accelerators while also reducing memory\\n  usage, typically without sacrificing model accuracy.\\n\\n  Note: While the mixed precision rewrite changes the datatype of various\\n  layers throughout the model, the same accuracy reached in float32 is\\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\\n  update for that batch is skipped. In this case, the global step count is not\\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\\n  has been shown to achieve the same accuracy as float32 and, in most cases,\\n  better training throughput.\\n\\n  Example:\\n\\n  ```python\\n  model = tf.keras.models.Sequential([\\n      tf.keras.layers.Dense(64, activation=\\'relu\\'),\\n      tf.keras.layers.Dense(64, activation=\\'softmax\\'),\\n  ])\\n\\n  opt = tf.keras.optimizers.SGD()\\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n  model.compile(loss=\"mse\", optimizer=opt)\\n\\n  x_train = np.random.random((1024, 64))\\n  y_train = np.random.random((1024, 64))\\n  model.fit(x_train, y_train)\\n  ```\\n\\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\\n  operation before computing gradients. The function additionally returns an\\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\\n  underflow in the float16 tensors during the backward pass. An optimizer of\\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\\n  to this function, which will then be wrapped to use loss scaling.\\n\\n  The graph rewrite operation changes the `dtype` of certain operations in the\\n  graph from float32 to float16. There are several categories of operations\\n  that are either included or excluded by this rewrite operation. The following\\n  categories of Ops are defined inside corresponding functions under the class\\n  `AutoMixedPrecisionLists` in\\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\\n  auto_mixed_precision_lists.h</a>:\\n\\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\\n  E.g. `ArgMax` and `Floor`.\\n  * `AllowList`: Ops that are considered numerically safe for execution in\\n  float16, and thus are always converted. E.g. `Conv2D`.\\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\\n  can negatively affect downstream nodes. E.g. `Softmax`.\\n  * `GrayList`: Ops that are considered numerically safe for execution in\\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\\n\\n  When this function is used, gradients should only be computed and applied\\n  with the returned optimizer, either by calling `opt.minimize()` or\\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\\n  This is because the returned optimizer will apply loss scaling, and\\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\\n  float16 underflow problems.\\n\\n  When eager execution is enabled, the mixed precision graph rewrite is only\\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\\n\\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\\n  (such as batch size, input size, output size, and channel counts)\\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\\n  256. For more information, check out the\\n  [NVIDIA Deep Learning Performance Guide](\\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\\n\\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\\n\\n  Raises:\\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\\n    API can be used.\\n\\n  Args:\\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\\n      `tf.train.Optimizer`.\\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\\n      is recommended to keep this as its default value of `\"dynamic\"`, which\\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\\n\\n  Returns:\\n    A version of `opt` that will use loss scaling to prevent underflow.\\n  '\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt",
            "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Enable mixed precision via a graph rewrite.\\n\\n  Mixed precision is the use of both float32 and float16 data types when\\n  training a model to improve performance. This is achieved via a graph rewrite\\n  operation and a loss-scale optimizer.\\n\\n  Performing arithmetic operations in float16 takes advantage of specialized\\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\\n  throughput. However, due to the smaller representable range, performing the\\n  entire training with float16 can result in gradient underflow, that is, small\\n  gradient values becoming zeroes. Instead, performing only select arithmetic\\n  operations in float16 results in higher throughput and decreased training\\n  time when using compatible hardware accelerators while also reducing memory\\n  usage, typically without sacrificing model accuracy.\\n\\n  Note: While the mixed precision rewrite changes the datatype of various\\n  layers throughout the model, the same accuracy reached in float32 is\\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\\n  update for that batch is skipped. In this case, the global step count is not\\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\\n  has been shown to achieve the same accuracy as float32 and, in most cases,\\n  better training throughput.\\n\\n  Example:\\n\\n  ```python\\n  model = tf.keras.models.Sequential([\\n      tf.keras.layers.Dense(64, activation=\\'relu\\'),\\n      tf.keras.layers.Dense(64, activation=\\'softmax\\'),\\n  ])\\n\\n  opt = tf.keras.optimizers.SGD()\\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n  model.compile(loss=\"mse\", optimizer=opt)\\n\\n  x_train = np.random.random((1024, 64))\\n  y_train = np.random.random((1024, 64))\\n  model.fit(x_train, y_train)\\n  ```\\n\\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\\n  operation before computing gradients. The function additionally returns an\\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\\n  underflow in the float16 tensors during the backward pass. An optimizer of\\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\\n  to this function, which will then be wrapped to use loss scaling.\\n\\n  The graph rewrite operation changes the `dtype` of certain operations in the\\n  graph from float32 to float16. There are several categories of operations\\n  that are either included or excluded by this rewrite operation. The following\\n  categories of Ops are defined inside corresponding functions under the class\\n  `AutoMixedPrecisionLists` in\\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\\n  auto_mixed_precision_lists.h</a>:\\n\\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\\n  E.g. `ArgMax` and `Floor`.\\n  * `AllowList`: Ops that are considered numerically safe for execution in\\n  float16, and thus are always converted. E.g. `Conv2D`.\\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\\n  can negatively affect downstream nodes. E.g. `Softmax`.\\n  * `GrayList`: Ops that are considered numerically safe for execution in\\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\\n\\n  When this function is used, gradients should only be computed and applied\\n  with the returned optimizer, either by calling `opt.minimize()` or\\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\\n  This is because the returned optimizer will apply loss scaling, and\\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\\n  float16 underflow problems.\\n\\n  When eager execution is enabled, the mixed precision graph rewrite is only\\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\\n\\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\\n  (such as batch size, input size, output size, and channel counts)\\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\\n  256. For more information, check out the\\n  [NVIDIA Deep Learning Performance Guide](\\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\\n\\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\\n\\n  Raises:\\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\\n    API can be used.\\n\\n  Args:\\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\\n      `tf.train.Optimizer`.\\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\\n      is recommended to keep this as its default value of `\"dynamic\"`, which\\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\\n\\n  Returns:\\n    A version of `opt` that will use loss scaling to prevent underflow.\\n  '\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt",
            "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Enable mixed precision via a graph rewrite.\\n\\n  Mixed precision is the use of both float32 and float16 data types when\\n  training a model to improve performance. This is achieved via a graph rewrite\\n  operation and a loss-scale optimizer.\\n\\n  Performing arithmetic operations in float16 takes advantage of specialized\\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\\n  throughput. However, due to the smaller representable range, performing the\\n  entire training with float16 can result in gradient underflow, that is, small\\n  gradient values becoming zeroes. Instead, performing only select arithmetic\\n  operations in float16 results in higher throughput and decreased training\\n  time when using compatible hardware accelerators while also reducing memory\\n  usage, typically without sacrificing model accuracy.\\n\\n  Note: While the mixed precision rewrite changes the datatype of various\\n  layers throughout the model, the same accuracy reached in float32 is\\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\\n  update for that batch is skipped. In this case, the global step count is not\\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\\n  has been shown to achieve the same accuracy as float32 and, in most cases,\\n  better training throughput.\\n\\n  Example:\\n\\n  ```python\\n  model = tf.keras.models.Sequential([\\n      tf.keras.layers.Dense(64, activation=\\'relu\\'),\\n      tf.keras.layers.Dense(64, activation=\\'softmax\\'),\\n  ])\\n\\n  opt = tf.keras.optimizers.SGD()\\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n  model.compile(loss=\"mse\", optimizer=opt)\\n\\n  x_train = np.random.random((1024, 64))\\n  y_train = np.random.random((1024, 64))\\n  model.fit(x_train, y_train)\\n  ```\\n\\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\\n  operation before computing gradients. The function additionally returns an\\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\\n  underflow in the float16 tensors during the backward pass. An optimizer of\\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\\n  to this function, which will then be wrapped to use loss scaling.\\n\\n  The graph rewrite operation changes the `dtype` of certain operations in the\\n  graph from float32 to float16. There are several categories of operations\\n  that are either included or excluded by this rewrite operation. The following\\n  categories of Ops are defined inside corresponding functions under the class\\n  `AutoMixedPrecisionLists` in\\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\\n  auto_mixed_precision_lists.h</a>:\\n\\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\\n  E.g. `ArgMax` and `Floor`.\\n  * `AllowList`: Ops that are considered numerically safe for execution in\\n  float16, and thus are always converted. E.g. `Conv2D`.\\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\\n  can negatively affect downstream nodes. E.g. `Softmax`.\\n  * `GrayList`: Ops that are considered numerically safe for execution in\\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\\n\\n  When this function is used, gradients should only be computed and applied\\n  with the returned optimizer, either by calling `opt.minimize()` or\\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\\n  This is because the returned optimizer will apply loss scaling, and\\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\\n  float16 underflow problems.\\n\\n  When eager execution is enabled, the mixed precision graph rewrite is only\\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\\n\\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\\n  (such as batch size, input size, output size, and channel counts)\\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\\n  256. For more information, check out the\\n  [NVIDIA Deep Learning Performance Guide](\\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\\n\\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\\n\\n  Raises:\\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\\n    API can be used.\\n\\n  Args:\\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\\n      `tf.train.Optimizer`.\\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\\n      is recommended to keep this as its default value of `\"dynamic\"`, which\\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\\n\\n  Returns:\\n    A version of `opt` that will use loss scaling to prevent underflow.\\n  '\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt",
            "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Enable mixed precision via a graph rewrite.\\n\\n  Mixed precision is the use of both float32 and float16 data types when\\n  training a model to improve performance. This is achieved via a graph rewrite\\n  operation and a loss-scale optimizer.\\n\\n  Performing arithmetic operations in float16 takes advantage of specialized\\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\\n  throughput. However, due to the smaller representable range, performing the\\n  entire training with float16 can result in gradient underflow, that is, small\\n  gradient values becoming zeroes. Instead, performing only select arithmetic\\n  operations in float16 results in higher throughput and decreased training\\n  time when using compatible hardware accelerators while also reducing memory\\n  usage, typically without sacrificing model accuracy.\\n\\n  Note: While the mixed precision rewrite changes the datatype of various\\n  layers throughout the model, the same accuracy reached in float32 is\\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\\n  update for that batch is skipped. In this case, the global step count is not\\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\\n  has been shown to achieve the same accuracy as float32 and, in most cases,\\n  better training throughput.\\n\\n  Example:\\n\\n  ```python\\n  model = tf.keras.models.Sequential([\\n      tf.keras.layers.Dense(64, activation=\\'relu\\'),\\n      tf.keras.layers.Dense(64, activation=\\'softmax\\'),\\n  ])\\n\\n  opt = tf.keras.optimizers.SGD()\\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n  model.compile(loss=\"mse\", optimizer=opt)\\n\\n  x_train = np.random.random((1024, 64))\\n  y_train = np.random.random((1024, 64))\\n  model.fit(x_train, y_train)\\n  ```\\n\\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\\n  operation before computing gradients. The function additionally returns an\\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\\n  underflow in the float16 tensors during the backward pass. An optimizer of\\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\\n  to this function, which will then be wrapped to use loss scaling.\\n\\n  The graph rewrite operation changes the `dtype` of certain operations in the\\n  graph from float32 to float16. There are several categories of operations\\n  that are either included or excluded by this rewrite operation. The following\\n  categories of Ops are defined inside corresponding functions under the class\\n  `AutoMixedPrecisionLists` in\\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\\n  auto_mixed_precision_lists.h</a>:\\n\\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\\n  E.g. `ArgMax` and `Floor`.\\n  * `AllowList`: Ops that are considered numerically safe for execution in\\n  float16, and thus are always converted. E.g. `Conv2D`.\\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\\n  can negatively affect downstream nodes. E.g. `Softmax`.\\n  * `GrayList`: Ops that are considered numerically safe for execution in\\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\\n\\n  When this function is used, gradients should only be computed and applied\\n  with the returned optimizer, either by calling `opt.minimize()` or\\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\\n  This is because the returned optimizer will apply loss scaling, and\\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\\n  float16 underflow problems.\\n\\n  When eager execution is enabled, the mixed precision graph rewrite is only\\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\\n\\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\\n  (such as batch size, input size, output size, and channel counts)\\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\\n  256. For more information, check out the\\n  [NVIDIA Deep Learning Performance Guide](\\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\\n\\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\\n\\n  Raises:\\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\\n    API can be used.\\n\\n  Args:\\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\\n      `tf.train.Optimizer`.\\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\\n      is recommended to keep this as its default value of `\"dynamic\"`, which\\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\\n\\n  Returns:\\n    A version of `opt` that will use loss scaling to prevent underflow.\\n  '\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt",
            "@deprecation.deprecated_endpoints('train.experimental.enable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.enable_mixed_precision_graph_rewrite', 'train.experimental.enable_mixed_precision_graph_rewrite'])\ndef enable_mixed_precision_graph_rewrite_v1(opt, loss_scale='dynamic'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Enable mixed precision via a graph rewrite.\\n\\n  Mixed precision is the use of both float32 and float16 data types when\\n  training a model to improve performance. This is achieved via a graph rewrite\\n  operation and a loss-scale optimizer.\\n\\n  Performing arithmetic operations in float16 takes advantage of specialized\\n  processing units, such as NVIDIA Tensor Cores, for much higher arithmetic\\n  throughput. However, due to the smaller representable range, performing the\\n  entire training with float16 can result in gradient underflow, that is, small\\n  gradient values becoming zeroes. Instead, performing only select arithmetic\\n  operations in float16 results in higher throughput and decreased training\\n  time when using compatible hardware accelerators while also reducing memory\\n  usage, typically without sacrificing model accuracy.\\n\\n  Note: While the mixed precision rewrite changes the datatype of various\\n  layers throughout the model, the same accuracy reached in float32 is\\n  expected. If a `NaN` gradient occurs with dynamic loss scaling, the model\\n  update for that batch is skipped. In this case, the global step count is not\\n  incremented, and the `LossScaleOptimizer` attempts to decrease the loss\\n  scaling value to avoid `NaN` values in subsequent iterations. This approach\\n  has been shown to achieve the same accuracy as float32 and, in most cases,\\n  better training throughput.\\n\\n  Example:\\n\\n  ```python\\n  model = tf.keras.models.Sequential([\\n      tf.keras.layers.Dense(64, activation=\\'relu\\'),\\n      tf.keras.layers.Dense(64, activation=\\'softmax\\'),\\n  ])\\n\\n  opt = tf.keras.optimizers.SGD()\\n  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n  model.compile(loss=\"mse\", optimizer=opt)\\n\\n  x_train = np.random.random((1024, 64))\\n  y_train = np.random.random((1024, 64))\\n  model.fit(x_train, y_train)\\n  ```\\n\\n  Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\\n  operation before computing gradients. The function additionally returns an\\n  `Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\\n  underflow in the float16 tensors during the backward pass. An optimizer of\\n  type `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed\\n  to this function, which will then be wrapped to use loss scaling.\\n\\n  The graph rewrite operation changes the `dtype` of certain operations in the\\n  graph from float32 to float16. There are several categories of operations\\n  that are either included or excluded by this rewrite operation. The following\\n  categories of Ops are defined inside corresponding functions under the class\\n  `AutoMixedPrecisionLists` in\\n  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/\\n  core/grappler/optimizers/auto_mixed_precision_lists.h\">\\n  auto_mixed_precision_lists.h</a>:\\n\\n  * `ClearList`: Ops that do not have numerically significant adverse effects.\\n  E.g. `ArgMax` and `Floor`.\\n  * `AllowList`: Ops that are considered numerically safe for execution in\\n  float16, and thus are always converted. E.g. `Conv2D`.\\n  * `DenyList`: Ops that are numerically unsafe to execute in float16 and\\n  can negatively affect downstream nodes. E.g. `Softmax`.\\n  * `GrayList`: Ops that are considered numerically safe for execution in\\n  float16 unless downstream from a DenyList Op. E.g. `Add` and `AvgPool`.\\n\\n  When this function is used, gradients should only be computed and applied\\n  with the returned optimizer, either by calling `opt.minimize()` or\\n  `opt.compute_gradients()` followed by `opt.apply_gradients()`.\\n  Gradients should not be computed with `tf.gradients` or `tf.GradientTape`.\\n  This is because the returned optimizer will apply loss scaling, and\\n  `tf.gradients` or `tf.GradientTape` will not. If you do directly use\\n  `tf.gradients` or `tf.GradientTape`, your model may not converge due to\\n  float16 underflow problems.\\n\\n  When eager execution is enabled, the mixed precision graph rewrite is only\\n  enabled within `tf.function`s, as outside `tf.function`s, there is no graph.\\n\\n  For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\\n  (such as batch size, input size, output size, and channel counts)\\n  should be powers of two if under 256, or  otherwise divisible by 8 if above\\n  256. For more information, check out the\\n  [NVIDIA Deep Learning Performance Guide](\\n  https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html).\\n\\n  Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\\n  Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The\\n  parts of the graph on CPUs and TPUs are untouched by the graph rewrite.\\n\\n  Raises:\\n    `ValueError`, if the `tf.keras.mixed_precision` API is also used by calling\\n    `tf.keras.mixed_precision.set_global_policy`. Only one mixed precision\\n    API can be used.\\n\\n  Args:\\n    opt: An instance of a `tf.keras.optimizers.Optimizer` or a\\n      `tf.train.Optimizer`.\\n    loss_scale: Either an int/float, the string `\"dynamic\"`, or an instance of\\n      a `tf.mixed_precision.experimental.LossScale`. The loss scale to use. It\\n      is recommended to keep this as its default value of `\"dynamic\"`, which\\n      will adjust the scaling automatically to prevent `Inf` or `NaN` values.\\n\\n  Returns:\\n    A version of `opt` that will use loss scaling to prevent underflow.\\n  '\n    if mixed_precision_global_state.is_using_mixed_precision_policy():\n        raise ValueError('The mixed precision graph rewrite cannot be enabled, because the global Keras dtype Policy has been set to a mixed precision policy. At most, one of the following can be called:\\n\\n  1. tf.keras.mixed_precision.set_global_policy() with a mixed precision policy (You called this first)\\n\\n  2. tf.train.experimental.enable_mixed_precision_graph_rewrite() (You called this second)\\nYou called both functions, which is an error, because both functions enable you to use mixed precision. If in doubt which function to use, use the first, as it supports Eager execution and is more customizable.')\n    if mixed_precision_global_state.non_mixed_precision_session_created():\n        tf_logging.warn('You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.')\n    opt = _wrap_optimizer(opt, loss_scale)\n    config.set_optimizer_experimental_options({'auto_mixed_precision': True})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(True)\n    return opt"
        ]
    },
    {
        "func_name": "disable_mixed_precision_graph_rewrite_v1",
        "original": "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    \"\"\"Disables the mixed precision graph rewrite.\n\n  After this is called, the mixed precision graph rewrite will no longer run for\n  new Sessions, and so float32 operations will no longer be converted to float16\n  in such Sessions. However, any existing Sessions will continue to have the\n  graph rewrite enabled if they were created after\n  `enable_mixed_precision_graph_rewrite` was called but before\n  `disable_mixed_precision_graph_rewrite` was called.\n\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\n  LossScaleOptimizer will continue to do loss scaling, although this loss\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\n  the graph rewrite no longer converts the graph to use float16.\n\n  This function is useful for unit testing. A unit tests can test using the\n  mixed precision graph rewrite, then disable it so future unit tests continue\n  using float32. If this is done, unit tests should not share a single session,\n  as `enable_mixed_precision_graph_rewrite` and\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\n  \"\"\"\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)",
        "mutated": [
            "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    if False:\n        i = 10\n    'Disables the mixed precision graph rewrite.\\n\\n  After this is called, the mixed precision graph rewrite will no longer run for\\n  new Sessions, and so float32 operations will no longer be converted to float16\\n  in such Sessions. However, any existing Sessions will continue to have the\\n  graph rewrite enabled if they were created after\\n  `enable_mixed_precision_graph_rewrite` was called but before\\n  `disable_mixed_precision_graph_rewrite` was called.\\n\\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\\n  LossScaleOptimizer will continue to do loss scaling, although this loss\\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\\n  the graph rewrite no longer converts the graph to use float16.\\n\\n  This function is useful for unit testing. A unit tests can test using the\\n  mixed precision graph rewrite, then disable it so future unit tests continue\\n  using float32. If this is done, unit tests should not share a single session,\\n  as `enable_mixed_precision_graph_rewrite` and\\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\\n  '\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)",
            "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Disables the mixed precision graph rewrite.\\n\\n  After this is called, the mixed precision graph rewrite will no longer run for\\n  new Sessions, and so float32 operations will no longer be converted to float16\\n  in such Sessions. However, any existing Sessions will continue to have the\\n  graph rewrite enabled if they were created after\\n  `enable_mixed_precision_graph_rewrite` was called but before\\n  `disable_mixed_precision_graph_rewrite` was called.\\n\\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\\n  LossScaleOptimizer will continue to do loss scaling, although this loss\\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\\n  the graph rewrite no longer converts the graph to use float16.\\n\\n  This function is useful for unit testing. A unit tests can test using the\\n  mixed precision graph rewrite, then disable it so future unit tests continue\\n  using float32. If this is done, unit tests should not share a single session,\\n  as `enable_mixed_precision_graph_rewrite` and\\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\\n  '\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)",
            "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Disables the mixed precision graph rewrite.\\n\\n  After this is called, the mixed precision graph rewrite will no longer run for\\n  new Sessions, and so float32 operations will no longer be converted to float16\\n  in such Sessions. However, any existing Sessions will continue to have the\\n  graph rewrite enabled if they were created after\\n  `enable_mixed_precision_graph_rewrite` was called but before\\n  `disable_mixed_precision_graph_rewrite` was called.\\n\\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\\n  LossScaleOptimizer will continue to do loss scaling, although this loss\\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\\n  the graph rewrite no longer converts the graph to use float16.\\n\\n  This function is useful for unit testing. A unit tests can test using the\\n  mixed precision graph rewrite, then disable it so future unit tests continue\\n  using float32. If this is done, unit tests should not share a single session,\\n  as `enable_mixed_precision_graph_rewrite` and\\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\\n  '\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)",
            "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Disables the mixed precision graph rewrite.\\n\\n  After this is called, the mixed precision graph rewrite will no longer run for\\n  new Sessions, and so float32 operations will no longer be converted to float16\\n  in such Sessions. However, any existing Sessions will continue to have the\\n  graph rewrite enabled if they were created after\\n  `enable_mixed_precision_graph_rewrite` was called but before\\n  `disable_mixed_precision_graph_rewrite` was called.\\n\\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\\n  LossScaleOptimizer will continue to do loss scaling, although this loss\\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\\n  the graph rewrite no longer converts the graph to use float16.\\n\\n  This function is useful for unit testing. A unit tests can test using the\\n  mixed precision graph rewrite, then disable it so future unit tests continue\\n  using float32. If this is done, unit tests should not share a single session,\\n  as `enable_mixed_precision_graph_rewrite` and\\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\\n  '\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)",
            "@deprecation.deprecated_endpoints('train.experimental.disable_mixed_precision_graph_rewrite')\n@tf_export(v1=['mixed_precision.disable_mixed_precision_graph_rewrite', 'train.experimental.disable_mixed_precision_graph_rewrite'])\ndef disable_mixed_precision_graph_rewrite_v1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Disables the mixed precision graph rewrite.\\n\\n  After this is called, the mixed precision graph rewrite will no longer run for\\n  new Sessions, and so float32 operations will no longer be converted to float16\\n  in such Sessions. However, any existing Sessions will continue to have the\\n  graph rewrite enabled if they were created after\\n  `enable_mixed_precision_graph_rewrite` was called but before\\n  `disable_mixed_precision_graph_rewrite` was called.\\n\\n  This does not undo the effects of loss scaling. Any optimizers wrapped with a\\n  LossScaleOptimizer will continue to do loss scaling, although this loss\\n  scaling will no longer be useful if the optimizer is used in new Sessions, as\\n  the graph rewrite no longer converts the graph to use float16.\\n\\n  This function is useful for unit testing. A unit tests can test using the\\n  mixed precision graph rewrite, then disable it so future unit tests continue\\n  using float32. If this is done, unit tests should not share a single session,\\n  as `enable_mixed_precision_graph_rewrite` and\\n  `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\\n  '\n    if not mixed_precision_global_state.is_mixed_precision_graph_rewrite_enabled():\n        tf_logging.warn('disable_mixed_precision_graph_rewrite() called when mixed precision is already disabled.')\n    config.set_optimizer_experimental_options({'auto_mixed_precision': False})\n    mixed_precision_global_state.set_mixed_precision_graph_rewrite_enabled(False)"
        ]
    }
]