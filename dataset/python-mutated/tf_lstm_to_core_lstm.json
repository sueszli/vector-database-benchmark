[
    {
        "func_name": "tf_lstm_to_core_lstm",
        "original": "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    \"\"\"\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\n    `lstm` in the core op set if compatible. They are compatible if all of the\n    followings are satisfied:\n\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\n      core lstm only supports sequence output for hidden state h, and thus if\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\n      core op set.\n\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\n      are consumed. Similar to above.\n\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\n\n    Inputs:\n\n        prog: Program\n    \"\"\"\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)",
        "mutated": [
            "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    if False:\n        i = 10\n    '\\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\\n    `lstm` in the core op set if compatible. They are compatible if all of the\\n    followings are satisfied:\\n\\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\\n      core lstm only supports sequence output for hidden state h, and thus if\\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\\n      core op set.\\n\\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\\n      are consumed. Similar to above.\\n\\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\\n\\n    Inputs:\\n\\n        prog: Program\\n    '\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)",
            "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\\n    `lstm` in the core op set if compatible. They are compatible if all of the\\n    followings are satisfied:\\n\\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\\n      core lstm only supports sequence output for hidden state h, and thus if\\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\\n      core op set.\\n\\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\\n      are consumed. Similar to above.\\n\\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\\n\\n    Inputs:\\n\\n        prog: Program\\n    '\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)",
            "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\\n    `lstm` in the core op set if compatible. They are compatible if all of the\\n    followings are satisfied:\\n\\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\\n      core lstm only supports sequence output for hidden state h, and thus if\\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\\n      core op set.\\n\\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\\n      are consumed. Similar to above.\\n\\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\\n\\n    Inputs:\\n\\n        prog: Program\\n    '\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)",
            "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\\n    `lstm` in the core op set if compatible. They are compatible if all of the\\n    followings are satisfied:\\n\\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\\n      core lstm only supports sequence output for hidden state h, and thus if\\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\\n      core op set.\\n\\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\\n      are consumed. Similar to above.\\n\\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\\n\\n    Inputs:\\n\\n        prog: Program\\n    '\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)",
            "@register_pass(namespace='tensorflow')\ndef tf_lstm_to_core_lstm(prog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Try to map TF dialect ops `tf_lstm_block` and `tf_lstm_block_cell` to\\n    `lstm` in the core op set if compatible. They are compatible if all of the\\n    followings are satisfied:\\n\\n    - If tf_lstm_block: only h output is consumed. tf_lstm_block has 7\\n      sequence outputs: [i, cs, f, o, ci, co, h]. Each of them (e.g., i) has\\n      shape [seq_len, batch, hidden_dim] (see tf_lstm_block op doc string).\\n      core lstm only supports sequence output for hidden state h, and thus if\\n      any outputs other than `h` is consumed, we cannot convert to lstm in the\\n      core op set.\\n\\n    - If tf_lstm_block_cell: only cs, h output (outputs[1], outputs[6])\\n      are consumed. Similar to above.\\n\\n    - batch size == 1 (due to bugs in core lstm backend impl rdar://62475041)\\n\\n    Inputs:\\n\\n        prog: Program\\n    '\n    for (f_name, f) in prog.functions.items():\n        tf_lstm_to_core_lstm_block(f)"
        ]
    },
    {
        "func_name": "tf_lstm_to_core_lstm_block",
        "original": "def tf_lstm_to_core_lstm_block(block):\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))",
        "mutated": [
            "def tf_lstm_to_core_lstm_block(block):\n    if False:\n        i = 10\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))",
            "def tf_lstm_to_core_lstm_block(block):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))",
            "def tf_lstm_to_core_lstm_block(block):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))",
            "def tf_lstm_to_core_lstm_block(block):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))",
            "def tf_lstm_to_core_lstm_block(block):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for op in block.operations[:]:\n        for b in op.blocks:\n            tf_lstm_to_core_lstm_block(b)\n        if op.op_type in ['tf_lstm_block_cell', 'tf_lstm_block']:\n            if try_replace_with_core_lstm(op):\n                logging.info('Successfully map {} to lstm'.format(op.op_type))\n            else:\n                logging.info('Unable to map {} to lstm'.format(op.op_type))"
        ]
    },
    {
        "func_name": "try_replace_with_core_lstm",
        "original": "def try_replace_with_core_lstm(op):\n    \"\"\"\n    Inputs:\n\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\n\n    Returns:\n\n    True if op can be represented by mb.lstm op in SSA. False otherwise\n    \"\"\"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True",
        "mutated": [
            "def try_replace_with_core_lstm(op):\n    if False:\n        i = 10\n    \"\\n    Inputs:\\n\\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\\n\\n    Returns:\\n\\n    True if op can be represented by mb.lstm op in SSA. False otherwise\\n    \"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True",
            "def try_replace_with_core_lstm(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Inputs:\\n\\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\\n\\n    Returns:\\n\\n    True if op can be represented by mb.lstm op in SSA. False otherwise\\n    \"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True",
            "def try_replace_with_core_lstm(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Inputs:\\n\\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\\n\\n    Returns:\\n\\n    True if op can be represented by mb.lstm op in SSA. False otherwise\\n    \"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True",
            "def try_replace_with_core_lstm(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Inputs:\\n\\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\\n\\n    Returns:\\n\\n    True if op can be represented by mb.lstm op in SSA. False otherwise\\n    \"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True",
            "def try_replace_with_core_lstm(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Inputs:\\n\\n    op (Operation): op.op_type must be 'tf_lstm_block_cell' or `tf_lstm_block`\\n\\n    Returns:\\n\\n    True if op can be represented by mb.lstm op in SSA. False otherwise\\n    \"\n    if op.op_type == 'tf_lstm_block_cell':\n        batch = op.x.shape[0]\n    else:\n        batch = op.x.shape[1]\n    if op.use_peephole.val:\n        return False\n    if op.cell_clip is not None:\n        return False\n    (i, cs, f, o, ci, co, h) = op.outputs\n    if op.op_type == 'tf_lstm_block_cell':\n        unsupported_outputs = [i, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    else:\n        unsupported_outputs = [i, cs, f, o, ci, co]\n        for ov in unsupported_outputs:\n            if len(ov.child_ops) > 0 or len(ov.consuming_blocks) > 0:\n                return False\n    hidden_dim = op.c_prev.shape[1]\n    mb_peep = None\n    if op.use_peephole.val:\n        mb_peep = np.stack([op.weight_peep_i.val, op.weight_peep_f.val, op.weight_peep_o.val])\n    tf_w = op.weight.val\n    (tf_w_i, tf_w_c, tf_w_f, tf_w_o) = np.split(tf_w, 4, axis=1)\n    w = np.concatenate([tf_w_i, tf_w_f, tf_w_o, tf_w_c], axis=1)\n    tf_b = op.bias.val\n    (tf_b_i, tf_b_c, tf_b_f, tf_b_o) = np.split(tf_b, 4, axis=0)\n    tf_b_f += op.forget_bias.val\n    bias = np.concatenate([tf_b_i, tf_b_f, tf_b_o, tf_b_c], axis=0)\n    bias = np.stack([np.zeros_like(bias), bias])\n    cell_clip = None if op.cell_clip is None else op.cell_clip.val\n    output_sequence = op.op_type == 'tf_lstm_block'\n    block = op.enclosing_block\n    with block:\n        if op.op_type == 'tf_lstm_block_cell':\n            x = mb.expand_dims(x=op.x, axes=[0], before_op=op)\n        else:\n            x = op.x\n        (new_h_all, new_h, new_cs) = mb.lstm(x=x, initial_c=op.c_prev, initial_h=op.h_prev, weight=w, bias=bias, activations=('sigmoid', 'tanh', 'tanh'), peephole=mb_peep, clip=cell_clip, output_sequence=output_sequence, name=op.name, before_op=op)\n    if op.op_type == 'tf_lstm_block_cell':\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=cs, new_var=new_cs)\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h)\n    else:\n        block.replace_uses_of_var_after_op(anchor_op=op, old_var=h, new_var=new_h_all)\n    block.remove_ops([op])\n    return True"
        ]
    }
]