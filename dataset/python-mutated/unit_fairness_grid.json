[
    {
        "func_name": "test_train_subset_models",
        "original": "def test_train_subset_models():\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
        "mutated": [
            "def test_train_subset_models():\n    if False:\n        i = 10\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = ['loan_amount', 'loan_to_value_ratio', 'no_intro_rate_period', 'intro_rate_period', 'property_value', 'income', 'debt_to_income_ratio', 'term_360', 'conforming']\n    y = 'high_priced'\n    train = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv')\n    test = h2o.import_file('https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv')\n    train = train[~train['black'].isna()[0] & ~train['hispanic'].isna()[0] & ~train['male'].isna()[0] & ~train['agegte62'].isna()[0], :]\n    test = test[~test['black'].isna()[0] & ~test['hispanic'].isna()[0] & ~test['male'].isna()[0] & ~test['agegte62'].isna()[0], :]\n    protected_columns = ['ethnic', 'sex']\n    reference = ['white', 'M']\n    favorable_class = '0'\n    for d in [train, test]:\n        d[:, y] = d[:, y].asfactor()\n        d[d['black'] == 1, 'ethnic'] = 'black'\n        d[d['asian'] == 1, 'ethnic'] = 'asian'\n        d[d['white'] == 1, 'ethnic'] = 'white'\n        d[d['amind'] == 1, 'ethnic'] = 'amind'\n        d[d['hipac'] == 1, 'ethnic'] = 'hipac'\n        d[d['hispanic'] == 1, 'ethnic'] = 'hispanic'\n        d['sex'] = 'NA'\n        d[d['female'] == 1, 'sex'] = 'F'\n        d[d['male'] == 1, 'sex'] = 'M'\n        d['ethnic'] = d['ethnic'].asfactor()\n        d['sex'] = d['sex'].asfactor()\n    ig = H2OInfogram(protected_columns=['ethnic', 'sex'])\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * 9 + 9\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * 9\n    assert (da[:, 'air_min'] > 0.7).all()\n    assert (da[:, 'air_max'] < 1.2).all()\n    assert ((da[:, 'air_min'] > 0.8) & (da[:, 'air_max'] < 1.25)).any()\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()"
        ]
    },
    {
        "func_name": "test_train_subset_models_taiwan",
        "original": "def test_train_subset_models_taiwan():\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
        "mutated": [
            "def test_train_subset_models_taiwan():\n    if False:\n        i = 10\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models_taiwan():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models_taiwan():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models_taiwan():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()",
            "def test_train_subset_models_taiwan():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig = H2OInfogram(protected_columns=protected_columns)\n    ig.train(x, y, training_frame=train)\n    da = ig.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OAutoML, y, train, test, protected_columns, reference, favorable_class, max_models=2)\n    assert da.nrows == 2 * len(x) + len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()\n    da = ig.train_subset_models(H2OGridSearch, y, train, test, protected_columns, reference, favorable_class, model=H2OGradientBoostingEstimator(), hyper_params=dict(ntrees=[1, 3, 5]))\n    assert da.nrows == 3 * len(x)\n    assert ((da[:, 'cair'] > 0.8) & (da[:, 'cair'] < 1.25)).any()"
        ]
    },
    {
        "func_name": "test_train_subset_models_metrics",
        "original": "def test_train_subset_models_metrics():\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)",
        "mutated": [
            "def test_train_subset_models_metrics():\n    if False:\n        i = 10\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)",
            "def test_train_subset_models_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)",
            "def test_train_subset_models_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)",
            "def test_train_subset_models_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)",
            "def test_train_subset_models_metrics():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data = h2o.import_file(pyunit_utils.locate('smalldata/admissibleml_test/taiwan_credit_card_uci.csv'))\n    x = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    y = 'default payment next month'\n    protected_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n    for c in [y] + protected_columns:\n        data[c] = data[c].asfactor()\n    (train, test) = data.split_frame([0.8])\n    reference = ['1', '2', '2']\n    favorable_class = '0'\n    ig_fair = H2OInfogram(protected_columns=protected_columns)\n    ig_fair.train(x, y, training_frame=train)\n    ig_core = H2OInfogram()\n    ig_core.train(x, y, training_frame=train)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class)\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_fair.train_subset_models(H2OGradientBoostingEstimator, y, train, test, protected_columns, reference, favorable_class, feature_selection_metrics=['safety_index', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test)\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'])\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='manhattan')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, metric='maximum')\n    assert da.nrows == len(x)\n    da = ig_core.train_subset_models(H2OGradientBoostingEstimator, y, train, test, feature_selection_metrics=['total_information', 'admissible_index'], metric='maximum')\n    assert da.nrows == len(x)"
        ]
    }
]