[
    {
        "func_name": "__init__",
        "original": "def __init__(self, *args, **kwargs):\n    super(PPOMod, self).__init__(*args, **kwargs)",
        "mutated": [
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n    super(PPOMod, self).__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(PPOMod, self).__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(PPOMod, self).__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(PPOMod, self).__init__(*args, **kwargs)",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(PPOMod, self).__init__(*args, **kwargs)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'",
        "mutated": [
            "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    if False:\n        i = 10\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'",
            "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'",
            "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'",
            "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'",
            "def __init__(self, seed_value=None, render_sleep_time=0.001):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTesting, self).__init__()\n    self.seed_value = seed_value\n    self.load_prefix = 'history_'\n    self.deterministic = True\n    self.warn = True\n    self.render = None\n    self.crosstest_flag = None\n    self.render_sleep_time = render_sleep_time\n    self._env_name = 'Testing'"
        ]
    },
    {
        "func_name": "_import_original_configs",
        "original": "def _import_original_configs(self):\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config",
        "mutated": [
            "def _import_original_configs(self):\n    if False:\n        i = 10\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config",
            "def _import_original_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config",
            "def _import_original_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config",
            "def _import_original_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config",
            "def _import_original_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_config_file_path = os.path.join(testing_config['path'], 'experiment_config.json')\n        if os.path.isfile(agent_config_file_path):\n            self.clilog.info(f'Parse from json file in {agent_config_file_path}')\n            (_experiment_configs, _agents_configs, _evaluation_configs, _testing_configs, _merged_config) = ExperimentParser.load(agent_config_file_path)\n            agent_original_config = _agents_configs[k]\n            self.agents_configs[k] = agent_original_config"
        ]
    },
    {
        "func_name": "_init_testing",
        "original": "def _init_testing(self, experiment_filename, logdir, wandb):\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)",
        "mutated": [
            "def _init_testing(self, experiment_filename, logdir, wandb):\n    if False:\n        i = 10\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)",
            "def _init_testing(self, experiment_filename, logdir, wandb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)",
            "def _init_testing(self, experiment_filename, logdir, wandb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)",
            "def _init_testing(self, experiment_filename, logdir, wandb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)",
            "def _init_testing(self, experiment_filename, logdir, wandb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTesting, self)._init_exp(experiment_filename, logdir, wandb)\n    self._import_original_configs()\n    self.render = self.testing_configs.get('render', True)\n    self.crosstest_flag = self.testing_configs.get('crosstest', False)\n    self.best_flag = self.testing_configs.get('best', False)\n    self.clilog.info(f'----- Load testing conditions')\n    self._load_testing_conditions(experiment_filename)"
        ]
    },
    {
        "func_name": "_init_argparse",
        "original": "def _init_argparse(self):\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
        "mutated": [
            "def _init_argparse(self):\n    if False:\n        i = 10\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')",
            "def _init_argparse(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTesting, self)._init_argparse(description='Self-play experiment testing script', help='The experiemnt configuration file path and name which the experiment should be loaded')"
        ]
    },
    {
        "func_name": "_generate_log_dir",
        "original": "def _generate_log_dir(self):\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')",
        "mutated": [
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')",
            "def _generate_log_dir(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(SelfPlayTesting, self)._generate_log_dir(dir_postfix='test')"
        ]
    },
    {
        "func_name": "_load_testing_conditions",
        "original": "def _load_testing_conditions(self, path):\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])",
        "mutated": [
            "def _load_testing_conditions(self, path):\n    if False:\n        i = 10\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])",
            "def _load_testing_conditions(self, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])",
            "def _load_testing_conditions(self, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])",
            "def _load_testing_conditions(self, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])",
            "def _load_testing_conditions(self, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.testing_conditions = {}\n    self.testing_modes = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        agent_testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        agent_testing_path = os.path.join(agent_testing_path, self.testing_configs[agent_name]['dirname'])\n        mode = testing_config['mode']\n        self.testing_conditions[agent_name] = {'path': agent_testing_path}\n        self.testing_modes[agent_name] = mode\n        num_rounds = self.experiment_configs['num_rounds']\n        if mode == 'limit':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'limit_s':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], num_rounds - 1, testing_config['freq']]\n        elif mode == 'limit_e':\n            self.testing_conditions[agent_name]['limits'] = [0, testing_config['gens'], testing_config['freq']]\n        elif mode == 'gen':\n            self.testing_conditions[agent_name]['limits'] = [testing_config['gens'], testing_config['gens'], testing_config['freq']]\n        elif mode == 'all':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        elif mode == 'random':\n            self.testing_conditions[agent_name]['limits'] = [None, None, testing_config['freq']]\n        elif mode == 'round':\n            self.testing_conditions[agent_name]['limits'] = [0, num_rounds - 1, testing_config['freq']]\n        self.clilog.debug(self.testing_conditions[agent_name]['limits'])"
        ]
    },
    {
        "func_name": "_get_opponent_algorithm_class",
        "original": "def _get_opponent_algorithm_class(self, agent_configs):\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class",
        "mutated": [
            "def _get_opponent_algorithm_class(self, agent_configs):\n    if False:\n        i = 10\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class",
            "def _get_opponent_algorithm_class(self, agent_configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class",
            "def _get_opponent_algorithm_class(self, agent_configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class",
            "def _get_opponent_algorithm_class(self, agent_configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class",
            "def _get_opponent_algorithm_class(self, agent_configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    algorithm_class = None\n    opponent_algorithm_class_cfg = agent_configs.get('opponent_rl_algorithm', agent_configs['rl_algorithm'])\n    if opponent_algorithm_class_cfg == 'PPO':\n        algorithm_class = PPOMod\n    elif opponent_algorithm_class_cfg == 'SAC':\n        algorithm_class = SAC\n    return algorithm_class"
        ]
    },
    {
        "func_name": "_init_envs",
        "original": "def _init_envs(self):\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env",
        "mutated": [
            "def _init_envs(self):\n    if False:\n        i = 10\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env",
            "def _init_envs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.envs = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        env = super(SelfPlayTesting, self).create_env(key=k, name=self._env_name, opponent_archive=None, algorithm_class=algorithm_class, gui=True)\n        self.envs[agent_name] = env"
        ]
    },
    {
        "func_name": "_init_archives",
        "original": "def _init_archives(self):\n    raise NotImplementedError('_init_archives() not implemented')",
        "mutated": [
            "def _init_archives(self):\n    if False:\n        i = 10\n    raise NotImplementedError('_init_archives() not implemented')",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    raise NotImplementedError('_init_archives() not implemented')",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    raise NotImplementedError('_init_archives() not implemented')",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    raise NotImplementedError('_init_archives() not implemented')",
            "def _init_archives(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    raise NotImplementedError('_init_archives() not implemented')"
        ]
    },
    {
        "func_name": "_init_models",
        "original": "def _init_models(self):\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class",
        "mutated": [
            "def _init_models(self):\n    if False:\n        i = 10\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class",
            "def _init_models(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.models = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.models[agent_name] = algorithm_class"
        ]
    },
    {
        "func_name": "render_callback",
        "original": "def render_callback(self, ret):\n    return ret",
        "mutated": [
            "def render_callback(self, ret):\n    if False:\n        i = 10\n    return ret",
            "def render_callback(self, ret):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return ret",
            "def render_callback(self, ret):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return ret",
            "def render_callback(self, ret):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return ret",
            "def render_callback(self, ret):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return ret"
        ]
    },
    {
        "func_name": "_run_one_evaluation",
        "original": "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)",
        "mutated": [
            "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    if False:\n        i = 10\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)",
            "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)",
            "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)",
            "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)",
            "def _run_one_evaluation(self, agent_conifgs_key, sampled_agent, sampled_opponents, n_eval_episodes, render_extra_info, env=None, render=None, agent_model=None, seed_value=None, return_episode_rewards=False, eval_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.clilog.info('----------------------------------------')\n    self.clilog.info(render_extra_info)\n    self.make_deterministic(cuda_check=False)\n    if env is None and agent_model is None:\n        agent_configs = self.agents_configs[agent_conifgs_key]\n        opponent_algorithm_class = self._get_opponent_algorithm_class(agent_configs)\n        (env, seed_value) = super(SelfPlayTesting, self).create_env(key=agent_conifgs_key, name='Testing', opponent_archive=None, algorithm_class=opponent_algorithm_class, seed_value=seed_value, ret_seed=True, gui=True)\n        algorithm_class = None\n        if agent_configs['rl_algorithm'] == 'PPO':\n            algorithm_class = PPOMod\n        elif agent_configs['rl_algorithm'] == 'SAC':\n            algorithm_class = SAC\n        self.clilog.debug(f'loading agent model: {sampled_agent}, {algorithm_class}, {env}')\n        agent_model = algorithm_class.load(sampled_agent, env)\n    (mean_reward, std_reward, win_rate, std_win_rate, render_ret) = evaluate_policy_simple(agent_model, env, n_eval_episodes=n_eval_episodes, render=self.render if render is None else render, deterministic=self.deterministic, return_episode_rewards=return_episode_rewards, warn=self.warn, callback=None, sampled_opponents=sampled_opponents, render_extra_info=render_extra_info, render_callback=self.render_callback, sleep_time=self.render_sleep_time, seed_value=eval_seed, trajectory_heatmap=self.testing_configs.get('trajectory_heatmap', False))\n    (mean_reward_, std_reward_, win_rate_, std_win_rate_) = (mean_reward, std_reward, win_rate, std_win_rate)\n    if return_episode_rewards:\n        mean_reward_ = np.mean(mean_reward)\n        std_reward_ = np.std(mean_reward)\n        win_rate_ = np.mean(win_rate)\n    self.clilog.info(f'{render_extra_info} -> win rate: {100 * win_rate_:.2f}% +/- {std_win_rate_:.2f}\\trewards: {mean_reward_:.2f} +/- {std_reward_:.2f}')\n    env.close()\n    return (mean_reward, std_reward, win_rate, std_win_rate, render_ret)"
        ]
    },
    {
        "func_name": "_test_round_by_round",
        "original": "def _test_round_by_round(self, key, n_eval_episodes):\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')",
        "mutated": [
            "def _test_round_by_round(self, key, n_eval_episodes):\n    if False:\n        i = 10\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')",
            "def _test_round_by_round(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')",
            "def _test_round_by_round(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')",
            "def _test_round_by_round(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')",
            "def _test_round_by_round(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for round_num in range(0, self.experiment_configs['num_rounds'], self.testing_conditions[agent_name]['limits'][2]):\n        startswith_keyword = f'{self.load_prefix}{round_num}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=startswith_keyword)\n        if len(opponent_latest) == 0:\n            continue\n        sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n        sampled_opponents = [sampled_opponent]\n        self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{round_num} vs {round_num}')"
        ]
    },
    {
        "func_name": "_test_different_rounds",
        "original": "def _test_different_rounds(self, key, n_eval_episodes):\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')",
        "mutated": [
            "def _test_different_rounds(self, key, n_eval_episodes):\n    if False:\n        i = 10\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')",
            "def _test_different_rounds(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')",
            "def _test_different_rounds(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')",
            "def _test_different_rounds(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')",
            "def _test_different_rounds(self, key, n_eval_episodes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    agent_configs = self.agents_configs[key]\n    agent_name = agent_configs['name']\n    opponent_name = agent_configs['opponent_name']\n    for i in range(self.testing_conditions[agent_name]['limits'][0], self.testing_conditions[agent_name]['limits'][1] + 1, self.testing_conditions[agent_name]['limits'][2]):\n        agent_startswith_keyword = f'{self.load_prefix}{i}_'\n        agent_latest = utos.get_latest(self.testing_conditions[agent_name]['path'], startswith=agent_startswith_keyword)\n        if len(agent_latest) == 0:\n            continue\n        sampled_agent = os.path.join(self.testing_conditions[agent_name]['path'], agent_latest[0])\n        for j in range(self.testing_conditions[opponent_name]['limits'][0], self.testing_conditions[opponent_name]['limits'][1] + 1, self.testing_conditions[opponent_name]['limits'][2]):\n            opponent_startswith_keyword = f'{self.load_prefix}{j}_'\n            opponent_latest = utos.get_latest(self.testing_conditions[opponent_name]['path'], startswith=opponent_startswith_keyword)\n            if len(opponent_latest) == 0:\n                continue\n            sampled_opponent = os.path.join(self.testing_conditions[opponent_name]['path'], opponent_latest[0])\n            sampled_opponents = [sampled_opponent]\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{i} vs {j}')"
        ]
    },
    {
        "func_name": "get_latest_agent_path",
        "original": "def get_latest_agent_path(self, idx, path, population_idx):\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)",
        "mutated": [
            "def get_latest_agent_path(self, idx, path, population_idx):\n    if False:\n        i = 10\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)",
            "def get_latest_agent_path(self, idx, path, population_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)",
            "def get_latest_agent_path(self, idx, path, population_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)",
            "def get_latest_agent_path(self, idx, path, population_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)",
            "def get_latest_agent_path(self, idx, path, population_idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    agent_startswith_keyword = f'{self.load_prefix}{idx}_'\n    agent_latest = utos.get_latest(path, startswith=agent_startswith_keyword, population_idx=population_idx)\n    ret = True\n    if len(agent_latest) == 0:\n        ret = False\n    latest_agent = os.path.join(path, agent_latest[0])\n    return (ret, latest_agent)"
        ]
    },
    {
        "func_name": "_compute_performance",
        "original": "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length",
        "mutated": [
            "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    if False:\n        i = 10\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length",
            "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length",
            "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length",
            "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length",
            "def _compute_performance(self, agent, opponent, key, n_eval_episodes=1, n_seeds=1, negative_score_flag=False, render=False, render_extra_info=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    lengths = []\n    for i in range(n_seeds):\n        random_seed = None if n_seeds == 1 else datetime.now().microsecond // 1000\n        (episodes_reward, episodes_length, win_rates, std_win_rate, render_ret) = self._run_one_evaluation(key, agent, [opponent], n_eval_episodes, render=render, render_extra_info=f'{agent} vs {opponent}' if render_extra_info is None else render_extra_info, return_episode_rewards=True, eval_seed=random_seed)\n        length = np.mean(episodes_length)\n        lengths.append(length)\n    length = np.mean(lengths)\n    limits = [0, 1000]\n    normalized_length = length\n    if negative_score_flag:\n        normalized_length = MAX_NUM_STEPS - length\n    normalized_length /= MAX_NUM_STEPS\n    self.clilog.debug(f'Nomralized: {normalized_length}, {length}')\n    return normalized_length"
        ]
    },
    {
        "func_name": "crosstest",
        "original": "def crosstest(self, n_eval_episodes, n_seeds):\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)",
        "mutated": [
            "def crosstest(self, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)",
            "def crosstest(self, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)",
            "def crosstest(self, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)",
            "def crosstest(self, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)",
            "def crosstest(self, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.deterministic = False\n    self._env_name = 'Evaluation'\n    self.clilog.info(f'---------------- Running Crosstest ----------------')\n    methods_experiments_parent_path = self.testing_configs.get('crosstest_methods_parent_path')\n    methods_experiments_path = self.testing_configs.get('crosstest_methods_path')\n    methods_experiments_best_agents = {}\n    filtered_methods_experiments_path = {}\n    for m in list(methods_experiments_path.keys()):\n        self.clilog.info(f'=============== Method: {m} ===============')\n        method_parent_path = methods_experiments_parent_path[m]\n        self.clilog.info(f'Parent path: {method_parent_path}')\n        methods_experiments_best_agents[m] = {}\n        filtered_methods_experiments_path[m] = []\n        if len(methods_experiments_path[m]) == 0:\n            self.clilog.warn(f'For method {m}, the script will automatically get all the experiments inside this method parent directory {method_parent_path}')\n            methods_experiments_path[m] = next(os.walk(method_parent_path))[1]\n        for exp in methods_experiments_path[m]:\n            self.clilog.info(f'Exp Path: {exp}')\n            experiment_path = os.path.join(method_parent_path, exp, 'experiment_config.json')\n            _evaluation_configs = None\n            try:\n                (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n            except Exception as e:\n                self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n                self.clilog.error(e)\n                continue\n            self.clilog.debug(_evaluation_configs)\n            filtered_methods_experiments_path[m].append(exp)\n            methods_experiments_best_agents[m][exp] = {}\n            for k in self.agents_configs.keys():\n                agent_name = self.agents_configs[k]['name']\n                methods_experiments_best_agents[m][exp][agent_name] = os.path.join(method_parent_path, exp, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n                self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {methods_experiments_best_agents[m][exp][agent_name]}\")\n        self.clilog.info(f'========================================================================================')\n    method_names = list(filtered_methods_experiments_path.keys())\n    agents_names = [self.agents_configs[k]['name'] for k in self.agents_configs.keys()]\n    best_agents_method = {}\n    for m in method_names:\n        self.clilog.info(f'=============== Method: {m} ===============')\n        best_agents_method[m] = {i: methods_experiments_best_agents[m][filtered_methods_experiments_path[m][0]][i] for i in agents_names}\n        for i in range(len(filtered_methods_experiments_path[m]) - 1):\n            break\n            self.clilog.info(f'Exp {i} vs {i + 1}')\n            other_agents_method = methods_experiments_best_agents[m][filtered_methods_experiments_path[m][i + 1]]\n            (best_agents_idx, _, _) = self._crosstest(best_agents_method[m], other_agents_method, agents_names, n_eval_episodes, n_seeds, None, None, False)\n            for agent_name in agents_names:\n                eps = 0.5\n                if best_agents_idx[agent_name] == 2 or (best_agents_idx[agent_name] == 0 and np.random.rand() > eps):\n                    best_agents_method[m][agent_name] = other_agents_method[agent_name]\n        self.clilog.info(f'========================================================================================')\n        self.clilog.info(f'Best agents for method {m}: {best_agents_method[m]}')\n        self.clilog.info(f'========================================================================================')\n    render = self.testing_configs.get('render')\n    num_methods = len(method_names)\n    crosstest_mat = np.zeros((num_methods, num_methods))\n    crosstest_mat2 = np.zeros((num_methods, num_methods))\n    for i in range(num_methods):\n        method1 = best_agents_method[method_names[i]]\n        for j in range(i, num_methods):\n            method2 = best_agents_method[method_names[j]]\n            (best_agents_idx, best_method_idx, scores) = self._crosstest(method1, method2, agents_names, n_eval_episodes, n_seeds, method_names[i], method_names[j], render)\n            gain_score = scores[2][2]\n            crosstest_mat[i, j] = gain_score\n            crosstest_mat2[i, j] = scores[2][3]\n    self.clilog.critical(method_names)\n    self.clilog.critical(f'Mat1: (g1+g2)\\n{crosstest_mat}')\n    self.clilog.critical(f'Mat2: (g1-g2)\\n{crosstest_mat2}')\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}1.npy\", crosstest_mat)\n    np.save(f\"{self.testing_configs.get('crosstest_save_name', 'crosstest_res')}2.npy\", crosstest_mat2)"
        ]
    },
    {
        "func_name": "_crosstest",
        "original": "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])",
        "mutated": [
            "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    if False:\n        i = 10\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])",
            "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])",
            "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])",
            "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])",
            "def _crosstest(self, method1_agents, method2_agents, agent_names, n_eval_episodes, n_seeds, approach1_path=None, approach2_path=None, render=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    best_agents_idx = []\n    (agent_name, opponent_name) = agent_names\n    (best_agent1, best_opponent1) = [method1_agents[a] for a in agent_names]\n    (best_agent2, best_opponent2) = [method2_agents[a] for a in agent_names]\n    render = self.testing_configs.get('render')\n    self.clilog.info(f'################# Agent1 vs Opponent2 #################')\n    perf_agent1_opponent2 = self._compute_performance(best_agent1, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent1 vs Opponent1 #################')\n    perf_agent1_opponent1 = self._compute_performance(best_agent1, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent2 #################')\n    perf_agent2_opponent2 = self._compute_performance(best_agent2, best_opponent2, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Agent2 vs Opponent1 #################')\n    perf_agent2_opponent1 = self._compute_performance(best_agent2, best_opponent1, agent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=True, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent2 #################')\n    perf_opponent1_agent2 = self._compute_performance(best_opponent1, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent1 vs Agent1 #################')\n    perf_opponent1_agent1 = self._compute_performance(best_opponent1, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent2 #################')\n    perf_opponent2_agent2 = self._compute_performance(best_opponent2, best_agent2, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    self.clilog.info(f'################# Opponent2 vs Agent1 #################')\n    perf_opponent2_agent1 = self._compute_performance(best_opponent2, best_agent1, opponent_name, n_eval_episodes=n_eval_episodes, n_seeds=n_seeds, negative_score_flag=False, render=render)\n    perf_agent1 = perf_agent1_opponent2 - perf_agent1_opponent1\n    perf_agent2 = perf_agent2_opponent2 - perf_agent2_opponent1\n    perf_opponent1 = perf_opponent1_agent2 - perf_opponent1_agent1\n    perf_opponent2 = perf_opponent2_agent2 - perf_opponent2_agent1\n    gain1 = perf_agent1 + perf_opponent1\n    gain2 = perf_agent2 + perf_opponent2\n    self.clilog.info('-----------------------------------------------------------------')\n    self.clilog.info(f'perf_agent1: {perf_agent1}\\tperf_opponent1: {perf_opponent1}\\tgain1: {gain1}')\n    self.clilog.info(f'perf_agent2: {perf_agent2}\\tperf_opponent2: {perf_opponent2}\\tgain2: {gain2}')\n    self.clilog.info(f'perf_agent: {perf_agent1 + perf_agent2}\\tperf_opponent: {perf_opponent1 + perf_opponent2}\\tgain(sum): {gain1 + gain2}')\n    gain = [gain1, gain2, gain1 + gain2, gain1 - gain2]\n    perf_agent = [perf_agent1, perf_agent2, perf_agent1 + perf_agent2]\n    perf_opponent = [perf_opponent1, perf_opponent2, perf_opponent1 + perf_opponent2]\n    best_agents_idx = {}\n    best_method_idx = None\n    i = 2\n    self.clilog.critical(f'{approach1_path} vs {approach2_path}')\n    self.clilog.critical(f' ----- Part {i + 1} ----- ')\n    eps = 0.001\n    if perf_agent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate preys (path: {approach1_path})')\n        best_agents_idx[agent_name] = 1\n    elif -eps <= perf_agent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate preys ({approach1_path}, {approach2_path})')\n        best_agents_idx[agent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate preys (path: {approach2_path})')\n        best_agents_idx[agent_name] = 2\n    if perf_opponent[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} to generate predators (path: {approach1_path})')\n        best_agents_idx[opponent_name] = 1\n    elif -eps <= perf_opponent[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other to generate predators ({approach1_path}, {approach2_path})')\n        best_agents_idx[opponent_name] = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} to generate predators (path: {approach2_path})')\n        best_agents_idx[opponent_name] = 2\n    if gain[i] > 0:\n        self.clilog.critical(f'Configuration 1 is better {1} (path: {approach1_path})')\n        best_method_idx = 1\n    elif -eps <= gain[i] <= eps:\n        self.clilog.critical(f'Configuration 1 & 2 are close to each other ({approach1_path}, {approach2_path})')\n        best_method_idx = 0\n    else:\n        self.clilog.critical(f'Configuration 2 is better {2} (path: {approach2_path})')\n        best_method_idx = 2\n    return (best_agents_idx, best_method_idx, [perf_agent, perf_opponent, gain])"
        ]
    },
    {
        "func_name": "best_testing",
        "original": "def best_testing(self, path, n_eval_episodes, n_seeds):\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')",
        "mutated": [
            "def best_testing(self, path, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')",
            "def best_testing(self, path, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')",
            "def best_testing(self, path, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')",
            "def best_testing(self, path, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')",
            "def best_testing(self, path, n_eval_episodes, n_seeds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    agents = []\n    agent_names = {}\n    for k in self.agents_configs.keys():\n        agent_configs = self.agents_configs[k]\n        agent_name = agent_configs['name']\n        testing_config = self.testing_configs[agent_name]\n        testing_path = os.path.join(path, agent_name) if testing_config['path'] is None else testing_config['path']\n        experiment_path = os.path.join(testing_path, 'experiment_config.json')\n        _evaluation_configs = None\n        try:\n            (_, _, _evaluation_configs, _, _) = ExperimentParser.load(experiment_path)\n        except Exception as e:\n            self.clilog.error('This experiment is not finished and does not have experiment configuration as a result of finish the experiment')\n            self.clilog.error(e)\n            continue\n        best_agent_name = os.path.join(testing_path, agent_name, _evaluation_configs[agent_name]['best_agent_name'])\n        self.clilog.info(f\"Best {agent_name}: {_evaluation_configs[agent_name]['best_agent_name']}\\nPath: {best_agent_name}\")\n        agents.append(best_agent_name)\n        agent_names[best_agent_name] = agent_name\n    for a in agents:\n        for b in set(agents) - {a}:\n            sampled_opponents = [b]\n            sampled_agent = a\n            key = agent_names[a]\n            self.clilog.critical('------------------------------------------------')\n            self.clilog.info(f'Env: {key}')\n            self.clilog.info(f'{a} vs {b}')\n            self._run_one_evaluation(key, sampled_agent, sampled_opponents, n_eval_episodes, f'{a} vs {b}')\n            self.clilog.critical('------------------------------------------------')"
        ]
    },
    {
        "func_name": "test",
        "original": "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)",
        "mutated": [
            "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    if False:\n        i = 10\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)",
            "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)",
            "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)",
            "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)",
            "def test(self, experiment_filename=None, logdir=False, wandb=False, n_eval_episodes=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._init_testing(experiment_filename=experiment_filename, logdir=logdir, wandb=wandb)\n    self.render_sleep_time = self.render_sleep_time if self.args.rendersleep <= 0 else self.args.rendersleep\n    n_eval_episodes_configs = self.testing_configs.get('repetition', None)\n    n_eval_episodes = n_eval_episodes_configs if n_eval_episodes_configs is not None else n_eval_episodes\n    if self.crosstest_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.crosstest(n_eval_episodes, n_seeds)\n    elif self.best_flag:\n        n_seeds = self.testing_configs.get('n_seeds', 1)\n        self.best_testing(experiment_filename, n_eval_episodes, n_seeds)\n    else:\n        already_evaluated_agents = []\n        self.clilog.debug(self.testing_modes)\n        keys = self.agents_configs.keys()\n        keys = ['pred', 'prey']\n        for k in keys:\n            agent_configs = self.agents_configs[k]\n            agent_name = agent_configs['name']\n            agent_opponent_joint = sorted([agent_name, agent_configs['opponent_name']])\n            if 'round' in self.testing_modes.values():\n                if agent_opponent_joint in already_evaluated_agents:\n                    continue\n                self._test_round_by_round(k, n_eval_episodes)\n            else:\n                self._test_different_rounds(k, n_eval_episodes)\n            already_evaluated_agents.append(agent_opponent_joint)"
        ]
    }
]