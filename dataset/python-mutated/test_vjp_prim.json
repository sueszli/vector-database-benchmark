[
    {
        "func_name": "get_ir_divide_program",
        "original": "def get_ir_divide_program():\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
        "mutated": [
            "def get_ir_divide_program():\n    if False:\n        i = 10\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_divide_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_divide_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_divide_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_divide_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        y = paddle.tensor.fill_constant(shape=[4], dtype='float32', value=1.0)\n        y.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[1, 4], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.divide(x, y)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program"
        ]
    },
    {
        "func_name": "get_ir_sum_program",
        "original": "def get_ir_sum_program():\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
        "mutated": [
            "def get_ir_sum_program():\n    if False:\n        i = 10\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_sum_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_sum_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_sum_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program",
            "def get_ir_sum_program():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.enable_static()\n    (main_program, start_program) = (paddle.static.Program(), paddle.static.Program())\n    with paddle.static.program_guard(main_program, start_program):\n        x = paddle.tensor.fill_constant(shape=[4, 5], dtype='float32', value=2.0)\n        x.stop_gradient = False\n        dout = paddle.tensor.fill_constant(shape=[], dtype='float32', value=1.0)\n        dout.stop_gradient = False\n        out = paddle.sum(x)\n    pir_program = pir.translate_to_pir(main_program.desc)\n    return pir_program"
        ]
    },
    {
        "func_name": "test_divide_grad_prim_case1",
        "original": "def test_divide_grad_prim_case1(self):\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
        "mutated": [
            "def test_divide_grad_prim_case1(self):\n    if False:\n        i = 10\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_divide_grad_prim_case1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_divide_grad_prim_case1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_divide_grad_prim_case1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_divide_grad_prim_case1(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-2].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False], [False]]\n        divide_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n        reshape_op2 = pir_program.global_block().ops[-1]\n        reshape_op1 = pir_program.global_block().ops[-8]\n        self.assertEqual(len(grad_outs), 2)\n        self.assertEqual(len(pir_program.global_block().ops), 21)\n        self.assertEqual(reshape_op2.result(0), grad_outs[0][0])\n        self.assertEqual(reshape_op1.result(0), grad_outs[1][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full', 'pd_op.divide', 'pd_op.full', 'pd_op.elementwise_pow', 'pd_op.divide', 'pd_op.full', 'pd_op.scale', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full', 'pd_op.divide', 'pd_op.multiply', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)"
        ]
    },
    {
        "func_name": "test_divide_grad_no_prim",
        "original": "def test_divide_grad_no_prim(self):\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
        "mutated": [
            "def test_divide_grad_no_prim(self):\n    if False:\n        i = 10\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_divide_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_divide_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_divide_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_divide_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pir_program = get_ir_divide_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False], [False]]\n    divide_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(divide_op, [[value] for value in divide_op.operands_source()], [[value] for value in divide_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 2)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(grad_outs[1][0].get_defining_op().name(), 'pd_op.divide_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)"
        ]
    },
    {
        "func_name": "test_sum_grad_prim",
        "original": "def test_sum_grad_prim(self):\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
        "mutated": [
            "def test_sum_grad_prim(self):\n    if False:\n        i = 10\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_sum_grad_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_sum_grad_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_sum_grad_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)",
            "def test_sum_grad_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(True)\n    with paddle.pir_utils.IrGuard():\n        dout = pir_program.global_block().ops[-3].result(0)\n        out_grads = [[dout]]\n        stop_gradients = [[False]]\n        sum_op = pir_program.global_block().ops[-1]\n        with paddle.pir.core.program_guard(pir_program):\n            grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n        expand_op = pir_program.global_block().ops[-1]\n        self.assertEqual(len(grad_outs), 1)\n        self.assertEqual(len(pir_program.global_block().ops), 8)\n        self.assertEqual(expand_op.result(0), grad_outs[0][0])\n        all_op_names = ['pd_op.full', 'pd_op.full', 'pd_op.full_int_array', 'pd_op.sum', 'pd_op.full_int_array', 'pd_op.reshape', 'pd_op.full_int_array', 'pd_op.expand']\n        for (idx, op) in enumerate(pir_program.global_block().ops):\n            self.assertEqual(op.name(), all_op_names[idx])\n        paddle.framework.core._set_prim_backward_enabled(False)"
        ]
    },
    {
        "func_name": "test_sum_grad_no_prim",
        "original": "def test_sum_grad_no_prim(self):\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
        "mutated": [
            "def test_sum_grad_no_prim(self):\n    if False:\n        i = 10\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_sum_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_sum_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_sum_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)",
            "def test_sum_grad_no_prim(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pir_program = get_ir_sum_program()\n    paddle.framework.core._set_prim_backward_enabled(False)\n    dout = pir_program.global_block().ops[-2].result(0)\n    out_grads = [[dout]]\n    stop_gradients = [[False]]\n    sum_op = pir_program.global_block().ops[-1]\n    with paddle.pir.core.program_guard(pir_program):\n        grad_outs = call_vjp(sum_op, [[value] for value in sum_op.operands_source()], [[value] for value in sum_op.results()], out_grads, stop_gradients)\n    self.assertEqual(len(grad_outs), 1)\n    self.assertEqual(grad_outs[0][0].get_defining_op().name(), 'pd_op.sum_grad')\n    self.assertEqual(len(pir_program.global_block().ops), 5)"
        ]
    }
]