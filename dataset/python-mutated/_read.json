[
    {
        "func_name": "_read_parquet_iterator",
        "original": "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)",
        "mutated": [
            "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    if False:\n        i = 10\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)",
            "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)",
            "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)",
            "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)",
            "def _read_parquet_iterator(path: str, keep_files: bool, use_threads: Union[bool, int], chunked: Union[bool, int], dtype_backend: Literal['numpy_nullable', 'pyarrow'], boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, str]], pyarrow_additional_kwargs: Optional[Dict[str, Any]]) -> Iterator[pd.DataFrame]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dfs: Iterator[pd.DataFrame] = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n    yield from dfs\n    if keep_files is False:\n        s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)"
        ]
    },
    {
        "func_name": "read_sql_query",
        "original": "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Return a DataFrame corresponding to the result set of the query string.\n\n    Note\n    ----\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\n\n    Parameters\n    ----------\n    sql : str\n        SQL query.\n    con : redshift_connector.Connection\n        Use redshift_connector.connect() to use \"\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\n    index_col : Union[str, List[str]], optional\n        Column(s) to set as index(MultiIndex).\n    params :  Union[List, Tuple, Dict], optional\n        List of parameters to pass to execute method.\n        The syntax used to pass parameters is database driver dependent.\n        Check your database driver documentation for which of the five syntax styles,\n        described in PEP 249\u2019s paramstyle, is supported.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    chunksize : int, optional\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\n    dtype : Dict[str, pyarrow.DataType], optional\n        Specifying the datatype for columns.\n        The keys should be the column names and the values should be the PyArrow types.\n    safe : bool\n        Check for overflows or other unsafe data type conversions.\n    timestamp_as_object : bool\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\n        Result as Pandas DataFrame(s).\n\n    Examples\n    --------\n    Reading from Redshift using a Glue Catalog Connections\n\n    >>> import awswrangler as wr\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\n    >>> df = wr.redshift.read_sql_query(\n    ...     sql=\"SELECT * FROM public.my_table\",\n    ...     con=con\n    ... )\n    >>> con.close()\n\n    \"\"\"\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
        "mutated": [
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Return a DataFrame corresponding to the result set of the query string.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\u2019s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_query(\\n    ...     sql=\"SELECT * FROM public.my_table\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return a DataFrame corresponding to the result set of the query string.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\u2019s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_query(\\n    ...     sql=\"SELECT * FROM public.my_table\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return a DataFrame corresponding to the result set of the query string.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\u2019s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_query(\\n    ...     sql=\"SELECT * FROM public.my_table\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return a DataFrame corresponding to the result set of the query string.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\u2019s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_query(\\n    ...     sql=\"SELECT * FROM public.my_table\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_query(sql: str, con: 'redshift_connector.Connection', index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return a DataFrame corresponding to the result set of the query string.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\u2019s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_query(\\n    ...     sql=\"SELECT * FROM public.my_table\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    _validate_connection(con=con)\n    return _db_utils.read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)"
        ]
    },
    {
        "func_name": "read_sql_table",
        "original": "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Return a DataFrame corresponding the table.\n\n    Note\n    ----\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\n\n    Parameters\n    ----------\n    table : str\n        Table name.\n    con : redshift_connector.Connection\n        Use redshift_connector.connect() to use \"\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\n    schema : str, optional\n        Name of SQL schema in database to query (if database flavor supports this).\n        Uses default schema if None (default).\n    index_col : Union[str, List[str]], optional\n        Column(s) to set as index(MultiIndex).\n    params :  Union[List, Tuple, Dict], optional\n        List of parameters to pass to execute method.\n        The syntax used to pass parameters is database driver dependent.\n        Check your database driver documentation for which of the five syntax styles,\n        described in PEP 249's paramstyle, is supported.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    chunksize : int, optional\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\n    dtype : Dict[str, pyarrow.DataType], optional\n        Specifying the datatype for columns.\n        The keys should be the column names and the values should be the PyArrow types.\n    safe : bool\n        Check for overflows or other unsafe data type conversions.\n    timestamp_as_object : bool\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\n        Result as Pandas DataFrame(s).\n\n    Examples\n    --------\n    Reading from Redshift using a Glue Catalog Connections\n\n    >>> import awswrangler as wr\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\n    >>> df = wr.redshift.read_sql_table(\n    ...     table=\"my_table\",\n    ...     schema=\"public\",\n    ...     con=con\n    ... )\n    >>> con.close()\n\n    \"\"\"\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
        "mutated": [
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Return a DataFrame corresponding the table.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    table : str\\n        Table name.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    schema : str, optional\\n        Name of SQL schema in database to query (if database flavor supports this).\\n        Uses default schema if None (default).\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_table(\\n    ...     table=\"my_table\",\\n    ...     schema=\"public\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return a DataFrame corresponding the table.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    table : str\\n        Table name.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    schema : str, optional\\n        Name of SQL schema in database to query (if database flavor supports this).\\n        Uses default schema if None (default).\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_table(\\n    ...     table=\"my_table\",\\n    ...     schema=\"public\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return a DataFrame corresponding the table.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    table : str\\n        Table name.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    schema : str, optional\\n        Name of SQL schema in database to query (if database flavor supports this).\\n        Uses default schema if None (default).\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_table(\\n    ...     table=\"my_table\",\\n    ...     schema=\"public\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return a DataFrame corresponding the table.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    table : str\\n        Table name.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    schema : str, optional\\n        Name of SQL schema in database to query (if database flavor supports this).\\n        Uses default schema if None (default).\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_table(\\n    ...     table=\"my_table\",\\n    ...     schema=\"public\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)",
            "@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef read_sql_table(table: str, con: 'redshift_connector.Connection', schema: Optional[str]=None, index_col: Optional[Union[str, List[str]]]=None, params: Optional[Union[List[Any], Tuple[Any, ...], Dict[Any, Any]]]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunksize: Optional[int]=None, dtype: Optional[Dict[str, pa.DataType]]=None, safe: bool=True, timestamp_as_object: bool=False) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return a DataFrame corresponding the table.\\n\\n    Note\\n    ----\\n    For large extractions (1K+ rows) consider the function **wr.redshift.unload()**.\\n\\n    Parameters\\n    ----------\\n    table : str\\n        Table name.\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    schema : str, optional\\n        Name of SQL schema in database to query (if database flavor supports this).\\n        Uses default schema if None (default).\\n    index_col : Union[str, List[str]], optional\\n        Column(s) to set as index(MultiIndex).\\n    params :  Union[List, Tuple, Dict], optional\\n        List of parameters to pass to execute method.\\n        The syntax used to pass parameters is database driver dependent.\\n        Check your database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunksize : int, optional\\n        If specified, return an iterator where chunksize is the number of rows to include in each chunk.\\n    dtype : Dict[str, pyarrow.DataType], optional\\n        Specifying the datatype for columns.\\n        The keys should be the column names and the values should be the PyArrow types.\\n    safe : bool\\n        Check for overflows or other unsafe data type conversions.\\n    timestamp_as_object : bool\\n        Cast non-nanosecond timestamps (np.datetime64) to objects.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    Reading from Redshift using a Glue Catalog Connections\\n\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.read_sql_table(\\n    ...     table=\"my_table\",\\n    ...     schema=\"public\",\\n    ...     con=con\\n    ... )\\n    >>> con.close()\\n\\n    '\n    sql: str = f'SELECT * FROM \"{table}\"' if schema is None else f'SELECT * FROM \"{schema}\".\"{table}\"'\n    return read_sql_query(sql=sql, con=con, index_col=index_col, params=params, chunksize=chunksize, dtype=dtype, safe=safe, timestamp_as_object=timestamp_as_object, dtype_backend=dtype_backend)"
        ]
    },
    {
        "func_name": "unload_to_files",
        "original": "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    \"\"\"Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\n\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Parameters\n    ----------\n    sql: str\n        SQL query.\n    path : Union[str, List[str]]\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\n    con : redshift_connector.Connection\n        Use redshift_connector.connect() to use \"\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\n    iam_role : str, optional\n        AWS IAM role with the related permissions.\n    aws_access_key_id : str, optional\n        The access key for your AWS account.\n    aws_secret_access_key : str, optional\n        The secret key for your AWS account.\n    aws_session_token : str, optional\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\n    region : str, optional\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn't in the\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\n        assumes that the target Amazon S3 bucket is located in the same AWS\n        Region as the Amazon Redshift cluster.\n    unload_format: str, optional\n        Format of the unloaded S3 objects from the query.\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\n    parallel: bool\n        Whether to unload to multiple files in parallel. Defaults to True.\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\n        sorted absolutely according to the ORDER BY clause, if one is used.\n    max_file_size : float, optional\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\n        maximum file size is 6200.0 MB.\n    kms_key_id : str, optional\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\n        used to encrypt data files on Amazon S3.\n    manifest : bool\n        Unload a manifest file on S3.\n    partition_cols: List[str], optional\n        Specifies the partition keys for the unload operation.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\n    >>> wr.redshift.unload_to_files(\n    ...     sql=\"SELECT * FROM public.mytable\",\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\n    ...     con=con,\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\n    ... )\n    >>> con.close()\n\n\n    \"\"\"\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)",
        "mutated": [
            "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    if False:\n        i = 10\n    'Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql: str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    unload_format: str, optional\\n        Format of the unloaded S3 objects from the query.\\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    manifest : bool\\n        Unload a manifest file on S3.\\n    partition_cols: List[str], optional\\n        Specifies the partition keys for the unload operation.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n\\n    Returns\\n    -------\\n    None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> wr.redshift.unload_to_files(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n\\n    '\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)",
            "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql: str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    unload_format: str, optional\\n        Format of the unloaded S3 objects from the query.\\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    manifest : bool\\n        Unload a manifest file on S3.\\n    partition_cols: List[str], optional\\n        Specifies the partition keys for the unload operation.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n\\n    Returns\\n    -------\\n    None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> wr.redshift.unload_to_files(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n\\n    '\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)",
            "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql: str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    unload_format: str, optional\\n        Format of the unloaded S3 objects from the query.\\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    manifest : bool\\n        Unload a manifest file on S3.\\n    partition_cols: List[str], optional\\n        Specifies the partition keys for the unload operation.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n\\n    Returns\\n    -------\\n    None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> wr.redshift.unload_to_files(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n\\n    '\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)",
            "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql: str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    unload_format: str, optional\\n        Format of the unloaded S3 objects from the query.\\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    manifest : bool\\n        Unload a manifest file on S3.\\n    partition_cols: List[str], optional\\n        Specifies the partition keys for the unload operation.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n\\n    Returns\\n    -------\\n    None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> wr.redshift.unload_to_files(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n\\n    '\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)",
            "@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload_to_files(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, unload_format: Optional[Literal['CSV', 'PARQUET']]=None, parallel: bool=True, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, manifest: bool=False, partition_cols: Optional[List[str]]=None, boto3_session: Optional[boto3.Session]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Unload Parquet files on s3 from a Redshift query result (Through the UNLOAD command).\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql: str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    unload_format: str, optional\\n        Format of the unloaded S3 objects from the query.\\n        Valid values: \"CSV\", \"PARQUET\". Case sensitive. Defaults to PARQUET.\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    manifest : bool\\n        Unload a manifest file on S3.\\n    partition_cols: List[str], optional\\n        Specifies the partition keys for the unload operation.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n\\n    Returns\\n    -------\\n    None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> wr.redshift.unload_to_files(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n\\n    '\n    _logger.debug('Unloading to S3 path: %s', path)\n    if unload_format not in [None, 'CSV', 'PARQUET']:\n        raise exceptions.InvalidArgumentValue(\"<unload_format> argument must be 'CSV' or 'PARQUET'\")\n    with con.cursor() as cursor:\n        format_str: str = unload_format or 'PARQUET'\n        partition_str: str = f\"\\nPARTITION BY ({','.join(partition_cols)})\" if partition_cols else ''\n        manifest_str: str = '\\nmanifest' if manifest is True else ''\n        region_str: str = f\"\\nREGION AS '{region}'\" if region is not None else ''\n        parallel_str: str = '\\nPARALLEL ON' if parallel else '\\nPARALLEL OFF'\n        if not max_file_size and engine.get() == EngineEnum.RAY:\n            _logger.warning('Unload `MAXFILESIZE` is not specified. Defaulting to `512.0 MB` corresponding to the recommended Ray target block size.')\n            max_file_size = 512.0\n        max_file_size_str: str = f'\\nMAXFILESIZE AS {max_file_size} MB' if max_file_size is not None else ''\n        kms_key_id_str: str = f\"\\nKMS_KEY_ID '{kms_key_id}'\" if kms_key_id is not None else ''\n        auth_str: str = _make_s3_auth_string(iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, boto3_session=boto3_session)\n        sql = sql.replace(\"'\", \"''\")\n        unload_sql = f\"UNLOAD ('{sql}')\\nTO '{path}'\\n{auth_str}ALLOWOVERWRITE\\n{parallel_str}\\nFORMAT {format_str}\\nENCRYPTED{kms_key_id_str}{partition_str}{region_str}{max_file_size_str}{manifest_str};\"\n        cursor.execute(unload_sql)"
        ]
    },
    {
        "func_name": "unload",
        "original": "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    \"\"\"Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\n\n    This is a **HIGH** latency and **HIGH** throughput alternative to\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\n\n    This strategy has more overhead and requires more IAM privileges\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\n    so it is only recommended to fetch 1k+ rows at once.\n\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\n\n    Note\n    ----\n    ``Batching`` (`chunked` argument) (Memory Friendly):\n\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\n\n    There are two batching strategies on awswrangler:\n\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\n\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\n\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\n    in the number of rows for each DataFrame.\n\n\n    Note\n    ----\n    In case of `use_threads=True` the number of threads\n    that will be spawned will be gotten from os.cpu_count().\n\n    Parameters\n    ----------\n    sql : str\n        SQL query.\n    path : Union[str, List[str]]\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\n    con : redshift_connector.Connection\n        Use redshift_connector.connect() to use \"\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\n    iam_role : str, optional\n        AWS IAM role with the related permissions.\n    aws_access_key_id : str, optional\n        The access key for your AWS account.\n    aws_secret_access_key : str, optional\n        The secret key for your AWS account.\n    aws_session_token : str, optional\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\n    region : str, optional\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn't in the\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\n        assumes that the target Amazon S3 bucket is located in the same AWS\n        Region as the Amazon Redshift cluster.\n    max_file_size : float, optional\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\n        maximum file size is 6200.0 MB.\n    kms_key_id : str, optional\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\n        used to encrypt data files on Amazon S3.\n    keep_files : bool\n        Should keep stage files?\n    parallel: bool\n        Whether to unload to multiple files in parallel. Defaults to True.\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\n        sorted absolutely according to the ORDER BY clause, if one is used.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    chunked : Union[int, bool]\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\n    use_threads : bool, int\n        True to enable concurrent requests, False to disable multiple threads.\n        If enabled os.cpu_count() will be used as the max number of threads.\n        If integer is provided, specified number is used.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    s3_additional_kwargs : Dict[str, str], optional\n        Forward to botocore requests.\n    pyarrow_additional_kwargs : Dict[str, Any], optional\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\n        e.g. pyarrow_additional_kwargs={'split_blocks': True}.\n\n    Returns\n    -------\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\n        Result as Pandas DataFrame(s).\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\n    >>> df = wr.redshift.unload(\n    ...     sql=\"SELECT * FROM public.mytable\",\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\n    ...     con=con,\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\n    ... )\n    >>> con.close()\n\n    \"\"\"\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)",
        "mutated": [
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n    'Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\\n\\n    This is a **HIGH** latency and **HIGH** throughput alternative to\\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\\n\\n    This strategy has more overhead and requires more IAM privileges\\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\\n    so it is only recommended to fetch 1k+ rows at once.\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    ``Batching`` (`chunked` argument) (Memory Friendly):\\n\\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\\n\\n    There are two batching strategies on awswrangler:\\n\\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\\n\\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\\n\\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\\n    in the number of rows for each DataFrame.\\n\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    keep_files : bool\\n        Should keep stage files?\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunked : Union[int, bool]\\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Dict[str, str], optional\\n        Forward to botocore requests.\\n    pyarrow_additional_kwargs : Dict[str, Any], optional\\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\\n        e.g. pyarrow_additional_kwargs={\\'split_blocks\\': True}.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.unload(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n    '\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\\n\\n    This is a **HIGH** latency and **HIGH** throughput alternative to\\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\\n\\n    This strategy has more overhead and requires more IAM privileges\\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\\n    so it is only recommended to fetch 1k+ rows at once.\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    ``Batching`` (`chunked` argument) (Memory Friendly):\\n\\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\\n\\n    There are two batching strategies on awswrangler:\\n\\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\\n\\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\\n\\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\\n    in the number of rows for each DataFrame.\\n\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    keep_files : bool\\n        Should keep stage files?\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunked : Union[int, bool]\\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Dict[str, str], optional\\n        Forward to botocore requests.\\n    pyarrow_additional_kwargs : Dict[str, Any], optional\\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\\n        e.g. pyarrow_additional_kwargs={\\'split_blocks\\': True}.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.unload(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n    '\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\\n\\n    This is a **HIGH** latency and **HIGH** throughput alternative to\\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\\n\\n    This strategy has more overhead and requires more IAM privileges\\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\\n    so it is only recommended to fetch 1k+ rows at once.\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    ``Batching`` (`chunked` argument) (Memory Friendly):\\n\\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\\n\\n    There are two batching strategies on awswrangler:\\n\\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\\n\\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\\n\\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\\n    in the number of rows for each DataFrame.\\n\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    keep_files : bool\\n        Should keep stage files?\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunked : Union[int, bool]\\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Dict[str, str], optional\\n        Forward to botocore requests.\\n    pyarrow_additional_kwargs : Dict[str, Any], optional\\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\\n        e.g. pyarrow_additional_kwargs={\\'split_blocks\\': True}.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.unload(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n    '\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\\n\\n    This is a **HIGH** latency and **HIGH** throughput alternative to\\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\\n\\n    This strategy has more overhead and requires more IAM privileges\\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\\n    so it is only recommended to fetch 1k+ rows at once.\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    ``Batching`` (`chunked` argument) (Memory Friendly):\\n\\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\\n\\n    There are two batching strategies on awswrangler:\\n\\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\\n\\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\\n\\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\\n    in the number of rows for each DataFrame.\\n\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    keep_files : bool\\n        Should keep stage files?\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunked : Union[int, bool]\\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Dict[str, str], optional\\n        Forward to botocore requests.\\n    pyarrow_additional_kwargs : Dict[str, Any], optional\\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\\n        e.g. pyarrow_additional_kwargs={\\'split_blocks\\': True}.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.unload(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n    '\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)",
            "@_utils.validate_distributed_kwargs(unsupported_kwargs=['boto3_session', 's3_additional_kwargs'])\n@apply_configs\n@_utils.check_optional_dependency(redshift_connector, 'redshift_connector')\ndef unload(sql: str, path: str, con: 'redshift_connector.Connection', iam_role: Optional[str]=None, aws_access_key_id: Optional[str]=None, aws_secret_access_key: Optional[str]=None, aws_session_token: Optional[str]=None, region: Optional[str]=None, max_file_size: Optional[float]=None, kms_key_id: Optional[str]=None, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', chunked: Union[bool, int]=False, keep_files: bool=False, parallel: bool=True, use_threads: Union[bool, int]=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> Union[pd.DataFrame, Iterator[pd.DataFrame]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load Pandas DataFrame from a Amazon Redshift query result using Parquet files on s3 as stage.\\n\\n    This is a **HIGH** latency and **HIGH** throughput alternative to\\n    `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` to extract large\\n    Amazon Redshift data into a Pandas DataFrames through the **UNLOAD command**.\\n\\n    This strategy has more overhead and requires more IAM privileges\\n    than the regular `wr.redshift.read_sql_query()`/`wr.redshift.read_sql_table()` function,\\n    so it is only recommended to fetch 1k+ rows at once.\\n\\n    https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html\\n\\n    Note\\n    ----\\n    ``Batching`` (`chunked` argument) (Memory Friendly):\\n\\n    Will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.\\n\\n    There are two batching strategies on awswrangler:\\n\\n    - If **chunked=True**, depending on the size of the data, one or more data frames are returned per file.\\n      Unlike **chunked=INTEGER**, rows from different files are not be mixed in the resulting data frames.\\n\\n    - If **chunked=INTEGER**, awswrangler iterates on the data by number of rows (equal to the received INTEGER).\\n\\n    `P.S.` `chunked=True` is faster and uses less memory while `chunked=INTEGER` is more precise\\n    in the number of rows for each DataFrame.\\n\\n\\n    Note\\n    ----\\n    In case of `use_threads=True` the number of threads\\n    that will be spawned will be gotten from os.cpu_count().\\n\\n    Parameters\\n    ----------\\n    sql : str\\n        SQL query.\\n    path : Union[str, List[str]]\\n        S3 path to write stage files (e.g. s3://bucket_name/any_name/)\\n    con : redshift_connector.Connection\\n        Use redshift_connector.connect() to use \"\\n        \"credentials directly or wr.redshift.connect() to fetch it from the Glue Catalog.\\n    iam_role : str, optional\\n        AWS IAM role with the related permissions.\\n    aws_access_key_id : str, optional\\n        The access key for your AWS account.\\n    aws_secret_access_key : str, optional\\n        The secret key for your AWS account.\\n    aws_session_token : str, optional\\n        The session key for your AWS account. This is only needed when you are using temporary credentials.\\n    region : str, optional\\n        Specifies the AWS Region where the target Amazon S3 bucket is located.\\n        REGION is required for UNLOAD to an Amazon S3 bucket that isn\\'t in the\\n        same AWS Region as the Amazon Redshift cluster. By default, UNLOAD\\n        assumes that the target Amazon S3 bucket is located in the same AWS\\n        Region as the Amazon Redshift cluster.\\n    max_file_size : float, optional\\n        Specifies the maximum size (MB) of files that UNLOAD creates in Amazon S3.\\n        Specify a decimal value between 5.0 MB and 6200.0 MB. If None, the default\\n        maximum file size is 6200.0 MB.\\n    kms_key_id : str, optional\\n        Specifies the key ID for an AWS Key Management Service (AWS KMS) key to be\\n        used to encrypt data files on Amazon S3.\\n    keep_files : bool\\n        Should keep stage files?\\n    parallel: bool\\n        Whether to unload to multiple files in parallel. Defaults to True.\\n        By default, UNLOAD writes data in parallel to multiple files, according to the number of\\n        slices in the cluster. If parallel is False, UNLOAD writes to one or more data files serially,\\n        sorted absolutely according to the ORDER BY clause, if one is used.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    chunked : Union[int, bool]\\n        If passed will split the data in a Iterable of DataFrames (Memory friendly).\\n        If `True` awswrangler iterates on the data by files in the most efficient way without guarantee of chunksize.\\n        If an `INTEGER` is passed awswrangler will iterate on the data by number of rows equal the received INTEGER.\\n    use_threads : bool, int\\n        True to enable concurrent requests, False to disable multiple threads.\\n        If enabled os.cpu_count() will be used as the max number of threads.\\n        If integer is provided, specified number is used.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    s3_additional_kwargs : Dict[str, str], optional\\n        Forward to botocore requests.\\n    pyarrow_additional_kwargs : Dict[str, Any], optional\\n        Forwarded to `to_pandas` method converting from PyArrow tables to Pandas DataFrame.\\n        Valid values include \"split_blocks\", \"self_destruct\", \"ignore_metadata\".\\n        e.g. pyarrow_additional_kwargs={\\'split_blocks\\': True}.\\n\\n    Returns\\n    -------\\n    Union[pandas.DataFrame, Iterator[pandas.DataFrame]]\\n        Result as Pandas DataFrame(s).\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> con = wr.redshift.connect(\"MY_GLUE_CONNECTION\")\\n    >>> df = wr.redshift.unload(\\n    ...     sql=\"SELECT * FROM public.mytable\",\\n    ...     path=\"s3://bucket/extracted_parquet_files/\",\\n    ...     con=con,\\n    ...     iam_role=\"arn:aws:iam::XXX:role/XXX\"\\n    ... )\\n    >>> con.close()\\n\\n    '\n    path = path if path.endswith('/') else f'{path}/'\n    unload_to_files(sql=sql, path=path, con=con, iam_role=iam_role, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_session_token=aws_session_token, region=region, max_file_size=max_file_size, kms_key_id=kms_key_id, manifest=False, parallel=parallel, boto3_session=boto3_session)\n    if chunked is False:\n        df: pd.DataFrame = s3.read_parquet(path=path, chunked=chunked, dataset=False, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, pyarrow_additional_kwargs=pyarrow_additional_kwargs)\n        if keep_files is False:\n            _logger.debug('Deleting objects in S3 path: %s', path)\n            s3.delete_objects(path=path, use_threads=use_threads, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs)\n        return df\n    return _read_parquet_iterator(path=path, chunked=chunked, use_threads=use_threads, dtype_backend=dtype_backend, boto3_session=boto3_session, s3_additional_kwargs=s3_additional_kwargs, keep_files=keep_files, pyarrow_additional_kwargs=pyarrow_additional_kwargs)"
        ]
    }
]