[
    {
        "func_name": "__init__",
        "original": "def __init__(self, mega_args, depth, vocab_size):\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth",
        "mutated": [
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.mega_args = mega_args\n    self.embedding_layer = nn.Embedding(vocab_size, self.mega_args.encoder_embed_dim)\n    self.encoders = nn.ModuleList([MegaEncoderLayer(self.mega_args) for _ in range(depth)])\n    self.depth = depth"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    \"\"\"\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\n\n        Other options:\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\n            aligns with the HF tokenizer behavior)\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\n            which aligns with HF tokenizer)\n        \"\"\"\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds",
        "mutated": [
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n    '\\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\\n\\n        Other options:\\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\\n            aligns with the HF tokenizer behavior)\\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\\n            which aligns with HF tokenizer)\\n        '\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\\n\\n        Other options:\\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\\n            aligns with the HF tokenizer behavior)\\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\\n            which aligns with HF tokenizer)\\n        '\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\\n\\n        Other options:\\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\\n            aligns with the HF tokenizer behavior)\\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\\n            which aligns with HF tokenizer)\\n        '\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\\n\\n        Other options:\\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\\n            aligns with the HF tokenizer behavior)\\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\\n            which aligns with HF tokenizer)\\n        '\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Code for a forward pass - expects input_ids and attention_mask to come from a Hugging Face tokenizer as PyTorch\\n        tensors, and returns a tensor of size (batch, n_classes) containing classification logits\\n\\n        Other options:\\n          - batch_first: boolean indicating whether the batch dimension is first in input_ids (default: True, which\\n            aligns with the HF tokenizer behavior)\\n          - ignore_mask_value: the value in attention_mask that identifies tokens that should be ignored (default: 0,\\n            which aligns with HF tokenizer)\\n        '\n    if batch_first:\n        input_ids = input_ids.T\n    if ignore_mask_value == 0:\n        attention_mask = 1 - attention_mask\n    embeds = self.embedding_layer(input_ids)\n    for encoder in self.encoders:\n        embeds = encoder(embeds, attention_mask)\n    if batch_first:\n        return torch.transpose(embeds, 0, 1)\n    else:\n        return embeds"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, mega_args, depth, vocab_size):\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)",
        "mutated": [
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)",
            "def __init__(self, mega_args, depth, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.mega = MegaLM(mega_args, depth, vocab_size)\n    self.mlm_head = nn.Linear(mega_args.encoder_embed_dim, vocab_size)\n    self.dropout = nn.Dropout(p=0.1)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    \"\"\"\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\n        entry.\n\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\n        size, Sequence length, Vocab size); otherwise (S, B, V)\n        \"\"\"\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))",
        "mutated": [
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n    '\\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\\n        entry.\\n\\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\\n        size, Sequence length, Vocab size); otherwise (S, B, V)\\n        '\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\\n        entry.\\n\\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\\n        size, Sequence length, Vocab size); otherwise (S, B, V)\\n        '\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\\n        entry.\\n\\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\\n        size, Sequence length, Vocab size); otherwise (S, B, V)\\n        '\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\\n        entry.\\n\\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\\n        size, Sequence length, Vocab size); otherwise (S, B, V)\\n        '\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))",
            "def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Perform a forward pass through the Mega encoder and the masked LM head. Returns logits for each vocabulary\\n        entry.\\n\\n        If `batch_first` (default to align with Hugging Face tokenizer behavior), output will have the shape (Batch\\n        size, Sequence length, Vocab size); otherwise (S, B, V)\\n        '\n    encoder_output = self.mega(input_ids, attention_mask, batch_first, ignore_mask_value)\n    return self.mlm_head(self.dropout(encoder_output))"
        ]
    },
    {
        "func_name": "convert_checkpoint_to_huggingface",
        "original": "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)",
        "mutated": [
            "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    if False:\n        i = 10\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)",
            "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)",
            "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)",
            "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)",
            "def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open(os.path.join(pretrained_checkpoint_path, 'model_args.pkl'), 'rb') as f:\n        mega_original_args = pkl.load(f)\n    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()\n    print('Original Mega encoder:', original_mlm.mega.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'encoder_weights.pt'), map_location='cpu')))\n    print('Original Mega MLM layer:', original_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    hf_config = MegaConfig(num_hidden_layers=mega_original_args['depth'], vocab_size=mega_original_args['vocab_size'], hidden_size=mega_original_args['mega_args'].encoder_embed_dim, shared_representation_size=mega_original_args['mega_args'].encoder_z_dim, intermediate_size=mega_original_args['mega_args'].encoder_hidden_dim, ema_projection_size=mega_original_args['mega_args'].encoder_n_dim, dropout_prob=mega_original_args['mega_args'].dropout, attention_probs_dropout_prob=mega_original_args['mega_args'].attention_dropout, hidden_dropout_prob=mega_original_args['mega_args'].hidden_dropout, activation=mega_original_args['mega_args'].activation_fn, attention_activation=mega_original_args['mega_args'].attention_activation_fn, bidirectional=mega_original_args['mega_args'].bidirectional, use_chunking=mega_original_args['mega_args'].encoder_chunk_size > 0, chunk_size=mega_original_args['mega_args'].encoder_chunk_size, truncation=mega_original_args['mega_args'].truncation_length, normalization_type=mega_original_args['mega_args'].normalization_type, normalize_before_mega=True, norm_affine=True, use_feature_dropout=mega_original_args['mega_args'].feature_dropout, relative_positional_bias=mega_original_args['mega_args'].rel_pos_bias, max_positions=mega_original_args['mega_args'].max_source_positions, nffn_hidden_size=mega_original_args['mega_args'].encoder_ffn_embed_dim, normalize_before_ffn=mega_original_args['mega_args'].normalize_before, nffn_activation_dropout_prob=0.0, add_token_type_embeddings=False, add_lm_hidden_dense_layer=False)\n    hf_mlm = MegaForMaskedLM(hf_config).eval()\n    hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n    original_state_dict = original_mlm.mega.encoders.state_dict()\n    updated_keys = {}\n    for module_name in original_state_dict.keys():\n        new_module_name = None\n        if 'beta' in module_name:\n            if 'move.beta' in module_name:\n                new_module_name = module_name.replace('move.beta', 'ema_gate.ema_expansion_matrix')\n            elif 'mega_layer.beta' in module_name:\n                new_module_name = module_name.replace('beta', 'qk_bias')\n            else:\n                new_module_name = module_name.replace('beta', 'b_param')\n        elif 'gamma' in module_name:\n            if 'move.gamma' in module_name:\n                new_module_name = module_name.replace('move.gamma', 'ema_gate.kernel_projection_matrix')\n            elif 'mega_layer.gamma' in module_name:\n                new_module_name = module_name.replace('gamma', 'qk_weight')\n            else:\n                new_module_name = module_name.replace('gamma', 'g_param')\n        elif 'move.alpha' in module_name:\n            new_module_name = module_name.replace('move.alpha', 'ema_gate.decay_factor')\n        elif 'move.delta' in module_name:\n            new_module_name = module_name.replace('move.delta', 'ema_gate.damping_factor')\n        elif 'omega' in module_name:\n            new_module_name = module_name.replace('move.omega', 'ema_gate.residual_weight')\n        if new_module_name:\n            updated_keys[module_name] = new_module_name\n    if len(updated_keys) != 0:\n        print(f'Renaming these keys: {updated_keys.keys()}')\n    else:\n        print('No need to rename state dict entries')\n    for (old, new) in updated_keys.items():\n        original_state_dict[new] = original_state_dict.pop(old)\n    print('HF Mega encoder:', hf_mlm.mega.layers.load_state_dict(original_state_dict))\n    print('HF Mega MLM layer:', hf_mlm.mlm_head.load_state_dict(torch.load(os.path.join(pretrained_checkpoint_path, 'mlm_head_weights.pt'), map_location='cpu')))\n    input_ids = torch.randint(0, hf_config.vocab_size, size=(4, 256))\n    input_mask = torch.ones_like(input_ids)\n    input_mask[:, -10:] = 0\n    original_output = original_mlm(input_ids, input_mask, batch_first=True, ignore_mask_value=0)\n    hf_output = hf_mlm(input_ids, input_mask)[0]\n    print(f'original output {original_output.shape}')\n    print(f'hf output {hf_output.shape}')\n    print(f'max diff: {(original_output - hf_output).max()}')\n    success = torch.allclose(original_output, hf_output, atol=0.001)\n    if success:\n        print('Yay!')\n        hf_mlm.save_pretrained(output_path)\n    else:\n        raise RuntimeError(f\"Something's broken :(\\nOriginal:\\n{original_output}\\n\\nHF\\n{hf_output}\\n{hf_mlm}\")\n    if includes_tokenizer:\n        print('Transferring tokenizer')\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_checkpoint_path)\n        tokenizer.save_pretrained(output_path)"
        ]
    }
]