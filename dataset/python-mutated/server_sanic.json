[
    {
        "func_name": "run",
        "original": "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    \"\"\"Run the server programatically\"\"\"\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)",
        "mutated": [
            "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    if False:\n        i = 10\n    'Run the server programatically'\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)",
            "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run the server programatically'\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)",
            "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run the server programatically'\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)",
            "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run the server programatically'\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)",
            "def run(port: int, workers: int, trained_models: Dict[str, DemoModel], static_dir: str=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run the server programatically'\n    print('Starting a sanic server on port {}.'.format(port))\n    if port != 8000:\n        logger.warning('The demo requires the API to be run on port 8000.')\n    demo_db = PostgresDemoDatabase.from_environment()\n    app = make_app(static_dir, demo_db)\n    CORS(app)\n    for (name, demo_model) in trained_models.items():\n        predictor = demo_model.predictor()\n        app.predictors[name] = predictor\n    app.run(port=port, host='0.0.0.0', workers=workers)"
        ]
    },
    {
        "func_name": "_caching_prediction",
        "original": "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n    return model.predict_json(json.loads(data))",
        "mutated": [
            "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    if False:\n        i = 10\n    '\\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\\n        '\n    return model.predict_json(json.loads(data))",
            "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\\n        '\n    return model.predict_json(json.loads(data))",
            "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\\n        '\n    return model.predict_json(json.loads(data))",
            "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\\n        '\n    return model.predict_json(json.loads(data))",
            "@lru_cache(maxsize=cache_size)\ndef _caching_prediction(model: Predictor, data: str) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\\n        '\n    return model.predict_json(json.loads(data))"
        ]
    },
    {
        "func_name": "make_app",
        "original": "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app",
        "mutated": [
            "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    if False:\n        i = 10\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app",
            "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app",
            "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app",
            "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app",
            "def make_app(build_dir: str=None, demo_db: Optional[DemoDatabase]=None) -> Sanic:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    app = Sanic(__name__)\n    start_time = datetime.now(pytz.utc)\n    start_time_str = start_time.strftime('%Y-%m-%d %H:%M:%S %Z')\n    if build_dir is None:\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        build_dir = os.path.join(dir_path, '../../demo/build')\n    if not os.path.exists(build_dir):\n        logger.error('app directory %s does not exist, aborting', build_dir)\n        sys.exit(-1)\n    app.predictors = {}\n    try:\n        cache_size = int(CACHE_SIZE)\n    except ValueError:\n        logger.warning('unable to parse cache size %s as int, disabling cache', CACHE_SIZE)\n        cache_size = 0\n\n    @lru_cache(maxsize=cache_size)\n    def _caching_prediction(model: Predictor, data: str) -> JsonDict:\n        \"\"\"\n        Just a wrapper around ``model.predict_json`` that allows us to use a cache decorator.\n        \"\"\"\n        return model.predict_json(json.loads(data))\n\n    @app.route('/permadata', methods=['POST', 'OPTIONS'])\n    async def permadata(req: request.Request) -> response.HTTPResponse:\n        \"\"\"\n        If the user requests a permalink, the front end will POST here with the payload\n            { slug: slug }\n        which we convert to an integer id and use to retrieve saved results from the database.\n        \"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        if demo_db is None:\n            raise ServerError('Permalinks are not enabled', 400)\n        slug = req.json['slug']\n        perma_id = slug_to_int(slug)\n        if perma_id is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        try:\n            permadata = demo_db.get_result(perma_id)\n        except psycopg2.Error:\n            logger.exception('Unable to get results from database: perma_id %s', perma_id)\n            raise ServerError('Database trouble', 500)\n        if permadata is None:\n            raise ServerError('Unrecognized permalink: {}'.format(slug), 400)\n        return response.json({'modelName': permadata.model_name, 'requestData': permadata.request_data, 'responseData': permadata.response_data})\n\n    @app.route('/predict/<model_name>', methods=['POST', 'OPTIONS'])\n    async def predict(req: request.Request, model_name: str) -> response.HTTPResponse:\n        \"\"\"make a prediction using the specified model and return the results\"\"\"\n        if req.method == 'OPTIONS':\n            return response.text('')\n        model = app.predictors.get(model_name.lower())\n        if model is None:\n            raise ServerError('unknown model: {}'.format(model_name), status_code=400)\n        data = req.json\n        log_blob = {'model': model_name, 'inputs': data, 'cached': False, 'outputs': {}}\n        pre_hits = _caching_prediction.cache_info().hits\n        try:\n            if cache_size > 0:\n                prediction = _caching_prediction(model, json.dumps(data))\n            else:\n                prediction = model.predict_json(data)\n        except KeyError as err:\n            raise ServerError('Required JSON field not found: ' + err.args[0], status_code=400)\n        post_hits = _caching_prediction.cache_info().hits\n        if demo_db is not None:\n            try:\n                perma_id = demo_db.add_result(headers=req.headers, model_name=model_name, inputs=data, outputs=prediction)\n                if perma_id is not None:\n                    slug = int_to_slug(perma_id)\n                    prediction['slug'] = slug\n                    log_blob['slug'] = slug\n            except Exception:\n                logger.exception('Unable to add result to database', exc_info=True)\n        if post_hits > pre_hits:\n            log_blob['cached'] = True\n            await asyncio.sleep(0.25)\n        if model_name == 'machine-comprehension':\n            log_blob['outputs']['best_span_str'] = prediction['best_span_str']\n        elif model_name == 'coreference-resolution':\n            log_blob['outputs']['clusters'] = prediction['clusters']\n            log_blob['outputs']['document'] = prediction['document']\n        elif model_name == 'textual-entailment':\n            log_blob['outputs']['label_probs'] = prediction['label_probs']\n        elif model_name == 'named-entity-recognition':\n            log_blob['outputs']['tags'] = prediction['tags']\n        elif model_name == 'semantic-role-labeling':\n            verbs = []\n            for verb in prediction['verbs']:\n                good_tags = [tag for tag in verb['tags'] if tag != '0']\n                if len(good_tags) > 1:\n                    verbs.append({'verb': verb['verb'], 'description': verb['description']})\n            log_blob['outputs']['verbs'] = verbs\n        logger.info('prediction: %s', json.dumps(log_blob))\n        return response.json(prediction)\n\n    @app.route('/models')\n    async def list_models(req: request.Request) -> response.HTTPResponse:\n        \"\"\"list the available models\"\"\"\n        return response.json({'models': list(app.predictors.keys())})\n\n    @app.route('/info')\n    async def info(req: request.Request) -> response.HTTPResponse:\n        \"\"\"List metadata about the running webserver\"\"\"\n        uptime = str(datetime.now(pytz.utc) - start_time)\n        git_version = os.environ.get('SOURCE_COMMIT') or ''\n        return response.json({'start_time': start_time_str, 'uptime': uptime, 'git_version': git_version, 'githubUrl': 'http://github.com/allenai/allennlp/commit/' + git_version})\n\n    @app.route('/semantic-role-labeling')\n    @app.route('/machine-comprehension')\n    @app.route('/textual-entailment')\n    @app.route('/coreference-resolution')\n    @app.route('/named-entity-recognition')\n    @app.route('/semantic-role-labeling/<permalink>')\n    @app.route('/machine-comprehension/<permalink>')\n    @app.route('/textual-entailment/<permalink>')\n    @app.route('/coreference-resolution/<permalink>')\n    @app.route('/named-entity-recognition/<permalink>')\n    async def return_page(req: request.Request, permalink: str=None) -> response.HTTPResponse:\n        \"\"\"return the page\"\"\"\n        return await response.file(os.path.join(build_dir, 'index.html'))\n    app.static('/', os.path.join(build_dir, 'index.html'))\n    app.static('/', build_dir)\n    return app"
        ]
    }
]