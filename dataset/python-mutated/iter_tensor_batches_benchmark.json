[
    {
        "func_name": "iter_torch_batches",
        "original": "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
        "mutated": [
            "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds",
            "def iter_torch_batches(ds: Dataset, batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, prefetch_batches: int=0, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_batches = 0\n    if use_default_params:\n        for batch in ds.iter_torch_batches():\n            num_batches += 1\n    else:\n        for batch in ds.iter_torch_batches(batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size, prefetch_batches=prefetch_batches):\n            num_batches += 1\n    print('iter_torch_batches done, block_format:', 'pyarrow', 'num_rows:', ds.count(), 'num_blocks:', ds.num_blocks(), 'num_batches:', num_batches)\n    return ds"
        ]
    },
    {
        "func_name": "to_tf",
        "original": "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds",
        "mutated": [
            "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds",
            "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds",
            "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds",
            "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds",
            "def to_tf(ds: Dataset, feature_columns: Union[str, List[str]], label_columns: Union[str, List[str]], batch_size: Optional[int]=None, local_shuffle_buffer_size: Optional[int]=None, use_default_params: bool=False) -> Dataset:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if use_default_params:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns)\n    else:\n        ds.to_tf(feature_columns=feature_columns, label_columns=label_columns, batch_size=batch_size, local_shuffle_buffer_size=local_shuffle_buffer_size)\n    return ds"
        ]
    },
    {
        "func_name": "add_label",
        "original": "def add_label(batch):\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch",
        "mutated": [
            "def add_label(batch):\n    if False:\n        i = 10\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch",
            "def add_label(batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch",
            "def add_label(batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch",
            "def add_label(batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch",
            "def add_label(batch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    label = np.ones(shape=(len(batch), 1))\n    batch['label'] = label\n    return batch"
        ]
    },
    {
        "func_name": "run_iter_tensor_batches_benchmark",
        "original": "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)",
        "mutated": [
            "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    if False:\n        i = 10\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)",
            "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)",
            "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)",
            "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)",
            "def run_iter_tensor_batches_benchmark(benchmark: Benchmark, data_size_gb: int, block_size_mb: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ctx = ray.data.context.DataContext.get_current()\n    ctx.target_max_block_size = block_size_mb * 1024 * 1024\n    ds = ray.data.read_images(f's3://anonymous@air-example-data-2/{data_size_gb}G-image-data-synthetic-raw')\n\n    def add_label(batch):\n        label = np.ones(shape=(len(batch), 1))\n        batch['label'] = label\n        return batch\n    ds = ds.map_batches(add_label, batch_format='pandas').materialize()\n    benchmark.run_materialize_ds('iter-torch-batches-default', iter_torch_batches, ds=ds, use_default_params=True)\n    benchmark.run_materialize_ds('to-tf-default', to_tf, ds=ds, feature_columns='image', label_columns='label', use_default_params=True)\n    batch_sizes = [16, 32]\n    for batch_size in batch_sizes:\n        benchmark.run_materialize_ds(f'iter-torch-batches-{batch_size}-block-size-{block_size_mb}', iter_torch_batches, ds=ds, batch_size=batch_size)\n    prefetch_batches = [0, 1, 4]\n    for prefetch_batch in prefetch_batches:\n        for shuffle_buffer_size in [None, 64]:\n            test_name = f'iter-torch-batches-bs-{32}-prefetch-{prefetch_batch}-shuffle{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=32, prefetch_batches=prefetch_batch)\n    for batch_size in batch_sizes:\n        for shuffle_buffer_size in [batch_size, 2 * batch_size]:\n            test_name = f'iter-torch-batches-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, iter_torch_batches, ds=ds, batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)\n            test_name = f'to-tf-shuffle-{batch_size}-{shuffle_buffer_size}'\n            benchmark.run_materialize_ds(test_name, to_tf, ds=ds, feature_columns='image', label_columns='label', batch_size=batch_size, local_shuffle_buffer_size=shuffle_buffer_size)"
        ]
    }
]