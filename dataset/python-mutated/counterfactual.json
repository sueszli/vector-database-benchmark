[
    {
        "func_name": "dist",
        "original": "def dist(x_reference, x_counterfact):\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)",
        "mutated": [
            "def dist(x_reference, x_counterfact):\n    if False:\n        i = 10\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)",
            "def dist(x_reference, x_counterfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)",
            "def dist(x_reference, x_counterfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)",
            "def dist(x_reference, x_counterfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)",
            "def dist(x_reference, x_counterfact):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    numerator = np.abs(x_reference - x_counterfact)\n    return np.sum(numerator / mad)"
        ]
    },
    {
        "func_name": "loss",
        "original": "def loss(x_counterfact, lammbda):\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)",
        "mutated": [
            "def loss(x_counterfact, lammbda):\n    if False:\n        i = 10\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)",
            "def loss(x_counterfact, lammbda):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)",
            "def loss(x_counterfact, lammbda):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)",
            "def loss(x_counterfact, lammbda):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)",
            "def loss(x_counterfact, lammbda):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if use_proba:\n        y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n    else:\n        y_predict = model.predict(x_counterfact.reshape(1, -1))\n    diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n    return diff + dist(x_reference, x_counterfact)"
        ]
    },
    {
        "func_name": "create_counterfactual",
        "original": "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    \"\"\"\n    Implementation of the counterfactual method by Wachter et al. 2017\n\n    References:\n\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\n    Counterfactual explanations without opening the black box:\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\n     https://arxiv.org/abs/1711.00399\n\n    Parameters\n    ----------\n\n    x_reference : array-like, shape=[m_features]\n        The data instance (training example) to be explained.\n\n    y_desired : int\n        The desired class label for `x_reference`.\n\n    model : estimator\n        A (scikit-learn) estimator implementing `.predict()` and/or\n        `predict_proba()`.\n        - If `model` supports `predict_proba()`, then this is used by\n        default for the first loss term,\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\n        - Otherwise, method will fall back to `predict`.\n\n    X_dataset : array-like, shape=[n_examples, m_features]\n        A (training) dataset for picking the initial counterfactual\n        as initial value for starting the optimization procedure.\n\n    y_desired_proba : float (default: None)\n        A float within the range [0, 1] designating the desired\n        class probability for `y_desired`.\n        - If `y_desired_proba=None` (default), the first loss term\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\n        is a class label\n        - If `y_desired_proba` is not None, the first loss term\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\n\n    lammbda : Weighting parameter for the first loss term,\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\n\n    random_seed : int (default=None)\n        If int, random_seed is the seed used by\n        the random number generator for selecting the inital counterfactual\n        from `X_dataset`.\n\n    \"\"\"\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact",
        "mutated": [
            "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    if False:\n        i = 10\n    '\\n    Implementation of the counterfactual method by Wachter et al. 2017\\n\\n    References:\\n\\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\\n    Counterfactual explanations without opening the black box:\\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\\n     https://arxiv.org/abs/1711.00399\\n\\n    Parameters\\n    ----------\\n\\n    x_reference : array-like, shape=[m_features]\\n        The data instance (training example) to be explained.\\n\\n    y_desired : int\\n        The desired class label for `x_reference`.\\n\\n    model : estimator\\n        A (scikit-learn) estimator implementing `.predict()` and/or\\n        `predict_proba()`.\\n        - If `model` supports `predict_proba()`, then this is used by\\n        default for the first loss term,\\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\\n        - Otherwise, method will fall back to `predict`.\\n\\n    X_dataset : array-like, shape=[n_examples, m_features]\\n        A (training) dataset for picking the initial counterfactual\\n        as initial value for starting the optimization procedure.\\n\\n    y_desired_proba : float (default: None)\\n        A float within the range [0, 1] designating the desired\\n        class probability for `y_desired`.\\n        - If `y_desired_proba=None` (default), the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\\n        is a class label\\n        - If `y_desired_proba` is not None, the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\\n\\n    lammbda : Weighting parameter for the first loss term,\\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\\n\\n    random_seed : int (default=None)\\n        If int, random_seed is the seed used by\\n        the random number generator for selecting the inital counterfactual\\n        from `X_dataset`.\\n\\n    '\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact",
            "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Implementation of the counterfactual method by Wachter et al. 2017\\n\\n    References:\\n\\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\\n    Counterfactual explanations without opening the black box:\\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\\n     https://arxiv.org/abs/1711.00399\\n\\n    Parameters\\n    ----------\\n\\n    x_reference : array-like, shape=[m_features]\\n        The data instance (training example) to be explained.\\n\\n    y_desired : int\\n        The desired class label for `x_reference`.\\n\\n    model : estimator\\n        A (scikit-learn) estimator implementing `.predict()` and/or\\n        `predict_proba()`.\\n        - If `model` supports `predict_proba()`, then this is used by\\n        default for the first loss term,\\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\\n        - Otherwise, method will fall back to `predict`.\\n\\n    X_dataset : array-like, shape=[n_examples, m_features]\\n        A (training) dataset for picking the initial counterfactual\\n        as initial value for starting the optimization procedure.\\n\\n    y_desired_proba : float (default: None)\\n        A float within the range [0, 1] designating the desired\\n        class probability for `y_desired`.\\n        - If `y_desired_proba=None` (default), the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\\n        is a class label\\n        - If `y_desired_proba` is not None, the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\\n\\n    lammbda : Weighting parameter for the first loss term,\\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\\n\\n    random_seed : int (default=None)\\n        If int, random_seed is the seed used by\\n        the random number generator for selecting the inital counterfactual\\n        from `X_dataset`.\\n\\n    '\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact",
            "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Implementation of the counterfactual method by Wachter et al. 2017\\n\\n    References:\\n\\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\\n    Counterfactual explanations without opening the black box:\\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\\n     https://arxiv.org/abs/1711.00399\\n\\n    Parameters\\n    ----------\\n\\n    x_reference : array-like, shape=[m_features]\\n        The data instance (training example) to be explained.\\n\\n    y_desired : int\\n        The desired class label for `x_reference`.\\n\\n    model : estimator\\n        A (scikit-learn) estimator implementing `.predict()` and/or\\n        `predict_proba()`.\\n        - If `model` supports `predict_proba()`, then this is used by\\n        default for the first loss term,\\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\\n        - Otherwise, method will fall back to `predict`.\\n\\n    X_dataset : array-like, shape=[n_examples, m_features]\\n        A (training) dataset for picking the initial counterfactual\\n        as initial value for starting the optimization procedure.\\n\\n    y_desired_proba : float (default: None)\\n        A float within the range [0, 1] designating the desired\\n        class probability for `y_desired`.\\n        - If `y_desired_proba=None` (default), the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\\n        is a class label\\n        - If `y_desired_proba` is not None, the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\\n\\n    lammbda : Weighting parameter for the first loss term,\\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\\n\\n    random_seed : int (default=None)\\n        If int, random_seed is the seed used by\\n        the random number generator for selecting the inital counterfactual\\n        from `X_dataset`.\\n\\n    '\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact",
            "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Implementation of the counterfactual method by Wachter et al. 2017\\n\\n    References:\\n\\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\\n    Counterfactual explanations without opening the black box:\\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\\n     https://arxiv.org/abs/1711.00399\\n\\n    Parameters\\n    ----------\\n\\n    x_reference : array-like, shape=[m_features]\\n        The data instance (training example) to be explained.\\n\\n    y_desired : int\\n        The desired class label for `x_reference`.\\n\\n    model : estimator\\n        A (scikit-learn) estimator implementing `.predict()` and/or\\n        `predict_proba()`.\\n        - If `model` supports `predict_proba()`, then this is used by\\n        default for the first loss term,\\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\\n        - Otherwise, method will fall back to `predict`.\\n\\n    X_dataset : array-like, shape=[n_examples, m_features]\\n        A (training) dataset for picking the initial counterfactual\\n        as initial value for starting the optimization procedure.\\n\\n    y_desired_proba : float (default: None)\\n        A float within the range [0, 1] designating the desired\\n        class probability for `y_desired`.\\n        - If `y_desired_proba=None` (default), the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\\n        is a class label\\n        - If `y_desired_proba` is not None, the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\\n\\n    lammbda : Weighting parameter for the first loss term,\\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\\n\\n    random_seed : int (default=None)\\n        If int, random_seed is the seed used by\\n        the random number generator for selecting the inital counterfactual\\n        from `X_dataset`.\\n\\n    '\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact",
            "def create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Implementation of the counterfactual method by Wachter et al. 2017\\n\\n    References:\\n\\n    - Wachter, S., Mittelstadt, B., & Russell, C. (2017).\\n    Counterfactual explanations without opening the black box:\\n     Automated decisions and the GDPR. Harv. JL & Tech., 31, 841.,\\n     https://arxiv.org/abs/1711.00399\\n\\n    Parameters\\n    ----------\\n\\n    x_reference : array-like, shape=[m_features]\\n        The data instance (training example) to be explained.\\n\\n    y_desired : int\\n        The desired class label for `x_reference`.\\n\\n    model : estimator\\n        A (scikit-learn) estimator implementing `.predict()` and/or\\n        `predict_proba()`.\\n        - If `model` supports `predict_proba()`, then this is used by\\n        default for the first loss term,\\n        `(lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2`\\n        - Otherwise, method will fall back to `predict`.\\n\\n    X_dataset : array-like, shape=[n_examples, m_features]\\n        A (training) dataset for picking the initial counterfactual\\n        as initial value for starting the optimization procedure.\\n\\n    y_desired_proba : float (default: None)\\n        A float within the range [0, 1] designating the desired\\n        class probability for `y_desired`.\\n        - If `y_desired_proba=None` (default), the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired)^2` where `y_desired`\\n        is a class label\\n        - If `y_desired_proba` is not None, the first loss term\\n        is `(lambda * model(x_counterfact) - y_desired_proba)^2`\\n\\n    lammbda : Weighting parameter for the first loss term,\\n        `(lambda * model(x_counterfact) - y_desired[_proba])^2`\\n\\n    random_seed : int (default=None)\\n        If int, random_seed is the seed used by\\n        the random number generator for selecting the inital counterfactual\\n        from `X_dataset`.\\n\\n    '\n    if y_desired_proba is not None:\n        use_proba = True\n        if not hasattr(model, 'predict_proba'):\n            raise AttributeError('Your `model` does not support `predict_proba`. Set `y_desired_proba`  to `None` to use `predict`instead.')\n    else:\n        use_proba = False\n    if y_desired_proba is None:\n        y_to_be_annealed_to = y_desired\n    else:\n        y_to_be_annealed_to = y_desired_proba\n    rng = np.random.RandomState(random_seed)\n    x_counterfact = X_dataset[rng.randint(X_dataset.shape[0])]\n    mad = np.abs(np.median(X_dataset, axis=0) - x_reference)\n\n    def dist(x_reference, x_counterfact):\n        numerator = np.abs(x_reference - x_counterfact)\n        return np.sum(numerator / mad)\n\n    def loss(x_counterfact, lammbda):\n        if use_proba:\n            y_predict = model.predict_proba(x_counterfact.reshape(1, -1)).flatten()[y_desired]\n        else:\n            y_predict = model.predict(x_counterfact.reshape(1, -1))\n        diff = lammbda * (y_predict - y_to_be_annealed_to) ** 2\n        return diff + dist(x_reference, x_counterfact)\n    res = minimize(loss, x_counterfact, args=lammbda, method='Nelder-Mead')\n    if not res['success']:\n        warnings.warn(res['message'])\n    x_counterfact = res['x']\n    return x_counterfact"
        ]
    }
]