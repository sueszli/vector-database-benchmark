[
    {
        "func_name": "isotonic_regression",
        "original": "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    \"\"\"Nonparametric isotonic regression.\n\n    A (not strictly) monotonically increasing array `x` with the same length\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\n    [1]_. See the Notes section for more details.\n\n    Parameters\n    ----------\n    y : (N,) array_like\n        Response variable.\n    weights : (N,) array_like or None\n        Case weights.\n    increasing : bool\n        If True, fit monotonic increasing, i.e. isotonic, regression.\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\n        Default is True.\n\n    Returns\n    -------\n    res : OptimizeResult\n        The optimization result represented as a ``OptimizeResult`` object.\n        Important attributes are:\n\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\n          decreasing) array of the same length than y, with elements in the\n          range from min(y) to max(y).\n        - ``weights`` : Array with the sum of case weights for each block\n          (or pool) B.\n        - ``blocks``: Array of length B+1 with the indices of the start\n          positions of each block (or pool) B. The j-th block is given by\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\n\n    Notes\n    -----\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\n    solves the following optimization problem:\n\n    .. math::\n\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\n\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\n    that :math:`x` is increasing (but not strictly), i.e.\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\n    The solution consists of pools or blocks, i.e. neighboring elements of\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\n    value.\n\n    Most interestingly, the solution stays the same if the squared loss is\n    replaced by the wide class of Bregman functions which are the unique\n    class of strictly consistent scoring functions for the mean, see [2]_\n    and references therein.\n\n    The implemented version of PAVA according to [1]_ has a computational\n    complexity of O(N) with input size N.\n\n    References\n    ----------\n    .. [1] Busing, F. M. T. A. (2022).\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\n           :doi:`10.18637/jss.v102.c01`\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\n           Characterizing the optimal solutions to the isotonic regression\n           problem for identifiable functionals.\n           Ann Inst Stat Math 74, 489-514 (2022).\n           :doi:`10.1007/s10463-021-00808-0`\n\n    Examples\n    --------\n    This example demonstrates that ``isotonic_regression`` really solves a\n    constrained optimization problem.\n\n    >>> import numpy as np\n    >>> from scipy.optimize import isotonic_regression, minimize\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\n    >>> def objective(yhat, y):\n    ...     return np.sum((yhat - y)**2)\n    >>> def constraint(yhat, y):\n    ...     # This is for a monotonically increasing regression.\n    ...     return np.diff(yhat)\n    >>> result = minimize(objective, x0=y, args=(y,),\n    ...                   constraints=[{'type': 'ineq',\n    ...                                 'fun': lambda x: constraint(x, y)}])\n    >>> result.x\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n           9.25      ])\n    >>> result = isotonic_regression(y)\n    >>> result.x\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n           9.25      ])\n\n    The big advantage of ``isotonic_regression`` compared to calling\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\n    define objective and constraint functions, and that it is orders of\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\n    input y of length 1000, the minimizer takes about 4 seconds, while\n    ``isotonic_regression`` takes about 200 microseconds.\n    \"\"\"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)",
        "mutated": [
            "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    if False:\n        i = 10\n    \"Nonparametric isotonic regression.\\n\\n    A (not strictly) monotonically increasing array `x` with the same length\\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\\n    [1]_. See the Notes section for more details.\\n\\n    Parameters\\n    ----------\\n    y : (N,) array_like\\n        Response variable.\\n    weights : (N,) array_like or None\\n        Case weights.\\n    increasing : bool\\n        If True, fit monotonic increasing, i.e. isotonic, regression.\\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\\n        Default is True.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        The optimization result represented as a ``OptimizeResult`` object.\\n        Important attributes are:\\n\\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\\n          decreasing) array of the same length than y, with elements in the\\n          range from min(y) to max(y).\\n        - ``weights`` : Array with the sum of case weights for each block\\n          (or pool) B.\\n        - ``blocks``: Array of length B+1 with the indices of the start\\n          positions of each block (or pool) B. The j-th block is given by\\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\\n\\n    Notes\\n    -----\\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\\n    solves the following optimization problem:\\n\\n    .. math::\\n\\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\\n\\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\\n    that :math:`x` is increasing (but not strictly), i.e.\\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\\n    The solution consists of pools or blocks, i.e. neighboring elements of\\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\\n    value.\\n\\n    Most interestingly, the solution stays the same if the squared loss is\\n    replaced by the wide class of Bregman functions which are the unique\\n    class of strictly consistent scoring functions for the mean, see [2]_\\n    and references therein.\\n\\n    The implemented version of PAVA according to [1]_ has a computational\\n    complexity of O(N) with input size N.\\n\\n    References\\n    ----------\\n    .. [1] Busing, F. M. T. A. (2022).\\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\\n           :doi:`10.18637/jss.v102.c01`\\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\\n           Characterizing the optimal solutions to the isotonic regression\\n           problem for identifiable functionals.\\n           Ann Inst Stat Math 74, 489-514 (2022).\\n           :doi:`10.1007/s10463-021-00808-0`\\n\\n    Examples\\n    --------\\n    This example demonstrates that ``isotonic_regression`` really solves a\\n    constrained optimization problem.\\n\\n    >>> import numpy as np\\n    >>> from scipy.optimize import isotonic_regression, minimize\\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\\n    >>> def objective(yhat, y):\\n    ...     return np.sum((yhat - y)**2)\\n    >>> def constraint(yhat, y):\\n    ...     # This is for a monotonically increasing regression.\\n    ...     return np.diff(yhat)\\n    >>> result = minimize(objective, x0=y, args=(y,),\\n    ...                   constraints=[{'type': 'ineq',\\n    ...                                 'fun': lambda x: constraint(x, y)}])\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n    >>> result = isotonic_regression(y)\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n\\n    The big advantage of ``isotonic_regression`` compared to calling\\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\\n    define objective and constraint functions, and that it is orders of\\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\\n    input y of length 1000, the minimizer takes about 4 seconds, while\\n    ``isotonic_regression`` takes about 200 microseconds.\\n    \"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)",
            "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Nonparametric isotonic regression.\\n\\n    A (not strictly) monotonically increasing array `x` with the same length\\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\\n    [1]_. See the Notes section for more details.\\n\\n    Parameters\\n    ----------\\n    y : (N,) array_like\\n        Response variable.\\n    weights : (N,) array_like or None\\n        Case weights.\\n    increasing : bool\\n        If True, fit monotonic increasing, i.e. isotonic, regression.\\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\\n        Default is True.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        The optimization result represented as a ``OptimizeResult`` object.\\n        Important attributes are:\\n\\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\\n          decreasing) array of the same length than y, with elements in the\\n          range from min(y) to max(y).\\n        - ``weights`` : Array with the sum of case weights for each block\\n          (or pool) B.\\n        - ``blocks``: Array of length B+1 with the indices of the start\\n          positions of each block (or pool) B. The j-th block is given by\\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\\n\\n    Notes\\n    -----\\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\\n    solves the following optimization problem:\\n\\n    .. math::\\n\\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\\n\\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\\n    that :math:`x` is increasing (but not strictly), i.e.\\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\\n    The solution consists of pools or blocks, i.e. neighboring elements of\\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\\n    value.\\n\\n    Most interestingly, the solution stays the same if the squared loss is\\n    replaced by the wide class of Bregman functions which are the unique\\n    class of strictly consistent scoring functions for the mean, see [2]_\\n    and references therein.\\n\\n    The implemented version of PAVA according to [1]_ has a computational\\n    complexity of O(N) with input size N.\\n\\n    References\\n    ----------\\n    .. [1] Busing, F. M. T. A. (2022).\\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\\n           :doi:`10.18637/jss.v102.c01`\\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\\n           Characterizing the optimal solutions to the isotonic regression\\n           problem for identifiable functionals.\\n           Ann Inst Stat Math 74, 489-514 (2022).\\n           :doi:`10.1007/s10463-021-00808-0`\\n\\n    Examples\\n    --------\\n    This example demonstrates that ``isotonic_regression`` really solves a\\n    constrained optimization problem.\\n\\n    >>> import numpy as np\\n    >>> from scipy.optimize import isotonic_regression, minimize\\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\\n    >>> def objective(yhat, y):\\n    ...     return np.sum((yhat - y)**2)\\n    >>> def constraint(yhat, y):\\n    ...     # This is for a monotonically increasing regression.\\n    ...     return np.diff(yhat)\\n    >>> result = minimize(objective, x0=y, args=(y,),\\n    ...                   constraints=[{'type': 'ineq',\\n    ...                                 'fun': lambda x: constraint(x, y)}])\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n    >>> result = isotonic_regression(y)\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n\\n    The big advantage of ``isotonic_regression`` compared to calling\\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\\n    define objective and constraint functions, and that it is orders of\\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\\n    input y of length 1000, the minimizer takes about 4 seconds, while\\n    ``isotonic_regression`` takes about 200 microseconds.\\n    \"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)",
            "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Nonparametric isotonic regression.\\n\\n    A (not strictly) monotonically increasing array `x` with the same length\\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\\n    [1]_. See the Notes section for more details.\\n\\n    Parameters\\n    ----------\\n    y : (N,) array_like\\n        Response variable.\\n    weights : (N,) array_like or None\\n        Case weights.\\n    increasing : bool\\n        If True, fit monotonic increasing, i.e. isotonic, regression.\\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\\n        Default is True.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        The optimization result represented as a ``OptimizeResult`` object.\\n        Important attributes are:\\n\\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\\n          decreasing) array of the same length than y, with elements in the\\n          range from min(y) to max(y).\\n        - ``weights`` : Array with the sum of case weights for each block\\n          (or pool) B.\\n        - ``blocks``: Array of length B+1 with the indices of the start\\n          positions of each block (or pool) B. The j-th block is given by\\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\\n\\n    Notes\\n    -----\\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\\n    solves the following optimization problem:\\n\\n    .. math::\\n\\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\\n\\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\\n    that :math:`x` is increasing (but not strictly), i.e.\\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\\n    The solution consists of pools or blocks, i.e. neighboring elements of\\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\\n    value.\\n\\n    Most interestingly, the solution stays the same if the squared loss is\\n    replaced by the wide class of Bregman functions which are the unique\\n    class of strictly consistent scoring functions for the mean, see [2]_\\n    and references therein.\\n\\n    The implemented version of PAVA according to [1]_ has a computational\\n    complexity of O(N) with input size N.\\n\\n    References\\n    ----------\\n    .. [1] Busing, F. M. T. A. (2022).\\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\\n           :doi:`10.18637/jss.v102.c01`\\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\\n           Characterizing the optimal solutions to the isotonic regression\\n           problem for identifiable functionals.\\n           Ann Inst Stat Math 74, 489-514 (2022).\\n           :doi:`10.1007/s10463-021-00808-0`\\n\\n    Examples\\n    --------\\n    This example demonstrates that ``isotonic_regression`` really solves a\\n    constrained optimization problem.\\n\\n    >>> import numpy as np\\n    >>> from scipy.optimize import isotonic_regression, minimize\\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\\n    >>> def objective(yhat, y):\\n    ...     return np.sum((yhat - y)**2)\\n    >>> def constraint(yhat, y):\\n    ...     # This is for a monotonically increasing regression.\\n    ...     return np.diff(yhat)\\n    >>> result = minimize(objective, x0=y, args=(y,),\\n    ...                   constraints=[{'type': 'ineq',\\n    ...                                 'fun': lambda x: constraint(x, y)}])\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n    >>> result = isotonic_regression(y)\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n\\n    The big advantage of ``isotonic_regression`` compared to calling\\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\\n    define objective and constraint functions, and that it is orders of\\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\\n    input y of length 1000, the minimizer takes about 4 seconds, while\\n    ``isotonic_regression`` takes about 200 microseconds.\\n    \"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)",
            "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Nonparametric isotonic regression.\\n\\n    A (not strictly) monotonically increasing array `x` with the same length\\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\\n    [1]_. See the Notes section for more details.\\n\\n    Parameters\\n    ----------\\n    y : (N,) array_like\\n        Response variable.\\n    weights : (N,) array_like or None\\n        Case weights.\\n    increasing : bool\\n        If True, fit monotonic increasing, i.e. isotonic, regression.\\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\\n        Default is True.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        The optimization result represented as a ``OptimizeResult`` object.\\n        Important attributes are:\\n\\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\\n          decreasing) array of the same length than y, with elements in the\\n          range from min(y) to max(y).\\n        - ``weights`` : Array with the sum of case weights for each block\\n          (or pool) B.\\n        - ``blocks``: Array of length B+1 with the indices of the start\\n          positions of each block (or pool) B. The j-th block is given by\\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\\n\\n    Notes\\n    -----\\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\\n    solves the following optimization problem:\\n\\n    .. math::\\n\\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\\n\\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\\n    that :math:`x` is increasing (but not strictly), i.e.\\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\\n    The solution consists of pools or blocks, i.e. neighboring elements of\\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\\n    value.\\n\\n    Most interestingly, the solution stays the same if the squared loss is\\n    replaced by the wide class of Bregman functions which are the unique\\n    class of strictly consistent scoring functions for the mean, see [2]_\\n    and references therein.\\n\\n    The implemented version of PAVA according to [1]_ has a computational\\n    complexity of O(N) with input size N.\\n\\n    References\\n    ----------\\n    .. [1] Busing, F. M. T. A. (2022).\\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\\n           :doi:`10.18637/jss.v102.c01`\\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\\n           Characterizing the optimal solutions to the isotonic regression\\n           problem for identifiable functionals.\\n           Ann Inst Stat Math 74, 489-514 (2022).\\n           :doi:`10.1007/s10463-021-00808-0`\\n\\n    Examples\\n    --------\\n    This example demonstrates that ``isotonic_regression`` really solves a\\n    constrained optimization problem.\\n\\n    >>> import numpy as np\\n    >>> from scipy.optimize import isotonic_regression, minimize\\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\\n    >>> def objective(yhat, y):\\n    ...     return np.sum((yhat - y)**2)\\n    >>> def constraint(yhat, y):\\n    ...     # This is for a monotonically increasing regression.\\n    ...     return np.diff(yhat)\\n    >>> result = minimize(objective, x0=y, args=(y,),\\n    ...                   constraints=[{'type': 'ineq',\\n    ...                                 'fun': lambda x: constraint(x, y)}])\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n    >>> result = isotonic_regression(y)\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n\\n    The big advantage of ``isotonic_regression`` compared to calling\\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\\n    define objective and constraint functions, and that it is orders of\\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\\n    input y of length 1000, the minimizer takes about 4 seconds, while\\n    ``isotonic_regression`` takes about 200 microseconds.\\n    \"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)",
            "def isotonic_regression(y: npt.ArrayLike, *, weights: npt.ArrayLike | None=None, increasing: bool=True) -> OptimizeResult:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Nonparametric isotonic regression.\\n\\n    A (not strictly) monotonically increasing array `x` with the same length\\n    as `y` is calculated by the pool adjacent violators algorithm (PAVA), see\\n    [1]_. See the Notes section for more details.\\n\\n    Parameters\\n    ----------\\n    y : (N,) array_like\\n        Response variable.\\n    weights : (N,) array_like or None\\n        Case weights.\\n    increasing : bool\\n        If True, fit monotonic increasing, i.e. isotonic, regression.\\n        If False, fit a monotonic decreasing, i.e. antitonic, regression.\\n        Default is True.\\n\\n    Returns\\n    -------\\n    res : OptimizeResult\\n        The optimization result represented as a ``OptimizeResult`` object.\\n        Important attributes are:\\n\\n        - ``x``: The isotonic regression solution, i.e. an increasing (or\\n          decreasing) array of the same length than y, with elements in the\\n          range from min(y) to max(y).\\n        - ``weights`` : Array with the sum of case weights for each block\\n          (or pool) B.\\n        - ``blocks``: Array of length B+1 with the indices of the start\\n          positions of each block (or pool) B. The j-th block is given by\\n          ``x[blocks[j]:blocks[j+1]]`` for which all values are the same.\\n\\n    Notes\\n    -----\\n    Given data :math:`y` and case weights :math:`w`, the isotonic regression\\n    solves the following optimization problem:\\n\\n    .. math::\\n\\n        \\\\operatorname{argmin}_{x_i} \\\\sum_i w_i (y_i - x_i)^2 \\\\quad\\n        \\\\text{subject to } x_i \\\\leq x_j \\\\text{ whenever } i \\\\leq j \\\\,.\\n\\n    For every input value :math:`y_i`, it generates a value :math:`x_i` such\\n    that :math:`x` is increasing (but not strictly), i.e.\\n    :math:`x_i \\\\leq x_{i+1}`. This is accomplished by the PAVA.\\n    The solution consists of pools or blocks, i.e. neighboring elements of\\n    :math:`x`, e.g. :math:`x_i` and :math:`x_{i+1}`, that all have the same\\n    value.\\n\\n    Most interestingly, the solution stays the same if the squared loss is\\n    replaced by the wide class of Bregman functions which are the unique\\n    class of strictly consistent scoring functions for the mean, see [2]_\\n    and references therein.\\n\\n    The implemented version of PAVA according to [1]_ has a computational\\n    complexity of O(N) with input size N.\\n\\n    References\\n    ----------\\n    .. [1] Busing, F. M. T. A. (2022).\\n           Monotone Regression: A Simple and Fast O(n) PAVA Implementation.\\n           Journal of Statistical Software, Code Snippets, 102(1), 1-25.\\n           :doi:`10.18637/jss.v102.c01`\\n    .. [2] Jordan, A.I., M\u00fchlemann, A. & Ziegel, J.F.\\n           Characterizing the optimal solutions to the isotonic regression\\n           problem for identifiable functionals.\\n           Ann Inst Stat Math 74, 489-514 (2022).\\n           :doi:`10.1007/s10463-021-00808-0`\\n\\n    Examples\\n    --------\\n    This example demonstrates that ``isotonic_regression`` really solves a\\n    constrained optimization problem.\\n\\n    >>> import numpy as np\\n    >>> from scipy.optimize import isotonic_regression, minimize\\n    >>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\\n    >>> def objective(yhat, y):\\n    ...     return np.sum((yhat - y)**2)\\n    >>> def constraint(yhat, y):\\n    ...     # This is for a monotonically increasing regression.\\n    ...     return np.diff(yhat)\\n    >>> result = minimize(objective, x0=y, args=(y,),\\n    ...                   constraints=[{'type': 'ineq',\\n    ...                                 'fun': lambda x: constraint(x, y)}])\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n    >>> result = isotonic_regression(y)\\n    >>> result.x\\n    array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\\n           5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\\n           9.25      ])\\n\\n    The big advantage of ``isotonic_regression`` compared to calling\\n    ``minimize`` is that it is more user friendly, i.e. one does not need to\\n    define objective and constraint functions, and that it is orders of\\n    magnitudes faster. On commodity hardware (in 2023), for normal distributed\\n    input y of length 1000, the minimizer takes about 4 seconds, while\\n    ``isotonic_regression`` takes about 200 microseconds.\\n    \"\n    yarr = np.asarray(y)\n    if weights is None:\n        warr = np.ones_like(yarr)\n    else:\n        warr = np.asarray(weights)\n        if not (yarr.ndim == warr.ndim == 1 and yarr.shape[0] == warr.shape[0]):\n            raise ValueError('Input arrays y and w must have one dimension of equal length.')\n        if np.any(warr <= 0):\n            raise ValueError('Weights w must be strictly positive.')\n    order = slice(None) if increasing else slice(None, None, -1)\n    x = np.array(yarr[order], order='C', dtype=np.float64, copy=True)\n    wx = np.array(warr[order], order='C', dtype=np.float64, copy=True)\n    n = x.shape[0]\n    r = np.full(shape=n + 1, fill_value=-1, dtype=np.intp)\n    (x, wx, r, b) = pava(x, wx, r)\n    r = r[:b + 1]\n    wx = wx[:b]\n    if not increasing:\n        x = x[::-1]\n        wx = wx[::-1]\n        r = r[-1] - r[::-1]\n    return OptimizeResult(x=x, weights=wx, blocks=r)"
        ]
    }
]