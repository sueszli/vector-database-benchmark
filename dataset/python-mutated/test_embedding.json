[
    {
        "func_name": "_run_sharded_embedding",
        "original": "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)",
        "mutated": [
            "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    if False:\n        i = 10\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)",
            "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)",
            "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)",
            "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)",
            "def _run_sharded_embedding(self, spec, input_size, num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, padding_idx=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.manual_seed(0)\n    local_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx).cuda(self.rank)\n    sharded_embedding = torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_embedding.weight = clone_module_parameter(local_embedding, 'weight')\n    shard_parameter(sharded_embedding, 'weight', spec)\n    torch.manual_seed(self.rank)\n    inp = torch.randint(0, num_embeddings, tuple(input_size)).cuda(self.rank)\n    sharded_output = sharded_embedding(inp)\n    if max_norm is not None:\n        gathered_inputs = [torch.zeros_like(inp) for _ in range(TEST_GPU_NUM)]\n        dist.all_gather(gathered_inputs, inp)\n        unique_inp = torch.unique(torch.cat(gathered_inputs))\n        local_embedding(unique_inp)\n    local_output = local_embedding(inp)\n    if max_norm is not None:\n        sharded_dim = spec.dim\n        sharded_weight = sharded_embedding.weight.local_shards()[0].tensor\n        (start_pos, chunk_size) = generate_local_weight_sharding_params_for_test(local_embedding.weight, sharded_dim, TEST_GPU_NUM, spec, self.rank)\n        local_weight_narrowed = local_embedding.weight.narrow(sharded_dim, start_pos, chunk_size)\n        self.assertEqual(local_weight_narrowed, sharded_weight)\n    self.assertEqual(local_output, sharded_output)\n    local_output = torch.nn.functional.embedding(inp, local_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    sharded_output = torch.nn.functional.embedding(inp, sharded_embedding.weight, max_norm=max_norm, norm_type=norm_type, padding_idx=padding_idx)\n    self.assertEqual(local_output, sharded_output)"
        ]
    },
    {
        "func_name": "test_sharded_embedding_colwise",
        "original": "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)",
        "mutated": [
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_colwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for spec in generate_chunk_sharding_specs_for_test(1):\n        self._run_sharded_embedding(spec, [5, 4], 17, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4, 7], 23, 16)\n        self._run_sharded_embedding(spec, [4], 15, 14)\n        self._run_sharded_embedding(spec, [34], 15, 14, padding_idx=10)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 23, 13, padding_idx=12)\n        self._run_sharded_embedding(spec, [4, 5, 6], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [12, 7, 16], 23, 13, max_norm=2.5)\n        self._run_sharded_embedding(spec, [8, 16, 20], 12, 12, max_norm=1.25, norm_type=1.0)\n        self._run_sharded_embedding(spec, [30], 15, 14, max_norm=2.0)"
        ]
    },
    {
        "func_name": "test_sharded_embedding_rowwise",
        "original": "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)",
        "mutated": [
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)",
            "@with_comms(init_rpc=False)\n@skip_if_lt_x_gpu(TEST_GPU_NUM)\n@requires_nccl()\ndef test_sharded_embedding_rowwise(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for spec in generate_chunk_sharding_specs_for_test(0):\n        self._run_sharded_embedding(spec, [5, 12], 16, 22)\n        self._run_sharded_embedding(spec, [5, 4], 32, 12)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11)\n        self._run_sharded_embedding(spec, [5, 12], 16, 22, max_norm=2.5)\n        self._run_sharded_embedding(spec, [6, 7, 6], 64, 11, padding_idx=30)\n        self._run_sharded_embedding(spec, [6, 5, 3], 26, 11, max_norm=2.0)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 19, 11)\n        self._run_sharded_embedding(spec, [6, 7, 6], 21, 11)\n        self._run_sharded_embedding(spec, [4], 21, 11)\n        self._run_sharded_embedding(spec, [8, 6, 5, 4], 21, 11, padding_idx=10)\n        self._run_sharded_embedding(spec, [6, 5, 8], 28, 5, max_norm=2.0)\n        self._run_sharded_embedding(spec, [4], 14, 11, max_norm=2.5)"
        ]
    }
]