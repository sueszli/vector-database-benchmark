[
    {
        "func_name": "last",
        "original": "def last(key):\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k",
        "mutated": [
            "def last(key):\n    if False:\n        i = 10\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k",
            "def last(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k",
            "def last(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k",
            "def last(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k",
            "def last(key):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if key.size(2) < self.pre_decision_ratio:\n        return key\n    else:\n        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n        if key.size(-1) % self.pre_decision_ratio != 0:\n            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n        return k"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, args):\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError",
        "mutated": [
            "def __init__(self, args):\n    if False:\n        i = 10\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError",
            "def __init__(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError",
            "def __init__(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError",
            "def __init__(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError",
            "def __init__(self, args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.waitk_lagging = 0\n    self.num_heads = 0\n    self.noise_mean = 0.0\n    self.noise_var = 0.0\n    super().__init__(args)\n    self.pre_decision_type = args.fixed_pre_decision_type\n    self.pre_decision_ratio = args.fixed_pre_decision_ratio\n    self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n    assert self.pre_decision_ratio > 1\n    if args.fixed_pre_decision_type == 'average':\n        self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n    elif args.fixed_pre_decision_type == 'last':\n\n        def last(key):\n            if key.size(2) < self.pre_decision_ratio:\n                return key\n            else:\n                k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                if key.size(-1) % self.pre_decision_ratio != 0:\n                    k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                return k\n        self.pooling_layer = last\n    else:\n        raise NotImplementedError"
        ]
    },
    {
        "func_name": "add_args",
        "original": "@staticmethod\ndef add_args(parser):\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')",
        "mutated": [
            "@staticmethod\ndef add_args(parser):\n    if False:\n        i = 10\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')",
            "@staticmethod\ndef add_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')",
            "@staticmethod\ndef add_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')",
            "@staticmethod\ndef add_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')",
            "@staticmethod\ndef add_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n    parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n    parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n    parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')"
        ]
    },
    {
        "func_name": "insert_zeros",
        "original": "def insert_zeros(self, x):\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)",
        "mutated": [
            "def insert_zeros(self, x):\n    if False:\n        i = 10\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)",
            "def insert_zeros(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)",
            "def insert_zeros(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)",
            "def insert_zeros(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)",
            "def insert_zeros(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (bsz_num_heads, tgt_len, src_len) = x.size()\n    stride = self.pre_decision_ratio\n    weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n    x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n    return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)"
        ]
    },
    {
        "func_name": "p_choose",
        "original": "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose",
        "mutated": [
            "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    if False:\n        i = 10\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose",
            "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose",
            "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose",
            "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose",
            "def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert key is not None\n    assert query is not None\n    src_len = key.size(0)\n    tgt_len = query.size(0)\n    batch_size = query.size(1)\n    key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n    if key_padding_mask is not None:\n        key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n        key_padding_mask_pool[:, 0] = 0\n    else:\n        key_padding_mask_pool = None\n    if incremental_state is not None:\n        if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n            key_pool = key_pool[:-1]\n            if key_padding_mask_pool is not None:\n                key_padding_mask_pool = key_padding_mask_pool[:-1]\n    p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n    p_choose = self.insert_zeros(p_choose_pooled)\n    if p_choose.size(-1) < src_len:\n        p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n    else:\n        p_choose = p_choose[:, :, :src_len]\n        p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n    assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n    return p_choose"
        ]
    },
    {
        "func_name": "create_model",
        "original": "def create_model(monotonic_attention, klass):\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention",
        "mutated": [
            "def create_model(monotonic_attention, klass):\n    if False:\n        i = 10\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention",
            "def create_model(monotonic_attention, klass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention",
            "def create_model(monotonic_attention, klass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention",
            "def create_model(monotonic_attention, klass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention",
            "def create_model(monotonic_attention, klass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    class FixedStrideMonotonicAttention(monotonic_attention):\n\n        def __init__(self, args):\n            self.waitk_lagging = 0\n            self.num_heads = 0\n            self.noise_mean = 0.0\n            self.noise_var = 0.0\n            super().__init__(args)\n            self.pre_decision_type = args.fixed_pre_decision_type\n            self.pre_decision_ratio = args.fixed_pre_decision_ratio\n            self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n            assert self.pre_decision_ratio > 1\n            if args.fixed_pre_decision_type == 'average':\n                self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n            elif args.fixed_pre_decision_type == 'last':\n\n                def last(key):\n                    if key.size(2) < self.pre_decision_ratio:\n                        return key\n                    else:\n                        k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                        if key.size(-1) % self.pre_decision_ratio != 0:\n                            k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                        return k\n                self.pooling_layer = last\n            else:\n                raise NotImplementedError\n\n        @staticmethod\n        def add_args(parser):\n            super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n            parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n            parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n            parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n        def insert_zeros(self, x):\n            (bsz_num_heads, tgt_len, src_len) = x.size()\n            stride = self.pre_decision_ratio\n            weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n            x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n            return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n        def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n            assert key is not None\n            assert query is not None\n            src_len = key.size(0)\n            tgt_len = query.size(0)\n            batch_size = query.size(1)\n            key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n            if key_padding_mask is not None:\n                key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                key_padding_mask_pool[:, 0] = 0\n            else:\n                key_padding_mask_pool = None\n            if incremental_state is not None:\n                if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                    key_pool = key_pool[:-1]\n                    if key_padding_mask_pool is not None:\n                        key_padding_mask_pool = key_padding_mask_pool[:-1]\n            p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n            p_choose = self.insert_zeros(p_choose_pooled)\n            if p_choose.size(-1) < src_len:\n                p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n            else:\n                p_choose = p_choose[:, :, :src_len]\n                p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n            assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n            return p_choose\n    FixedStrideMonotonicAttention.__name__ = klass.__name__\n    return FixedStrideMonotonicAttention"
        ]
    },
    {
        "func_name": "fixed_pooling_monotonic_attention",
        "original": "def fixed_pooling_monotonic_attention(monotonic_attention):\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)",
        "mutated": [
            "def fixed_pooling_monotonic_attention(monotonic_attention):\n    if False:\n        i = 10\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)",
            "def fixed_pooling_monotonic_attention(monotonic_attention):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)",
            "def fixed_pooling_monotonic_attention(monotonic_attention):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)",
            "def fixed_pooling_monotonic_attention(monotonic_attention):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)",
            "def fixed_pooling_monotonic_attention(monotonic_attention):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def create_model(monotonic_attention, klass):\n\n        class FixedStrideMonotonicAttention(monotonic_attention):\n\n            def __init__(self, args):\n                self.waitk_lagging = 0\n                self.num_heads = 0\n                self.noise_mean = 0.0\n                self.noise_var = 0.0\n                super().__init__(args)\n                self.pre_decision_type = args.fixed_pre_decision_type\n                self.pre_decision_ratio = args.fixed_pre_decision_ratio\n                self.pre_decision_pad_threshold = args.fixed_pre_decision_pad_threshold\n                assert self.pre_decision_ratio > 1\n                if args.fixed_pre_decision_type == 'average':\n                    self.pooling_layer = torch.nn.AvgPool1d(kernel_size=self.pre_decision_ratio, stride=self.pre_decision_ratio, ceil_mode=True)\n                elif args.fixed_pre_decision_type == 'last':\n\n                    def last(key):\n                        if key.size(2) < self.pre_decision_ratio:\n                            return key\n                        else:\n                            k = key[:, :, self.pre_decision_ratio - 1::self.pre_decision_ratio].contiguous()\n                            if key.size(-1) % self.pre_decision_ratio != 0:\n                                k = torch.cat([k, key[:, :, -1:]], dim=-1).contiguous()\n                            return k\n                    self.pooling_layer = last\n                else:\n                    raise NotImplementedError\n\n            @staticmethod\n            def add_args(parser):\n                super(FixedStrideMonotonicAttention, FixedStrideMonotonicAttention).add_args(parser)\n                parser.add_argument('--fixed-pre-decision-ratio', type=int, required=True, help='Ratio for the fixed pre-decision,indicating how many encoder steps will startsimultaneous decision making process.')\n                parser.add_argument('--fixed-pre-decision-type', default='average', choices=['average', 'last'], help='Pooling type')\n                parser.add_argument('--fixed-pre-decision-pad-threshold', type=float, default=0.3, help='If a part of the sequence has pad,the threshold the pooled part is a pad.')\n\n            def insert_zeros(self, x):\n                (bsz_num_heads, tgt_len, src_len) = x.size()\n                stride = self.pre_decision_ratio\n                weight = F.pad(torch.ones(1, 1, 1).to(x), (stride - 1, 0))\n                x_upsample = F.conv_transpose1d(x.view(-1, src_len).unsqueeze(1), weight, stride=stride, padding=0)\n                return x_upsample.squeeze(1).view(bsz_num_heads, tgt_len, -1)\n\n            def p_choose(self, query: Optional[Tensor], key: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):\n                assert key is not None\n                assert query is not None\n                src_len = key.size(0)\n                tgt_len = query.size(0)\n                batch_size = query.size(1)\n                key_pool = self.pooling_layer(key.transpose(0, 2)).transpose(0, 2)\n                if key_padding_mask is not None:\n                    key_padding_mask_pool = self.pooling_layer(key_padding_mask.unsqueeze(0).float()).squeeze(0).gt(self.pre_decision_pad_threshold)\n                    key_padding_mask_pool[:, 0] = 0\n                else:\n                    key_padding_mask_pool = None\n                if incremental_state is not None:\n                    if max(1, math.floor(key.size(0) / self.pre_decision_ratio)) < key_pool.size(0):\n                        key_pool = key_pool[:-1]\n                        if key_padding_mask_pool is not None:\n                            key_padding_mask_pool = key_padding_mask_pool[:-1]\n                p_choose_pooled = self.p_choose_from_qk(query, key_pool, key_padding_mask_pool, incremental_state=incremental_state)\n                p_choose = self.insert_zeros(p_choose_pooled)\n                if p_choose.size(-1) < src_len:\n                    p_choose = torch.cat([p_choose, torch.zeros(p_choose.size(0), tgt_len, src_len - p_choose.size(-1)).to(p_choose)], dim=2)\n                else:\n                    p_choose = p_choose[:, :, :src_len]\n                    p_choose[:, :, -1] = p_choose_pooled[:, :, -1]\n                assert list(p_choose.size()) == [batch_size * self.num_heads, tgt_len, src_len]\n                return p_choose\n        FixedStrideMonotonicAttention.__name__ = klass.__name__\n        return FixedStrideMonotonicAttention\n    return partial(create_model, monotonic_attention)"
        ]
    }
]