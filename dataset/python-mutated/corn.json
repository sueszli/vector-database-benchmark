[
    {
        "func_name": "corn_loss",
        "original": "def corn_loss(logits, y_train, num_classes):\n    \"\"\"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\n    Regression based on Conditional Probabilities' manuscript.\n\n    Parameters\n    ----------\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\n        Outputs of the CORN layer.\n\n    y_train : torch.tensor, shape=(num_examples)\n        Torch tensor containing the class labels.\n\n    num_classes : int\n        Number of unique class labels (class labels should start at 0).\n\n    Returns\n    ----------\n        loss : torch.tensor\n        A torch.tensor containing a single loss value.\n\n    Examples\n    ----------\n    >>> # Consider 8 training examples\n    >>> _  = torch.manual_seed(123)\n    >>> X_train = torch.rand(8, 99)\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\n    >>> NUM_CLASSES = 5\n    >>> #\n    >>> #\n    >>> # def __init__(self):\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\n    >>> #\n    >>> #\n    >>> # def forward(self, X_train):\n    >>> logits = corn_net(X_train)\n    >>> logits.shape\n    torch.Size([8, 4])\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\n    tensor(0.7127, grad_fn=<DivBackward0>)\n    \"\"\"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples",
        "mutated": [
            "def corn_loss(logits, y_train, num_classes):\n    if False:\n        i = 10\n    \"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\\n    Regression based on Conditional Probabilities' manuscript.\\n\\n    Parameters\\n    ----------\\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\\n        Outputs of the CORN layer.\\n\\n    y_train : torch.tensor, shape=(num_examples)\\n        Torch tensor containing the class labels.\\n\\n    num_classes : int\\n        Number of unique class labels (class labels should start at 0).\\n\\n    Returns\\n    ----------\\n        loss : torch.tensor\\n        A torch.tensor containing a single loss value.\\n\\n    Examples\\n    ----------\\n    >>> # Consider 8 training examples\\n    >>> _  = torch.manual_seed(123)\\n    >>> X_train = torch.rand(8, 99)\\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\\n    >>> NUM_CLASSES = 5\\n    >>> #\\n    >>> #\\n    >>> # def __init__(self):\\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\\n    >>> #\\n    >>> #\\n    >>> # def forward(self, X_train):\\n    >>> logits = corn_net(X_train)\\n    >>> logits.shape\\n    torch.Size([8, 4])\\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\\n    tensor(0.7127, grad_fn=<DivBackward0>)\\n    \"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples",
            "def corn_loss(logits, y_train, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\\n    Regression based on Conditional Probabilities' manuscript.\\n\\n    Parameters\\n    ----------\\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\\n        Outputs of the CORN layer.\\n\\n    y_train : torch.tensor, shape=(num_examples)\\n        Torch tensor containing the class labels.\\n\\n    num_classes : int\\n        Number of unique class labels (class labels should start at 0).\\n\\n    Returns\\n    ----------\\n        loss : torch.tensor\\n        A torch.tensor containing a single loss value.\\n\\n    Examples\\n    ----------\\n    >>> # Consider 8 training examples\\n    >>> _  = torch.manual_seed(123)\\n    >>> X_train = torch.rand(8, 99)\\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\\n    >>> NUM_CLASSES = 5\\n    >>> #\\n    >>> #\\n    >>> # def __init__(self):\\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\\n    >>> #\\n    >>> #\\n    >>> # def forward(self, X_train):\\n    >>> logits = corn_net(X_train)\\n    >>> logits.shape\\n    torch.Size([8, 4])\\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\\n    tensor(0.7127, grad_fn=<DivBackward0>)\\n    \"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples",
            "def corn_loss(logits, y_train, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\\n    Regression based on Conditional Probabilities' manuscript.\\n\\n    Parameters\\n    ----------\\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\\n        Outputs of the CORN layer.\\n\\n    y_train : torch.tensor, shape=(num_examples)\\n        Torch tensor containing the class labels.\\n\\n    num_classes : int\\n        Number of unique class labels (class labels should start at 0).\\n\\n    Returns\\n    ----------\\n        loss : torch.tensor\\n        A torch.tensor containing a single loss value.\\n\\n    Examples\\n    ----------\\n    >>> # Consider 8 training examples\\n    >>> _  = torch.manual_seed(123)\\n    >>> X_train = torch.rand(8, 99)\\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\\n    >>> NUM_CLASSES = 5\\n    >>> #\\n    >>> #\\n    >>> # def __init__(self):\\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\\n    >>> #\\n    >>> #\\n    >>> # def forward(self, X_train):\\n    >>> logits = corn_net(X_train)\\n    >>> logits.shape\\n    torch.Size([8, 4])\\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\\n    tensor(0.7127, grad_fn=<DivBackward0>)\\n    \"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples",
            "def corn_loss(logits, y_train, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\\n    Regression based on Conditional Probabilities' manuscript.\\n\\n    Parameters\\n    ----------\\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\\n        Outputs of the CORN layer.\\n\\n    y_train : torch.tensor, shape=(num_examples)\\n        Torch tensor containing the class labels.\\n\\n    num_classes : int\\n        Number of unique class labels (class labels should start at 0).\\n\\n    Returns\\n    ----------\\n        loss : torch.tensor\\n        A torch.tensor containing a single loss value.\\n\\n    Examples\\n    ----------\\n    >>> # Consider 8 training examples\\n    >>> _  = torch.manual_seed(123)\\n    >>> X_train = torch.rand(8, 99)\\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\\n    >>> NUM_CLASSES = 5\\n    >>> #\\n    >>> #\\n    >>> # def __init__(self):\\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\\n    >>> #\\n    >>> #\\n    >>> # def forward(self, X_train):\\n    >>> logits = corn_net(X_train)\\n    >>> logits.shape\\n    torch.Size([8, 4])\\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\\n    tensor(0.7127, grad_fn=<DivBackward0>)\\n    \"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples",
            "def corn_loss(logits, y_train, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes the CORN loss described in our forthcoming 'Deep Neural Networks for Rank Consistent Ordinal\\n    Regression based on Conditional Probabilities' manuscript.\\n\\n    Parameters\\n    ----------\\n    logits : torch.tensor, shape=(num_examples, num_classes-1)\\n        Outputs of the CORN layer.\\n\\n    y_train : torch.tensor, shape=(num_examples)\\n        Torch tensor containing the class labels.\\n\\n    num_classes : int\\n        Number of unique class labels (class labels should start at 0).\\n\\n    Returns\\n    ----------\\n        loss : torch.tensor\\n        A torch.tensor containing a single loss value.\\n\\n    Examples\\n    ----------\\n    >>> # Consider 8 training examples\\n    >>> _  = torch.manual_seed(123)\\n    >>> X_train = torch.rand(8, 99)\\n    >>> y_train = torch.tensor([0, 1, 2, 2, 2, 3, 4, 4])\\n    >>> NUM_CLASSES = 5\\n    >>> #\\n    >>> #\\n    >>> # def __init__(self):\\n    >>> corn_net = torch.nn.Linear(99, NUM_CLASSES-1)\\n    >>> #\\n    >>> #\\n    >>> # def forward(self, X_train):\\n    >>> logits = corn_net(X_train)\\n    >>> logits.shape\\n    torch.Size([8, 4])\\n    >>> corn_loss(logits, y_train, NUM_CLASSES)\\n    tensor(0.7127, grad_fn=<DivBackward0>)\\n    \"\n    sets = []\n    for i in range(num_classes - 1):\n        label_mask = y_train > i - 1\n        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n        sets.append((label_mask, label_tensor))\n    num_examples = 0\n    losses = 0.0\n    for (task_index, s) in enumerate(sets):\n        train_examples = s[0]\n        train_labels = s[1]\n        if len(train_labels) < 1:\n            continue\n        num_examples += len(train_labels)\n        pred = logits[train_examples, task_index]\n        loss = -torch.sum(F.logsigmoid(pred) * train_labels + (F.logsigmoid(pred) - pred) * (1 - train_labels))\n        losses += loss\n    return losses / num_examples"
        ]
    }
]