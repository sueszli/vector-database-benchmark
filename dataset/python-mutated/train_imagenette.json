[
    {
        "func_name": "pr",
        "original": "def pr(s):\n    if rank_distrib() == 0:\n        print(s)",
        "mutated": [
            "def pr(s):\n    if False:\n        i = 10\n    if rank_distrib() == 0:\n        print(s)",
            "def pr(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if rank_distrib() == 0:\n        print(s)",
            "def pr(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if rank_distrib() == 0:\n        print(s)",
            "def pr(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if rank_distrib() == 0:\n        print(s)",
            "def pr(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if rank_distrib() == 0:\n        print(s)"
        ]
    },
    {
        "func_name": "get_dls",
        "original": "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)",
        "mutated": [
            "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    if False:\n        i = 10\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)",
            "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)",
            "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)",
            "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)",
            "def get_dls(size, woof, pct_noise, bs, sh=0.0, workers=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert pct_noise in [0, 5, 50], '`pct_noise` must be 0,5 or 50.'\n    if size <= 224:\n        path = URLs.IMAGEWOOF_320 if woof else URLs.IMAGENETTE_320\n    else:\n        path = URLs.IMAGEWOOF if woof else URLs.IMAGENETTE\n    source = untar_data(path)\n    workers = ifnone(workers, min(8, num_cpus()))\n    blocks = (ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    if sh:\n        batch_tfms.append(RandomErasing(p=0.3, max_count=3, sh=sh))\n    csv_file = 'noisy_imagewoof.csv' if woof else 'noisy_imagenette.csv'\n    inp = pd.read_csv(source / csv_file)\n    dblock = DataBlock(blocks=blocks, splitter=ColSplitter(), get_x=ColReader('path', pref=source), get_y=ColReader(f'noisy_labels_{pct_noise}'), item_tfms=tfms, batch_tfms=batch_tfms)\n    return dblock.dataloaders(inp, path=source, bs=bs, num_workers=workers)"
        ]
    },
    {
        "func_name": "main",
        "original": "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    \"\"\"Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`\"\"\"\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)",
        "mutated": [
            "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    if False:\n        i = 10\n    'Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`'\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)",
            "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`'\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)",
            "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`'\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)",
            "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`'\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)",
            "@call_parse\ndef main(woof: Param('Use imagewoof (otherwise imagenette)', int)=0, pct_noise: Param('Percentage of noise in training set (0,5,50%)', int)=0, lr: Param('Learning rate', float)=0.01, size: Param('Size (px: 128,192,256)', int)=128, sqrmom: Param('sqr_mom', float)=0.99, mom: Param('Momentum', float)=0.9, eps: Param('Epsilon', float)=1e-06, wd: Param('Weight decay', float)=0.01, epochs: Param('Number of epochs', int)=5, bs: Param('Batch size', int)=64, mixup: Param('Mixup', float)=0.0, opt: Param('Optimizer (adam,rms,sgd,ranger)', str)='ranger', arch: Param('Architecture', str)='xresnet50', sh: Param('Random erase max proportion', float)=0.0, sa: Param('Self-attention', store_true)=False, sym: Param('Symmetry for self-attention', int)=0, beta: Param('SAdam softplus beta', float)=0.0, act_fn: Param('Activation function', str)='Mish', fp16: Param('Use mixed precision training', store_true)=False, pool: Param('Pooling method', str)='AvgPool', dump: Param(\"Print model; don't train\", int)=0, runs: Param('Number of times to repeat training', int)=1, meta: Param('Metadata (ignored)', str)='', workers: Param('Number of workers', int)=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Training of Imagenette. Call with `python -m fastai.launch` for distributed training.\\n        Note for testing -- the following should result in accuracy top-5 of 75%+:\\n        `python train_imagenette.py --fp16 --epochs 3 --size 192`'\n    if opt == 'adam':\n        opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt == 'rms':\n        opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt == 'sgd':\n        opt_func = partial(SGD, mom=mom)\n    elif opt == 'ranger':\n        opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n    dls = rank0_first(get_dls, size, woof, pct_noise, bs, sh=sh, workers=workers)\n    pr(f'pct_noise: {pct_noise}; epochs: {epochs}; lr: {lr}; size: {size}; sqrmom: {sqrmom}; mom: {mom}; eps: {eps}')\n    (m, act_fn, pool) = [globals()[o] for o in (arch, act_fn, pool)]\n    for run in range(runs):\n        pr(f'Run: {run}')\n        learn = Learner(dls, m(n_out=10, act_cls=act_fn, sa=sa, sym=sym, pool=pool), opt_func=opt_func, metrics=[accuracy, top_k_accuracy], loss_func=LabelSmoothingCrossEntropy())\n        if dump:\n            pr(learn.model)\n            exit()\n        if fp16:\n            learn = learn.to_fp16()\n        cbs = MixUp(mixup) if mixup else []\n        n_gpu = torch.cuda.device_count()\n        ctx = learn.distrib_ctx if num_distrib() and n_gpu else learn.parallel_ctx\n        with ctx():\n            learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)"
        ]
    }
]