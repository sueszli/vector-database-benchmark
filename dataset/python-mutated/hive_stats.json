[
    {
        "func_name": "__init__",
        "original": "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'",
        "mutated": [
            "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if False:\n        i = 10\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'",
            "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'",
            "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'",
            "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'",
            "def __init__(self, *, table: str, partition: Any, extra_exprs: dict[str, Any] | None=None, excluded_columns: list[str] | None=None, assignment_func: Callable[[str, str], dict[Any, Any] | None] | None=None, metastore_conn_id: str='metastore_default', presto_conn_id: str='presto_default', mysql_conn_id: str='airflow_db', **kwargs: Any) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'col_blacklist' in kwargs:\n        warnings.warn(f\"col_blacklist kwarg passed to {self.__class__.__name__} (task_id: {kwargs.get('task_id')}) is deprecated, please rename it to excluded_columns instead\", category=FutureWarning, stacklevel=2)\n        excluded_columns = kwargs.pop('col_blacklist')\n    super().__init__(**kwargs)\n    self.table = table\n    self.partition = partition\n    self.extra_exprs = extra_exprs or {}\n    self.excluded_columns: list[str] = excluded_columns or []\n    self.metastore_conn_id = metastore_conn_id\n    self.presto_conn_id = presto_conn_id\n    self.mysql_conn_id = mysql_conn_id\n    self.assignment_func = assignment_func\n    self.ds = '{{ ds }}'\n    self.dttm = '{{ execution_date.isoformat() }}'"
        ]
    },
    {
        "func_name": "get_default_exprs",
        "original": "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    \"\"\"Get default expressions.\"\"\"\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp",
        "mutated": [
            "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    if False:\n        i = 10\n    'Get default expressions.'\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp",
            "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get default expressions.'\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp",
            "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get default expressions.'\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp",
            "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get default expressions.'\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp",
            "def get_default_exprs(self, col: str, col_type: str) -> dict[Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get default expressions.'\n    if col in self.excluded_columns:\n        return {}\n    exp = {(col, 'non_null'): f'COUNT({col})'}\n    if col_type in {'double', 'int', 'bigint', 'float'}:\n        exp[col, 'sum'] = f'SUM({col})'\n        exp[col, 'min'] = f'MIN({col})'\n        exp[col, 'max'] = f'MAX({col})'\n        exp[col, 'avg'] = f'AVG({col})'\n    elif col_type == 'boolean':\n        exp[col, 'true'] = f'SUM(CASE WHEN {col} THEN 1 ELSE 0 END)'\n        exp[col, 'false'] = f'SUM(CASE WHEN NOT {col} THEN 1 ELSE 0 END)'\n    elif col_type == 'string':\n        exp[col, 'len'] = f'SUM(CAST(LENGTH({col}) AS BIGINT))'\n        exp[col, 'approx_distinct'] = f'APPROX_DISTINCT({col})'\n    return exp"
        ]
    },
    {
        "func_name": "execute",
        "original": "def execute(self, context: Context) -> None:\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])",
        "mutated": [
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])",
            "def execute(self, context: Context) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    metastore = HiveMetastoreHook(metastore_conn_id=self.metastore_conn_id)\n    table = metastore.get_table(table_name=self.table)\n    field_types = {col.name: col.type for col in table.sd.cols}\n    exprs: Any = {('', 'count'): 'COUNT(*)'}\n    for (col, col_type) in list(field_types.items()):\n        if self.assignment_func:\n            assign_exprs = self.assignment_func(col, col_type)\n            if assign_exprs is None:\n                assign_exprs = self.get_default_exprs(col, col_type)\n        else:\n            assign_exprs = self.get_default_exprs(col, col_type)\n        exprs.update(assign_exprs)\n    exprs.update(self.extra_exprs)\n    exprs_str = ',\\n        '.join((f'{v} AS {k[0]}__{k[1]}' for (k, v) in exprs.items()))\n    where_clause_ = [f\"{k} = '{v}'\" for (k, v) in self.partition.items()]\n    where_clause = ' AND\\n        '.join(where_clause_)\n    sql = f'SELECT {exprs_str} FROM {self.table} WHERE {where_clause};'\n    presto = PrestoHook(presto_conn_id=self.presto_conn_id)\n    self.log.info('Executing SQL check: %s', sql)\n    row = presto.get_first(sql)\n    self.log.info('Record: %s', row)\n    if not row:\n        raise AirflowException('The query returned None')\n    part_json = json.dumps(self.partition, sort_keys=True)\n    self.log.info('Deleting rows from previous runs if they exist')\n    mysql = MySqlHook(self.mysql_conn_id)\n    sql = f\"\\n        SELECT 1 FROM hive_stats\\n        WHERE\\n            table_name='{self.table}' AND\\n            partition_repr='{part_json}' AND\\n            dttm='{self.dttm}'\\n        LIMIT 1;\\n        \"\n    if mysql.get_records(sql):\n        sql = f\"\\n            DELETE FROM hive_stats\\n            WHERE\\n                table_name='{self.table}' AND\\n                partition_repr='{part_json}' AND\\n                dttm='{self.dttm}';\\n            \"\n        mysql.run(sql)\n    self.log.info('Pivoting and loading cells into the Airflow db')\n    rows = [(self.ds, self.dttm, self.table, part_json) + (r[0][0], r[0][1], r[1]) for r in zip(exprs, row)]\n    mysql.insert_rows(table='hive_stats', rows=rows, target_fields=['ds', 'dttm', 'table_name', 'partition_repr', 'col', 'metric', 'value'])"
        ]
    }
]