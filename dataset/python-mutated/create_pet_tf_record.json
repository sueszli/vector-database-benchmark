[
    {
        "func_name": "get_class_name_from_filename",
        "original": "def get_class_name_from_filename(file_name):\n    \"\"\"Gets the class name from a file.\n\n  Args:\n    file_name: The file name to get the class name from.\n               ie. \"american_pit_bull_terrier_105.jpg\"\n\n  Returns:\n    A string of the class name.\n  \"\"\"\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]",
        "mutated": [
            "def get_class_name_from_filename(file_name):\n    if False:\n        i = 10\n    'Gets the class name from a file.\\n\\n  Args:\\n    file_name: The file name to get the class name from.\\n               ie. \"american_pit_bull_terrier_105.jpg\"\\n\\n  Returns:\\n    A string of the class name.\\n  '\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]",
            "def get_class_name_from_filename(file_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Gets the class name from a file.\\n\\n  Args:\\n    file_name: The file name to get the class name from.\\n               ie. \"american_pit_bull_terrier_105.jpg\"\\n\\n  Returns:\\n    A string of the class name.\\n  '\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]",
            "def get_class_name_from_filename(file_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Gets the class name from a file.\\n\\n  Args:\\n    file_name: The file name to get the class name from.\\n               ie. \"american_pit_bull_terrier_105.jpg\"\\n\\n  Returns:\\n    A string of the class name.\\n  '\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]",
            "def get_class_name_from_filename(file_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Gets the class name from a file.\\n\\n  Args:\\n    file_name: The file name to get the class name from.\\n               ie. \"american_pit_bull_terrier_105.jpg\"\\n\\n  Returns:\\n    A string of the class name.\\n  '\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]",
            "def get_class_name_from_filename(file_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Gets the class name from a file.\\n\\n  Args:\\n    file_name: The file name to get the class name from.\\n               ie. \"american_pit_bull_terrier_105.jpg\"\\n\\n  Returns:\\n    A string of the class name.\\n  '\n    match = re.match('([A-Za-z_]+)(_[0-9]+\\\\.jpg)', file_name, re.I)\n    return match.groups()[0]"
        ]
    },
    {
        "func_name": "dict_to_tf_example",
        "original": "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    \"\"\"Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    mask_path: String path to PNG encoded mask.\n    label_map_dict: A map from string label names to integers ids.\n    image_subdirectory: String specifying subdirectory within the\n      Pascal dataset directory holding the actual image data.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\n      generates bounding boxes (as well as segmentations for full pet bodies).\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\n      smaller file sizes.\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n  \"\"\"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example",
        "mutated": [
            "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n    \"Convert XML derived dict to tf.Example proto.\\n\\n  Notice that this function normalizes the bounding box coordinates provided\\n  by the raw data.\\n\\n  Args:\\n    data: dict holding PASCAL XML fields for a single image (obtained by\\n      running dataset_util.recursive_parse_xml_to_dict)\\n    mask_path: String path to PNG encoded mask.\\n    label_map_dict: A map from string label names to integers ids.\\n    image_subdirectory: String specifying subdirectory within the\\n      Pascal dataset directory holding the actual image data.\\n    ignore_difficult_instances: Whether to skip difficult instances in the\\n      dataset  (default: False).\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n\\n  Returns:\\n    example: The converted tf.Example.\\n\\n  Raises:\\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\\n  \"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example",
            "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Convert XML derived dict to tf.Example proto.\\n\\n  Notice that this function normalizes the bounding box coordinates provided\\n  by the raw data.\\n\\n  Args:\\n    data: dict holding PASCAL XML fields for a single image (obtained by\\n      running dataset_util.recursive_parse_xml_to_dict)\\n    mask_path: String path to PNG encoded mask.\\n    label_map_dict: A map from string label names to integers ids.\\n    image_subdirectory: String specifying subdirectory within the\\n      Pascal dataset directory holding the actual image data.\\n    ignore_difficult_instances: Whether to skip difficult instances in the\\n      dataset  (default: False).\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n\\n  Returns:\\n    example: The converted tf.Example.\\n\\n  Raises:\\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\\n  \"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example",
            "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Convert XML derived dict to tf.Example proto.\\n\\n  Notice that this function normalizes the bounding box coordinates provided\\n  by the raw data.\\n\\n  Args:\\n    data: dict holding PASCAL XML fields for a single image (obtained by\\n      running dataset_util.recursive_parse_xml_to_dict)\\n    mask_path: String path to PNG encoded mask.\\n    label_map_dict: A map from string label names to integers ids.\\n    image_subdirectory: String specifying subdirectory within the\\n      Pascal dataset directory holding the actual image data.\\n    ignore_difficult_instances: Whether to skip difficult instances in the\\n      dataset  (default: False).\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n\\n  Returns:\\n    example: The converted tf.Example.\\n\\n  Raises:\\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\\n  \"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example",
            "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Convert XML derived dict to tf.Example proto.\\n\\n  Notice that this function normalizes the bounding box coordinates provided\\n  by the raw data.\\n\\n  Args:\\n    data: dict holding PASCAL XML fields for a single image (obtained by\\n      running dataset_util.recursive_parse_xml_to_dict)\\n    mask_path: String path to PNG encoded mask.\\n    label_map_dict: A map from string label names to integers ids.\\n    image_subdirectory: String specifying subdirectory within the\\n      Pascal dataset directory holding the actual image data.\\n    ignore_difficult_instances: Whether to skip difficult instances in the\\n      dataset  (default: False).\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n\\n  Returns:\\n    example: The converted tf.Example.\\n\\n  Raises:\\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\\n  \"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example",
            "def dict_to_tf_example(data, mask_path, label_map_dict, image_subdirectory, ignore_difficult_instances=False, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Convert XML derived dict to tf.Example proto.\\n\\n  Notice that this function normalizes the bounding box coordinates provided\\n  by the raw data.\\n\\n  Args:\\n    data: dict holding PASCAL XML fields for a single image (obtained by\\n      running dataset_util.recursive_parse_xml_to_dict)\\n    mask_path: String path to PNG encoded mask.\\n    label_map_dict: A map from string label names to integers ids.\\n    image_subdirectory: String specifying subdirectory within the\\n      Pascal dataset directory holding the actual image data.\\n    ignore_difficult_instances: Whether to skip difficult instances in the\\n      dataset  (default: False).\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n\\n  Returns:\\n    example: The converted tf.Example.\\n\\n  Raises:\\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\\n  \"\n    img_path = os.path.join(image_subdirectory, data['filename'])\n    with tf.gfile.GFile(img_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = PIL.Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n    key = hashlib.sha256(encoded_jpg).hexdigest()\n    with tf.gfile.GFile(mask_path, 'rb') as fid:\n        encoded_mask_png = fid.read()\n    encoded_png_io = io.BytesIO(encoded_mask_png)\n    mask = PIL.Image.open(encoded_png_io)\n    if mask.format != 'PNG':\n        raise ValueError('Mask format not PNG')\n    mask_np = np.asarray(mask)\n    nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n    nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n    nonzero_x_indices = np.where(nonbackground_indices_x)\n    nonzero_y_indices = np.where(nonbackground_indices_y)\n    width = int(data['size']['width'])\n    height = int(data['size']['height'])\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    classes = []\n    classes_text = []\n    truncated = []\n    poses = []\n    difficult_obj = []\n    masks = []\n    if 'object' in data:\n        for obj in data['object']:\n            difficult = bool(int(obj['difficult']))\n            if ignore_difficult_instances and difficult:\n                continue\n            difficult_obj.append(int(difficult))\n            if faces_only:\n                xmin = float(obj['bndbox']['xmin'])\n                xmax = float(obj['bndbox']['xmax'])\n                ymin = float(obj['bndbox']['ymin'])\n                ymax = float(obj['bndbox']['ymax'])\n            else:\n                xmin = float(np.min(nonzero_x_indices))\n                xmax = float(np.max(nonzero_x_indices))\n                ymin = float(np.min(nonzero_y_indices))\n                ymax = float(np.max(nonzero_y_indices))\n            xmins.append(xmin / width)\n            ymins.append(ymin / height)\n            xmaxs.append(xmax / width)\n            ymaxs.append(ymax / height)\n            class_name = get_class_name_from_filename(data['filename'])\n            classes_text.append(class_name.encode('utf8'))\n            classes.append(label_map_dict[class_name])\n            truncated.append(int(obj['truncated']))\n            poses.append(obj['pose'].encode('utf8'))\n            if not faces_only:\n                mask_remapped = (mask_np != 2).astype(np.uint8)\n                masks.append(mask_remapped)\n    feature_dict = {'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/source_id': dataset_util.bytes_feature(data['filename'].encode('utf8')), 'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), 'image/object/difficult': dataset_util.int64_list_feature(difficult_obj), 'image/object/truncated': dataset_util.int64_list_feature(truncated), 'image/object/view': dataset_util.bytes_list_feature(poses)}\n    if not faces_only:\n        if mask_type == 'numerical':\n            mask_stack = np.stack(masks).astype(np.float32)\n            masks_flattened = np.reshape(mask_stack, [-1])\n            feature_dict['image/object/mask'] = dataset_util.float_list_feature(masks_flattened.tolist())\n        elif mask_type == 'png':\n            encoded_mask_png_list = []\n            for mask in masks:\n                img = PIL.Image.fromarray(mask)\n                output = io.BytesIO()\n                img.save(output, format='PNG')\n                encoded_mask_png_list.append(output.getvalue())\n            feature_dict['image/object/mask'] = dataset_util.bytes_list_feature(encoded_mask_png_list)\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example"
        ]
    },
    {
        "func_name": "create_tf_record",
        "original": "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    \"\"\"Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    num_shards: Number of shards for output file.\n    label_map_dict: The label map dictionary.\n    annotations_dir: Directory where annotation files are stored.\n    image_dir: Directory where image files are stored.\n    examples: Examples to parse and save to tf record.\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\n      generates bounding boxes (as well as segmentations for full pet bodies).\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\n      smaller file sizes.\n  \"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)",
        "mutated": [
            "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n    \"Creates a TFRecord file from examples.\\n\\n  Args:\\n    output_filename: Path to where output file is saved.\\n    num_shards: Number of shards for output file.\\n    label_map_dict: The label map dictionary.\\n    annotations_dir: Directory where annotation files are stored.\\n    image_dir: Directory where image files are stored.\\n    examples: Examples to parse and save to tf record.\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n  \"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)",
            "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Creates a TFRecord file from examples.\\n\\n  Args:\\n    output_filename: Path to where output file is saved.\\n    num_shards: Number of shards for output file.\\n    label_map_dict: The label map dictionary.\\n    annotations_dir: Directory where annotation files are stored.\\n    image_dir: Directory where image files are stored.\\n    examples: Examples to parse and save to tf record.\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n  \"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)",
            "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Creates a TFRecord file from examples.\\n\\n  Args:\\n    output_filename: Path to where output file is saved.\\n    num_shards: Number of shards for output file.\\n    label_map_dict: The label map dictionary.\\n    annotations_dir: Directory where annotation files are stored.\\n    image_dir: Directory where image files are stored.\\n    examples: Examples to parse and save to tf record.\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n  \"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)",
            "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Creates a TFRecord file from examples.\\n\\n  Args:\\n    output_filename: Path to where output file is saved.\\n    num_shards: Number of shards for output file.\\n    label_map_dict: The label map dictionary.\\n    annotations_dir: Directory where annotation files are stored.\\n    image_dir: Directory where image files are stored.\\n    examples: Examples to parse and save to tf record.\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n  \"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)",
            "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir, image_dir, examples, faces_only=True, mask_type='png'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Creates a TFRecord file from examples.\\n\\n  Args:\\n    output_filename: Path to where output file is saved.\\n    num_shards: Number of shards for output file.\\n    label_map_dict: The label map dictionary.\\n    annotations_dir: Directory where annotation files are stored.\\n    image_dir: Directory where image files are stored.\\n    examples: Examples to parse and save to tf record.\\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\\n      generates bounding boxes (as well as segmentations for full pet bodies).\\n    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to\\n      smaller file sizes.\\n  \"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack, output_filename, num_shards)\n        for (idx, example) in enumerate(examples):\n            if idx % 100 == 0:\n                logging.info('On image %d of %d', idx, len(examples))\n            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n            mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n            if not os.path.exists(xml_path):\n                logging.warning('Could not find %s, ignoring example.', xml_path)\n                continue\n            with tf.gfile.GFile(xml_path, 'r') as fid:\n                xml_str = fid.read()\n            xml = etree.fromstring(xml_str)\n            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n            try:\n                tf_example = dict_to_tf_example(data, mask_path, label_map_dict, image_dir, faces_only=faces_only, mask_type=mask_type)\n                if tf_example:\n                    shard_idx = idx % num_shards\n                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n            except ValueError:\n                logging.warning('Invalid example: %s, ignoring.', xml_path)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(_):\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)",
        "mutated": [
            "def main(_):\n    if False:\n        i = 10\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)",
            "def main(_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    data_dir = FLAGS.data_dir\n    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n    logging.info('Reading from Pet dataset.')\n    image_dir = os.path.join(data_dir, 'images')\n    annotations_dir = os.path.join(data_dir, 'annotations')\n    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n    examples_list = dataset_util.read_examples_list(examples_path)\n    random.seed(42)\n    random.shuffle(examples_list)\n    num_examples = len(examples_list)\n    num_train = int(0.7 * num_examples)\n    train_examples = examples_list[:num_train]\n    val_examples = examples_list[num_train:]\n    logging.info('%d training and %d validation examples.', len(train_examples), len(val_examples))\n    train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')\n    val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')\n    if not FLAGS.faces_only:\n        train_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_train.record')\n        val_output_path = os.path.join(FLAGS.output_dir, 'pets_fullbody_with_masks_val.record')\n    create_tf_record(train_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, train_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)\n    create_tf_record(val_output_path, FLAGS.num_shards, label_map_dict, annotations_dir, image_dir, val_examples, faces_only=FLAGS.faces_only, mask_type=FLAGS.mask_type)"
        ]
    }
]