[
    {
        "func_name": "get_xclip_config",
        "original": "def get_xclip_config(model_name, num_frames):\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config",
        "mutated": [
            "def get_xclip_config(model_name, num_frames):\n    if False:\n        i = 10\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config",
            "def get_xclip_config(model_name, num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config",
            "def get_xclip_config(model_name, num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config",
            "def get_xclip_config(model_name, num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config",
            "def get_xclip_config(model_name, num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    text_config = XCLIPTextConfig()\n    start_idx = model_name.find('patch')\n    patch_size = int(model_name[start_idx + len('patch'):start_idx + len('patch') + 2])\n    vision_config = XCLIPVisionConfig(patch_size=patch_size, num_frames=num_frames)\n    if 'large' in model_name:\n        text_config.hidden_size = 768\n        text_config.intermediate_size = 3072\n        text_config.num_attention_heads = 12\n        vision_config.hidden_size = 1024\n        vision_config.intermediate_size = 4096\n        vision_config.num_attention_heads = 16\n        vision_config.num_hidden_layers = 24\n        vision_config.mit_hidden_size = 768\n        vision_config.mit_intermediate_size = 3072\n    if model_name == 'xclip-large-patch14-16-frames':\n        vision_config.image_size = 336\n    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n    if 'large' in model_name:\n        config.projection_dim = 768\n    return config"
        ]
    },
    {
        "func_name": "rename_key",
        "original": "def rename_key(name):\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name",
        "mutated": [
            "def rename_key(name):\n    if False:\n        i = 10\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name",
            "def rename_key(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name",
            "def rename_key(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name",
            "def rename_key(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name",
            "def rename_key(name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name == 'token_embedding.weight':\n        name = name.replace('token_embedding.weight', 'text_model.embeddings.token_embedding.weight')\n    if name == 'positional_embedding':\n        name = name.replace('positional_embedding', 'text_model.embeddings.position_embedding.weight')\n    if 'ln_1' in name:\n        name = name.replace('ln_1', 'layer_norm1')\n    if 'ln_2' in name:\n        name = name.replace('ln_2', 'layer_norm2')\n    if 'c_fc' in name:\n        name = name.replace('c_fc', 'fc1')\n    if 'c_proj' in name:\n        name = name.replace('c_proj', 'fc2')\n    if name.startswith('transformer.resblocks'):\n        name = name.replace('transformer.resblocks', 'text_model.encoder.layers')\n    if 'attn.out_proj' in name and 'message' not in name:\n        name = name.replace('attn.out_proj', 'self_attn.out_proj')\n    if 'ln_final' in name:\n        name = name.replace('ln_final', 'text_model.final_layer_norm')\n    if name == 'visual.class_embedding':\n        name = name.replace('visual.class_embedding', 'vision_model.embeddings.class_embedding')\n    if name == 'visual.positional_embedding':\n        name = name.replace('visual.positional_embedding', 'vision_model.embeddings.position_embedding.weight')\n    if name.startswith('visual.transformer.resblocks'):\n        name = name.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')\n    if 'visual.conv1' in name:\n        name = name.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')\n    if 'visual.ln_pre' in name:\n        name = name.replace('visual.ln_pre', 'vision_model.pre_layernorm')\n    if 'visual.ln_post' in name:\n        name = name.replace('visual.ln_post', 'vision_model.post_layernorm')\n    if 'visual.proj' in name:\n        name = name.replace('visual.proj', 'visual_projection.weight')\n    if 'text_projection' in name:\n        name = name.replace('text_projection', 'text_projection.weight')\n    if 'prompts_visual_proj' in name:\n        name = name.replace('prompts_visual_proj', 'prompts_visual_projection')\n    if 'prompts_visual_ln' in name:\n        name = name.replace('prompts_visual_ln', 'prompts_visual_layernorm')\n    if name == 'mit.positional_embedding':\n        name = name.replace('positional', 'position')\n    if name.startswith('mit.resblocks'):\n        name = name.replace('mit.resblocks', 'mit.encoder.layers')\n    if name.startswith('prompts_generator.norm'):\n        name = name.replace('prompts_generator.norm', 'prompts_generator.layernorm')\n    return name"
        ]
    },
    {
        "func_name": "convert_state_dict",
        "original": "def convert_state_dict(orig_state_dict, config):\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict",
        "mutated": [
            "def convert_state_dict(orig_state_dict, config):\n    if False:\n        i = 10\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict",
            "def convert_state_dict(orig_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict",
            "def convert_state_dict(orig_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict",
            "def convert_state_dict(orig_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict",
            "def convert_state_dict(orig_state_dict, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for key in orig_state_dict.copy().keys():\n        val = orig_state_dict.pop(key)\n        if 'attn.in_proj' in key:\n            key_split = key.split('.')\n            if key.startswith('visual'):\n                layer_num = key_split[3]\n                dim = config.vision_config.hidden_size\n                if 'message_attn' in key:\n                    if 'weight' in key:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.weight'] = val[:dim, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.weight'] = val[-dim:, :]\n                    else:\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.q_proj.bias'] = val[:dim]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.k_proj.bias'] = val[dim:dim * 2]\n                        orig_state_dict[f'vision_model.encoder.layers.{layer_num}.message_attn.v_proj.bias'] = val[-dim:]\n                elif 'weight' in key:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'vision_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            elif key.startswith('mit'):\n                layer_num = key_split[2]\n                dim = config.vision_config.mit_hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'mit.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n            else:\n                layer_num = key_split[2]\n                dim = config.text_config.hidden_size\n                if 'weight' in key:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.weight'] = val[:dim, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.weight'] = val[dim:dim * 2, :]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.weight'] = val[-dim:, :]\n                else:\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.q_proj.bias'] = val[:dim]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.k_proj.bias'] = val[dim:dim * 2]\n                    orig_state_dict[f'text_model.encoder.layers.{layer_num}.self_attn.v_proj.bias'] = val[-dim:]\n        else:\n            new_key_name = rename_key(key)\n            if new_key_name in ['visual_projection.weight', 'text_projection.weight']:\n                val = val.T\n            orig_state_dict[new_key_name] = val\n    return orig_state_dict"
        ]
    },
    {
        "func_name": "prepare_video",
        "original": "def prepare_video(num_frames):\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)",
        "mutated": [
            "def prepare_video(num_frames):\n    if False:\n        i = 10\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)",
            "def prepare_video(num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)",
            "def prepare_video(num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)",
            "def prepare_video(num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)",
            "def prepare_video(num_frames):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if num_frames == 8:\n        filename = 'eating_spaghetti_8_frames.npy'\n    elif num_frames == 16:\n        filename = 'eating_spaghetti.npy'\n    elif num_frames == 32:\n        filename = 'eating_spaghetti_32_frames.npy'\n    file = hf_hub_download(repo_id='hf-internal-testing/spaghetti-video', filename=filename, repo_type='dataset')\n    video = np.load(file)\n    return list(video)"
        ]
    },
    {
        "func_name": "convert_xclip_checkpoint",
        "original": "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')",
        "mutated": [
            "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    if False:\n        i = 10\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')",
            "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')",
            "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')",
            "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')",
            "def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_to_url = {'xclip-base-patch32': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_8.pth', 'xclip-base-patch32-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_32_16.pth', 'xclip-base-patch16': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_8.pth', 'xclip-base-patch16-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k400_16_16.pth', 'xclip-large-patch14': 'https://drive.google.com/u/0/uc?id=1NUOImq0o5DlQTST17iIP3vG7DgmHQuCx&amp;export=download&amp;confirm=t&amp;uuid=b26caedc-88e2-473e-830a-9d158b653cdb', 'xclip-large-patch14-16-frames': 'https://drive.google.com/u/0/uc?id=1FOYgnJc097OJ4lGwtRCCydQyVPJEOH7d&amp;export=download&amp;confirm=t&amp;uuid=538fa810-e671-4050-b385-9a623f89804f', 'xclip-base-patch16-kinetics-600': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_8.pth', 'xclip-base-patch16-kinetics-600-16-frames': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/k600_16_16.pth', 'xclip-large-patch14-kinetics-600': 'https://drive.google.com/u/0/uc?id=1FV8C1INuM91sLAN4ImjzePLIlpMSihwV&amp;export=download&amp;confirm=t&amp;uuid=141d4977-4a65-44ae-864f-4b0c19f838be', 'xclip-base-patch16-hmdb-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_2.pth', 'xclip-base-patch16-hmdb-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_4.pth', 'xclip-base-patch16-hmdb-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_8.pth', 'xclip-base-patch16-hmdb-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_hmdb_16.pth', 'xclip-base-patch16-ucf-2-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_2.pth', 'xclip-base-patch16-ucf-4-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_4.pth', 'xclip-base-patch16-ucf-8-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_8.pth', 'xclip-base-patch16-ucf-16-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/few_ucf_16.pth', 'xclip-base-patch16-zero-shot': 'https://github.com/nbl97/X-CLIP_Model_Zoo/releases/download/v1.0/zero.pth'}\n    checkpoint_url = model_to_url[model_name]\n    num_frames = 8\n    if '16-frames' in model_name:\n        num_frames = 16\n    elif 'shot' in model_name:\n        num_frames = 32\n    config = get_xclip_config(model_name, num_frames)\n    model = XCLIPModel(config)\n    model.eval()\n    if 'drive' in checkpoint_url:\n        output = 'pytorch_model.bin'\n        gdown.cached_download(checkpoint_url, output, quiet=False)\n        state_dict = torch.load(output, map_location='cpu')['model']\n    else:\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)['model']\n    state_dict = convert_state_dict(state_dict, config)\n    model = XCLIPModel(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n    model.eval()\n    size = 336 if model_name == 'xclip-large-patch14-16-frames' else 224\n    image_processor = VideoMAEImageProcessor(size=size)\n    slow_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')\n    fast_tokenizer = CLIPTokenizerFast.from_pretrained('openai/clip-vit-base-patch32')\n    processor = XCLIPProcessor(image_processor=image_processor, tokenizer=fast_tokenizer)\n    video = prepare_video(num_frames)\n    inputs = processor(text=['playing sports', 'eating spaghetti', 'go shopping'], videos=video, return_tensors='pt', padding=True)\n    print('Shape of pixel values:', inputs.pixel_values.shape)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits_per_video = outputs.logits_per_video\n    probs = logits_per_video.softmax(dim=1)\n    print('Probs:', probs)\n    if model_name == 'xclip-base-patch32':\n        expected_probs = torch.tensor([[0.0019, 0.9951, 0.003]])\n    elif model_name == 'xclip-base-patch32-16-frames':\n        expected_probs = torch.tensor([[0.00070999, 0.99883, 0.0004558]])\n    elif model_name == 'xclip-base-patch16':\n        expected_probs = torch.tensor([[0.0083, 0.9681, 0.0236]])\n    elif model_name == 'xclip-base-patch16-16-frames':\n        expected_probs = torch.tensor([[0.00076937, 0.99728, 0.0019473]])\n    elif model_name == 'xclip-large-patch14':\n        expected_probs = torch.tensor([[0.0062, 0.9864, 0.0075]])\n    elif model_name == 'xclip-large-patch14-16-frames':\n        expected_probs = torch.tensor([[0.00033877, 0.99937, 0.00028888]])\n    elif model_name == 'xclip-base-patch16-kinetics-600':\n        expected_probs = torch.tensor([[0.0555, 0.8914, 0.0531]])\n    elif model_name == 'xclip-base-patch16-kinetics-600-16-frames':\n        expected_probs = torch.tensor([[0.00038554, 0.99929, 0.00032754]])\n    elif model_name == 'xclip-large-patch14-kinetics-600':\n        expected_probs = torch.tensor([[0.0036, 0.992, 0.0045]])\n    elif model_name == 'xclip-base-patch16-hmdb-2-shot':\n        expected_probs = torch.tensor([[7.189e-06, 0.99994, 5.6559e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-4-shot':\n        expected_probs = torch.tensor([[1.032e-05, 0.99993, 6.2435e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-8-shot':\n        expected_probs = torch.tensor([[4.1377e-06, 0.9999, 9.8386e-05]])\n    elif model_name == 'xclip-base-patch16-hmdb-16-shot':\n        expected_probs = torch.tensor([[4.1347e-05, 0.99962, 0.00033411]])\n    elif model_name == 'xclip-base-patch16-ucf-2-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-4-shot':\n        expected_probs = torch.tensor([[8.5857e-05, 0.99928, 0.00063291]])\n    elif model_name == 'xclip-base-patch16-ucf-8-shot':\n        expected_probs = torch.tensor([[0.0027, 0.9904, 0.007]])\n    elif model_name == 'xclip-base-patch16-ucf-16-shot':\n        expected_probs = torch.tensor([[0.00098219, 0.99593, 0.0030863]])\n    elif model_name == 'xclip-base-patch16-zero-shot':\n        expected_probs = torch.tensor([[0.00035082, 0.99785, 0.0017966]])\n    else:\n        raise ValueError(f'Model name {model_name} not supported')\n    assert torch.allclose(probs, expected_probs, atol=0.001)\n    print('Looks ok!')\n    if pytorch_dump_folder_path is not None:\n        print(f'Saving model {model_name} to {pytorch_dump_folder_path}')\n        model.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print('Pushing model, processor and slow tokenizer files to the hub...')\n        model.push_to_hub(model_name, organization='nielsr')\n        processor.push_to_hub(model_name, organization='nielsr')\n        slow_tokenizer.push_to_hub(model_name, organization='nielsr')"
        ]
    }
]