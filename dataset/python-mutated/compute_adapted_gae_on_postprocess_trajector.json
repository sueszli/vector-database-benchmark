[
    {
        "func_name": "on_postprocess_trajectory",
        "original": "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)",
        "mutated": [
            "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    if False:\n        i = 10\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)",
            "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)",
            "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)",
            "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)",
            "@override(DefaultCallbacks)\ndef on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().on_postprocess_trajectory(worker=worker, episode=episode, agent_id=agent_id, policy_id=policy_id, policies=policies, postprocessed_batch=postprocessed_batch, original_batches=original_batches, **kwargs)\n    if policies[policy_id].config.get('use_adapted_gae', False):\n        policy = policies[policy_id]\n        assert policy.config['use_gae'], \"Can't use adapted gae without use_gae=True!\"\n        info_dicts = postprocessed_batch[SampleBatch.INFOS]\n        assert np.all(['d_ts' in info_dict for info_dict in info_dicts]), \"Info dicts in sample batch must contain data 'd_ts'                 (=ts[i+1]-ts[i] length of time steps)!\"\n        d_ts = np.array([np.float(info_dict.get('d_ts')) for info_dict in info_dicts])\n        assert np.all([e.is_integer() for e in d_ts]), \"Elements of 'd_ts' (length of time steps) must be integer!\"\n        if postprocessed_batch[SampleBatch.TERMINATEDS][-1]:\n            last_r = 0.0\n        else:\n            input_dict = postprocessed_batch.get_single_step_input_dict(policy.model.view_requirements, index='last')\n            last_r = policy._value(**input_dict)\n        gamma = policy.config['gamma']\n        lambda_ = policy.config['lambda']\n        vpred_t = np.concatenate([postprocessed_batch[SampleBatch.VF_PREDS], np.array([last_r])])\n        delta_t = postprocessed_batch[SampleBatch.REWARDS] + gamma ** d_ts * vpred_t[1:] - vpred_t[:-1]\n        postprocessed_batch[Postprocessing.ADVANTAGES] = generalized_discount_cumsum(delta_t, d_ts[:-1], gamma * lambda_)\n        postprocessed_batch[Postprocessing.VALUE_TARGETS] = (postprocessed_batch[Postprocessing.ADVANTAGES] + postprocessed_batch[SampleBatch.VF_PREDS]).astype(np.float32)\n        postprocessed_batch[Postprocessing.ADVANTAGES] = postprocessed_batch[Postprocessing.ADVANTAGES].astype(np.float32)"
        ]
    },
    {
        "func_name": "generalized_discount_cumsum",
        "original": "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    \"\"\"Calculates the 'time-dependent' discounted cumulative sum over a\n    (reward) sequence `x`.\n\n    Recursive equations:\n\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\n\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\n    reversed(x)[t]\n\n    Args:\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\n        deltas (np.ndarray): A sequence of time step deltas (length of time\n            steps).\n        gamma: The discount factor gamma.\n\n    Returns:\n        np.ndarray: The sequence containing the 'time-dependent' discounted\n            cumulative sums for each individual element in `x` till the end of\n            the trajectory.\n\n    .. testcode::\n        :skipif: True\n\n        x = np.array([0.0, 1.0, 2.0, 3.0])\n        deltas = np.array([1.0, 4.0, 15.0])\n        gamma = 0.9\n        generalized_discount_cumsum(x, deltas, gamma)\n\n    .. testoutput::\n\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\n               2.0 + 0.9^15.0*3.0,\n               3.0])\n    \"\"\"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]",
        "mutated": [
            "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    if False:\n        i = 10\n    \"Calculates the 'time-dependent' discounted cumulative sum over a\\n    (reward) sequence `x`.\\n\\n    Recursive equations:\\n\\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\\n\\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\\n    reversed(x)[t]\\n\\n    Args:\\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\\n        deltas (np.ndarray): A sequence of time step deltas (length of time\\n            steps).\\n        gamma: The discount factor gamma.\\n\\n    Returns:\\n        np.ndarray: The sequence containing the 'time-dependent' discounted\\n            cumulative sums for each individual element in `x` till the end of\\n            the trajectory.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        x = np.array([0.0, 1.0, 2.0, 3.0])\\n        deltas = np.array([1.0, 4.0, 15.0])\\n        gamma = 0.9\\n        generalized_discount_cumsum(x, deltas, gamma)\\n\\n    .. testoutput::\\n\\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               2.0 + 0.9^15.0*3.0,\\n               3.0])\\n    \"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]",
            "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Calculates the 'time-dependent' discounted cumulative sum over a\\n    (reward) sequence `x`.\\n\\n    Recursive equations:\\n\\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\\n\\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\\n    reversed(x)[t]\\n\\n    Args:\\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\\n        deltas (np.ndarray): A sequence of time step deltas (length of time\\n            steps).\\n        gamma: The discount factor gamma.\\n\\n    Returns:\\n        np.ndarray: The sequence containing the 'time-dependent' discounted\\n            cumulative sums for each individual element in `x` till the end of\\n            the trajectory.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        x = np.array([0.0, 1.0, 2.0, 3.0])\\n        deltas = np.array([1.0, 4.0, 15.0])\\n        gamma = 0.9\\n        generalized_discount_cumsum(x, deltas, gamma)\\n\\n    .. testoutput::\\n\\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               2.0 + 0.9^15.0*3.0,\\n               3.0])\\n    \"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]",
            "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Calculates the 'time-dependent' discounted cumulative sum over a\\n    (reward) sequence `x`.\\n\\n    Recursive equations:\\n\\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\\n\\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\\n    reversed(x)[t]\\n\\n    Args:\\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\\n        deltas (np.ndarray): A sequence of time step deltas (length of time\\n            steps).\\n        gamma: The discount factor gamma.\\n\\n    Returns:\\n        np.ndarray: The sequence containing the 'time-dependent' discounted\\n            cumulative sums for each individual element in `x` till the end of\\n            the trajectory.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        x = np.array([0.0, 1.0, 2.0, 3.0])\\n        deltas = np.array([1.0, 4.0, 15.0])\\n        gamma = 0.9\\n        generalized_discount_cumsum(x, deltas, gamma)\\n\\n    .. testoutput::\\n\\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               2.0 + 0.9^15.0*3.0,\\n               3.0])\\n    \"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]",
            "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Calculates the 'time-dependent' discounted cumulative sum over a\\n    (reward) sequence `x`.\\n\\n    Recursive equations:\\n\\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\\n\\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\\n    reversed(x)[t]\\n\\n    Args:\\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\\n        deltas (np.ndarray): A sequence of time step deltas (length of time\\n            steps).\\n        gamma: The discount factor gamma.\\n\\n    Returns:\\n        np.ndarray: The sequence containing the 'time-dependent' discounted\\n            cumulative sums for each individual element in `x` till the end of\\n            the trajectory.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        x = np.array([0.0, 1.0, 2.0, 3.0])\\n        deltas = np.array([1.0, 4.0, 15.0])\\n        gamma = 0.9\\n        generalized_discount_cumsum(x, deltas, gamma)\\n\\n    .. testoutput::\\n\\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               2.0 + 0.9^15.0*3.0,\\n               3.0])\\n    \"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]",
            "def generalized_discount_cumsum(x: np.ndarray, deltas: np.ndarray, gamma: float) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Calculates the 'time-dependent' discounted cumulative sum over a\\n    (reward) sequence `x`.\\n\\n    Recursive equations:\\n\\n    y[t] - gamma**deltas[t+1]*y[t+1] = x[t]\\n\\n    reversed(y)[t] - gamma**reversed(deltas)[t-1]*reversed(y)[t-1] =\\n    reversed(x)[t]\\n\\n    Args:\\n        x (np.ndarray): A sequence of rewards or one-step TD residuals.\\n        deltas (np.ndarray): A sequence of time step deltas (length of time\\n            steps).\\n        gamma: The discount factor gamma.\\n\\n    Returns:\\n        np.ndarray: The sequence containing the 'time-dependent' discounted\\n            cumulative sums for each individual element in `x` till the end of\\n            the trajectory.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        x = np.array([0.0, 1.0, 2.0, 3.0])\\n        deltas = np.array([1.0, 4.0, 15.0])\\n        gamma = 0.9\\n        generalized_discount_cumsum(x, deltas, gamma)\\n\\n    .. testoutput::\\n\\n        array([0.0 + 0.9^1.0*1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               1.0 + 0.9^4.0*2.0 + 0.9^15.0*3.0,\\n               2.0 + 0.9^15.0*3.0,\\n               3.0])\\n    \"\n    reversed_x = x[::-1]\n    reversed_deltas = deltas[::-1]\n    reversed_y = np.empty_like(x)\n    reversed_y[0] = reversed_x[0]\n    for i in range(1, x.size):\n        reversed_y[i] = reversed_x[i] + gamma ** reversed_deltas[i - 1] * reversed_y[i - 1]\n    return reversed_y[::-1]"
        ]
    }
]