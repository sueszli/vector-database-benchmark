[
    {
        "func_name": "gmean",
        "original": "def gmean(s):\n    return s.product() ** (1 / len(s))",
        "mutated": [
            "def gmean(s):\n    if False:\n        i = 10\n    return s.product() ** (1 / len(s))",
            "def gmean(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return s.product() ** (1 / len(s))",
            "def gmean(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return s.product() ** (1 / len(s))",
            "def gmean(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return s.product() ** (1 / len(s))",
            "def gmean(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return s.product() ** (1 / len(s))"
        ]
    },
    {
        "func_name": "is_csv",
        "original": "def is_csv(f):\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')",
        "mutated": [
            "def is_csv(f):\n    if False:\n        i = 10\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')",
            "def is_csv(f):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')",
            "def is_csv(f):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')",
            "def is_csv(f):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')",
            "def is_csv(f):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if perf_compare:\n        regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n        return re.match(regex, f) is not None\n    else:\n        return f.endswith('_performance.csv')"
        ]
    },
    {
        "func_name": "find_csv_files",
        "original": "def find_csv_files(path, perf_compare):\n    \"\"\"\n    Recursively search for all CSV files in directory and subdirectories whose\n    name contains a target string.\n    \"\"\"\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files",
        "mutated": [
            "def find_csv_files(path, perf_compare):\n    if False:\n        i = 10\n    '\\n    Recursively search for all CSV files in directory and subdirectories whose\\n    name contains a target string.\\n    '\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files",
            "def find_csv_files(path, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Recursively search for all CSV files in directory and subdirectories whose\\n    name contains a target string.\\n    '\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files",
            "def find_csv_files(path, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Recursively search for all CSV files in directory and subdirectories whose\\n    name contains a target string.\\n    '\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files",
            "def find_csv_files(path, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Recursively search for all CSV files in directory and subdirectories whose\\n    name contains a target string.\\n    '\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files",
            "def find_csv_files(path, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Recursively search for all CSV files in directory and subdirectories whose\\n    name contains a target string.\\n    '\n\n    def is_csv(f):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            return re.match(regex, f) is not None\n        else:\n            return f.endswith('_performance.csv')\n    csv_files = []\n    for (root, dirs, files) in os.walk(path):\n        for file in files:\n            if is_csv(file):\n                csv_files.append(os.path.join(root, file))\n    return csv_files"
        ]
    },
    {
        "func_name": "main",
        "original": "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    \"\"\"\n    Given a directory containing multiple CSVs from --performance benchmark\n    runs, aggregates and generates summary statistics similar to the web UI at\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\n\n    This is most useful if you've downloaded CSVs from CI and need to quickly\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\n    naming convention that is used in CI.\n\n    You may also be interested in\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\n    which explains how to interpret the raw csv data.\n    \"\"\"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()",
        "mutated": [
            "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    if False:\n        i = 10\n    \"\\n    Given a directory containing multiple CSVs from --performance benchmark\\n    runs, aggregates and generates summary statistics similar to the web UI at\\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\\n\\n    This is most useful if you've downloaded CSVs from CI and need to quickly\\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\\n    naming convention that is used in CI.\\n\\n    You may also be interested in\\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\\n    which explains how to interpret the raw csv data.\\n    \"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()",
            "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Given a directory containing multiple CSVs from --performance benchmark\\n    runs, aggregates and generates summary statistics similar to the web UI at\\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\\n\\n    This is most useful if you've downloaded CSVs from CI and need to quickly\\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\\n    naming convention that is used in CI.\\n\\n    You may also be interested in\\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\\n    which explains how to interpret the raw csv data.\\n    \"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()",
            "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Given a directory containing multiple CSVs from --performance benchmark\\n    runs, aggregates and generates summary statistics similar to the web UI at\\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\\n\\n    This is most useful if you've downloaded CSVs from CI and need to quickly\\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\\n    naming convention that is used in CI.\\n\\n    You may also be interested in\\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\\n    which explains how to interpret the raw csv data.\\n    \"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()",
            "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Given a directory containing multiple CSVs from --performance benchmark\\n    runs, aggregates and generates summary statistics similar to the web UI at\\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\\n\\n    This is most useful if you've downloaded CSVs from CI and need to quickly\\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\\n    naming convention that is used in CI.\\n\\n    You may also be interested in\\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\\n    which explains how to interpret the raw csv data.\\n    \"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()",
            "@click.command()\n@click.argument('directory', default='artifacts')\n@click.option('--amp', is_flag=True)\n@click.option('--float32', is_flag=True)\n@click.option('--perf-compare', is_flag=True, help='Set if the CSVs were generated by running manually the action rather than picking them from the nightly job')\ndef main(directory, amp, float32, perf_compare):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Given a directory containing multiple CSVs from --performance benchmark\\n    runs, aggregates and generates summary statistics similar to the web UI at\\n    https://torchci-git-fork-huydhn-add-compilers-bench-74abf8-fbopensource.vercel.app/benchmark/compilers\\n\\n    This is most useful if you've downloaded CSVs from CI and need to quickly\\n    look at aggregate stats.  The CSVs are expected to follow exactly the same\\n    naming convention that is used in CI.\\n\\n    You may also be interested in\\n    https://docs.google.com/document/d/1DQQxIgmKa3eF0HByDTLlcJdvefC4GwtsklJUgLs09fQ/edit#\\n    which explains how to interpret the raw csv data.\\n    \"\n    dtypes = ['amp', 'float32']\n    if amp and (not float32):\n        dtypes = ['amp']\n    if float32 and (not amp):\n        dtypes = ['float32']\n    dfs = defaultdict(list)\n    for f in find_csv_files(directory, perf_compare):\n        try:\n            dfs[os.path.basename(f)].append(pd.read_csv(f))\n        except Exception:\n            logging.warning('failed parsing %s', f)\n            raise\n    results = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n    for (k, v) in sorted(dfs.items()):\n        if perf_compare:\n            regex = 'training_(torchbench|huggingface|timm_models)\\\\.csv'\n            m = re.match(regex, k)\n            assert m is not None, k\n            compiler = 'inductor'\n            benchmark = m.group(1)\n            dtype = 'float32'\n            mode = 'training'\n            device = 'cuda'\n        else:\n            regex = '(.+)_(torchbench|huggingface|timm_models)_(float32|amp)_(inference|training)_(cpu|cuda)_performance\\\\.csv'\n            m = re.match(regex, k)\n            compiler = m.group(1)\n            benchmark = m.group(2)\n            dtype = m.group(3)\n            mode = m.group(4)\n            device = m.group(5)\n        df = pd.concat(v)\n        df = df.dropna().query('speedup != 0')\n        statistics = {'speedup': gmean(df['speedup']), 'comptime': df['compilation_latency'].mean(), 'memory': gmean(df['compression_ratio'])}\n        if dtype not in dtypes:\n            continue\n        for (statistic, v) in statistics.items():\n            results[f'{device} {dtype} {mode}'][statistic][benchmark][compiler] = v\n    descriptions = {'speedup': 'Geometric mean speedup', 'comptime': 'Mean compilation time', 'memory': 'Peak memory compression ratio'}\n    for (dtype_mode, r) in results.items():\n        print(f'# {dtype_mode} performance results')\n        for (statistic, data) in r.items():\n            print(f'## {descriptions[statistic]}')\n            table = []\n            for row_name in data[list(data.keys())[0]]:\n                row = [row_name]\n                for col_name in data:\n                    row.append(round(data[col_name][row_name], 2))\n                table.append(row)\n            headers = list(data.keys())\n            print(tabulate(table, headers=headers))\n            print()"
        ]
    }
]