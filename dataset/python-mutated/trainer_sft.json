[
    {
        "func_name": "compute_metrics",
        "original": "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out",
        "mutated": [
            "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    if False:\n        i = 10\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out",
            "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out",
            "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out",
            "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out",
            "def compute_metrics(eval_pred, preprocess_fns, metrics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    out = {}\n    for (metric, preprocess_fn) in zip(metrics, preprocess_fns):\n        (preds, labels) = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n    return out"
        ]
    },
    {
        "func_name": "preprocess_logits_for_metrics",
        "original": "def preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids",
        "mutated": [
            "def preprocess_logits_for_metrics(logits, labels):\n    if False:\n        i = 10\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids",
            "def preprocess_logits_for_metrics(logits, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids",
            "def preprocess_logits_for_metrics(logits, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids",
            "def preprocess_logits_for_metrics(logits, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids",
            "def preprocess_logits_for_metrics(logits, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler",
        "mutated": [
            "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    if False:\n        i = 10\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler",
            "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler",
            "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler",
            "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler",
            "def __init__(self, model: Union[PreTrainedModel, nn.Module]=None, args: TrainingArguments=None, sampler: torch.utils.data.sampler.Sampler=None, loss_function: str='CrossEntropyLoss', poly_eps: float=1.0, train_collate_fn: Callable=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(model, args, **kwargs)\n    self.train_collate_fn = train_collate_fn\n    self.loss_fct = get_loss(loss_function, poly_eps)\n    self.sampler = sampler"
        ]
    },
    {
        "func_name": "compute_loss",
        "original": "def compute_loss(self, model, inputs, return_outputs=False):\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss",
        "mutated": [
            "def compute_loss(self, model, inputs, return_outputs=False):\n    if False:\n        i = 10\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss",
            "def compute_loss(self, model, inputs, return_outputs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss",
            "def compute_loss(self, model, inputs, return_outputs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss",
            "def compute_loss(self, model, inputs, return_outputs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss",
            "def compute_loss(self, model, inputs, return_outputs=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, outputs) if return_outputs else loss"
        ]
    },
    {
        "func_name": "_compute_loss",
        "original": "def _compute_loss(self, model, inputs):\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)",
        "mutated": [
            "def _compute_loss(self, model, inputs):\n    if False:\n        i = 10\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)",
            "def _compute_loss(self, model, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)",
            "def _compute_loss(self, model, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)",
            "def _compute_loss(self, model, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)",
            "def _compute_loss(self, model, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    inputs = self._prepare_inputs(inputs)\n    labels_mask = inputs.pop('label_masks')\n    targets = inputs.pop('targets')\n    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs.get('attention_mask', None), use_cache=False)\n    logits = outputs.get('logits')\n    loss = self.loss_fct(outputs.get('logits'), targets, mask=labels_mask)\n    return (loss, logits, targets, labels_mask)"
        ]
    },
    {
        "func_name": "prediction_step",
        "original": "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)",
        "mutated": [
            "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    if False:\n        i = 10\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)",
            "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)",
            "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)",
            "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)",
            "def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]=None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with torch.no_grad():\n        (loss, logits, labels, labels_mask) = self._compute_loss(model, inputs)\n        labels[~labels_mask.bool()] = -100\n    loss = loss.mean().detach()\n    if self.args.prediction_loss_only:\n        return (loss, None, None)\n    return (loss, logits, labels)"
        ]
    },
    {
        "func_name": "get_train_dataloader",
        "original": "def get_train_dataloader(self):\n    \"\"\"\n        Inject custom data sampling behaviour into training loop\n        and use custom task mixing collate function : train_collate_fn\n\n        rewrite from:\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\n        \"\"\"\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader",
        "mutated": [
            "def get_train_dataloader(self):\n    if False:\n        i = 10\n    '\\n        Inject custom data sampling behaviour into training loop\\n        and use custom task mixing collate function : train_collate_fn\\n\\n        rewrite from:\\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\\n        '\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader",
            "def get_train_dataloader(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Inject custom data sampling behaviour into training loop\\n        and use custom task mixing collate function : train_collate_fn\\n\\n        rewrite from:\\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\\n        '\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader",
            "def get_train_dataloader(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Inject custom data sampling behaviour into training loop\\n        and use custom task mixing collate function : train_collate_fn\\n\\n        rewrite from:\\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\\n        '\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader",
            "def get_train_dataloader(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Inject custom data sampling behaviour into training loop\\n        and use custom task mixing collate function : train_collate_fn\\n\\n        rewrite from:\\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\\n        '\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader",
            "def get_train_dataloader(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Inject custom data sampling behaviour into training loop\\n        and use custom task mixing collate function : train_collate_fn\\n\\n        rewrite from:\\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\\n        '\n    data_collator = self.train_collate_fn\n    train_dataset = self.train_dataset\n    if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n        train_dataset = self._remove_unused_columns(train_dataset, description='training')\n    if isinstance(train_dataset, torch.utils.data.IterableDataset):\n        if self.args.world_size > 1:\n            train_dataset = IterableDatasetShard(train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index)\n        return DataLoader(train_dataset, batch_size=self.args.per_device_train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory)\n    if self.sampler is None:\n        train_sampler = self._get_train_sampler()\n    else:\n        train_sampler = self.sampler\n        logging.warning('Custom sampler found!')\n    dataloader = DataLoader(train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker)\n    return dataloader"
        ]
    },
    {
        "func_name": "argument_parsing",
        "original": "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)",
        "mutated": [
            "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)",
            "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)",
            "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)",
            "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)",
            "def argument_parsing(notebook: bool=False, notebook_args: Sequence[str] | None=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--configs', nargs='+', required=True, help='\\n        Multiple configs can be passed to set different options.\\n        For example, run as:\\n\\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\\n\\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\\n    ')\n    parser.add_argument('--local_rank', type=int, default=-1)\n    parser.add_argument('--deepspeed', action='store_true')\n    parser.add_argument('--no-deepspeed', dest='deepspeed', action='store_false')\n    parser.add_argument('--wandb-entity', type=str, default='open-assistant')\n    parser.add_argument('--resume_from_checkpoint', action='store_true', help='Resume from last saved checkpoint')\n    parser.add_argument('--rng_seed', type=int, help='rng seed')\n    parser.add_argument('--show_dataset_stats', action='store_true', help='Show dataset stats', default=False)\n    parser.set_defaults(deepspeed=False)\n    if notebook:\n        (args, remaining) = parser.parse_known_args(notebook_args)\n    else:\n        (args, remaining) = parser.parse_known_args()\n    conf = {}\n    configs = read_yamls('./configs')\n    conf.update(configs['defaults'])\n    try:\n        for name in args.configs:\n            if ',' in name:\n                for n in name.split(','):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n    conf['wandb_entity'] = args.wandb_entity\n    conf['local_rank'] = args.local_rank\n    conf['deepspeed'] = args.deepspeed\n    conf['resume_from_checkpoint'] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf['rng_seed'] = args.rng_seed\n    conf['show_dataset_stats'] = args.show_dataset_stats\n    if conf['deepspeed']:\n        conf['world_size'] = int(os.getenv('WORLD_SIZE', default='1'))\n    else:\n        conf['world_size'] = 1\n    parser = argparse.ArgumentParser()\n    for (key, value) in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f'--{key}', type=type_, default=value)\n        parser.add_argument(f'--no-{key}', dest=key, action='store_const', const=None)\n    return parser.parse_args(remaining)"
        ]
    },
    {
        "func_name": "tokenizer_sanity_check",
        "original": "def tokenizer_sanity_check(tokenizer):\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)",
        "mutated": [
            "def tokenizer_sanity_check(tokenizer):\n    if False:\n        i = 10\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)",
            "def tokenizer_sanity_check(tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)",
            "def tokenizer_sanity_check(tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)",
            "def tokenizer_sanity_check(tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)",
            "def tokenizer_sanity_check(tokenizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('Tokenizer sanity check:')\n    print(f'Type: {type(tokenizer).__name__}')\n    print('special_tokens_map:', tokenizer.special_tokens_map)\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n    ds_entry = create_dataset_entry_qa(mode='sft', questions=['Q1', 'Q2'], answers=['A1', 'A2'], lang='en', context='ctx')\n    in_text = ds_entry.get_formatted(tokenizer.eos_token, use_system_tag=True, system_property_dropout=0, system_add_length=True)\n    in_text = ''.join(in_text)\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Question'])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS['Answer'])\n    print(f'prompter_token_id={prompter_token_id!r}, assistant_token_id={assistant_token_id!r}')\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n    print('encoding result:', tr)\n    for (i, xs) in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n    print('message_indices:', message_indices)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f'trainig_conf = {training_conf}')\n    output_dir = training_conf.output_dir if training_conf.output_dir else f'{training_conf.model_name}-{training_conf.log_dir}-finetuned'\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n    args = TrainingArguments(output_dir=output_dir, num_train_epochs=training_conf.num_train_epochs, warmup_steps=training_conf.warmup_steps, learning_rate=float(training_conf.learning_rate), deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None, optim=optimizer, fp16=training_conf.dtype in ['fp16', 'float16'], bf16=training_conf.dtype in ['bf16', 'bfloat16'], local_rank=training_conf.local_rank, gradient_checkpointing=training_conf.gradient_checkpointing, gradient_accumulation_steps=training_conf.gradient_accumulation_steps, per_device_train_batch_size=training_conf.per_device_train_batch_size, per_device_eval_batch_size=training_conf.per_device_eval_batch_size, adam_beta1=training_conf.adam_beta1, adam_beta2=training_conf.adam_beta2, adam_epsilon=float(training_conf.adam_epsilon), weight_decay=training_conf.weight_decay, max_grad_norm=training_conf.max_grad_norm, logging_steps=training_conf.logging_steps, save_total_limit=training_conf.save_total_limit, evaluation_strategy='steps', eval_steps=training_conf.eval_steps, save_strategy=training_conf.save_strategy, save_steps=training_conf.save_steps, eval_accumulation_steps=training_conf.eval_accumulation_steps, resume_from_checkpoint=training_conf.resume_from_checkpoint, report_to='wandb' if training_conf.log_wandb else None)\n    init_rng(training_conf)\n    tokenizer = get_tokenizer(training_conf)\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n    train_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=training_conf.samples_mixing, pad_to_multiple_of=16, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n    eval_collate_fn = DialogueDataCollator(tokenizer, max_length=training_conf.val_max_length, random_offset_probability=training_conf.random_offset_probability, label_masking=training_conf.label_masking, samples_mixing=False, use_system_prefix=training_conf.use_system_prefix, system_prefix=training_conf.system_prefix, use_system_tag=training_conf.use_system_tag, system_property_dropout=training_conf.system_property_dropout, system_add_length=training_conf.system_add_length)\n    (train, evals) = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (not training_conf.deepspeed or training_conf.local_rank == 0)\n    if show_dataset_stats:\n        print('Training dataset sizes (before sampling):')\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f'Subset of {type(d.dataset).__name__}'\n                if hasattr(d.dataset, 'name'):\n                    name += f' ({d.dataset.name})'\n            else:\n                name = type(d).__name__\n                if hasattr(d, 'name'):\n                    name += f' ({d.name})'\n            print(f'{name}: {len(d)} ({len(d) / total:.2%})')\n        print(f'\\nTotal train: {total}')\n        print('-' * 80)\n        print('Evaluation set sizes:')\n        total_eval = sum((len(x) for x in evals.values()))\n        for (k, d) in evals.items():\n            print(f'{k}: {len(d)} ({len(d) / total_eval:.2%})')\n        print(f'\\nTotal eval: {total_eval}')\n        print('-' * 80)\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(map(lambda x: train_collate_fn.process_one(x, return_length=True), tqdm(train, desc='Calculating lengths per sample')))\n        sampler = PerDatasetSampler.build_sampler_from_config(training_conf, train.datasets, rank=training_conf.local_rank, world_size=training_conf.world_size, samples_length=samples_length, verbose=show_dataset_stats)\n    else:\n        sampler = None\n    (metrics, preprocess_fns) = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n    print(f'rope_scaling: {model.config.rope_scaling}')\n    print(f'max_position_embeddings: {model.config.max_position_embeddings}')\n    if training_conf.peft_model:\n        print('Using PEFT model')\n        model = peft_model(model, training_conf)\n    if training_conf.quantization:\n        import bitsandbytes\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(module, 'weight', {'optim_bits': 32})\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n    if not training_conf.log_wandb:\n        os.environ['WANDB_MODE'] = 'offline'\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n        wandb_name = training_conf.model_name.replace(os.getenv('HOME', '/home/ubuntu'), '')\n        wandb.init(project='supervised-finetuning', entity=training_conf.wandb_entity, resume=training_conf.resume_from_checkpoint, name=f'{wandb_name}-{training_conf.log_dir}-finetuned', config=training_conf)\n        wandb.config['_max_length'] = training_conf.max_length\n        wandb.config['_val_max_length'] = training_conf.val_max_length\n    trainer = SFTTrainer(model=model, args=args, sampler=sampler, train_collate_fn=train_collate_fn, loss_function=training_conf.loss_fn, poly_eps=training_conf.poly_eps, train_dataset=train, eval_dataset=evals, data_collator=eval_collate_fn, tokenizer=tokenizer, compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns), preprocess_logits_for_metrics=preprocess_logits_for_metrics)\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)"
        ]
    }
]