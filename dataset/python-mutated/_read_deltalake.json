[
    {
        "func_name": "_set_default_storage_options_kwargs",
        "original": "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}",
        "mutated": [
            "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    if False:\n        i = 10\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}",
            "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}",
            "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}",
            "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}",
            "def _set_default_storage_options_kwargs(boto3_session: Optional[boto3.Session], s3_additional_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    defaults = {key.upper(): value for (key, value) in _utils.boto3_to_primitives(boto3_session=boto3_session).items()}\n    defaults['AWS_REGION'] = defaults.pop('REGION_NAME')\n    s3_additional_kwargs = s3_additional_kwargs or {}\n    return {**defaults, **s3_additional_kwargs}"
        ]
    },
    {
        "func_name": "read_deltalake",
        "original": "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    \"\"\"Load a Deltalake table data from an S3 path.\n\n    This function requires the `deltalake package\n    <https://delta-io.github.io/delta-rs/python>`__.\n    See the `How to load a Delta table\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\n    guide for loading instructions.\n\n    Parameters\n    ----------\n    path: str\n        The path of the DeltaTable.\n    version: Optional[int]\n        The version of the DeltaTable.\n    partitions: Optional[List[Tuple[str, str, Any]]\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\n        for filter syntax.\n    columns: Optional[List[str]]\n        The columns to project. This can be a list of column names to include\n        (order and duplicates are preserved).\n    without_files: bool\n        If True, load the table without tracking files (memory-friendly).\n        Some append-only applications might not need to track files.\n    dtype_backend: str, optional\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\n        nullable dtypes are used for all dtypes that have a nullable implementation when\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\n\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\n    use_threads : bool\n        True to enable concurrent requests, False to disable multiple threads.\n        When enabled, os.cpu_count() is used as the max number of threads.\n    boto3_session: Optional[boto3.Session()]\n        Boto3 Session. If None, the default boto3 session is used.\n    s3_additional_kwargs: Optional[Dict[str, str]]\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\n        Forwarded to the PyArrow to_pandas method.\n\n    Returns\n    -------\n    df: pd.DataFrame\n        DataFrame with the results.\n\n    See Also\n    --------\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\n    \"\"\"\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)",
        "mutated": [
            "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Load a Deltalake table data from an S3 path.\\n\\n    This function requires the `deltalake package\\n    <https://delta-io.github.io/delta-rs/python>`__.\\n    See the `How to load a Delta table\\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\\n    guide for loading instructions.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        The path of the DeltaTable.\\n    version: Optional[int]\\n        The version of the DeltaTable.\\n    partitions: Optional[List[Tuple[str, str, Any]]\\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\\n        for filter syntax.\\n    columns: Optional[List[str]]\\n        The columns to project. This can be a list of column names to include\\n        (order and duplicates are preserved).\\n    without_files: bool\\n        If True, load the table without tracking files (memory-friendly).\\n        Some append-only applications might not need to track files.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    use_threads : bool\\n        True to enable concurrent requests, False to disable multiple threads.\\n        When enabled, os.cpu_count() is used as the max number of threads.\\n    boto3_session: Optional[boto3.Session()]\\n        Boto3 Session. If None, the default boto3 session is used.\\n    s3_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the PyArrow to_pandas method.\\n\\n    Returns\\n    -------\\n    df: pd.DataFrame\\n        DataFrame with the results.\\n\\n    See Also\\n    --------\\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\\n    '\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)",
            "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load a Deltalake table data from an S3 path.\\n\\n    This function requires the `deltalake package\\n    <https://delta-io.github.io/delta-rs/python>`__.\\n    See the `How to load a Delta table\\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\\n    guide for loading instructions.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        The path of the DeltaTable.\\n    version: Optional[int]\\n        The version of the DeltaTable.\\n    partitions: Optional[List[Tuple[str, str, Any]]\\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\\n        for filter syntax.\\n    columns: Optional[List[str]]\\n        The columns to project. This can be a list of column names to include\\n        (order and duplicates are preserved).\\n    without_files: bool\\n        If True, load the table without tracking files (memory-friendly).\\n        Some append-only applications might not need to track files.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    use_threads : bool\\n        True to enable concurrent requests, False to disable multiple threads.\\n        When enabled, os.cpu_count() is used as the max number of threads.\\n    boto3_session: Optional[boto3.Session()]\\n        Boto3 Session. If None, the default boto3 session is used.\\n    s3_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the PyArrow to_pandas method.\\n\\n    Returns\\n    -------\\n    df: pd.DataFrame\\n        DataFrame with the results.\\n\\n    See Also\\n    --------\\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\\n    '\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)",
            "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load a Deltalake table data from an S3 path.\\n\\n    This function requires the `deltalake package\\n    <https://delta-io.github.io/delta-rs/python>`__.\\n    See the `How to load a Delta table\\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\\n    guide for loading instructions.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        The path of the DeltaTable.\\n    version: Optional[int]\\n        The version of the DeltaTable.\\n    partitions: Optional[List[Tuple[str, str, Any]]\\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\\n        for filter syntax.\\n    columns: Optional[List[str]]\\n        The columns to project. This can be a list of column names to include\\n        (order and duplicates are preserved).\\n    without_files: bool\\n        If True, load the table without tracking files (memory-friendly).\\n        Some append-only applications might not need to track files.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    use_threads : bool\\n        True to enable concurrent requests, False to disable multiple threads.\\n        When enabled, os.cpu_count() is used as the max number of threads.\\n    boto3_session: Optional[boto3.Session()]\\n        Boto3 Session. If None, the default boto3 session is used.\\n    s3_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the PyArrow to_pandas method.\\n\\n    Returns\\n    -------\\n    df: pd.DataFrame\\n        DataFrame with the results.\\n\\n    See Also\\n    --------\\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\\n    '\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)",
            "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load a Deltalake table data from an S3 path.\\n\\n    This function requires the `deltalake package\\n    <https://delta-io.github.io/delta-rs/python>`__.\\n    See the `How to load a Delta table\\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\\n    guide for loading instructions.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        The path of the DeltaTable.\\n    version: Optional[int]\\n        The version of the DeltaTable.\\n    partitions: Optional[List[Tuple[str, str, Any]]\\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\\n        for filter syntax.\\n    columns: Optional[List[str]]\\n        The columns to project. This can be a list of column names to include\\n        (order and duplicates are preserved).\\n    without_files: bool\\n        If True, load the table without tracking files (memory-friendly).\\n        Some append-only applications might not need to track files.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    use_threads : bool\\n        True to enable concurrent requests, False to disable multiple threads.\\n        When enabled, os.cpu_count() is used as the max number of threads.\\n    boto3_session: Optional[boto3.Session()]\\n        Boto3 Session. If None, the default boto3 session is used.\\n    s3_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the PyArrow to_pandas method.\\n\\n    Returns\\n    -------\\n    df: pd.DataFrame\\n        DataFrame with the results.\\n\\n    See Also\\n    --------\\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\\n    '\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)",
            "@_utils.check_optional_dependency(deltalake, 'deltalake')\n@apply_configs\ndef read_deltalake(path: str, version: Optional[int]=None, partitions: Optional[List[Tuple[str, str, Any]]]=None, columns: Optional[List[str]]=None, without_files: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable', use_threads: bool=True, boto3_session: Optional[boto3.Session]=None, s3_additional_kwargs: Optional[Dict[str, str]]=None, pyarrow_additional_kwargs: Optional[Dict[str, Any]]=None) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load a Deltalake table data from an S3 path.\\n\\n    This function requires the `deltalake package\\n    <https://delta-io.github.io/delta-rs/python>`__.\\n    See the `How to load a Delta table\\n    <https://delta-io.github.io/delta-rs/python/usage.html#loading-a-delta-table>`__\\n    guide for loading instructions.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        The path of the DeltaTable.\\n    version: Optional[int]\\n        The version of the DeltaTable.\\n    partitions: Optional[List[Tuple[str, str, Any]]\\n        A list of partition filters, see help(DeltaTable.files_by_partitions)\\n        for filter syntax.\\n    columns: Optional[List[str]]\\n        The columns to project. This can be a list of column names to include\\n        (order and duplicates are preserved).\\n    without_files: bool\\n        If True, load the table without tracking files (memory-friendly).\\n        Some append-only applications might not need to track files.\\n    dtype_backend: str, optional\\n        Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,\\n        nullable dtypes are used for all dtypes that have a nullable implementation when\\n        \u201cnumpy_nullable\u201d is set, pyarrow is used for all dtypes if \u201cpyarrow\u201d is set.\\n\\n        The dtype_backends are still experimential. The \"pyarrow\" backend is only supported with Pandas 2.0 or above.\\n    use_threads : bool\\n        True to enable concurrent requests, False to disable multiple threads.\\n        When enabled, os.cpu_count() is used as the max number of threads.\\n    boto3_session: Optional[boto3.Session()]\\n        Boto3 Session. If None, the default boto3 session is used.\\n    s3_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the Delta Table class for the storage options of the S3 backend.\\n    pyarrow_additional_kwargs: Optional[Dict[str, str]]\\n        Forwarded to the PyArrow to_pandas method.\\n\\n    Returns\\n    -------\\n    df: pd.DataFrame\\n        DataFrame with the results.\\n\\n    See Also\\n    --------\\n    deltalake.DeltaTable : Create a DeltaTable instance with the deltalake library.\\n    '\n    arrow_kwargs = _data_types.pyarrow2pandas_defaults(use_threads=use_threads, kwargs=pyarrow_additional_kwargs, dtype_backend=dtype_backend)\n    storage_options = _set_default_storage_options_kwargs(boto3_session, s3_additional_kwargs)\n    return deltalake.DeltaTable(table_uri=path, version=version, storage_options=storage_options, without_files=without_files).to_pyarrow_table(partitions=partitions, columns=columns).to_pandas(**arrow_kwargs)"
        ]
    }
]