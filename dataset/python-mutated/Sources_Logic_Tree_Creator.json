[
    {
        "func_name": "__init__",
        "original": "def __init__(self, param, calculation_log_file):\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()",
        "mutated": [
            "def __init__(self, param, calculation_log_file):\n    if False:\n        i = 10\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()",
            "def __init__(self, param, calculation_log_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()",
            "def __init__(self, param, calculation_log_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()",
            "def __init__(self, param, calculation_log_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()",
            "def __init__(self, param, calculation_log_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.param = param\n    path = self.param['dirpath']\n    self.Run_Name = param['Run_Name']\n    if param['main']['fault_input_type'] == 'geojson':\n        self.faults_file = path + param['main']['faults_file']\n        self.File_geom = self.faults_file\n    elif param['main']['fault_input_type'] == 'txtsherifs':\n        self.File_geom = path + param['main']['File_geom']\n        self.File_prop = path + param['main']['File_prop']\n    if param['main']['background']['option_bg'] == 'smooth':\n        fbgpath = param['dirpath'] + param['main']['background']['smoothing_xml']\n        if os.path.isdir(fbgpath):\n            list_fbg = [f for f in listdir(fbgpath) if isfile(join(fbgpath, f))]\n        else:\n            list_fbg = fbgpath.split(' ')\n            while '' in list_fbg:\n                list_fbg.remove('')\n    else:\n        list_fbg = []\n        fbgpath = None\n    self.list_fbg = list_fbg\n    self.fbgpath = fbgpath\n    self.nb_random_sampling = param['main']['parameters']['nb_sample']\n    self.overwrite = param['main']['parameters']['overwrite_files']\n    self.calculation_log_file = calculation_log_file\n    self.Domain_in_model = []\n    self.initialize()"
        ]
    },
    {
        "func_name": "initialize",
        "original": "def initialize(self):\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)",
        "mutated": [
            "def initialize(self):\n    if False:\n        i = 10\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)",
            "def initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)",
            "def initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)",
            "def initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)",
            "def initialize(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    path = self.param['dirpath']\n    LT_file = path + self.Run_Name + '/Sources_Logic_tree.xml'\n    LT_log_name = path + self.param['main']['LT_file']\n    lt_info_file = open(path + self.Run_Name + '/ssm/lt_branches_id.txt', 'w')\n    lt_info_file.write('id\\tmodel\\tmfd\\trup_set\\tbackground\\tscaling\\t')\n    lt_info_file.write('sample\\n')\n    if not os.path.exists(LT_log_name):\n        print('ERROR : Please provide a LT_file file \\n              See the user manual for guidelines and               the example for file setup example.')\n        exit()\n    else:\n        LT = toml.load(LT_log_name)\n        model_hyps = LT['Models']\n        mfd_hyps = LT['MFD_shape']\n        bg_hyps = LT['Background']\n        sc_hyps = LT['scenario_set']\n        scL_hyps = LT['Scaling_Laws']\n        branches = []\n        for model in model_hyps:\n            for mfd in mfd_hyps:\n                for bg in bg_hyps:\n                    for sc in sc_hyps:\n                        for scl in scL_hyps:\n                            bi = [model, mfd, bg, sc, scl]\n                            branches.append(bi)\n        tmp = branches\n        branches = []\n        b_indexes = []\n        for i_b in range(len(tmp)):\n            for sample in range(1, self.nb_random_sampling + 1):\n                bi = tmp[i_b] + [sample]\n                branches.append(bi)\n                b_indexes.append(i_b)\n        force_rerun = self.param['main']['parameters']['force_rerun']\n        if force_rerun in ['False', 'false']:\n            if os.path.isfile(path + self.Run_Name + '/LOG/lt_branchs.pkl'):\n                old_branches = pickle.load(open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'rb'))\n                old_indexes = pickle.load(open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'rb'))\n            else:\n                old_branches = []\n                old_indexes = []\n        else:\n            old_branches = []\n            old_indexes = []\n        dict_LT = {}\n        used_id = []\n        for bi in branches:\n            if force_rerun in ['False', 'false']:\n                if bi in old_branches:\n                    rerun_bi = False\n                    i = old_branches.index(bi)\n                    id = old_indexes[i]\n                    if not id in used_id:\n                        used_id.append(id)\n                else:\n                    rerun_bi = True\n                    id = None\n            else:\n                rerun_bi = True\n                id = None\n            if rerun_bi == True:\n                i = 0\n                while i in used_id + old_indexes:\n                    i += 1\n                id = i\n            if not id in used_id:\n                used_id.append(id)\n            if self.overwrite in ['True', 'true']:\n                rerun_bi = True\n            dict_LT.update({id: {'run_branch': rerun_bi, 'model': bi[0], 'mfd': bi[1], 'set': bi[3], 'bg': bi[2], 'scl': bi[4], 'smp': bi[5]}})\n            lt_info_file.write(str(id) + '\\t')\n            lt_info_file.write(str(bi[0]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[1]]) + '\\t')\n            lt_info_file.write(str(bi[3]) + '\\t')\n            lt_info_file.write(str(bi[2]) + '\\t')\n            lt_info_file.write(' '.join([i for i in bi[4]]) + '\\t')\n            lt_info_file.write(str(bi[5]) + '\\n')\n        lt_info_file.close()\n        if not self.param['main']['background']['option_bg'] in ['None', 'none']:\n            try:\n                available_bg = read_input.extract_bg_input(path + 'input/' + self.Run_Name + '/bg_seismicity.txt')\n            except:\n                print('Error related to the background file \\n' + 'Please make sure input/run_name/bg_seismicity.txt                      is correctly set up')\n        else:\n            print('No background is used')\n        try:\n            rupt_file = path + self.param['main']['rupture_file']\n            available_sets = read_input.extract_sc_input(rupt_file)\n        except:\n            print('Error related to the rupture scenario set file \\n' + 'Please make sure input/run_name/ruptures.txt is correctly set up')\n    last_bg = 'impossible_name'\n    last_set = 'impossible_name'\n    last_model = 'impossible_name'\n    line = \"<?xml version='1.0' encoding='utf-8'?>\\n\"\n    line += '<nrml xmlns:gml=\"http://www.opengis.net/gml\"\\n'\n    line += '\\txmlns=\"http://openquake.org/xmlns/nrml/0.5\">\\n'\n    line += '\\t<logicTree logicTreeID=\"lt1\">\\n'\n    line += '\\t\\t<logicTreeBranchingLevel branchingLevelID=\"bl_1\">\\n'\n    line += '\\t\\t\\t<logicTreeBranchSet uncertaintyType=\"sourceModel\"\\n'\n    line += '\\t\\t\\t\\t\\t\\t\\tbranchSetID=\"bs_1\">\\n'\n    for id in used_id:\n        print('\\n*******\\nLOGIC TREE BRANCH', id, '\\n*******')\n        branch = dict_LT[id]\n        model_hyp = branch['model']\n        scl_hyp = branch['scl']\n        mfd_hyp = branch['mfd']\n        bg_hyp = branch['bg']\n        set_hyp = branch['set']\n        smp = branch['smp']\n        if branch['run_branch'] == True:\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            log_path = path + self.Run_Name + '/ssm/log_b_' + str(id)\n            if not os.path.exists(b_path):\n                os.makedirs(b_path)\n                print('running branch id ', str(id), ' for the first time')\n            elif self.param['main']['parameters']['force_rerun'] in ['true', 'True']:\n                files = glob.glob(b_path + '/*')\n                for f in files:\n                    os.remove(f)\n                print('rerunning branch id ', str(id))\n            if not os.path.exists(log_path):\n                os.makedirs(log_path)\n                '\\n                    files = glob.glob(log_path+\"/participation_rates/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    os.dir\\n                    files = glob.glob(log_path+\"/*\")\\n                    for f in files:\\n                        os.remove(f)\\n                    '\n            line += '\\t\\t\\t\\t<logicTreeBranch branchID= \"b_' + str(id) + '\">\\n'\n            self.calculation_log_file.write('\\n\\nRunning logic tree branch:')\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            if mfd_hyp[0] in ['GR', 'YC', 'tapered_GR']:\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            elif mfd_hyp[0] == 'YC_modified':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mf : ', mfd_hyp[2], ' size_of_bump : ', mfd_hyp[3])\n            elif mfd_hyp[0] == 'double_GR':\n                print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1], ' Mrupt : ', mfd_hyp[2])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            self.calculation_log_file.write('\\n' + str(model_hyp) + '-' + str(scl_hyp) + '-' + str(mfd_hyp) + '-' + str(bg_hyp) + '-' + str(set_hyp) + '-' + str(set_hyp) + '-' + str(smp))\n            if last_bg != bg_hyp:\n                bg_ratio = available_bg[bg_hyp]\n                last_bg = bg_hyp\n            if last_set != set_hyp:\n                rupture_set = available_sets[set_hyp]\n                index_scenario = 0\n                scenarios_names = []\n                if np.size(rupture_set) == 0:\n                    scenarios_names = []\n                else:\n                    for index_scenario in range(len(rupture_set)):\n                        faults_in_scenario = rupture_set[index_scenario]\n                        if len(faults_in_scenario) > 1:\n                            scenario = {}\n                            faults_done = []\n                            for i in range(len(faults_in_scenario)):\n                                if not str(faults_in_scenario[i]).replace('\\r', '') in faults_done:\n                                    scenario['f_%s' % str(i + 1)] = [str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', '')]\n                                    faults_done.append(str(faults_in_scenario[i]).replace('\\r', '').replace('\\t', '').replace('\\n', ''))\n                            if len(scenario) != 0:\n                                scenarios_names.append(scenario)\n                        index_scenario += 1\n                last_set = set_hyp\n            if last_model != model_hyp:\n                last_model = model_hyp\n                print('Importing faults')\n                if self.param['main']['fault_input_type'] == 'txtsherifs':\n                    Prop = np.genfromtxt(self.File_prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n                    Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n                    Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n                    index_model = np.where(np.array(Column_model_name) == model_hyp)[0]\n                    Prop = np.take(Prop, index_model)\n                    faults_names = np.array(Column_fault_name[index_model[0]:index_model[-1] + 1])\n                    faults_names = list(faults_names)\n                elif self.param['main']['fault_input_type'] == 'geojson':\n                    with open(self.faults_file) as f:\n                        gj = geojson.load(f)\n                    faults = gj['features']\n                    faults_names = []\n                    for fi in range(len(faults)):\n                        if faults[fi]['properties']['model'] == model_hyp:\n                            faults_names.append(str(faults[fi]['properties']['si']))\n                print('\\t - importing faults geometry')\n                faults_data = {}\n                index_fault = 0\n                if self.param['main']['parameters']['simplify_faults']:\n                    simplify = True\n                else:\n                    simplify\n                geom_scenar = Geometry_scenario.Geom_scenar(faults_names, self.File_geom, model_hyp, simplify)\n                faults_lon = geom_scenar.faults_lon\n                faults_lat = geom_scenar.faults_lat\n                for i_fault in range(len(faults_names)):\n                    pairs = []\n                    for (i, j) in zip(faults_lon[i_fault], faults_lat[i_fault]):\n                        pair = [i, j]\n                        if not pair in pairs:\n                            pairs.append(pair)\n                    if len(pairs) != len(faults_lon[i_fault]):\n                        faults_lon[i_fault] = [pair[0] for pair in pairs]\n                        faults_lat[i_fault] = [pair[1] for pair in pairs]\n                simplify_faults = self.param['main']['parameters']['simplify_faults']\n                if simplify_faults in ['True', 'true']:\n                    print('WARNING : fault simplification is applied!!')\n                    for i_fault in range(len(faults_names)):\n                        faults_lon[i_fault] = [faults_lon[i_fault][0], faults_lon[i_fault][-1]]\n                        faults_lat[i_fault] = [faults_lat[i_fault][0], faults_lat[i_fault][-1]]\n                self.FaultGeometry(model_hyp)\n                print('\\t - importing faults properties')\n                re_use = True\n                f_prop_tmp = path + str(self.Run_Name) + '/LOG/' + model_hyp + '_prop.pkl'\n                if not os.path.isfile(f_prop_tmp):\n                    re_use = False\n                if re_use == False:\n                    for Fault_name in faults_names:\n                        i_d = np.where(np.array(self.Column_Fault_name) == Fault_name)\n                        depth = list(map(lambda i: self.Depths[i], i_d[0]))\n                        self.FaultProperties(Fault_name, model_hyp)\n                        dip = self.dip\n                        upper_sismo_depth = self.upper_sismo_depth\n                        lower_sismo_depth = self.lower_sismo_depth\n                        width = (lower_sismo_depth - upper_sismo_depth) / math.sin(math.radians(dip)) * 1000.0\n                        length = geom_scenar.length[index_fault] * 1000.0\n                        area = length * width\n                        if self.rake > -135.0 and self.rake < -45:\n                            mecanism = 'N'\n                        elif self.rake < 135.0 and self.rake > 45:\n                            mecanism = 'R'\n                        else:\n                            mecanism = 'S'\n                        slip_rate_min = self.slip_rate_min\n                        slip_rate_moy = self.slip_rate_moy\n                        slip_rate_max = self.slip_rate_max\n                        faults_data.update({index_fault: {'name': Fault_name, 'dip': dip, 'oriented': self.oriented, 'upper_sismo_depth': upper_sismo_depth, 'lower_sismo_depth': lower_sismo_depth, 'width': width, 'length': length, 'area': area, 'mecanism': mecanism, 'rake': self.rake, 'slip_rate_min': slip_rate_min, 'slip_rate_moy': slip_rate_moy, 'slip_rate_max': slip_rate_max, 'shear_mod': float(self.shear_mod) * 10 ** 9, 'domain': self.Domain, 'lon': faults_lon[index_fault], 'lat': faults_lat[index_fault], 'depth': depth}})\n                        index_fault += 1\n                    with open(f_prop_tmp, 'wb') as f:\n                        pickle.dump(faults_data, f)\n                else:\n                    print('Reloading MFDs from pickle file')\n                    with open(f_prop_tmp, 'rb') as f:\n                        faults_data = pickle.load(f)\n                print('Faults imported.')\n            id_fault = 0\n            for Fault_name in faults_names:\n                faults_data[id_fault]['lon'] = faults_lon[id_fault]\n                faults_data[id_fault]['lat'] = faults_lat[id_fault]\n                id_fault += 1\n            if scl[2] in ['a', 'A']:\n                use_all_ScL_data = True\n            elif scl[2] in ['m', 'M']:\n                use_all_ScL_data = False\n            mfd_param = {}\n            mfd_param.update({'b_value': float(mfd_hyp[1])})\n            if mfd_hyp[0] == 'YC_modified':\n                mfd_param.update({'Mf': float(mfd_hyp[2])})\n                mfd_param.update({'size_of_bump': float(mfd_hyp[3])})\n            elif mfd_hyp[0] == 'double_GR':\n                mfd_param.update({'Mrupt': float(mfd_hyp[2])})\n            Source_model = Source_Model_Creator(b_path, log_path, self.param, model_hyp, rupture_set, smp, self.Domain_in_model, scl[0], scl[1], use_all_ScL_data, mfd_param, mfd_hyp[0], bg_ratio, self.calculation_log_file, faults_names, scenarios_names, faults_data, faults_lon, faults_lat, self.list_fbg, self.fbgpath, branch)\n            self.Domain_in_model = Source_model.Domain_in_the_model\n            list_src_files = Source_model.list_src_files\n        elif branch['run_branch'] == False:\n            print('\\nnot rerunning branch id ', str(id))\n            print()\n            print('Model : \\t', model_hyp)\n            print('Rupture set : \\t\\t', set_hyp)\n            print('Model : \\t\\t\\t', mfd_hyp[0], ' b : ', mfd_hyp[1])\n            print('Model : \\t\\t\\t\\t', bg_hyp)\n            print('Scaling law : \\t\\t\\t\\t\\t', ' '.join((i for i in scl_hyp)))\n            print('Sample : ', smp)\n            print()\n            b_path = path + self.Run_Name + '/ssm/b_' + str(id)\n            list_src_files = [f for f in listdir(b_path) if isfile(join(b_path, f))]\n        if not '/ssm/b_' + str(id) + '/single_sec_rup.xml' in list_src_files:\n            list_src_files.append('ssm/b_' + str(id) + '/single_sec_rup.xml')\n        line += '\\t\\t\\t\\t\\t<uncertaintyModel> \\n'\n        if self.param['main']['parameters']['use_multiF'] in ['True', 'true']:\n            line += '\\t\\t\\t\\t\\t\\t\\tssm/' + model + '_sections.xml \\n'\n        for f in list_src_files:\n            f = f.replace(path + self.Run_Name + '/', '')\n            f = f.replace(model + '/', '')\n            line += '\\t\\t\\t\\t\\t\\t\\t' + f + '\\n'\n        line += '\\t\\t\\t\\t\\t</uncertaintyModel>\\n'\n        line += '\\t\\t\\t\\t\\t<uncertaintyWeight>' + str(round(1.0 / float(len(branches)), 5)) + '</uncertaintyWeight>\\n'\n        line += '\\t\\t\\t\\t</logicTreeBranch>\\n'\n    line += '\\t\\t\\t</logicTreeBranchSet>\\n'\n    line += '\\t\\t</logicTreeBranchingLevel>\\n'\n    line += '\\t</logicTree>\\n'\n    line += '</nrml>\\n'\n    LT_file = path + str(self.Run_Name) + '/Sources_Logic_tree.xml'\n    XMLfile = open(LT_file, 'w')\n    XMLfile.write(line)\n    XMLfile.close()\n    with open(path + self.Run_Name + '/LOG/lt_branchs.pkl', 'wb') as f:\n        pickle.dump(branches, f)\n    with open(path + self.Run_Name + '/LOG/lt_b_id.pkl', 'wb') as f:\n        pickle.dump(used_id, f)"
        ]
    },
    {
        "func_name": "FaultProperties",
        "original": "def FaultProperties(self, Name_of_fault, Model):\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']",
        "mutated": [
            "def FaultProperties(self, Name_of_fault, Model):\n    if False:\n        i = 10\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']",
            "def FaultProperties(self, Name_of_fault, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']",
            "def FaultProperties(self, Name_of_fault, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']",
            "def FaultProperties(self, Name_of_fault, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']",
            "def FaultProperties(self, Name_of_fault, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.param['main']['fault_input_type'] == 'txtsherifs':\n        FileName_Prop = self.File_prop\n        Prop = np.genfromtxt(FileName_Prop, dtype=['U100', 'U100', 'f8', 'U100', 'U100', 'f8', 'f8', 'f8', 'f8', 'f8', 'U100', 'f8'], skip_header=1)\n        Column_model_name = list(map(lambda i: Prop[i][0], range(len(Prop))))\n        Column_fault_name = list(map(lambda i: Prop[i][1], range(len(Prop))))\n        index_model = np.where(np.array(Column_model_name) == Model)[0]\n        Prop = np.take(Prop, index_model)\n        index_fault = np.where(np.array(Column_fault_name[index_model[0]:index_model[-1] + 1]) == Name_of_fault)\n        Indexfault_final = index_fault[0]\n        self.dip = Prop[Indexfault_final][0][2]\n        self.oriented = Prop[Indexfault_final][0][3]\n        self.rake = Prop[Indexfault_final][0][4]\n        self.upper_sismo_depth = Prop[Indexfault_final][0][5]\n        self.lower_sismo_depth = Prop[Indexfault_final][0][6]\n        self.slip_rate_min = Prop[Indexfault_final][0][7]\n        self.slip_rate_moy = Prop[Indexfault_final][0][8]\n        self.slip_rate_max = Prop[Indexfault_final][0][9]\n        self.Domain = Prop[Indexfault_final][0][10]\n        self.shear_mod = Prop[Indexfault_final][0][11]\n        if self.rake == 'N':\n            self.rake = -90.0\n        if self.rake == 'S':\n            self.rake = 0.0\n        if self.rake == 'SS':\n            self.rake = 0.0\n        if self.rake == 'R':\n            self.rake = 90.0\n        self.rake = float(self.rake)\n        if len(str(self.dip)) == 0:\n            print('\\nError!!! please verify your input file for fault parameters\\n')\n    elif self.param['main']['fault_input_type'] == 'geojson':\n        with open(self.File_geom) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if str(faults[fi]['properties']['si']) == Name_of_fault:\n                if faults[fi]['properties']['model'] == Model:\n                    self.dip = faults[fi]['properties']['dip']\n                    self.oriented = faults[fi]['properties']['oriented']\n                    self.upper_sismo_depth = faults[fi]['properties']['up_s_d']\n                    self.lower_sismo_depth = faults[fi]['properties']['lo_s_d']\n                    self.slip_rate_min = faults[fi]['properties']['sr_min']\n                    self.slip_rate_moy = faults[fi]['properties']['sr_mean']\n                    self.slip_rate_max = faults[fi]['properties']['sr_max']\n                    self.Domain = faults[fi]['properties']['Domain']\n                    self.shear_mod = faults[fi]['properties']['shear_modulus']\n                    self.rake = faults[fi]['properties']['rake']"
        ]
    },
    {
        "func_name": "FaultGeometry",
        "original": "def FaultGeometry(self, Model):\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name",
        "mutated": [
            "def FaultGeometry(self, Model):\n    if False:\n        i = 10\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name",
            "def FaultGeometry(self, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name",
            "def FaultGeometry(self, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name",
            "def FaultGeometry(self, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name",
            "def FaultGeometry(self, Model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not '.geojson' in self.File_geom:\n        NomFichier_InfosZonage = self.File_geom\n        InfosZonage = np.genfromtxt(NomFichier_InfosZonage, dtype=['U100', 'U100', 'f8', 'f8', 'U100'], skip_header=1)\n        Column_model_name = list(map(lambda i: InfosZonage[i][0], range(len(InfosZonage))))\n        index_model = np.where(np.array(Column_model_name) == Model)\n        self.Column_Fault_name = list(map(lambda i: InfosZonage[i][1], index_model[0]))\n        self.Longitudes = list(map(lambda i: InfosZonage[i][2], index_model[0]))\n        self.Latitudes = list(map(lambda i: InfosZonage[i][3], index_model[0]))\n        self.Depths = list(map(lambda i: InfosZonage[i][4], index_model[0]))\n        ZoneSelec = self.Column_Fault_name\n        DicoZone = dict([(k, ZoneSelec.count(k)) for k in set(ZoneSelec)])\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for cle in DicoZone.keys():\n            indices_ZonesSelec = np.where(np.array(self.Column_Fault_name) == cle)\n            ColonneNomZone_inter = np.take(self.Column_Fault_name, indices_ZonesSelec)\n            Longitudes_inter = np.take(self.Longitudes, indices_ZonesSelec)\n            Latitudes_inter = np.take(self.Latitudes, indices_ZonesSelec)\n            depth_inter = np.take(self.Depths, indices_ZonesSelec)\n            Longitudes_inter = Longitudes_inter[0].tolist()\n            Latitudes_inter = Latitudes_inter[0].tolist()\n            depth_inter = depth_inter[0].tolist()\n            ColonneNomZone_inter = ColonneNomZone_inter[0].tolist()\n            compt = 0\n            for (xx, yy, nn, dd) in zip(Longitudes_inter, Latitudes_inter, ColonneNomZone_inter, depth_inter):\n                compt += 1\n                Longitudes.append(xx)\n                Latitudes.append(yy)\n                Depths.append(dd)\n                Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name\n    else:\n        with open(self.faults_file) as f:\n            gj = geojson.load(f)\n        faults = gj['features']\n        Longitudes = []\n        Latitudes = []\n        Depths = []\n        Column_Fault_name = []\n        for fi in range(len(faults)):\n            if faults[fi]['properties']['model'] == Model:\n                lons_i = [i[0] for i in faults[fi]['geometry']['coordinates']]\n                lats_i = [i[1] for i in faults[fi]['geometry']['coordinates']]\n                dd = 'sf'\n                nn = str(faults[fi]['properties']['si'])\n                for (xx, yy) in zip(lons_i, lats_i):\n                    Longitudes.append(xx)\n                    Latitudes.append(yy)\n                    Depths.append(dd)\n                    Column_Fault_name.append(nn)\n        self.Longitudes = Longitudes\n        self.Latitudes = Latitudes\n        self.Depths = Depths\n        self.Column_Fault_name = Column_Fault_name"
        ]
    }
]