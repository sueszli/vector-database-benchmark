[
    {
        "func_name": "maximum_spanning_tree_gradient",
        "original": "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    \"\"\"Returns a subgradient of the MaximumSpanningTree op.\n\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\n  and its |max_scores| output.\n\n  Args:\n    mst_op: The MaximumSpanningTree op being differentiated.\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\n                         loss w.r.t. entry b of the |max_scores| output of the\n                         |mst_op|.\n    *_: The gradients w.r.t. the other outputs; ignored.\n\n  Returns:\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\n       |d_loss_d_max_scores|.\n  \"\"\"\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)",
        "mutated": [
            "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    if False:\n        i = 10\n    'Returns a subgradient of the MaximumSpanningTree op.\\n\\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\\n  and its |max_scores| output.\\n\\n  Args:\\n    mst_op: The MaximumSpanningTree op being differentiated.\\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\\n                         loss w.r.t. entry b of the |max_scores| output of the\\n                         |mst_op|.\\n    *_: The gradients w.r.t. the other outputs; ignored.\\n\\n  Returns:\\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\\n       |d_loss_d_max_scores|.\\n  '\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)",
            "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a subgradient of the MaximumSpanningTree op.\\n\\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\\n  and its |max_scores| output.\\n\\n  Args:\\n    mst_op: The MaximumSpanningTree op being differentiated.\\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\\n                         loss w.r.t. entry b of the |max_scores| output of the\\n                         |mst_op|.\\n    *_: The gradients w.r.t. the other outputs; ignored.\\n\\n  Returns:\\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\\n       |d_loss_d_max_scores|.\\n  '\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)",
            "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a subgradient of the MaximumSpanningTree op.\\n\\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\\n  and its |max_scores| output.\\n\\n  Args:\\n    mst_op: The MaximumSpanningTree op being differentiated.\\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\\n                         loss w.r.t. entry b of the |max_scores| output of the\\n                         |mst_op|.\\n    *_: The gradients w.r.t. the other outputs; ignored.\\n\\n  Returns:\\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\\n       |d_loss_d_max_scores|.\\n  '\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)",
            "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a subgradient of the MaximumSpanningTree op.\\n\\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\\n  and its |max_scores| output.\\n\\n  Args:\\n    mst_op: The MaximumSpanningTree op being differentiated.\\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\\n                         loss w.r.t. entry b of the |max_scores| output of the\\n                         |mst_op|.\\n    *_: The gradients w.r.t. the other outputs; ignored.\\n\\n  Returns:\\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\\n       |d_loss_d_max_scores|.\\n  '\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)",
            "@tf.RegisterGradient('MaximumSpanningTree')\ndef maximum_spanning_tree_gradient(mst_op, d_loss_d_max_scores, *_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a subgradient of the MaximumSpanningTree op.\\n\\n  Note that MaximumSpanningTree is only differentiable w.r.t. its |scores| input\\n  and its |max_scores| output.\\n\\n  Args:\\n    mst_op: The MaximumSpanningTree op being differentiated.\\n    d_loss_d_max_scores: [B] vector where entry b is the gradient of the network\\n                         loss w.r.t. entry b of the |max_scores| output of the\\n                         |mst_op|.\\n    *_: The gradients w.r.t. the other outputs; ignored.\\n\\n  Returns:\\n    1. None, since the op is not differentiable w.r.t. its |num_nodes| input.\\n    2. [B,M,M] tensor where entry b,t,s is a subgradient of the network loss\\n       w.r.t. entry b,t,s of the |scores| input, with the same dtype as\\n       |d_loss_d_max_scores|.\\n  '\n    dtype = d_loss_d_max_scores.dtype.base_dtype\n    check.NotNone(dtype)\n    argmax_sources_bxm = mst_op.outputs[1]\n    input_dim = tf.shape(argmax_sources_bxm)[1]\n    indicators_bxmxm = tf.one_hot(argmax_sources_bxm, input_dim, dtype=dtype)\n    d_loss_d_max_scores_bx1 = tf.expand_dims(d_loss_d_max_scores, -1)\n    d_loss_d_max_scores_bx1x1 = tf.expand_dims(d_loss_d_max_scores_bx1, -1)\n    d_loss_d_scores_bxmxm = indicators_bxmxm * d_loss_d_max_scores_bx1x1\n    return (None, d_loss_d_scores_bxmxm)"
        ]
    },
    {
        "func_name": "log_partition_function",
        "original": "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    \"\"\"Returns the log of the sum-of-product of spanning trees or forests.\n\n  Computing the sum-of-product in the log domain reduces the chance of overflow\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\n  the log partition function anyways.  For similar reasons, the scores input is\n  assumed to be specified in the log domain.\n\n  The partition function is caluclated via application of the Matrix-Tree\n  theorem; see the following for details:\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\n\n  Computing the gradient of the log partition function requires inverting the\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\n  paper, we alleviated the numerical issues by clipping the difference between\n  the minimum and maximum score for each node to 20 (in the log domain).  The\n  |max_dynamic_range| argument can be used for this purpose.\n\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\n  directly, instead of using the indirect approach above.  For example, one\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\n\n  Args:\n    num_nodes: [B] vector of graph sizes per batch item.\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\n    forest: If true, sum over spanning forests instead of trees.\n    max_dynamic_range: If specified, incoming scores for each node are clipped\n      to at most this far from the maximum such score (in the log domain).\n\n  Returns:\n    [B] vector Z of log partition function values, where\n      Z[b] = log(\n          \\\\sum_{tree spanning batch item b}\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\n  \"\"\"\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)",
        "mutated": [
            "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    if False:\n        i = 10\n    'Returns the log of the sum-of-product of spanning trees or forests.\\n\\n  Computing the sum-of-product in the log domain reduces the chance of overflow\\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\\n  the log partition function anyways.  For similar reasons, the scores input is\\n  assumed to be specified in the log domain.\\n\\n  The partition function is caluclated via application of the Matrix-Tree\\n  theorem; see the following for details:\\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\\n\\n  Computing the gradient of the log partition function requires inverting the\\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\\n  paper, we alleviated the numerical issues by clipping the difference between\\n  the minimum and maximum score for each node to 20 (in the log domain).  The\\n  |max_dynamic_range| argument can be used for this purpose.\\n\\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\\n  directly, instead of using the indirect approach above.  For example, one\\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\\n\\n  Args:\\n    num_nodes: [B] vector of graph sizes per batch item.\\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\\n    forest: If true, sum over spanning forests instead of trees.\\n    max_dynamic_range: If specified, incoming scores for each node are clipped\\n      to at most this far from the maximum such score (in the log domain).\\n\\n  Returns:\\n    [B] vector Z of log partition function values, where\\n      Z[b] = log(\\n          \\\\sum_{tree spanning batch item b}\\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\\n  '\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)",
            "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the log of the sum-of-product of spanning trees or forests.\\n\\n  Computing the sum-of-product in the log domain reduces the chance of overflow\\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\\n  the log partition function anyways.  For similar reasons, the scores input is\\n  assumed to be specified in the log domain.\\n\\n  The partition function is caluclated via application of the Matrix-Tree\\n  theorem; see the following for details:\\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\\n\\n  Computing the gradient of the log partition function requires inverting the\\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\\n  paper, we alleviated the numerical issues by clipping the difference between\\n  the minimum and maximum score for each node to 20 (in the log domain).  The\\n  |max_dynamic_range| argument can be used for this purpose.\\n\\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\\n  directly, instead of using the indirect approach above.  For example, one\\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\\n\\n  Args:\\n    num_nodes: [B] vector of graph sizes per batch item.\\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\\n    forest: If true, sum over spanning forests instead of trees.\\n    max_dynamic_range: If specified, incoming scores for each node are clipped\\n      to at most this far from the maximum such score (in the log domain).\\n\\n  Returns:\\n    [B] vector Z of log partition function values, where\\n      Z[b] = log(\\n          \\\\sum_{tree spanning batch item b}\\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\\n  '\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)",
            "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the log of the sum-of-product of spanning trees or forests.\\n\\n  Computing the sum-of-product in the log domain reduces the chance of overflow\\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\\n  the log partition function anyways.  For similar reasons, the scores input is\\n  assumed to be specified in the log domain.\\n\\n  The partition function is caluclated via application of the Matrix-Tree\\n  theorem; see the following for details:\\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\\n\\n  Computing the gradient of the log partition function requires inverting the\\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\\n  paper, we alleviated the numerical issues by clipping the difference between\\n  the minimum and maximum score for each node to 20 (in the log domain).  The\\n  |max_dynamic_range| argument can be used for this purpose.\\n\\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\\n  directly, instead of using the indirect approach above.  For example, one\\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\\n\\n  Args:\\n    num_nodes: [B] vector of graph sizes per batch item.\\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\\n    forest: If true, sum over spanning forests instead of trees.\\n    max_dynamic_range: If specified, incoming scores for each node are clipped\\n      to at most this far from the maximum such score (in the log domain).\\n\\n  Returns:\\n    [B] vector Z of log partition function values, where\\n      Z[b] = log(\\n          \\\\sum_{tree spanning batch item b}\\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\\n  '\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)",
            "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the log of the sum-of-product of spanning trees or forests.\\n\\n  Computing the sum-of-product in the log domain reduces the chance of overflow\\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\\n  the log partition function anyways.  For similar reasons, the scores input is\\n  assumed to be specified in the log domain.\\n\\n  The partition function is caluclated via application of the Matrix-Tree\\n  theorem; see the following for details:\\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\\n\\n  Computing the gradient of the log partition function requires inverting the\\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\\n  paper, we alleviated the numerical issues by clipping the difference between\\n  the minimum and maximum score for each node to 20 (in the log domain).  The\\n  |max_dynamic_range| argument can be used for this purpose.\\n\\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\\n  directly, instead of using the indirect approach above.  For example, one\\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\\n\\n  Args:\\n    num_nodes: [B] vector of graph sizes per batch item.\\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\\n    forest: If true, sum over spanning forests instead of trees.\\n    max_dynamic_range: If specified, incoming scores for each node are clipped\\n      to at most this far from the maximum such score (in the log domain).\\n\\n  Returns:\\n    [B] vector Z of log partition function values, where\\n      Z[b] = log(\\n          \\\\sum_{tree spanning batch item b}\\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\\n  '\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)",
            "def log_partition_function(num_nodes, scores, forest=False, max_dynamic_range=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the log of the sum-of-product of spanning trees or forests.\\n\\n  Computing the sum-of-product in the log domain reduces the chance of overflow\\n  or underflow, and ML techniques (e.g., CRF loss functions) typically require\\n  the log partition function anyways.  For similar reasons, the scores input is\\n  assumed to be specified in the log domain.\\n\\n  The partition function is caluclated via application of the Matrix-Tree\\n  theorem; see the following for details:\\n    https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem\\n    http://www.aclweb.org/anthology/D/D07/D07-1015.pdf\\n\\n  Computing the gradient of the log partition function requires inverting the\\n  Laplacian matrix.  Numerical issues may occur if the Laplacian is singular or\\n  nearly-so.  (Intuitively, the Laplacian will be close to singular when the\\n  input scores strongly favor invalid structures such as cycles).  In the EMNLP\\n  paper, we alleviated the numerical issues by clipping the difference between\\n  the minimum and maximum score for each node to 20 (in the log domain).  The\\n  |max_dynamic_range| argument can be used for this purpose.\\n\\n  TODO(googleuser): Try improving the condition number of the Laplacian matrix\\n  directly, instead of using the indirect approach above.  For example, one\\n  could add c*I to the Laplacian (i.e., Tikhonov regularization).\\n\\n  Args:\\n    num_nodes: [B] vector of graph sizes per batch item.\\n    scores: [B,M,M] tensor of padded batched arc and root scores, in the format\\n      used by the maximum_spanning_tree() op.  Padding values must be finite.\\n    forest: If true, sum over spanning forests instead of trees.\\n    max_dynamic_range: If specified, incoming scores for each node are clipped\\n      to at most this far from the maximum such score (in the log domain).\\n\\n  Returns:\\n    [B] vector Z of log partition function values, where\\n      Z[b] = log(\\n          \\\\sum_{tree spanning batch item b}\\n              score(root_of(tree)) \\\\prod_{arc in tree} score(arc))\\n  '\n    orig_dtype = scores.dtype.base_dtype\n    scores_bxmxm = tf.to_double(scores)\n    shape_bxmxm = tf.shape(scores_bxmxm)\n    batch_size = shape_bxmxm[0]\n    max_nodes = shape_bxmxm[1]\n    total_nodes = batch_size * max_nodes\n    (_, valid_tokens_bxm) = digraph_ops.ValidArcAndTokenMasks(num_nodes, max_nodes, dtype=tf.int32)\n    valid_tokens_bx1xm = tf.expand_dims(valid_tokens_bxm, 1)\n    valid_sources_bxmxm = tf.tile(valid_tokens_bx1xm, [1, max_nodes, 1])\n    sequence_bm = 1 + tf.range(total_nodes, dtype=tf.int32)\n    sequence_bxmx1 = tf.reshape(sequence_bm, [batch_size, max_nodes, 1])\n    target_ids_bxmxm = valid_sources_bxmxm * sequence_bxmx1\n    max_scores_bm1 = tf.unsorted_segment_max(scores_bxmxm, target_ids_bxmxm, total_nodes + 1)\n    max_scores_bm = max_scores_bm1[1:]\n    sequence_b = 1 + tf.range(batch_size, dtype=tf.int32)\n    sequence_bx1 = tf.expand_dims(sequence_b, 1)\n    batch_ids_bxm = valid_tokens_bxm * sequence_bx1\n    batch_ids_bm = tf.reshape(batch_ids_bxm, [-1])\n    log_normalization_factor_b1 = tf.unsorted_segment_sum(max_scores_bm, batch_ids_bm, batch_size + 1)\n    log_normalization_factor_b = log_normalization_factor_b1[1:]\n    max_scores_bxmx1 = tf.reshape(max_scores_bm, [batch_size, max_nodes, 1])\n    scores_bxmxm -= max_scores_bxmx1\n    if max_dynamic_range is not None:\n        scores_bxmxm = tf.maximum(scores_bxmxm, -max_dynamic_range)\n    scores_bxmxm = tf.exp(scores_bxmxm)\n    exp_normalized_laplacian_bxmxm = digraph_ops.LaplacianMatrix(num_nodes, scores_bxmxm, forest=forest)\n    log_normalized_partition_function_b = tf.log(tf.matrix_determinant(exp_normalized_laplacian_bxmxm))\n    log_partition_function_b = log_normalized_partition_function_b + log_normalization_factor_b\n    return tf.cast(log_partition_function_b, orig_dtype)"
        ]
    }
]