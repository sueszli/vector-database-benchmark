[
    {
        "func_name": "_is_power_of_2",
        "original": "def _is_power_of_2(n):\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0",
        "mutated": [
            "def _is_power_of_2(n):\n    if False:\n        i = 10\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0",
            "def _is_power_of_2(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0",
            "def _is_power_of_2(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0",
            "def _is_power_of_2(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0",
            "def _is_power_of_2(n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(n, int) or n < 0:\n        raise ValueError('invalid input for _is_power_of_2: {} (type: {})'.format(n, type(n)))\n    return n & n - 1 == 0 and n != 0"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    \"\"\"Multi-Scale Deformable Attention Module\n\n        Args:\n            d_model: hidden dimension\n            n_levels: number of feature levels\n            n_heads: number of attention heads\n            n_points: number of sampling points per attention head per feature level\n        \"\"\"\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()",
        "mutated": [
            "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    if False:\n        i = 10\n    'Multi-Scale Deformable Attention Module\\n\\n        Args:\\n            d_model: hidden dimension\\n            n_levels: number of feature levels\\n            n_heads: number of attention heads\\n            n_points: number of sampling points per attention head per feature level\\n        '\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()",
            "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Multi-Scale Deformable Attention Module\\n\\n        Args:\\n            d_model: hidden dimension\\n            n_levels: number of feature levels\\n            n_heads: number of attention heads\\n            n_points: number of sampling points per attention head per feature level\\n        '\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()",
            "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Multi-Scale Deformable Attention Module\\n\\n        Args:\\n            d_model: hidden dimension\\n            n_levels: number of feature levels\\n            n_heads: number of attention heads\\n            n_points: number of sampling points per attention head per feature level\\n        '\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()",
            "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Multi-Scale Deformable Attention Module\\n\\n        Args:\\n            d_model: hidden dimension\\n            n_levels: number of feature levels\\n            n_heads: number of attention heads\\n            n_points: number of sampling points per attention head per feature level\\n        '\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()",
            "def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Multi-Scale Deformable Attention Module\\n\\n        Args:\\n            d_model: hidden dimension\\n            n_levels: number of feature levels\\n            n_heads: number of attention heads\\n            n_points: number of sampling points per attention head per feature level\\n        '\n    super().__init__()\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads, but got {} and {}'.format(d_model, n_heads))\n    _d_per_head = d_model // n_heads\n    if not _is_power_of_2(_d_per_head):\n        warnings.warn(\"You'd better set d_model in MSDeformAttn to make the dimension of each attention head a power of 2 which is more efficient in our CUDA implementation.\")\n    self.im2col_step = 128\n    self.d_model = d_model\n    self.n_levels = n_levels\n    self.n_heads = n_heads\n    self.n_points = n_points\n    self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * 2)\n    self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)\n    self.value_proj = nn.Linear(d_model, d_model)\n    self.output_proj = nn.Linear(d_model, d_model)\n    self._reset_parameters()"
        ]
    },
    {
        "func_name": "_reset_parameters",
        "original": "def _reset_parameters(self):\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)",
        "mutated": [
            "def _reset_parameters(self):\n    if False:\n        i = 10\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)",
            "def _reset_parameters(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)",
            "def _reset_parameters(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)",
            "def _reset_parameters(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)",
            "def _reset_parameters(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    constant_(self.sampling_offsets.weight.data, 0.0)\n    thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)\n    grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n    grid_init = (grid_init / grid_init.abs().max(-1, keepdim=True)[0]).view(self.n_heads, 1, 1, 2).repeat(1, self.n_levels, self.n_points, 1)\n    for i in range(self.n_points):\n        grid_init[:, :, i, :] *= i + 1\n    with torch.no_grad():\n        self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n    constant_(self.attention_weights.weight.data, 0.0)\n    constant_(self.attention_weights.bias.data, 0.0)\n    xavier_uniform_(self.value_proj.weight.data)\n    constant_(self.value_proj.bias.data, 0.0)\n    xavier_uniform_(self.output_proj.weight.data)\n    constant_(self.output_proj.bias.data, 0.0)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    \"\"\"\n        Args:\n            query: (N, Length_{query}, C)\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\n                add additional (w, h) to form reference boxes\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\n                False for non-padding elements\n\n        Returns:\n             output: (N, Length_{query}, C)\n        \"\"\"\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output",
        "mutated": [
            "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    if False:\n        i = 10\n    '\\n        Args:\\n            query: (N, Length_{query}, C)\\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\\n                add additional (w, h) to form reference boxes\\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\\n                False for non-padding elements\\n\\n        Returns:\\n             output: (N, Length_{query}, C)\\n        '\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output",
            "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Args:\\n            query: (N, Length_{query}, C)\\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\\n                add additional (w, h) to form reference boxes\\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\\n                False for non-padding elements\\n\\n        Returns:\\n             output: (N, Length_{query}, C)\\n        '\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output",
            "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Args:\\n            query: (N, Length_{query}, C)\\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\\n                add additional (w, h) to form reference boxes\\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\\n                False for non-padding elements\\n\\n        Returns:\\n             output: (N, Length_{query}, C)\\n        '\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output",
            "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Args:\\n            query: (N, Length_{query}, C)\\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\\n                add additional (w, h) to form reference boxes\\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\\n                False for non-padding elements\\n\\n        Returns:\\n             output: (N, Length_{query}, C)\\n        '\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output",
            "def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Args:\\n            query: (N, Length_{query}, C)\\n            reference_points: (N, Length_{query}, n_levels, 2), range in [0, 1], top-left (0,0),\\n                bottom-right (1, 1), including padding area or (N, Length_{query}, n_levels, 4),\\n                add additional (w, h) to form reference boxes\\n            input_flatten: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}, C)\\n            input_spatial_shapes: (n_levels, 2), [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]\\n            input_level_start_index: (n_levels, ), [0, H_0*W_0, H_0*W_0+H_1*W_1,\\n                H_0*W_0+H_1*W_1+H_2*W_2, ..., H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}]\\n            input_padding_mask: (N, H_0*W_0+H_1*W_1+...+H_{L-1}*W_{L-1}), True for padding elements,\\n                False for non-padding elements\\n\\n        Returns:\\n             output: (N, Length_{query}, C)\\n        '\n    (N, Len_q, _) = query.shape\n    (N, Len_in, _) = input_flatten.shape\n    assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in\n    value = self.value_proj(input_flatten)\n    if input_padding_mask is not None:\n        value = value.masked_fill(input_padding_mask[..., None], float(0))\n    value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)\n    sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, 2)\n    attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)\n    attention_weights = F.softmax(attention_weights, -1).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n    if reference_points.shape[-1] == 2:\n        offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)\n        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n    elif reference_points.shape[-1] == 4:\n        sampling_locations = reference_points[:, :, None, :, None, :2] + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5\n    else:\n        raise ValueError('Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))\n    try:\n        output = MultiScaleDeformableAttnFunction.apply(value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)\n    except Exception:\n        output = multi_scale_deformable_attn_pytorch(value, input_spatial_shapes, sampling_locations, attention_weights)\n    output = self.output_proj(output)\n    return output"
        ]
    }
]