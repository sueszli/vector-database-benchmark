[
    {
        "func_name": "transform_inplace_ops",
        "original": "def transform_inplace_ops(graph, name_remap_dict=None):\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v",
        "mutated": [
            "def transform_inplace_ops(graph, name_remap_dict=None):\n    if False:\n        i = 10\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v",
            "def transform_inplace_ops(graph, name_remap_dict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v",
            "def transform_inplace_ops(graph, name_remap_dict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v",
            "def transform_inplace_ops(graph, name_remap_dict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v",
            "def transform_inplace_ops(graph, name_remap_dict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name_remap_dict is None:\n        name_remap_dict = {}\n    for node in graph.nodes:\n        for (k, v) in name_remap_dict.items():\n            node.replace_name(k, v)\n        if node.kind == 'append':\n            if isinstance(node.parent, InternalTorchIRGraph):\n                name_remap_dict[node.inputs[0]] = node.outputs[0]\n            elif node.parent.parent.kind == 'loop':\n                global_input = node.inputs[0]\n                local_input = node.parent.parent.name + '.0'\n                local_output = node.outputs[0]\n                global_output = local_output + '.out'\n                name_remap_dict[global_input] = global_output\n                node.parent.parent.inputs.append(global_input)\n                node.parent.inputs.append(local_input)\n                node.replace_name(global_input, local_input)\n                node.parent.outputs.append(local_output)\n                node.parent.parent.outputs.append(global_output)\n                node.parent.parent.name = node.parent.parent.outputs[0]\n            elif node.parent.parent.kind == 'if':\n                raise NotImplementedError(\"inplace_ops pass doesn't yet support append op inside conditional\")\n        for block in node.blocks:\n            transform_inplace_ops(block, name_remap_dict)\n    for (k, v) in name_remap_dict.items():\n        try:\n            idx = graph.outputs.index(k)\n        except ValueError:\n            pass\n        else:\n            graph.outputs[idx] = v"
        ]
    },
    {
        "func_name": "flatten_graph_input_values",
        "original": "def flatten_graph_input_values(graph):\n    \"\"\" CoreML can't handle nested iterables of tensors, so we flatten the\n        inputs of any graph that expects them.\n    \"\"\"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes",
        "mutated": [
            "def flatten_graph_input_values(graph):\n    if False:\n        i = 10\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        inputs of any graph that expects them.\\n    \"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes",
            "def flatten_graph_input_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        inputs of any graph that expects them.\\n    \"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes",
            "def flatten_graph_input_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        inputs of any graph that expects them.\\n    \"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes",
            "def flatten_graph_input_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        inputs of any graph that expects them.\\n    \"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes",
            "def flatten_graph_input_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        inputs of any graph that expects them.\\n    \"\n    new_graph_inputs = graph.inputs\n    all_new_nodes = []\n    changed = True\n    notified = False\n    while changed:\n        old_graph_inputs = new_graph_inputs\n        new_graph_inputs = OrderedDict()\n        new_nodes = []\n        changed = False\n        for (_input_name, _input_val) in old_graph_inputs.items():\n            if isinstance(_input_val, (tuple, list)):\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph input. This will be flattened in the converted model.')\n                node_inputs = []\n                for (idx, item) in enumerate(_input_val):\n                    name = _input_name + '_{}'.format(idx)\n                    new_graph_inputs[name] = item\n                    node_inputs.append(name)\n                new_nodes.append(InternalTorchIRNode(inputs=node_inputs, outputs=[_input_name], kind='tupleconstruct'))\n            else:\n                new_graph_inputs[_input_name] = _input_val\n        all_new_nodes = new_nodes + all_new_nodes\n    graph.inputs = new_graph_inputs\n    graph.nodes = all_new_nodes + graph.nodes"
        ]
    },
    {
        "func_name": "flatten_graph_output_values",
        "original": "def flatten_graph_output_values(graph):\n    \"\"\" CoreML can't handle nested iterables of tensors, so we flatten the\n        outputs of any graph that produces them.\n    \"\"\"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs",
        "mutated": [
            "def flatten_graph_output_values(graph):\n    if False:\n        i = 10\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        outputs of any graph that produces them.\\n    \"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs",
            "def flatten_graph_output_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        outputs of any graph that produces them.\\n    \"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs",
            "def flatten_graph_output_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        outputs of any graph that produces them.\\n    \"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs",
            "def flatten_graph_output_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        outputs of any graph that produces them.\\n    \"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs",
            "def flatten_graph_output_values(graph):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \" CoreML can't handle nested iterables of tensors, so we flatten the\\n        outputs of any graph that produces them.\\n    \"\n    node_names = [node.name for node in graph.nodes]\n    new_graph_outputs = graph.outputs\n    changed = True\n    notified = False\n    while changed:\n        old_graph_outputs = new_graph_outputs\n        new_graph_outputs = []\n        changed = False\n        for outp in old_graph_outputs:\n            try:\n                node_idx = node_names.index(outp)\n            except:\n                new_graph_outputs.append(outp)\n                continue\n            if graph.nodes[node_idx].kind in ['tupleconstruct', 'listconstruct']:\n                new_graph_outputs.extend(graph.nodes[node_idx].inputs)\n                changed = True\n                if not notified:\n                    notified = True\n                    _logging.warning('Tuple detected at graph output. This will be flattened in the converted model.')\n            else:\n                new_graph_outputs.append(outp)\n    graph.outputs = new_graph_outputs"
        ]
    }
]