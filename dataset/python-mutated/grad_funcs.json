[
    {
        "func_name": "sweep_epsilon",
        "original": "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)",
        "mutated": [
            "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if False:\n        i = 10\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)",
            "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)",
            "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)",
            "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)",
            "def sweep_epsilon(layer, inp, pert_rng, out_shape=None, lshape=None, pert_frac=0.1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if out_shape is None:\n        inpa = layer.be.array(inp.copy())\n        in_shape = lshape if lshape is not None else inpa.shape[0]\n        layer.configure(in_shape)\n        out_shape = layer.out_shape\n    loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    pert_cnt = int(np.ceil(inpa.size * pert_frac))\n    pert_inds = np.random.permutation(inpa.size)[0:pert_cnt]\n    layer.be.rng_reset()\n    min_max_diff = -1.0\n    min_max_pert = None\n    neon_logger.display('epsilon, max diff')\n    for epsilon in pert_rng:\n        (max_abs, max_rel) = general_gradient_comp(layer, inp, epsilon=epsilon, loss_scale=loss_scale, lshape=lshape, pert_inds=pert_inds)\n        layer.be.rng_reset()\n        if min_max_diff < 0 or max_abs < min_max_diff:\n            min_max_diff = max_abs\n            min_max_pert = epsilon\n        neon_logger.display('%e %e %e' % (epsilon, max_abs, max_rel))\n        neon_logger.display('Min max diff : %e at Pert. Mag. %e' % (min_max_diff, min_max_pert))\n    return (min_max_pert, min_max_diff)"
        ]
    },
    {
        "func_name": "general_gradient_comp",
        "original": "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)",
        "mutated": [
            "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    if False:\n        i = 10\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)",
            "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)",
            "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)",
            "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)",
            "def general_gradient_comp(layer, inp, epsilon=1e-05, loss_scale=None, lshape=None, pert_inds=None, pooling=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    layer.reset()\n    inpa = layer.be.array(inp.copy())\n    in_shape = lshape if lshape is not None else inpa.shape[0]\n    layer.configure(in_shape)\n    if layer.owns_delta:\n        layer.prev_layer = True\n    layer.allocate()\n    dtree = DeltasTree()\n    layer.allocate_deltas(dtree)\n    dtree.allocate_buffers()\n    layer.set_deltas(dtree)\n    out = layer.fprop(inpa).get()\n    out_shape = out.shape\n    if loss_scale is None:\n        loss_scale = np.random.random(out_shape) * 2.0 - 1.0\n    bprop_deltas = layer.bprop(layer.be.array(loss_scale.copy())).get()\n    max_abs_err = -1.0\n    max_rel_err = -1.0\n    inp_pert = inp.copy()\n    if pert_inds is None:\n        pert_inds = list(range(inp.size))\n    for pert_ind in pert_inds:\n        save_val = inp_pert.flat[pert_ind]\n        inp_pert.flat[pert_ind] = save_val + epsilon\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        inpa = layer.be.array(inp_pert.copy())\n        out_pos = layer.fprop(inpa).get().copy()\n        inp_pert.flat[pert_ind] = save_val - epsilon\n        inpa = layer.be.array(inp_pert.copy())\n        layer.reset()\n        layer.configure(in_shape)\n        layer.allocate()\n        out_neg = layer.fprop(inpa).get().copy()\n        loss_pos = np.sum(loss_scale * out_pos)\n        loss_neg = np.sum(loss_scale * out_neg)\n        grad_est = 0.5 * (loss_pos - loss_neg) / epsilon\n        inp_pert.flat[pert_ind] = save_val\n        bprop_val = bprop_deltas.flat[pert_ind]\n        abs_err = abs(grad_est - bprop_val)\n        if abs_err > max_abs_err:\n            max_abs_err = abs_err\n            max_abs_vals = [grad_est, bprop_val]\n        if abs(grad_est) + abs(bprop_val) == 0.0:\n            rel_err = 0.0\n        else:\n            rel_err = float(abs_err) / (abs(grad_est) + abs(bprop_val))\n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            max_rel_vals = [grad_est, bprop_val]\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_abs_err, max_abs_vals[0], max_abs_vals[1]))\n    neon_logger.display('Worst case diff %e, vals grad: %e, bprop: %e' % (max_rel_err, max_rel_vals[0], max_rel_vals[1]))\n    return (max_abs_err, max_rel_err)"
        ]
    }
]