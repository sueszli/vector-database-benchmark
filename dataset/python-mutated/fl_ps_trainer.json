[
    {
        "func_name": "get_dataset",
        "original": "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)",
        "mutated": [
            "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    if False:\n        i = 10\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)",
            "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)",
            "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)",
            "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)",
            "def get_dataset(inputs, config, pipe_cmd, role='worker'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dataset = base.DatasetFactory().create_dataset()\n    dataset.set_use_var(inputs)\n    dataset.set_pipe_command(pipe_cmd)\n    dataset.set_batch_size(config.get('runner.batch_size'))\n    reader_thread_num = int(config.get('runner.reader_thread_num'))\n    dataset.set_thread(reader_thread_num)\n    train_files_path = config.get('runner.train_files_path')\n    print(f'train_data_files:{train_files_path}')\n    file_list = [os.path.join(train_files_path, x) for x in os.listdir(train_files_path)]\n    if role == 'worker':\n        file_list = fleet.util.get_file_shard(file_list)\n        print(f'worker file list: {file_list}')\n    elif role == 'heter_worker':\n        file_list = fleet.util.get_heter_file_shard(file_list)\n        print(f'heter worker file list: {file_list}')\n    return (dataset, file_list)"
        ]
    },
    {
        "func_name": "fl_ps_train",
        "original": "def fl_ps_train():\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')",
        "mutated": [
            "def fl_ps_train():\n    if False:\n        i = 10\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')",
            "def fl_ps_train():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')",
            "def fl_ps_train():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')",
            "def fl_ps_train():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')",
            "def fl_ps_train():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from paddle.distributed.fleet.base import role_maker\n    role_maker = role_maker.PaddleCloudRoleMaker()\n    role_maker._generate_role()\n    fleet.util._set_role_maker(role_maker)\n    from ps_dnn_trainer import StaticModel, YamlHelper, get_user_defined_strategy\n    yaml_helper = YamlHelper()\n    config_yaml_path = '../ps/fl_async_ps_config.yaml'\n    config = yaml_helper.load_yaml(config_yaml_path)\n    paddle.enable_static()\n    model = StaticModel(config)\n    feeds_list = model.create_feeds()\n    metrics = model.fl_net(feeds_list)\n    loss = model._cost\n    user_defined_strategy = get_user_defined_strategy(config)\n    a_sync_configs = user_defined_strategy.a_sync_configs\n    a_sync_configs['launch_barrier'] = True\n    user_defined_strategy.a_sync_configs = a_sync_configs\n    print('launch_barrier: ', user_defined_strategy.a_sync_configs['launch_barrier'])\n    learning_rate = config.get('hyper_parameters.optimizer.learning_rate')\n    inner_optimizer = paddle.optimizer.Adam(learning_rate, lazy_mode=True)\n    from paddle.distributed.fleet.meta_optimizers.ps_optimizer import ParameterServerOptimizer\n    ps_optimizer = ParameterServerOptimizer(inner_optimizer)\n    ps_optimizer._set_basic_info(loss, role_maker, inner_optimizer, user_defined_strategy)\n    ps_optimizer.minimize_impl(loss)\n    from paddle.distributed.ps.the_one_ps import TheOnePSRuntime\n    _runtime_handle = TheOnePSRuntime()\n    _runtime_handle._set_basic_info(ps_optimizer.pass_ctx._attrs)\n    epoch_num = int(config.get('runner.epoch_num'))\n    if role_maker._is_server():\n        _runtime_handle._init_server()\n        _runtime_handle._run_server()\n    elif role_maker._is_worker():\n        place = base.CPUPlace()\n        exe = base.Executor(place)\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        print('trainer get dataset')\n        inputs = feeds_list[1:-1]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_A.py')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            start_time = time.time()\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n            end_time = time.time()\n            print('trainer epoch %d finished, use time=%d\\n' % (epoch, end_time - start_time))\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partyA Trainer Success!')\n    else:\n        exe = base.Executor()\n        exe.run(base.default_startup_program())\n        _runtime_handle._init_worker()\n        inputs = [feeds_list[0], feeds_list[-1]]\n        (dataset, file_list) = get_dataset(inputs, config, 'python dataset_generator_B.py', 'heter_worker')\n        print('base.default_main_program: {}'.format(base.default_main_program()._heter_pipeline_opt))\n        for epoch in range(epoch_num):\n            dataset.set_filelist(file_list)\n            exe.train_from_dataset(program=base.default_main_program(), dataset=dataset, print_period=2, debug=False)\n        exe.close()\n        _runtime_handle._stop_worker()\n        print('Fl partB Trainer Success!')"
        ]
    }
]