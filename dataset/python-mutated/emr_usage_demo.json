[
    {
        "func_name": "status_poller",
        "original": "def status_poller(intro, done_status, func):\n    \"\"\"\n    Polls a function for status, sleeping for 10 seconds between each query,\n    until the specified status is returned.\n\n    :param intro: An introductory sentence that informs the reader what we're\n                  waiting for.\n    :param done_status: The status we're waiting for. This function polls the status\n                        function until it returns the specified status.\n    :param func: The function to poll for status. This function must eventually\n                 return the expected done_status or polling will continue indefinitely.\n    \"\"\"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)",
        "mutated": [
            "def status_poller(intro, done_status, func):\n    if False:\n        i = 10\n    \"\\n    Polls a function for status, sleeping for 10 seconds between each query,\\n    until the specified status is returned.\\n\\n    :param intro: An introductory sentence that informs the reader what we're\\n                  waiting for.\\n    :param done_status: The status we're waiting for. This function polls the status\\n                        function until it returns the specified status.\\n    :param func: The function to poll for status. This function must eventually\\n                 return the expected done_status or polling will continue indefinitely.\\n    \"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)",
            "def status_poller(intro, done_status, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Polls a function for status, sleeping for 10 seconds between each query,\\n    until the specified status is returned.\\n\\n    :param intro: An introductory sentence that informs the reader what we're\\n                  waiting for.\\n    :param done_status: The status we're waiting for. This function polls the status\\n                        function until it returns the specified status.\\n    :param func: The function to poll for status. This function must eventually\\n                 return the expected done_status or polling will continue indefinitely.\\n    \"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)",
            "def status_poller(intro, done_status, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Polls a function for status, sleeping for 10 seconds between each query,\\n    until the specified status is returned.\\n\\n    :param intro: An introductory sentence that informs the reader what we're\\n                  waiting for.\\n    :param done_status: The status we're waiting for. This function polls the status\\n                        function until it returns the specified status.\\n    :param func: The function to poll for status. This function must eventually\\n                 return the expected done_status or polling will continue indefinitely.\\n    \"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)",
            "def status_poller(intro, done_status, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Polls a function for status, sleeping for 10 seconds between each query,\\n    until the specified status is returned.\\n\\n    :param intro: An introductory sentence that informs the reader what we're\\n                  waiting for.\\n    :param done_status: The status we're waiting for. This function polls the status\\n                        function until it returns the specified status.\\n    :param func: The function to poll for status. This function must eventually\\n                 return the expected done_status or polling will continue indefinitely.\\n    \"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)",
            "def status_poller(intro, done_status, func):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Polls a function for status, sleeping for 10 seconds between each query,\\n    until the specified status is returned.\\n\\n    :param intro: An introductory sentence that informs the reader what we're\\n                  waiting for.\\n    :param done_status: The status we're waiting for. This function polls the status\\n                        function until it returns the specified status.\\n    :param func: The function to poll for status. This function must eventually\\n                 return the expected done_status or polling will continue indefinitely.\\n    \"\n    emr_basics.logger.setLevel(logging.WARNING)\n    status = None\n    print(intro)\n    print('Current status: ', end='')\n    while status != done_status:\n        prev_status = status\n        status = func()\n        if prev_status == status:\n            print('.', end='')\n        else:\n            print(status, end='')\n        sys.stdout.flush()\n        time.sleep(10)\n    print()\n    emr_basics.logger.setLevel(logging.INFO)"
        ]
    },
    {
        "func_name": "setup_bucket",
        "original": "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    \"\"\"\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\n\n    :param bucket_name: The name of the bucket to create.\n    :param script_file_name: The name of the script file to upload.\n    :param script_key: The key of the script object in the Amazon S3 bucket.\n    :param s3_resource: The Boto3 Amazon S3 resource object.\n    :return: The newly created bucket.\n    \"\"\"\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket",
        "mutated": [
            "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    if False:\n        i = 10\n    '\\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\\n\\n    :param bucket_name: The name of the bucket to create.\\n    :param script_file_name: The name of the script file to upload.\\n    :param script_key: The key of the script object in the Amazon S3 bucket.\\n    :param s3_resource: The Boto3 Amazon S3 resource object.\\n    :return: The newly created bucket.\\n    '\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket",
            "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\\n\\n    :param bucket_name: The name of the bucket to create.\\n    :param script_file_name: The name of the script file to upload.\\n    :param script_key: The key of the script object in the Amazon S3 bucket.\\n    :param s3_resource: The Boto3 Amazon S3 resource object.\\n    :return: The newly created bucket.\\n    '\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket",
            "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\\n\\n    :param bucket_name: The name of the bucket to create.\\n    :param script_file_name: The name of the script file to upload.\\n    :param script_key: The key of the script object in the Amazon S3 bucket.\\n    :param s3_resource: The Boto3 Amazon S3 resource object.\\n    :return: The newly created bucket.\\n    '\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket",
            "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\\n\\n    :param bucket_name: The name of the bucket to create.\\n    :param script_file_name: The name of the script file to upload.\\n    :param script_key: The key of the script object in the Amazon S3 bucket.\\n    :param s3_resource: The Boto3 Amazon S3 resource object.\\n    :return: The newly created bucket.\\n    '\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket",
            "def setup_bucket(bucket_name, script_file_name, script_key, s3_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Creates an Amazon S3 bucket and uploads the specified script file to it.\\n\\n    :param bucket_name: The name of the bucket to create.\\n    :param script_file_name: The name of the script file to upload.\\n    :param script_key: The key of the script object in the Amazon S3 bucket.\\n    :param s3_resource: The Boto3 Amazon S3 resource object.\\n    :return: The newly created bucket.\\n    '\n    try:\n        bucket = s3_resource.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': s3_resource.meta.client.meta.region_name})\n        bucket.wait_until_exists()\n        logger.info('Created bucket %s.', bucket_name)\n    except ClientError:\n        logger.exception(\"Couldn't create bucket %s.\", bucket_name)\n        raise\n    try:\n        bucket.upload_file(script_file_name, script_key)\n        logger.info('Uploaded script %s to %s.', script_file_name, f'{bucket_name}/{script_key}')\n    except ClientError:\n        logger.exception(\"Couldn't upload %s to %s.\", script_file_name, bucket_name)\n        raise\n    return bucket"
        ]
    },
    {
        "func_name": "delete_bucket",
        "original": "def delete_bucket(bucket):\n    \"\"\"\n    Deletes all objects in the specified bucket and deletes the bucket.\n\n    :param bucket: The bucket to delete.\n    \"\"\"\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise",
        "mutated": [
            "def delete_bucket(bucket):\n    if False:\n        i = 10\n    '\\n    Deletes all objects in the specified bucket and deletes the bucket.\\n\\n    :param bucket: The bucket to delete.\\n    '\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise",
            "def delete_bucket(bucket):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Deletes all objects in the specified bucket and deletes the bucket.\\n\\n    :param bucket: The bucket to delete.\\n    '\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise",
            "def delete_bucket(bucket):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Deletes all objects in the specified bucket and deletes the bucket.\\n\\n    :param bucket: The bucket to delete.\\n    '\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise",
            "def delete_bucket(bucket):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Deletes all objects in the specified bucket and deletes the bucket.\\n\\n    :param bucket: The bucket to delete.\\n    '\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise",
            "def delete_bucket(bucket):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Deletes all objects in the specified bucket and deletes the bucket.\\n\\n    :param bucket: The bucket to delete.\\n    '\n    try:\n        bucket.objects.delete()\n        bucket.delete()\n        logger.info('Emptied and removed bucket %s.', bucket.name)\n    except ClientError:\n        logger.exception(\"Couldn't remove bucket %s.\", bucket.name)\n        raise"
        ]
    },
    {
        "func_name": "create_roles",
        "original": "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    \"\"\"\n    Creates IAM roles for the job flow and for the service.\n\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\n\n    The service role is assumed by Amazon EMR and grants it permission to use various\n    Amazon EC2, Amazon S3, and other actions.\n\n    For demo purposes, these roles are fairly permissive. In practice, it's more\n    secure to restrict permissions to the minimum needed to perform the required\n    tasks.\n\n    :param job_flow_role_name: The name of the job flow role.\n    :param service_role_name: The name of the service role.\n    :param iam_resource: The Boto3 IAM resource object.\n    :return: The newly created roles.\n    \"\"\"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)",
        "mutated": [
            "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    if False:\n        i = 10\n    \"\\n    Creates IAM roles for the job flow and for the service.\\n\\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\\n\\n    The service role is assumed by Amazon EMR and grants it permission to use various\\n    Amazon EC2, Amazon S3, and other actions.\\n\\n    For demo purposes, these roles are fairly permissive. In practice, it's more\\n    secure to restrict permissions to the minimum needed to perform the required\\n    tasks.\\n\\n    :param job_flow_role_name: The name of the job flow role.\\n    :param service_role_name: The name of the service role.\\n    :param iam_resource: The Boto3 IAM resource object.\\n    :return: The newly created roles.\\n    \"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)",
            "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Creates IAM roles for the job flow and for the service.\\n\\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\\n\\n    The service role is assumed by Amazon EMR and grants it permission to use various\\n    Amazon EC2, Amazon S3, and other actions.\\n\\n    For demo purposes, these roles are fairly permissive. In practice, it's more\\n    secure to restrict permissions to the minimum needed to perform the required\\n    tasks.\\n\\n    :param job_flow_role_name: The name of the job flow role.\\n    :param service_role_name: The name of the service role.\\n    :param iam_resource: The Boto3 IAM resource object.\\n    :return: The newly created roles.\\n    \"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)",
            "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Creates IAM roles for the job flow and for the service.\\n\\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\\n\\n    The service role is assumed by Amazon EMR and grants it permission to use various\\n    Amazon EC2, Amazon S3, and other actions.\\n\\n    For demo purposes, these roles are fairly permissive. In practice, it's more\\n    secure to restrict permissions to the minimum needed to perform the required\\n    tasks.\\n\\n    :param job_flow_role_name: The name of the job flow role.\\n    :param service_role_name: The name of the service role.\\n    :param iam_resource: The Boto3 IAM resource object.\\n    :return: The newly created roles.\\n    \"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)",
            "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Creates IAM roles for the job flow and for the service.\\n\\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\\n\\n    The service role is assumed by Amazon EMR and grants it permission to use various\\n    Amazon EC2, Amazon S3, and other actions.\\n\\n    For demo purposes, these roles are fairly permissive. In practice, it's more\\n    secure to restrict permissions to the minimum needed to perform the required\\n    tasks.\\n\\n    :param job_flow_role_name: The name of the job flow role.\\n    :param service_role_name: The name of the service role.\\n    :param iam_resource: The Boto3 IAM resource object.\\n    :return: The newly created roles.\\n    \"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)",
            "def create_roles(job_flow_role_name, service_role_name, iam_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Creates IAM roles for the job flow and for the service.\\n\\n    The job flow role is assumed by the cluster's Amazon EC2 instances and grants\\n    them broad permission to use services like Amazon DynamoDB and Amazon S3.\\n\\n    The service role is assumed by Amazon EMR and grants it permission to use various\\n    Amazon EC2, Amazon S3, and other actions.\\n\\n    For demo purposes, these roles are fairly permissive. In practice, it's more\\n    secure to restrict permissions to the minimum needed to perform the required\\n    tasks.\\n\\n    :param job_flow_role_name: The name of the job flow role.\\n    :param service_role_name: The name of the service role.\\n    :param iam_resource: The Boto3 IAM resource object.\\n    :return: The newly created roles.\\n    \"\n    try:\n        job_flow_role = iam_resource.create_role(RoleName=job_flow_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Effect': 'Allow', 'Principal': {'Service': 'ec2.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=job_flow_role_name)\n        logger.info('Created job flow role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create job flow role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role')\n        logger.info('Attached policy to role %s.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to role %s.\", job_flow_role_name)\n        raise\n    try:\n        job_flow_inst_profile = iam_resource.create_instance_profile(InstanceProfileName=job_flow_role_name)\n        job_flow_inst_profile.add_role(RoleName=job_flow_role_name)\n        logger.info('Created instance profile %s and added job flow role.', job_flow_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create instance profile %s.\", job_flow_role_name)\n        raise\n    try:\n        service_role = iam_resource.create_role(RoleName=service_role_name, AssumeRolePolicyDocument=json.dumps({'Version': '2008-10-17', 'Statement': [{'Sid': '', 'Effect': 'Allow', 'Principal': {'Service': 'elasticmapreduce.amazonaws.com'}, 'Action': 'sts:AssumeRole'}]}))\n        waiter = iam_resource.meta.client.get_waiter('role_exists')\n        waiter.wait(RoleName=service_role_name)\n        logger.info('Created service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't create service role %s.\", service_role_name)\n        raise\n    try:\n        service_role.attach_policy(PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole')\n        logger.info('Attached policy to service role %s.', service_role_name)\n    except ClientError:\n        logger.exception(\"Couldn't attach policy to service role %s.\", service_role_name)\n        raise\n    return (job_flow_role, service_role)"
        ]
    },
    {
        "func_name": "delete_roles",
        "original": "def delete_roles(roles):\n    \"\"\"\n    Deletes the roles created for this demo.\n\n    :param roles: The roles to delete.\n    \"\"\"\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise",
        "mutated": [
            "def delete_roles(roles):\n    if False:\n        i = 10\n    '\\n    Deletes the roles created for this demo.\\n\\n    :param roles: The roles to delete.\\n    '\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise",
            "def delete_roles(roles):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Deletes the roles created for this demo.\\n\\n    :param roles: The roles to delete.\\n    '\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise",
            "def delete_roles(roles):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Deletes the roles created for this demo.\\n\\n    :param roles: The roles to delete.\\n    '\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise",
            "def delete_roles(roles):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Deletes the roles created for this demo.\\n\\n    :param roles: The roles to delete.\\n    '\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise",
            "def delete_roles(roles):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Deletes the roles created for this demo.\\n\\n    :param roles: The roles to delete.\\n    '\n    try:\n        for role in roles:\n            for policy in role.attached_policies.all():\n                role.detach_policy(PolicyArn=policy.arn)\n            for inst_profile in role.instance_profiles.all():\n                inst_profile.remove_role(RoleName=role.name)\n                inst_profile.delete()\n            role.delete()\n            logger.info('Detached policies and deleted role %s.', role.name)\n    except ClientError:\n        logger.exception(\"Couldn't delete roles %s.\", [role.name for role in roles])\n        raise"
        ]
    },
    {
        "func_name": "create_security_groups",
        "original": "def create_security_groups(prefix, ec2_resource):\n    \"\"\"\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\n\n    When the cluster is created, Amazon EMR adds all required rules to these\n    security groups. Because this demo needs only the default rules, it creates\n    empty security groups and lets Amazon EMR fill them in.\n\n    :param prefix: The name prefix for the security groups.\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\n    :return: The newly created security groups.\n    \"\"\"\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups",
        "mutated": [
            "def create_security_groups(prefix, ec2_resource):\n    if False:\n        i = 10\n    '\\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\\n\\n    When the cluster is created, Amazon EMR adds all required rules to these\\n    security groups. Because this demo needs only the default rules, it creates\\n    empty security groups and lets Amazon EMR fill them in.\\n\\n    :param prefix: The name prefix for the security groups.\\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\\n    :return: The newly created security groups.\\n    '\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups",
            "def create_security_groups(prefix, ec2_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\\n\\n    When the cluster is created, Amazon EMR adds all required rules to these\\n    security groups. Because this demo needs only the default rules, it creates\\n    empty security groups and lets Amazon EMR fill them in.\\n\\n    :param prefix: The name prefix for the security groups.\\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\\n    :return: The newly created security groups.\\n    '\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups",
            "def create_security_groups(prefix, ec2_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\\n\\n    When the cluster is created, Amazon EMR adds all required rules to these\\n    security groups. Because this demo needs only the default rules, it creates\\n    empty security groups and lets Amazon EMR fill them in.\\n\\n    :param prefix: The name prefix for the security groups.\\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\\n    :return: The newly created security groups.\\n    '\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups",
            "def create_security_groups(prefix, ec2_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\\n\\n    When the cluster is created, Amazon EMR adds all required rules to these\\n    security groups. Because this demo needs only the default rules, it creates\\n    empty security groups and lets Amazon EMR fill them in.\\n\\n    :param prefix: The name prefix for the security groups.\\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\\n    :return: The newly created security groups.\\n    '\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups",
            "def create_security_groups(prefix, ec2_resource):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Creates Amazon EC2 security groups for the instances contained in the cluster.\\n\\n    When the cluster is created, Amazon EMR adds all required rules to these\\n    security groups. Because this demo needs only the default rules, it creates\\n    empty security groups and lets Amazon EMR fill them in.\\n\\n    :param prefix: The name prefix for the security groups.\\n    :param ec2_resource: The Boto3 Amazon EC2 resource object.\\n    :return: The newly created security groups.\\n    '\n    try:\n        default_vpc = list(ec2_resource.vpcs.filter(Filters=[{'Name': 'isDefault', 'Values': ['true']}]))[0]\n        logger.info('Got default VPC %s.', default_vpc.id)\n    except ClientError:\n        logger.exception(\"Couldn't get VPCs.\")\n        raise\n    except IndexError:\n        logger.exception('No default VPC in the list.')\n        raise\n    groups = {'manager': None, 'worker': None}\n    for group in groups.keys():\n        try:\n            groups[group] = default_vpc.create_security_group(GroupName=f'{prefix}-{group}', Description=f'EMR {group} group.')\n            logger.info('Created security group %s in VPC %s.', groups[group].id, default_vpc.id)\n        except ClientError:\n            logger.exception(\"Couldn't create security group.\")\n            raise\n    return groups"
        ]
    },
    {
        "func_name": "delete_security_groups",
        "original": "def delete_security_groups(security_groups):\n    \"\"\"\n    Deletes the security groups used by the demo. When there are dependencies\n    on a security group, it cannot be deleted. Because it can take some time\n    to release all dependencies after a cluster is terminated, this function retries\n    the delete until it succeeds.\n\n    :param security_groups: The security groups to delete.\n    \"\"\"\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise",
        "mutated": [
            "def delete_security_groups(security_groups):\n    if False:\n        i = 10\n    '\\n    Deletes the security groups used by the demo. When there are dependencies\\n    on a security group, it cannot be deleted. Because it can take some time\\n    to release all dependencies after a cluster is terminated, this function retries\\n    the delete until it succeeds.\\n\\n    :param security_groups: The security groups to delete.\\n    '\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise",
            "def delete_security_groups(security_groups):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Deletes the security groups used by the demo. When there are dependencies\\n    on a security group, it cannot be deleted. Because it can take some time\\n    to release all dependencies after a cluster is terminated, this function retries\\n    the delete until it succeeds.\\n\\n    :param security_groups: The security groups to delete.\\n    '\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise",
            "def delete_security_groups(security_groups):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Deletes the security groups used by the demo. When there are dependencies\\n    on a security group, it cannot be deleted. Because it can take some time\\n    to release all dependencies after a cluster is terminated, this function retries\\n    the delete until it succeeds.\\n\\n    :param security_groups: The security groups to delete.\\n    '\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise",
            "def delete_security_groups(security_groups):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Deletes the security groups used by the demo. When there are dependencies\\n    on a security group, it cannot be deleted. Because it can take some time\\n    to release all dependencies after a cluster is terminated, this function retries\\n    the delete until it succeeds.\\n\\n    :param security_groups: The security groups to delete.\\n    '\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise",
            "def delete_security_groups(security_groups):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Deletes the security groups used by the demo. When there are dependencies\\n    on a security group, it cannot be deleted. Because it can take some time\\n    to release all dependencies after a cluster is terminated, this function retries\\n    the delete until it succeeds.\\n\\n    :param security_groups: The security groups to delete.\\n    '\n    try:\n        for sg in security_groups.values():\n            sg.revoke_ingress(IpPermissions=sg.ip_permissions)\n        max_tries = 5\n        while True:\n            try:\n                for sg in security_groups.values():\n                    sg.delete()\n                break\n            except ClientError as error:\n                max_tries -= 1\n                if max_tries > 0 and error.response['Error']['Code'] == 'DependencyViolation':\n                    logger.warning('Attempt to delete security group got DependencyViolation. Waiting for 10 seconds to let things propagate.')\n                    time.sleep(10)\n                else:\n                    raise\n        logger.info('Deleted security groups %s.', security_groups)\n    except ClientError:\n        logger.exception(\"Couldn't delete security groups %s.\", security_groups)\n        raise"
        ]
    },
    {
        "func_name": "add_top_product_step",
        "original": "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    \"\"\"\n    Adds a step to a cluster that queries historical Amazon review data to find\n    the top products in the specified category that contain a keyword.\n\n    This function then polls the cluster until the step completes.\n\n    After the step completes, this function gets the output from the bucket\n    and prints it in the command window.\n\n    :param count: The number of results to return.\n    :param category: The product category, such as Books or Grocery.\n    :param keyword: Each returned product must contain this keyword in its title.\n    :param cluster_id: The ID of the cluster that runs the step.\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\n                   that stores the output from the step.\n    :param script_key: The object key of the script that identifies it in the bucket.\n    :param emr_client: The Boto3 Amazon EMR client object.\n    \"\"\"\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)",
        "mutated": [
            "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    if False:\n        i = 10\n    '\\n    Adds a step to a cluster that queries historical Amazon review data to find\\n    the top products in the specified category that contain a keyword.\\n\\n    This function then polls the cluster until the step completes.\\n\\n    After the step completes, this function gets the output from the bucket\\n    and prints it in the command window.\\n\\n    :param count: The number of results to return.\\n    :param category: The product category, such as Books or Grocery.\\n    :param keyword: Each returned product must contain this keyword in its title.\\n    :param cluster_id: The ID of the cluster that runs the step.\\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\\n                   that stores the output from the step.\\n    :param script_key: The object key of the script that identifies it in the bucket.\\n    :param emr_client: The Boto3 Amazon EMR client object.\\n    '\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)",
            "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Adds a step to a cluster that queries historical Amazon review data to find\\n    the top products in the specified category that contain a keyword.\\n\\n    This function then polls the cluster until the step completes.\\n\\n    After the step completes, this function gets the output from the bucket\\n    and prints it in the command window.\\n\\n    :param count: The number of results to return.\\n    :param category: The product category, such as Books or Grocery.\\n    :param keyword: Each returned product must contain this keyword in its title.\\n    :param cluster_id: The ID of the cluster that runs the step.\\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\\n                   that stores the output from the step.\\n    :param script_key: The object key of the script that identifies it in the bucket.\\n    :param emr_client: The Boto3 Amazon EMR client object.\\n    '\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)",
            "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Adds a step to a cluster that queries historical Amazon review data to find\\n    the top products in the specified category that contain a keyword.\\n\\n    This function then polls the cluster until the step completes.\\n\\n    After the step completes, this function gets the output from the bucket\\n    and prints it in the command window.\\n\\n    :param count: The number of results to return.\\n    :param category: The product category, such as Books or Grocery.\\n    :param keyword: Each returned product must contain this keyword in its title.\\n    :param cluster_id: The ID of the cluster that runs the step.\\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\\n                   that stores the output from the step.\\n    :param script_key: The object key of the script that identifies it in the bucket.\\n    :param emr_client: The Boto3 Amazon EMR client object.\\n    '\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)",
            "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Adds a step to a cluster that queries historical Amazon review data to find\\n    the top products in the specified category that contain a keyword.\\n\\n    This function then polls the cluster until the step completes.\\n\\n    After the step completes, this function gets the output from the bucket\\n    and prints it in the command window.\\n\\n    :param count: The number of results to return.\\n    :param category: The product category, such as Books or Grocery.\\n    :param keyword: Each returned product must contain this keyword in its title.\\n    :param cluster_id: The ID of the cluster that runs the step.\\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\\n                   that stores the output from the step.\\n    :param script_key: The object key of the script that identifies it in the bucket.\\n    :param emr_client: The Boto3 Amazon EMR client object.\\n    '\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)",
            "def add_top_product_step(count, category, keyword, cluster_id, bucket, script_key, emr_client):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Adds a step to a cluster that queries historical Amazon review data to find\\n    the top products in the specified category that contain a keyword.\\n\\n    This function then polls the cluster until the step completes.\\n\\n    After the step completes, this function gets the output from the bucket\\n    and prints it in the command window.\\n\\n    :param count: The number of results to return.\\n    :param category: The product category, such as Books or Grocery.\\n    :param keyword: Each returned product must contain this keyword in its title.\\n    :param cluster_id: The ID of the cluster that runs the step.\\n    :param bucket: The Amazon S3 bucket that contains the script for the step and\\n                   that stores the output from the step.\\n    :param script_key: The object key of the script that identifies it in the bucket.\\n    :param emr_client: The Boto3 Amazon EMR client object.\\n    '\n    print(f\"Adding a step to calculate the top {count} products in {category} that contain the word '{keyword}'...\")\n    output_folder = f'top-{count}-{category}-{keyword}'\n    step_id = emr_basics.add_step(cluster_id, f'Calculate {output_folder}', f's3://{bucket.name}/{script_key}', ['--category', category, '--title_keyword', keyword, '--count', count, '--output_uri', f's3://{bucket.name}/{output_folder}'], emr_client)\n    status_poller('Waiting for step to complete...', 'COMPLETED', lambda : emr_basics.describe_step(cluster_id, step_id, emr_client)['Status']['State'])\n    print(f'The output for this step is in Amazon S3 bucket {bucket.name}/{output_folder}.')\n    print('-' * 88)\n    for obj in bucket.objects.filter(Prefix=output_folder):\n        print(obj.get()['Body'].read().decode())\n    print('-' * 88)"
        ]
    },
    {
        "func_name": "demo_short_lived_cluster",
        "original": "def demo_short_lived_cluster():\n    \"\"\"\n    Shows how to create a short-lived cluster that runs a step and automatically\n    terminates after the step completes.\n    \"\"\"\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')",
        "mutated": [
            "def demo_short_lived_cluster():\n    if False:\n        i = 10\n    '\\n    Shows how to create a short-lived cluster that runs a step and automatically\\n    terminates after the step completes.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')",
            "def demo_short_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Shows how to create a short-lived cluster that runs a step and automatically\\n    terminates after the step completes.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')",
            "def demo_short_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Shows how to create a short-lived cluster that runs a step and automatically\\n    terminates after the step completes.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')",
            "def demo_short_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Shows how to create a short-lived cluster that runs a step and automatically\\n    terminates after the step completes.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')",
            "def demo_short_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Shows how to create a short-lived cluster that runs a step and automatically\\n    terminates after the step completes.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR short-lived cluster demo.')\n    print('-' * 88)\n    prefix = f'demo-short-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_estimate_pi.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    output_prefix = 'pi-calc-output'\n    pi_step = {'name': 'estimate-pi-step', 'script_uri': f's3://{bucket_name}/{script_key}', 'script_args': ['--partitions', '3', '--output_uri', f's3://{bucket_name}/{output_prefix}']}\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', False, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [pi_step], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'RUNNING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    status_poller('Waiting for step to complete...', 'PENDING', lambda : emr_basics.list_steps(cluster_id, emr_client)[0]['Status']['State'])\n    status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    print(f'Job complete!. The script, logs, and output for this demo are in Amazon S3 bucket {bucket_name}. The output is:')\n    for obj in bucket.objects.filter(Prefix=output_prefix):\n        print(obj.get()['Body'].read().decode())\n    remove_everything = input(f'Do you want to delete the security roles, groups, and bucket (y/n)? ')\n    if remove_everything.lower() == 'y':\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that objects kept in an Amazon S3 bucket can incur chargesagainst your account.')\n    print('Thanks for watching!')"
        ]
    },
    {
        "func_name": "demo_long_lived_cluster",
        "original": "def demo_long_lived_cluster():\n    \"\"\"\n    Shows how to create a long-lived cluster that waits after all steps are run so\n    that more steps can be run. At the end of the demo, the cluster is optionally\n    terminated.\n    \"\"\"\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')",
        "mutated": [
            "def demo_long_lived_cluster():\n    if False:\n        i = 10\n    '\\n    Shows how to create a long-lived cluster that waits after all steps are run so\\n    that more steps can be run. At the end of the demo, the cluster is optionally\\n    terminated.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')",
            "def demo_long_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Shows how to create a long-lived cluster that waits after all steps are run so\\n    that more steps can be run. At the end of the demo, the cluster is optionally\\n    terminated.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')",
            "def demo_long_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Shows how to create a long-lived cluster that waits after all steps are run so\\n    that more steps can be run. At the end of the demo, the cluster is optionally\\n    terminated.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')",
            "def demo_long_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Shows how to create a long-lived cluster that waits after all steps are run so\\n    that more steps can be run. At the end of the demo, the cluster is optionally\\n    terminated.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')",
            "def demo_long_lived_cluster():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Shows how to create a long-lived cluster that waits after all steps are run so\\n    that more steps can be run. At the end of the demo, the cluster is optionally\\n    terminated.\\n    '\n    print('-' * 88)\n    print(f'Welcome to the Amazon EMR long-lived cluster demo.')\n    print('-' * 88)\n    prefix = 'demo-long-emr'\n    s3_resource = boto3.resource('s3')\n    iam_resource = boto3.resource('iam')\n    emr_client = boto3.client('emr')\n    ec2_resource = boto3.resource('ec2')\n    bucket_name = f'{prefix}-{time.time_ns()}'\n    script_file_name = 'pyspark_top_product_keyword.py'\n    script_key = f'scripts/{script_file_name}'\n    bucket = setup_bucket(bucket_name, script_file_name, script_key, s3_resource)\n    (job_flow_role, service_role) = create_roles(f'{prefix}-ec2-role', f'{prefix}-service-role', iam_resource)\n    security_groups = create_security_groups(prefix, ec2_resource)\n    print('Wait for 10 seconds to give roles and profiles time to propagate...')\n    time.sleep(10)\n    max_tries = 5\n    while True:\n        try:\n            cluster_id = emr_basics.run_job_flow(f'{prefix}-cluster', f's3://{bucket_name}/logs', True, ['Hadoop', 'Hive', 'Spark'], job_flow_role, service_role, security_groups, [], emr_client)\n            print(f'Running job flow for cluster {cluster_id}...')\n            break\n        except ClientError as error:\n            max_tries -= 1\n            if max_tries > 0 and error.response['Error']['Code'] == 'ValidationException':\n                print(\"Instance profile is not ready, let's give it more time...\")\n                time.sleep(10)\n            else:\n                raise\n    status_poller('Waiting for cluster, this typically takes several minutes...', 'WAITING', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n    add_top_product_step('20', 'Books', 'fire', cluster_id, bucket, script_key, emr_client)\n    add_top_product_step('20', 'Grocery', 'cheese', cluster_id, bucket, script_key, emr_client)\n    review_bucket_folders = s3_resource.meta.client.list_objects_v2(Bucket='demo-reviews-pds', Prefix='parquet/', Delimiter='/', MaxKeys=100)\n    categories = [cat['Prefix'].split('=')[1][:-1] for cat in review_bucket_folders['CommonPrefixes']]\n    while True:\n        while True:\n            input_cat = input(f\"Your turn! Possible categories are: {categories}. Which category would you like to search (enter 'none' when you're done)? \")\n            if input_cat.lower() == 'none' or input_cat in categories:\n                break\n            elif input_cat not in categories:\n                print(f'Sorry, {input_cat} is not an allowed category!')\n        if input_cat.lower() == 'none':\n            break\n        else:\n            input_keyword = input('What keyword would you like to search for? ')\n            input_count = input('How many items would you like to list? ')\n            add_top_product_step(input_count, input_cat, input_keyword, cluster_id, bucket, script_key, emr_client)\n    remove_everything = input(f'Do you want to terminate the cluster and delete the security roles, groups, bucket, and all of its contents (y/n)? ')\n    if remove_everything.lower() == 'y':\n        emr_basics.terminate_cluster(cluster_id, emr_client)\n        status_poller('Waiting for cluster to terminate.', 'TERMINATED', lambda : emr_basics.describe_cluster(cluster_id, emr_client)['Status']['State'])\n        delete_security_groups(security_groups)\n        delete_roles([job_flow_role, service_role])\n        delete_bucket(bucket)\n    else:\n        print(f'Remember that running Amazon EMR clusters and objects kept in an Amazon S3 bucket can incur charges against your account.')\n    print('Thanks for watching!')"
        ]
    }
]