[
    {
        "func_name": "index_data",
        "original": "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    \"\"\"\n    Returns the data import job that is creating the data asset.\n\n    :param data_index: DataIndex object.\n    :type data_index: azure.ai.ml.entities._dataindex\n    :param identity: Identity configuration for the job.\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\n    :param compute: The compute target to use for the job. Default: \"serverless\".\n    :type compute: str\n    :param serverless_instance_type: The instance type to use for serverless compute.\n    :type serverless_instance_type: Optional[str]\n    :param input_data_override: Input data override for the job.\n        Used to pipe output of step into DataIndex Job in a pipeline.\n    :type input_data_override: Optional[Input]\n    :param submit_job: Whether to submit the job to the service. Default: True.\n    :type submit_job: bool\n    :return: data import job object.\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\n    \"\"\"\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline",
        "mutated": [
            "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n    '\\n    Returns the data import job that is creating the data asset.\\n\\n    :param data_index: DataIndex object.\\n    :type data_index: azure.ai.ml.entities._dataindex\\n    :param identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :param compute: The compute target to use for the job. Default: \"serverless\".\\n    :type compute: str\\n    :param serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :param input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :param submit_job: Whether to submit the job to the service. Default: True.\\n    :type submit_job: bool\\n    :return: data import job object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline",
            "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Returns the data import job that is creating the data asset.\\n\\n    :param data_index: DataIndex object.\\n    :type data_index: azure.ai.ml.entities._dataindex\\n    :param identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :param compute: The compute target to use for the job. Default: \"serverless\".\\n    :type compute: str\\n    :param serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :param input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :param submit_job: Whether to submit the job to the service. Default: True.\\n    :type submit_job: bool\\n    :return: data import job object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline",
            "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Returns the data import job that is creating the data asset.\\n\\n    :param data_index: DataIndex object.\\n    :type data_index: azure.ai.ml.entities._dataindex\\n    :param identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :param compute: The compute target to use for the job. Default: \"serverless\".\\n    :type compute: str\\n    :param serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :param input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :param submit_job: Whether to submit the job to the service. Default: True.\\n    :type submit_job: bool\\n    :return: data import job object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline",
            "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Returns the data import job that is creating the data asset.\\n\\n    :param data_index: DataIndex object.\\n    :type data_index: azure.ai.ml.entities._dataindex\\n    :param identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :param compute: The compute target to use for the job. Default: \"serverless\".\\n    :type compute: str\\n    :param serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :param input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :param submit_job: Whether to submit the job to the service. Default: True.\\n    :type submit_job: bool\\n    :return: data import job object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline",
            "@monitor_with_activity(logger, 'Data.IndexData', ActivityType.PUBLICAPI)\n@experimental\ndef index_data(self, data_index: DataIndex, identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]=None, compute: str='serverless', serverless_instance_type: Optional[str]=None, input_data_override: Optional[Input]=None, submit_job: bool=True, **kwargs) -> PipelineJob:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Returns the data import job that is creating the data asset.\\n\\n    :param data_index: DataIndex object.\\n    :type data_index: azure.ai.ml.entities._dataindex\\n    :param identity: Identity configuration for the job.\\n    :type identity: Optional[Union[ManagedIdentityConfiguration, UserIdentityConfiguration]]\\n    :param compute: The compute target to use for the job. Default: \"serverless\".\\n    :type compute: str\\n    :param serverless_instance_type: The instance type to use for serverless compute.\\n    :type serverless_instance_type: Optional[str]\\n    :param input_data_override: Input data override for the job.\\n        Used to pipe output of step into DataIndex Job in a pipeline.\\n    :type input_data_override: Optional[Input]\\n    :param submit_job: Whether to submit the job to the service. Default: True.\\n    :type submit_job: bool\\n    :return: data import job object.\\n    :rtype: ~azure.ai.ml.entities.PipelineJob.\\n    '\n    from azure.ai.ml import MLClient\n    default_name = 'data_index_' + data_index.name\n    experiment_name = kwargs.pop('experiment_name', None) or default_name\n    data_index.type = AssetTypes.URI_FOLDER\n    _validate_auto_delete_setting_in_data_output(data_index.auto_delete_setting)\n    data_index.path = _validate_workspace_managed_datastore(data_index.path)\n    if '${{name}}' not in data_index.path and '{name}' not in data_index.path:\n        data_index.path = data_index.path.rstrip('/') + '/${{name}}'\n    index_job = index_data_func(description=data_index.description or kwargs.pop('description', None) or default_name, name=data_index.name or kwargs.pop('name', None), display_name=kwargs.pop('display_name', None) or default_name, experiment_name=experiment_name, compute=compute, serverless_instance_type=serverless_instance_type, data_index=data_index, ml_client=MLClient(subscription_id=self._subscription_id, resource_group_name=self._resource_group_name, workspace_name=self._workspace_name, credential=self._service_client._config.credential), identity=identity, input_data_override=input_data_override, **kwargs)\n    index_pipeline = PipelineJob(description=index_job.description, tags=index_job.tags, name=index_job.name, display_name=index_job.display_name, experiment_name=experiment_name, properties=index_job.properties or {}, settings=PipelineJobSettings(force_rerun=True, default_compute=compute), jobs={default_name: index_job})\n    index_pipeline.properties['azureml.mlIndexAssetName'] = data_index.name\n    index_pipeline.properties['azureml.mlIndexAssetKind'] = data_index.index.type\n    index_pipeline.properties['azureml.mlIndexAssetSource'] = kwargs.pop('mlindex_asset_source', 'Data Asset')\n    if submit_job:\n        return self._all_operations.all_operations[AzureMLResourceType.JOB].create_or_update(job=index_pipeline, skip_validation=True, **kwargs)\n    return index_pipeline"
        ]
    }
]