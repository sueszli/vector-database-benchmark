[
    {
        "func_name": "replicator",
        "original": "def replicator(state, fitness):\n    \"\"\"Continuous-time replicator dynamics.\n\n  This is the standard form of the continuous-time replicator dynamics also\n  known as selection dynamics.\n\n  For more details, see equation (5) page 9 in\n  https://jair.org/index.php/jair/article/view/10952\n\n  Args:\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\n\n  Returns:\n    Time derivative of the population state.\n  \"\"\"\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)",
        "mutated": [
            "def replicator(state, fitness):\n    if False:\n        i = 10\n    'Continuous-time replicator dynamics.\\n\\n  This is the standard form of the continuous-time replicator dynamics also\\n  known as selection dynamics.\\n\\n  For more details, see equation (5) page 9 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)",
            "def replicator(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Continuous-time replicator dynamics.\\n\\n  This is the standard form of the continuous-time replicator dynamics also\\n  known as selection dynamics.\\n\\n  For more details, see equation (5) page 9 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)",
            "def replicator(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Continuous-time replicator dynamics.\\n\\n  This is the standard form of the continuous-time replicator dynamics also\\n  known as selection dynamics.\\n\\n  For more details, see equation (5) page 9 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)",
            "def replicator(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Continuous-time replicator dynamics.\\n\\n  This is the standard form of the continuous-time replicator dynamics also\\n  known as selection dynamics.\\n\\n  For more details, see equation (5) page 9 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)",
            "def replicator(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Continuous-time replicator dynamics.\\n\\n  This is the standard form of the continuous-time replicator dynamics also\\n  known as selection dynamics.\\n\\n  For more details, see equation (5) page 9 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    avg_fitness = state.dot(fitness)\n    return state * (fitness - avg_fitness)"
        ]
    },
    {
        "func_name": "boltzmannq",
        "original": "def boltzmannq(state, fitness, temperature=1.0):\n    \"\"\"Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\n\n  For more details, see equation (10) page 15 in\n  https://jair.org/index.php/jair/article/view/10952\n\n  Args:\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\n    temperature: A scalar parameter determining the rate of exploration.\n\n  Returns:\n    Time derivative of the population state.\n  \"\"\"\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration",
        "mutated": [
            "def boltzmannq(state, fitness, temperature=1.0):\n    if False:\n        i = 10\n    'Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\\n\\n  For more details, see equation (10) page 15 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n    temperature: A scalar parameter determining the rate of exploration.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration",
            "def boltzmannq(state, fitness, temperature=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\\n\\n  For more details, see equation (10) page 15 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n    temperature: A scalar parameter determining the rate of exploration.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration",
            "def boltzmannq(state, fitness, temperature=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\\n\\n  For more details, see equation (10) page 15 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n    temperature: A scalar parameter determining the rate of exploration.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration",
            "def boltzmannq(state, fitness, temperature=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\\n\\n  For more details, see equation (10) page 15 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n    temperature: A scalar parameter determining the rate of exploration.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration",
            "def boltzmannq(state, fitness, temperature=1.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Selection-mutation dynamics modeling Q-learning with Boltzmann exploration.\\n\\n  For more details, see equation (10) page 15 in\\n  https://jair.org/index.php/jair/article/view/10952\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n    temperature: A scalar parameter determining the rate of exploration.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    exploitation = 1.0 / temperature * replicator(state, fitness)\n    exploration = np.log(state) - state.dot(np.log(state).transpose())\n    return exploitation - state * exploration"
        ]
    },
    {
        "func_name": "qpg",
        "original": "def qpg(state, fitness):\n    \"\"\"Q-based policy gradient dynamics (QPG).\n\n  For more details, see equation (12) on page 18 in\n  https://arxiv.org/pdf/1810.09026.pdf\n\n  Args:\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\n\n  Returns:\n    Time derivative of the population state.\n  \"\"\"\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))",
        "mutated": [
            "def qpg(state, fitness):\n    if False:\n        i = 10\n    'Q-based policy gradient dynamics (QPG).\\n\\n  For more details, see equation (12) on page 18 in\\n  https://arxiv.org/pdf/1810.09026.pdf\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))",
            "def qpg(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Q-based policy gradient dynamics (QPG).\\n\\n  For more details, see equation (12) on page 18 in\\n  https://arxiv.org/pdf/1810.09026.pdf\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))",
            "def qpg(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Q-based policy gradient dynamics (QPG).\\n\\n  For more details, see equation (12) on page 18 in\\n  https://arxiv.org/pdf/1810.09026.pdf\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))",
            "def qpg(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Q-based policy gradient dynamics (QPG).\\n\\n  For more details, see equation (12) on page 18 in\\n  https://arxiv.org/pdf/1810.09026.pdf\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))",
            "def qpg(state, fitness):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Q-based policy gradient dynamics (QPG).\\n\\n  For more details, see equation (12) on page 18 in\\n  https://arxiv.org/pdf/1810.09026.pdf\\n\\n  Args:\\n    state: Probability distribution as an `np.array(shape=num_strategies)`.\\n    fitness: Fitness vector as an `np.array(shape=num_strategies)`.\\n\\n  Returns:\\n    Time derivative of the population state.\\n  '\n    regret = fitness - state.dot(fitness)\n    return state * (state * regret - np.sum(state ** 2 * regret))"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, payoff_matrix, dynamics):\n    \"\"\"Initializes the single-population dynamics.\"\"\"\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics",
        "mutated": [
            "def __init__(self, payoff_matrix, dynamics):\n    if False:\n        i = 10\n    'Initializes the single-population dynamics.'\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics",
            "def __init__(self, payoff_matrix, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the single-population dynamics.'\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics",
            "def __init__(self, payoff_matrix, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the single-population dynamics.'\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics",
            "def __init__(self, payoff_matrix, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the single-population dynamics.'\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics",
            "def __init__(self, payoff_matrix, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the single-population dynamics.'\n    assert payoff_matrix.ndim == 3\n    assert payoff_matrix.shape[0] == 2\n    assert np.allclose(payoff_matrix[0], payoff_matrix[1].T)\n    self.payoff_matrix = payoff_matrix[0]\n    self.dynamics = dynamics"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, state=None, time=None):\n    \"\"\"Time derivative of the population state.\n\n    Args:\n      state: Probability distribution as list or\n        `numpy.ndarray(shape=num_strategies)`.\n      time: Time is ignored (time-invariant dynamics). Including the argument in\n        the function signature supports numerical integration via e.g.\n        `scipy.integrate.odeint` which requires that the callback function has\n        at least two arguments (state and time).\n\n    Returns:\n      Time derivative of the population state as\n      `numpy.ndarray(shape=num_strategies)`.\n    \"\"\"\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)",
        "mutated": [
            "def __call__(self, state=None, time=None):\n    if False:\n        i = 10\n    'Time derivative of the population state.\\n\\n    Args:\\n      state: Probability distribution as list or\\n        `numpy.ndarray(shape=num_strategies)`.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the population state as\\n      `numpy.ndarray(shape=num_strategies)`.\\n    '\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)",
            "def __call__(self, state=None, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Time derivative of the population state.\\n\\n    Args:\\n      state: Probability distribution as list or\\n        `numpy.ndarray(shape=num_strategies)`.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the population state as\\n      `numpy.ndarray(shape=num_strategies)`.\\n    '\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)",
            "def __call__(self, state=None, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Time derivative of the population state.\\n\\n    Args:\\n      state: Probability distribution as list or\\n        `numpy.ndarray(shape=num_strategies)`.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the population state as\\n      `numpy.ndarray(shape=num_strategies)`.\\n    '\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)",
            "def __call__(self, state=None, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Time derivative of the population state.\\n\\n    Args:\\n      state: Probability distribution as list or\\n        `numpy.ndarray(shape=num_strategies)`.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the population state as\\n      `numpy.ndarray(shape=num_strategies)`.\\n    '\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)",
            "def __call__(self, state=None, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Time derivative of the population state.\\n\\n    Args:\\n      state: Probability distribution as list or\\n        `numpy.ndarray(shape=num_strategies)`.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the population state as\\n      `numpy.ndarray(shape=num_strategies)`.\\n    '\n    state = np.array(state)\n    assert state.ndim == 1\n    assert state.shape[0] == self.payoff_matrix.shape[0]\n    fitness = np.matmul(state, self.payoff_matrix.T)\n    return self.dynamics(state, fitness)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, payoff_tensor, dynamics):\n    \"\"\"Initializes the multi-population dynamics.\"\"\"\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics",
        "mutated": [
            "def __init__(self, payoff_tensor, dynamics):\n    if False:\n        i = 10\n    'Initializes the multi-population dynamics.'\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics",
            "def __init__(self, payoff_tensor, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the multi-population dynamics.'\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics",
            "def __init__(self, payoff_tensor, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the multi-population dynamics.'\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics",
            "def __init__(self, payoff_tensor, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the multi-population dynamics.'\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics",
            "def __init__(self, payoff_tensor, dynamics):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the multi-population dynamics.'\n    if isinstance(dynamics, list) or isinstance(dynamics, tuple):\n        assert payoff_tensor.shape[0] == len(dynamics)\n    else:\n        dynamics = [dynamics] * payoff_tensor.shape[0]\n    self.payoff_tensor = payoff_tensor\n    self.dynamics = dynamics"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, state, time=None):\n    \"\"\"Time derivative of the population states.\n\n    Args:\n      state: Combined population state for all populations as a list or flat\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\n        order of the players.\n      time: Time is ignored (time-invariant dynamics). Including the argument in\n        the function signature supports numerical integration via e.g.\n        `scipy.integrate.odeint` which requires that the callback function has\n        at least two arguments (state and time).\n\n    Returns:\n      Time derivative of the combined population state as `numpy.ndarray`.\n    \"\"\"\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)",
        "mutated": [
            "def __call__(self, state, time=None):\n    if False:\n        i = 10\n    'Time derivative of the population states.\\n\\n    Args:\\n      state: Combined population state for all populations as a list or flat\\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\\n        order of the players.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the combined population state as `numpy.ndarray`.\\n    '\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)",
            "def __call__(self, state, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Time derivative of the population states.\\n\\n    Args:\\n      state: Combined population state for all populations as a list or flat\\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\\n        order of the players.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the combined population state as `numpy.ndarray`.\\n    '\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)",
            "def __call__(self, state, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Time derivative of the population states.\\n\\n    Args:\\n      state: Combined population state for all populations as a list or flat\\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\\n        order of the players.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the combined population state as `numpy.ndarray`.\\n    '\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)",
            "def __call__(self, state, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Time derivative of the population states.\\n\\n    Args:\\n      state: Combined population state for all populations as a list or flat\\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\\n        order of the players.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the combined population state as `numpy.ndarray`.\\n    '\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)",
            "def __call__(self, state, time=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Time derivative of the population states.\\n\\n    Args:\\n      state: Combined population state for all populations as a list or flat\\n        `numpy.ndarray` (ndim=1). Probability distributions are concatenated in\\n        order of the players.\\n      time: Time is ignored (time-invariant dynamics). Including the argument in\\n        the function signature supports numerical integration via e.g.\\n        `scipy.integrate.odeint` which requires that the callback function has\\n        at least two arguments (state and time).\\n\\n    Returns:\\n      Time derivative of the combined population state as `numpy.ndarray`.\\n    '\n    state = np.array(state)\n    n = self.payoff_tensor.shape[0]\n    ks = self.payoff_tensor.shape[1:]\n    assert state.shape[0] == sum(ks)\n    states = np.split(state, np.cumsum(ks)[:-1])\n    dstates = [None] * n\n    for i in range(n):\n        fitness = np.moveaxis(self.payoff_tensor[i], i, 0)\n        for i_ in set(range(n)) - {i}:\n            fitness = np.tensordot(states[i_], fitness, axes=[0, 1])\n        dstates[i] = self.dynamics[i](states[i], fitness)\n    return np.concatenate(dstates)"
        ]
    },
    {
        "func_name": "time_average",
        "original": "def time_average(traj):\n    \"\"\"Time-averaged population state trajectory.\n\n  Args:\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\n      types/strategies along the second.\n\n  Returns:\n    Time-averaged trajectory.\n  \"\"\"\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]",
        "mutated": [
            "def time_average(traj):\n    if False:\n        i = 10\n    'Time-averaged population state trajectory.\\n\\n  Args:\\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\\n      types/strategies along the second.\\n\\n  Returns:\\n    Time-averaged trajectory.\\n  '\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]",
            "def time_average(traj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Time-averaged population state trajectory.\\n\\n  Args:\\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\\n      types/strategies along the second.\\n\\n  Returns:\\n    Time-averaged trajectory.\\n  '\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]",
            "def time_average(traj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Time-averaged population state trajectory.\\n\\n  Args:\\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\\n      types/strategies along the second.\\n\\n  Returns:\\n    Time-averaged trajectory.\\n  '\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]",
            "def time_average(traj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Time-averaged population state trajectory.\\n\\n  Args:\\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\\n      types/strategies along the second.\\n\\n  Returns:\\n    Time-averaged trajectory.\\n  '\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]",
            "def time_average(traj):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Time-averaged population state trajectory.\\n\\n  Args:\\n    traj: Trajectory as `numpy.ndarray`. Time is along the first dimension,\\n      types/strategies along the second.\\n\\n  Returns:\\n    Time-averaged trajectory.\\n  '\n    n = traj.shape[0]\n    sum_traj = np.cumsum(traj, axis=0)\n    norm = 1.0 / np.arange(1, n + 1)\n    return sum_traj * norm[:, np.newaxis]"
        ]
    }
]